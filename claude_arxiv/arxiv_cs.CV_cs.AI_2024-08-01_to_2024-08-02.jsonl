{"title": "3D U-KAN Implementation for Multi-modal MRI Brain Tumor Segmentation", "authors": [], "abstract": "We explore the application of U-KAN, a U-Net based network enhanced with\nKolmogorov-Arnold Network (KAN) layers, for 3D brain tumor segmentation using\nmulti-modal MRI data. We adapt the original 2D U-KAN model to the 3D task, and\nintroduce a variant called UKAN-SE, which incorporates Squeeze-and-Excitation\nmodules for global attention. We compare the performance of U-KAN and UKAN-SE\nagainst existing methods such as U-Net, Attention U-Net, and Swin UNETR, using\nthe BraTS 2024 dataset. Our results show that U-KAN and UKAN-SE, with\napproximately 10.6 million parameters, achieve exceptional efficiency,\nrequiring only about 1/4 of the training time of U-Net and Attention U-Net, and\n1/6 that of Swin UNETR, while surpassing these models across most evaluation\nmetrics. Notably, UKAN-SE slightly outperforms U-KAN.", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00273"}
{"title": "QUITO: Accelerating Long-Context Reasoning through Query-Guided Context\n  Compression", "authors": [], "abstract": "In-context learning (ICL) capabilities are foundational to the success of\nlarge language models (LLMs). Recently, context compression has attracted\ngrowing interest since it can largely reduce reasoning complexities and\ncomputation costs of LLMs. In this paper, we introduce a novel Query-gUIded\naTtention cOmpression (QUITO) method, which leverages attention of the question\nover the contexts to filter useless information. Specifically, we take a\ntrigger token to calculate the attention distribution of the context in\nresponse to the question. Based on the distribution, we propose three different\nfiltering methods to satisfy the budget constraints of the context length. We\nevaluate the QUITO using two widely-used datasets, namely, NaturalQuestions and\nASQA. Experimental results demonstrate that QUITO significantly outperforms\nestablished baselines across various datasets and downstream LLMs, underscoring\nits effectiveness. Our code is available at\nhttps://github.com/Wenshansilvia/attention_compressor.", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00274"}
{"title": "High Performance Im2win and Direct Convolutions using Three Tensor\n  Layouts on SIMD Architectures", "authors": [], "abstract": "Convolution is the core component within deep neural networks and it is\ncomputationally intensive and time consuming. Tensor data layouts significantly\nimpact convolution operations in terms of memory access and computational\nefficiency. Yet, there is still a lack of comprehensive performance\ncharacterization on data layouts on SIMD architectures concerning convolution\nmethods. This paper proposes three novel data layouts for im2win convolution:\nNHWC, CHWN, and CHWN8, and introduces a set of general optimization techniques\nfor both direct and im2win convolutions. We compare the optimized im2win\nconvolution with the direct convolution and PyTorch's im2col-based convolution\nacross the aforementioned layouts on SIMD machines. The experiments\ndemonstrated that the im2win convolution with the new NHWC layout achieved up\nto 355% performance speedup over NCHW layout. Our optimizations also\nsignificantly improve the performance of both im2win and direct convolutions.\nOur optimized im2win and direct convolutions achieved up to 95% and 94% of\nmachine's theoretical peak performance, respectively.", "categories": ["cs.LG", "cs.AI", "cs.NE"], "primary_category": "cs.LG", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00278"}
{"title": "DMESA: Densely Matching Everything by Segmenting Anything", "authors": [], "abstract": "We propose MESA and DMESA as novel feature matching methods, which utilize\nSegment Anything Model (SAM) to effectively mitigate matching redundancy. The\nkey insight of our methods is to establish implicit-semantic area matching\nprior to point matching, based on advanced image understanding of SAM. Then,\ninformative area matches with consistent internal semantic are able to undergo\ndense feature comparison, facilitating precise inside-area point matching.\nSpecifically, MESA adopts a sparse matching framework and first obtains\ncandidate areas from SAM results through a novel Area Graph (AG). Then, area\nmatching among the candidates is formulated as graph energy minimization and\nsolved by graphical models derived from AG. To address the efficiency issue of\nMESA, we further propose DMESA as its dense counterpart, applying a dense\nmatching framework. After candidate areas are identified by AG, DMESA\nestablishes area matches through generating dense matching distributions. The\ndistributions are produced from off-the-shelf patch matching utilizing the\nGaussian Mixture Model and refined via the Expectation Maximization. With less\nrepetitive computation, DMESA showcases a speed improvement of nearly five\ntimes compared to MESA, while maintaining competitive accuracy. Our methods are\nextensively evaluated on five datasets encompassing indoor and outdoor scenes.\nThe results illustrate consistent performance improvements from our methods for\nfive distinct point matching baselines across all datasets. Furthermore, our\nmethods exhibit promise generalization and improved robustness against image\nresolution variations. The code is publicly available at\nhttps://github.com/Easonyesheng/A2PM-MESA.", "categories": ["cs.CV"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00279"}
{"title": "Towards Scalable GPU-Accelerated SNN Training via Temporal Fusion", "authors": [], "abstract": "Drawing on the intricate structures of the brain, Spiking Neural Networks\n(SNNs) emerge as a transformative development in artificial intelligence,\nclosely emulating the complex dynamics of biological neural networks. While\nSNNs show promising efficiency on specialized sparse-computational hardware,\ntheir practical training often relies on conventional GPUs. This reliance\nfrequently leads to extended computation times when contrasted with traditional\nArtificial Neural Networks (ANNs), presenting significant hurdles for advancing\nSNN research. To navigate this challenge, we present a novel temporal fusion\nmethod, specifically designed to expedite the propagation dynamics of SNNs on\nGPU platforms, which serves as an enhancement to the current significant\napproaches for handling deep learning tasks with SNNs. This method underwent\nthorough validation through extensive experiments in both authentic training\nscenarios and idealized conditions, confirming its efficacy and adaptability\nfor single and multi-GPU systems. Benchmarked against various existing SNN\nlibraries/implementations, our method achieved accelerations ranging from\n$5\\times$ to $40\\times$ on NVIDIA A100 GPUs. Publicly available experimental\ncodes can be found at https://github.com/EMI-Group/snn-temporal-fusion.", "categories": ["cs.AI", "cs.DC"], "primary_category": "cs.AI", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00280"}
{"title": "Navigating Text-to-Image Generative Bias across Indic Languages", "authors": [], "abstract": "This research investigates biases in text-to-image (TTI) models for the Indic\nlanguages widely spoken across India. It evaluates and compares the generative\nperformance and cultural relevance of leading TTI models in these languages\nagainst their performance in English. Using the proposed IndicTTI benchmark, we\ncomprehensively assess the performance of 30 Indic languages with two\nopen-source diffusion models and two commercial generation APIs. The primary\nobjective of this benchmark is to evaluate the support for Indic languages in\nthese models and identify areas needing improvement. Given the linguistic\ndiversity of 30 languages spoken by over 1.4 billion people, this benchmark\naims to provide a detailed and insightful analysis of TTI models' effectiveness\nwithin the Indic linguistic landscape. The data and code for the IndicTTI\nbenchmark can be accessed at\nhttps://iab-rubric.org/resources/other-databases/indictti.", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00283"}
{"title": "Diff3DETR:Agent-based Diffusion Model for Semi-supervised 3D Object\n  Detection", "authors": [], "abstract": "3D object detection is essential for understanding 3D scenes. Contemporary\ntechniques often require extensive annotated training data, yet obtaining\npoint-wise annotations for point clouds is time-consuming and laborious. Recent\ndevelopments in semi-supervised methods seek to mitigate this problem by\nemploying a teacher-student framework to generate pseudo-labels for unlabeled\npoint clouds. However, these pseudo-labels frequently suffer from insufficient\ndiversity and inferior quality. To overcome these hurdles, we introduce an\nAgent-based Diffusion Model for Semi-supervised 3D Object Detection\n(Diff3DETR). Specifically, an agent-based object query generator is designed to\nproduce object queries that effectively adapt to dynamic scenes while striking\na balance between sampling locations and content embedding. Additionally, a\nbox-aware denoising module utilizes the DDIM denoising process and the\nlong-range attention in the transformer decoder to refine bounding boxes\nincrementally. Extensive experiments on ScanNet and SUN RGB-D datasets\ndemonstrate that Diff3DETR outperforms state-of-the-art semi-supervised 3D\nobject detection methods.", "categories": ["cs.CV"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00286"}
{"title": "Gradient Harmonization in Unsupervised Domain Adaptation", "authors": [], "abstract": "Unsupervised domain adaptation (UDA) intends to transfer knowledge from a\nlabeled source domain to an unlabeled target domain. Many current methods focus\non learning feature representations that are both discriminative for\nclassification and invariant across domains by simultaneously optimizing domain\nalignment and classification tasks. However, these methods often overlook a\ncrucial challenge: the inherent conflict between these two tasks during\ngradient-based optimization. In this paper, we delve into this issue and\nintroduce two effective solutions known as Gradient Harmonization, including GH\nand GH++, to mitigate the conflict between domain alignment and classification\ntasks. GH operates by altering the gradient angle between different tasks from\nan obtuse angle to an acute angle, thus resolving the conflict and trade-offing\nthe two tasks in a coordinated manner. Yet, this would cause both tasks to\ndeviate from their original optimization directions. We thus further propose an\nimproved version, GH++, which adjusts the gradient angle between tasks from an\nobtuse angle to a vertical angle. This not only eliminates the conflict but\nalso minimizes deviation from the original gradient directions. Finally, for\noptimization convenience and efficiency, we evolve the gradient harmonization\nstrategies into a dynamically weighted loss function using an integral operator\non the harmonized gradient. Notably, GH/GH++ are orthogonal to UDA and can be\nseamlessly integrated into most existing UDA models. Theoretical insights and\nexperimental analyses demonstrate that the proposed approaches not only enhance\npopular UDA baselines but also improve recent state-of-the-art models.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00288"}
{"title": "Multi-Modal Parameter-Efficient Fine-tuning via Graph Neural Network", "authors": [], "abstract": "With the advent of the era of foundation models, pre-training and fine-tuning\nhave become common paradigms. Recently, parameter-efficient fine-tuning has\ngarnered widespread attention due to its better balance between the number of\nlearnable parameters and performance. However, some current parameter-efficient\nfine-tuning methods only model a single modality and lack the utilization of\nstructural knowledge in downstream tasks. To address this issue, this paper\nproposes a multi-modal parameter-efficient fine-tuning method based on graph\nnetworks. Each image is fed into a multi-modal large language model (MLLM) to\ngenerate a text description. The image and its corresponding text description\nare then processed by a frozen image encoder and text encoder to generate image\nfeatures and text features, respectively. A graph is constructed based on the\nsimilarity of the multi-modal feature nodes, and knowledge and relationships\nrelevant to these features are extracted from each node. Additionally, Elastic\nWeight Consolidation (EWC) regularization is incorporated into the loss\nfunction to mitigate the problem of forgetting during task learning. The\nproposed model achieves test accuracies on the OxfordPets, Flowers102, and\nFood101 datasets that improve by 4.45%, 2.92%, and 0.23%, respectively. The\ncode is available at https://github.com/yunche0/GA-Net/tree/master.", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00290"}
{"title": "RDP: Ranked Differential Privacy for Facial Feature Protection in\n  Multiscale Sparsified Subspace", "authors": [], "abstract": "With the widespread sharing of personal face images in applications' public\ndatabases, face recognition systems faces real threat of being breached by\npotential adversaries who are able to access users' face images and use them to\nintrude the face recognition systems. In this paper, we propose a novel privacy\nprotection method in the multiscale sparsified feature subspaces to protect\nsensitive facial features, by taking care of the influence or weight ranked\nfeature coefficients on the privacy budget, named \"Ranked Differential Privacy\n(RDP)\". After the multiscale feature decomposition, the lightweight Laplacian\nnoise is added to the dimension-reduced sparsified feature coefficients\naccording to the geometric superposition method. Then, we rigorously prove that\nthe RDP satisfies Differential Privacy. After that, the nonlinear Lagrange\nMultiplier (LM) method is formulated for the constraint optimization problem of\nmaximizing the utility of the visualization quality protected face images with\nsanitizing noise, under a given facial features privacy budget. Then, two\nmethods are proposed to solve the nonlinear LM problem and obtain the optimal\nnoise scale parameters: 1) the analytical Normalization Approximation (NA)\nmethod with identical average noise scale parameter for real-time online\napplications; and 2) the LM optimization Gradient Descent (LMGD) numerical\nmethod to obtain the nonlinear solution through iterative updating for more\naccurate offline applications. Experimental results on two real-world datasets\nshow that our proposed RDP outperforms other state-of-the-art methods: at a\nprivacy budget of 0.2, the PSNR (Peak Signal-to-Noise Ratio) of the RDP is\nabout ~10 dB higher than (10 times as high as) the highest PSNR of all compared\nmethods.", "categories": ["cs.CV", "cs.IR"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00294"}
{"title": "Contrastive Graph Representation Learning with Adversarial Cross-view\n  Reconstruction and Information Bottleneck", "authors": [], "abstract": "Graph Neural Networks (GNNs) have received extensive research attention due\nto their powerful information aggregation capabilities. Despite the success of\nGNNs, most of them suffer from the popularity bias issue in a graph caused by a\nsmall number of popular categories. Additionally, real graph datasets always\ncontain incorrect node labels, which hinders GNNs from learning effective node\nrepresentations. Graph contrastive learning (GCL) has been shown to be\neffective in solving the above problems for node classification tasks. Most\nexisting GCL methods are implemented by randomly removing edges and nodes to\ncreate multiple contrasting views, and then maximizing the mutual information\n(MI) between these contrasting views to improve the node feature\nrepresentation. However, maximizing the mutual information between multiple\ncontrasting views may lead the model to learn some redundant information\nirrelevant to the node classification task. To tackle this issue, we propose an\neffective Contrastive Graph Representation Learning with Adversarial Cross-view\nReconstruction and Information Bottleneck (CGRL) for node classification, which\ncan adaptively learn to mask the nodes and edges in the graph to obtain the\noptimal graph structure representation. Furthermore, we innovatively introduce\nthe information bottleneck theory into GCLs to remove redundant information in\nmultiple contrasting views while retaining as much information as possible\nabout node classification. Moreover, we add noise perturbations to the original\nviews and reconstruct the augmented views by constructing adversarial views to\nimprove the robustness of node feature representation. Extensive experiments on\nreal-world public datasets demonstrate that our method significantly\noutperforms existing state-of-the-art algorithms.", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00295"}
{"title": "Head360: Learning a Parametric 3D Full-Head for Free-View Synthesis in\n  360{\\deg}", "authors": [], "abstract": "Creating a 360{\\deg} parametric model of a human head is a very challenging\ntask. While recent advancements have demonstrated the efficacy of leveraging\nsynthetic data for building such parametric head models, their performance\nremains inadequate in crucial areas such as expression-driven animation,\nhairstyle editing, and text-based modifications. In this paper, we build a\ndataset of artist-designed high-fidelity human heads and propose to create a\nnovel parametric 360{\\deg} renderable parametric head model from it. Our scheme\ndecouples the facial motion/shape and facial appearance, which are represented\nby a classic parametric 3D mesh model and an attached neural texture,\nrespectively. We further propose a training method for decompositing hairstyle\nand facial appearance, allowing free-swapping of the hairstyle. A novel\ninversion fitting method is presented based on single image input with high\ngeneralization and fidelity. To the best of our knowledge, our model is the\nfirst parametric 3D full-head that achieves 360{\\deg} free-view synthesis,\nimage-based fitting, appearance editing, and animation within a single model.\nExperiments show that facial motions and appearances are well disentangled in\nthe parametric space, leading to SOTA performance in rendering and animating\nquality. The code and SynHead100 dataset are released at\nhttps://nju-3dv.github.io/projects/Head360.", "categories": ["cs.CV"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00296"}
{"title": "EmoTalk3D: High-Fidelity Free-View Synthesis of Emotional 3D Talking\n  Head", "authors": [], "abstract": "We present a novel approach for synthesizing 3D talking heads with\ncontrollable emotion, featuring enhanced lip synchronization and rendering\nquality. Despite significant progress in the field, prior methods still suffer\nfrom multi-view consistency and a lack of emotional expressiveness. To address\nthese issues, we collect EmoTalk3D dataset with calibrated multi-view videos,\nemotional annotations, and per-frame 3D geometry. By training on the EmoTalk3D\ndataset, we propose a \\textit{`Speech-to-Geometry-to-Appearance'} mapping\nframework that first predicts faithful 3D geometry sequence from the audio\nfeatures, then the appearance of a 3D talking head represented by 4D Gaussians\nis synthesized from the predicted geometry. The appearance is further\ndisentangled into canonical and dynamic Gaussians, learned from multi-view\nvideos, and fused to render free-view talking head animation. Moreover, our\nmodel enables controllable emotion in the generated talking heads and can be\nrendered in wide-range views. Our method exhibits improved rendering quality\nand stability in lip motion generation while capturing dynamic facial details\nsuch as wrinkles and subtle expressions. Experiments demonstrate the\neffectiveness of our approach in generating high-fidelity and\nemotion-controllable 3D talking heads. The code and EmoTalk3D dataset are\nreleased at https://nju-3dv.github.io/projects/EmoTalk3D.", "categories": ["cs.CV"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00297"}
{"title": "Tails Tell Tales: Chapter-Wide Manga Transcriptions with Character Names", "authors": [], "abstract": "Enabling engagement of manga by visually impaired individuals presents a\nsignificant challenge due to its inherently visual nature. With the goal of\nfostering accessibility, this paper aims to generate a dialogue transcript of a\ncomplete manga chapter, entirely automatically, with a particular emphasis on\nensuring narrative consistency. This entails identifying (i) what is being\nsaid, i.e., detecting the texts on each page and classifying them into\nessential vs non-essential, and (ii) who is saying it, i.e., attributing each\ndialogue to its speaker, while ensuring the same characters are named\nconsistently throughout the chapter.\n  To this end, we introduce: (i) Magiv2, a model that is capable of generating\nhigh-quality chapter-wide manga transcripts with named characters and\nsignificantly higher precision in speaker diarisation over prior works; (ii) an\nextension of the PopManga evaluation dataset, which now includes annotations\nfor speech-bubble tail boxes, associations of text to corresponding tails,\nclassifications of text as essential or non-essential, and the identity for\neach character box; and (iii) a new character bank dataset, which comprises\nover 11K characters from 76 manga series, featuring 11.5K exemplar character\nimages in total, as well as a list of chapters in which they appear. The code,\ntrained model, and both datasets can be found at:\nhttps://github.com/ragavsachdeva/magi", "categories": ["cs.CV"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00298"}
{"title": "Towards Flexible Evaluation for Generative Visual Question Answering", "authors": [], "abstract": "Throughout rapid development of multimodal large language models, a crucial\ningredient is a fair and accurate evaluation of their multimodal comprehension\nabilities. Although Visual Question Answering (VQA) could serve as a developed\ntest field, limitations of VQA evaluation, like the inflexible pattern of Exact\nMatch, have hindered MLLMs from demonstrating their real capability and\ndiscourage rich responses. Therefore, this paper proposes the use of\nsemantics-based evaluators for assessing unconstrained open-ended responses on\nVQA datasets. As characteristics of VQA have made such evaluation significantly\ndifferent than the traditional Semantic Textual Similarity (STS) task, to\nsystematically analyze the behaviour and compare the performance of various\nevaluators including LLM-based ones, we proposes three key properties, i.e.,\nAlignment, Consistency and Generalization, and a corresponding dataset\nAssessing VQA Evaluators (AVE) to facilitate analysis. In addition, this paper\nproposes a Semantically Flexible VQA Evaluator (SFVE) with meticulous design\nbased on the unique features of VQA evaluation. Experimental results verify the\nfeasibility of model-based VQA evaluation and effectiveness of the proposed\nevaluator that surpasses existing semantic evaluators by a large margin. The\nproposed training scheme generalizes to both the BERT-like encoders and\ndecoder-only LLM.", "categories": ["cs.CV", "cs.MM"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00300"}
{"title": "Neural Octahedral Field: Octahedral prior for simultaneous smoothing and\n  sharp edge regularization", "authors": [], "abstract": "Neural implicit representation, the parameterization of distance function as\na coordinate neural field, has emerged as a promising lead in tackling surface\nreconstruction from unoriented point clouds. To enforce consistent orientation,\nexisting methods focus on regularizing the gradient of the distance function,\nsuch as constraining it to be of the unit norm, minimizing its divergence, or\naligning it with the eigenvector of Hessian that corresponds to zero\neigenvalue. However, under the presence of large scanning noise, they tend to\neither overfit the noise input or produce an excessively smooth reconstruction.\nIn this work, we propose to guide the surface reconstruction under a new\nvariant of neural field, the octahedral field, leveraging the spherical\nharmonics representation of octahedral frames originated in the hexahedral\nmeshing. Such field automatically snaps to geometry features when constrained\nto be smooth, and naturally preserves sharp angles when interpolated over\ncreases. By simultaneously fitting and smoothing the octahedral field alongside\nthe implicit geometry, it behaves analogously to bilateral filtering, resulting\nin smooth reconstruction while preserving sharp edges. Despite being operated\npurely pointwise, our method outperforms various traditional and neural\napproaches across extensive experiments, and is very competitive with methods\nthat require normal and data priors. Our full implementation is available at:\nhttps://github.com/Ankbzpx/frame-field.", "categories": ["cs.CV", "cs.GR"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00303"}
{"title": "ABC Align: Large Language Model Alignment for Safety & Accuracy", "authors": [], "abstract": "Alignment of Large Language Models (LLMs) remains an unsolved problem. Human\npreferences are highly distributed and can be captured at multiple levels of\nabstraction, from the individual to diverse populations. Organisational\npreferences, represented by standards and principles, are defined to mitigate\nreputational risk or meet legislative obligations. In this paper, we present\nABC Align, a novel alignment methodology for LLMs that enables integration of\nthe standards and preferences of a large media organisation into the LLM\nitself. We combine a set of data and methods that build on recent breakthroughs\nin synthetic data generation, preference optimisation, and post-training model\nquantisation. Our unified approach mitigates bias and improves accuracy, while\npreserving reasoning capability, as measured against standard benchmarks.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00307"}
{"title": "Discretizing Continuous Action Space with Unimodal Probability\n  Distributions for On-Policy Reinforcement Learning", "authors": [], "abstract": "For on-policy reinforcement learning, discretizing action space for\ncontinuous control can easily express multiple modes and is straightforward to\noptimize. However, without considering the inherent ordering between the\ndiscrete atomic actions, the explosion in the number of discrete actions can\npossess undesired properties and induce a higher variance for the policy\ngradient estimator. In this paper, we introduce a straightforward architecture\nthat addresses this issue by constraining the discrete policy to be unimodal\nusing Poisson probability distributions. This unimodal architecture can better\nleverage the continuity in the underlying continuous action space using\nexplicit unimodal probability distributions. We conduct extensive experiments\nto show that the discrete policy with the unimodal probability distribution\nprovides significantly faster convergence and higher performance for on-policy\nreinforcement learning algorithms in challenging control tasks, especially in\nhighly complex tasks such as Humanoid. We provide theoretical analysis on the\nvariance of the policy gradient estimator, which suggests that our attentively\ndesigned unimodal discrete policy can retain a lower variance and yield a\nstable learning process.", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00309"}
{"title": "Translating Imaging to Genomics: Leveraging Transformers for Predictive\n  Modeling", "authors": [], "abstract": "In this study, we present a novel approach for predicting genomic information\nfrom medical imaging modalities using a transformer-based model. We aim to\nbridge the gap between imaging and genomics data by leveraging transformer\nnetworks, allowing for accurate genomic profile predictions from CT/MRI images.\nPresently most studies rely on the use of whole slide images (WSI) for the\nassociation, which are obtained via invasive methodologies. We propose using\nonly available CT/MRI images to predict genomic sequences. Our transformer\nbased approach is able to efficiently generate associations between multiple\nsequences based on CT/MRI images alone. This work paves the way for the use of\nnon-invasive imaging modalities for precise and personalized healthcare,\nallowing for a better understanding of diseases and treatment.", "categories": ["cs.CV"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00311"}
{"title": "ADBM: Adversarial diffusion bridge model for reliable adversarial\n  purification", "authors": [], "abstract": "Recently Diffusion-based Purification (DiffPure) has been recognized as an\neffective defense method against adversarial examples. However, we find\nDiffPure which directly employs the original pre-trained diffusion models for\nadversarial purification, to be suboptimal. This is due to an inherent\ntrade-off between noise purification performance and data recovery quality.\nAdditionally, the reliability of existing evaluations for DiffPure is\nquestionable, as they rely on weak adaptive attacks. In this work, we propose a\nnovel Adversarial Diffusion Bridge Model, termed ADBM. ADBM directly constructs\na reverse bridge from the diffused adversarial data back to its original clean\nexamples, enhancing the purification capabilities of the original diffusion\nmodels. Through theoretical analysis and experimental validation across various\nscenarios, ADBM has proven to be a superior and robust defense mechanism,\noffering significant promise for practical applications.", "categories": ["cs.LG", "cs.AI", "cs.CV"], "primary_category": "cs.LG", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00315"}
{"title": "OTAD: An Optimal Transport-Induced Robust Model for Agnostic Adversarial\n  Attack", "authors": [], "abstract": "Deep neural networks (DNNs) are vulnerable to small adversarial perturbations\nof the inputs, posing a significant challenge to their reliability and\nrobustness. Empirical methods such as adversarial training can defend against\nparticular attacks but remain vulnerable to more powerful attacks.\nAlternatively, Lipschitz networks provide certified robustness to unseen\nperturbations but lack sufficient expressive power. To harness the advantages\nof both approaches, we design a novel two-step Optimal Transport induced\nAdversarial Defense (OTAD) model that can fit the training data accurately\nwhile preserving the local Lipschitz continuity. First, we train a DNN with a\nregularizer derived from optimal transport theory, yielding a discrete optimal\ntransport map linking data to its features. By leveraging the map's inherent\nregularity, we interpolate the map by solving the convex integration problem\n(CIP) to guarantee the local Lipschitz property. OTAD is extensible to diverse\narchitectures of ResNet and Transformer, making it suitable for complex data.\nFor efficient computation, the CIP can be solved through training neural\nnetworks. OTAD opens a novel avenue for developing reliable and secure deep\nlearning systems through the regularity of optimal transport maps. Empirical\nresults demonstrate that OTAD can outperform other robust models on diverse\ndatasets.", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "primary_category": "cs.LG", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00329"}
{"title": "DECIDER: Leveraging Foundation Model Priors for Improved Model Failure\n  Detection and Explanation", "authors": [], "abstract": "Reliably detecting when a deployed machine learning model is likely to fail\non a given input is crucial for ensuring safe operation. In this work, we\npropose DECIDER (Debiasing Classifiers to Identify Errors Reliably), a novel\napproach that leverages priors from large language models (LLMs) and\nvision-language models (VLMs) to detect failures in image classification\nmodels. DECIDER utilizes LLMs to specify task-relevant core attributes and\nconstructs a ``debiased'' version of the classifier by aligning its visual\nfeatures to these core attributes using a VLM, and detects potential failure by\nmeasuring disagreement between the original and debiased models. In addition to\nproactively identifying samples on which the model would fail, DECIDER also\nprovides human-interpretable explanations for failure through a novel\nattribute-ablation strategy. Through extensive experiments across diverse\nbenchmarks spanning subpopulation shifts (spurious correlations, class\nimbalance) and covariate shifts (synthetic corruptions, domain shifts), DECIDER\nconsistently achieves state-of-the-art failure detection performance,\nsignificantly outperforming baselines in terms of the overall Matthews\ncorrelation coefficient as well as failure and success recall. Our codes can be\naccessed at~\\url{https://github.com/kowshikthopalli/DECIDER/}", "categories": ["cs.CV"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00331"}
{"title": "Vision-based Wearable Steering Assistance for People with Impaired\n  Vision in Jogging", "authors": [], "abstract": "Outdoor sports pose a challenge for people with impaired vision. The demand\nfor higher-speed mobility inspired us to develop a vision-based wearable\nsteering assistance. To ensure broad applicability, we focused on a\nrepresentative sports environment, the athletics track. Our efforts centered on\nimproving the speed and accuracy of perception, enhancing planning adaptability\nfor the real world, and providing swift and safe assistance for people with\nimpaired vision. In perception, we engineered a lightweight multitask network\ncapable of simultaneously detecting track lines and obstacles. Additionally,\ndue to the limitations of existing datasets for supporting multi-task detection\nin athletics tracks, we diligently collected and annotated a new dataset (MAT)\ncontaining 1000 images. In planning, we integrated the methods of sampling and\nspline curves, addressing the planning challenges of curves. Meanwhile, we\nutilized the positions of the track lines and obstacles as constraints to guide\npeople with impaired vision safely along the current track. Our system is\ndeployed on an embedded device, Jetson Orin NX. Through outdoor experiments, it\ndemonstrated adaptability in different sports scenarios, assisting users in\nachieving free movement of 400-meter at an average speed of 1.34 m/s, meeting\nthe level of normal people in jogging. Our MAT dataset is publicly available\nfrom https://github.com/snoopy-l/MAT", "categories": ["cs.CV", "cs.RO"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00332"}
{"title": "DistillGrasp: Integrating Features Correlation with Knowledge\n  Distillation for Depth Completion of Transparent Objects", "authors": [], "abstract": "Due to the visual properties of reflection and refraction, RGB-D cameras\ncannot accurately capture the depth of transparent objects, leading to\nincomplete depth maps. To fill in the missing points, recent studies tend to\nexplore new visual features and design complex networks to reconstruct the\ndepth, however, these approaches tremendously increase computation, and the\ncorrelation of different visual features remains a problem. To this end, we\npropose an efficient depth completion network named DistillGrasp which\ndistillates knowledge from the teacher branch to the student branch.\nSpecifically, in the teacher branch, we design a position correlation block\n(PCB) that leverages RGB images as the query and key to search for the\ncorresponding values, guiding the model to establish correct correspondence\nbetween two features and transfer it to the transparent areas. For the student\nbranch, we propose a consistent feature correlation module (CFCM) that retains\nthe reliable regions of RGB images and depth maps respectively according to the\nconsistency and adopts a CNN to capture the pairwise relationship for depth\ncompletion. To avoid the student branch only learning regional features from\nthe teacher branch, we devise a distillation loss that not only considers the\ndistance loss but also the object structure and edge information. Extensive\nexperiments conducted on the ClearGrasp dataset manifest that our teacher\nnetwork outperforms state-of-the-art methods in terms of accuracy and\ngeneralization, and the student network achieves competitive results with a\nhigher speed of 48 FPS. In addition, the significant improvement in a\nreal-world robotic grasping system illustrates the effectiveness and robustness\nof our proposed system.", "categories": ["cs.CV", "cs.IR"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00337"}
{"title": "MuJoCo MPC for Humanoid Control: Evaluation on HumanoidBench", "authors": [], "abstract": "We tackle the recently introduced benchmark for whole-body humanoid control\nHumanoidBench using MuJoCo MPC. We find that sparse reward functions of\nHumanoidBench yield undesirable and unrealistic behaviors when optimized;\ntherefore, we propose a set of regularization terms that stabilize the robot\nbehavior across tasks. Current evaluations on a subset of tasks demonstrate\nthat our proposed reward function allows achieving the highest HumanoidBench\nscores while maintaining realistic posture and smooth control signals. Our code\nis publicly available and will become a part of MuJoCo MPC, enabling rapid\nprototyping of robot behaviors.", "categories": ["cs.RO", "cs.AI", "cs.LG"], "primary_category": "cs.RO", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00342"}
{"title": "Neural Graph Matching for Video Retrieval in Large-Scale Video-driven\n  E-commerce", "authors": [], "abstract": "With the rapid development of the short video industry, traditional\ne-commerce has encountered a new paradigm, video-driven e-commerce, which\nleverages attractive videos for product showcases and provides both video and\nitem services for users. Benefitting from the dynamic and visualized\nintroduction of items,video-driven e-commerce has shown huge potential in\nstimulating consumer confidence and promoting sales. In this paper, we focus on\nthe video retrieval task, facing the following challenges: (1) Howto handle the\nheterogeneities among users, items, and videos? (2)How to mine the\ncomplementarity between items and videos for better user understanding? In this\npaper, we first leverage the dual graph to model the co-existing of user-video\nand user-item interactions in video-driven e-commerce and innovatively reduce\nuser preference understanding to a graph matching problem. To solve it, we\nfurther propose a novel bi-level Graph Matching Network(GMN), which mainly\nconsists of node- and preference-level graph matching. Given a user, node-level\ngraph matching aims to match videos and items, while preference-level graph\nmatching aims to match multiple user preferences extracted from both videos and\nitems. Then the proposed GMN can generate and improve user embedding by\naggregating matched nodes or preferences from the dual graph in a bi-level\nmanner. Comprehensive experiments show the superiority of the proposed GMN with\nsignificant improvements over state-of-the-art approaches (e.g., AUC+1.9% and\nCTR+7.15%). We have developed it on a well-known video-driven e-commerce\nplatform, serving hundreds of millions of users every day", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00346"}
{"title": "Advancing Medical Image Segmentation: Morphology-Driven Learning with\n  Diffusion Transformer", "authors": [], "abstract": "Understanding the morphological structure of medical images and precisely\nsegmenting the region of interest or abnormality is an important task that can\nassist in diagnosis. However, the unique properties of medical imaging make\nclear segmentation difficult, and the high cost and time-consuming task of\nlabeling leads to a coarse-grained representation of ground truth. Facing with\nthese problems, we propose a novel Diffusion Transformer Segmentation (DTS)\nmodel for robust segmentation in the presence of noise. We propose an\nalternative to the dominant Denoising U-Net encoder through experiments\napplying a transformer architecture, which captures global dependency through\nself-attention. Additionally, we propose k-neighbor label smoothing, reverse\nboundary attention, and self-supervised learning with morphology-driven\nlearning to improve the ability to identify complex structures. Our model,\nwhich analyzes the morphological representation of images, shows better results\nthan the previous models in various medical imaging modalities, including CT,\nMRI, and lesion images.", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00347"}
{"title": "Securing the Diagnosis of Medical Imaging: An In-depth Analysis of\n  AI-Resistant Attacks", "authors": [], "abstract": "Machine learning (ML) is a rapidly developing area of medicine that uses\nsignificant resources to apply computer science and statistics to medical\nissues. ML's proponents laud its capacity to handle vast, complicated, and\nerratic medical data. It's common knowledge that attackers might cause\nmisclassification by deliberately creating inputs for machine learning\nclassifiers. Research on adversarial examples has been extensively conducted in\nthe field of computer vision applications. Healthcare systems are thought to be\nhighly difficult because of the security and life-or-death considerations they\ninclude, and performance accuracy is very important. Recent arguments have\nsuggested that adversarial attacks could be made against medical image analysis\n(MedIA) technologies because of the accompanying technology infrastructure and\npowerful financial incentives. Since the diagnosis will be the basis for\nimportant decisions, it is essential to assess how strong medical DNN tasks are\nagainst adversarial attacks. Simple adversarial attacks have been taken into\naccount in several earlier studies. However, DNNs are susceptible to more risky\nand realistic attacks. The present paper covers recent proposed adversarial\nattack strategies against DNNs for medical imaging as well as countermeasures.\nIn this study, we review current techniques for adversarial imaging attacks,\ndetections. It also encompasses various facets of these techniques and offers\nsuggestions for the robustness of neural networks to be improved in the future.", "categories": ["cs.CR", "cs.AI", "eess.IV"], "primary_category": "cs.CR", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00348"}
{"title": "A Simple Background Augmentation Method for Object Detection with\n  Diffusion Model", "authors": [], "abstract": "In computer vision, it is well-known that a lack of data diversity will\nimpair model performance. In this study, we address the challenges of enhancing\nthe dataset diversity problem in order to benefit various downstream tasks such\nas object detection and instance segmentation. We propose a simple yet\neffective data augmentation approach by leveraging advancements in generative\nmodels, specifically text-to-image synthesis technologies like Stable\nDiffusion. Our method focuses on generating variations of labeled real images,\nutilizing generative object and background augmentation via inpainting to\naugment existing training data without the need for additional annotations. We\nfind that background augmentation, in particular, significantly improves the\nmodels' robustness and generalization capabilities. We also investigate how to\nadjust the prompt and mask to ensure the generated content comply with the\nexisting annotations. The efficacy of our augmentation techniques is validated\nthrough comprehensive evaluations of the COCO dataset and several other key\nobject detection benchmarks, demonstrating notable enhancements in model\nperformance across diverse scenarios. This approach offers a promising solution\nto the challenges of dataset enhancement, contributing to the development of\nmore accurate and robust computer vision models.", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00350"}
{"title": "Hierarchically Structured Neural Bones for Reconstructing Animatable\n  Objects from Casual Videos", "authors": [], "abstract": "We propose a new framework for creating and easily manipulating 3D models of\narbitrary objects using casually captured videos. Our core ingredient is a\nnovel hierarchy deformation model, which captures motions of objects with a\ntree-structured bones. Our hierarchy system decomposes motions based on the\ngranularity and reveals the correlations between parts without exploiting any\nprior structural knowledge. We further propose to regularize the bones to be\npositioned at the basis of motions, centers of parts, sufficiently covering\nrelated surfaces of the part. This is achieved by our bone occupancy function,\nwhich identifies whether a given 3D point is placed within the bone. Coupling\nthe proposed components, our framework offers several clear advantages: (1)\nusers can obtain animatable 3D models of the arbitrary objects in improved\nquality from their casual videos, (2) users can manipulate 3D models in an\nintuitive manner with minimal costs, and (3) users can interactively add or\ndelete control points as necessary. The experimental results demonstrate the\nefficacy of our framework on diverse instances, in reconstruction quality,\ninterpretability and easier manipulation. Our code is available at\nhttps://github.com/subin6/HSNB.", "categories": ["cs.CV"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00351"}
{"title": "Autonomous LLM-Enhanced Adversarial Attack for Text-to-Motion", "authors": [], "abstract": "Human motion generation driven by deep generative models has enabled\ncompelling applications, but the ability of text-to-motion (T2M) models to\nproduce realistic motions from text prompts raises security concerns if\nexploited maliciously. Despite growing interest in T2M, few methods focus on\nsafeguarding these models against adversarial attacks, with existing work on\ntext-to-image models proving insufficient for the unique motion domain. In the\npaper, we propose ALERT-Motion, an autonomous framework leveraging large\nlanguage models (LLMs) to craft targeted adversarial attacks against black-box\nT2M models. Unlike prior methods modifying prompts through predefined rules,\nALERT-Motion uses LLMs' knowledge of human motion to autonomously generate\nsubtle yet powerful adversarial text descriptions. It comprises two key\nmodules: an adaptive dispatching module that constructs an LLM-based agent to\niteratively refine and search for adversarial prompts; and a multimodal\ninformation contrastive module that extracts semantically relevant motion\ninformation to guide the agent's search. Through this LLM-driven approach,\nALERT-Motion crafts adversarial prompts querying victim models to produce\noutputs closely matching targeted motions, while avoiding obvious\nperturbations. Evaluations across popular T2M models demonstrate ALERT-Motion's\nsuperiority over previous methods, achieving higher attack success rates with\nstealthier adversarial prompts. This pioneering work on T2M adversarial attacks\nhighlights the urgency of developing defensive measures as motion generation\ntechnology advances, urging further research into safe and responsible\ndeployment.", "categories": ["cs.CV"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00352"}
{"title": "DNTextSpotter: Arbitrary-Shaped Scene Text Spotting via Improved\n  Denoising Training", "authors": [], "abstract": "More and more end-to-end text spotting methods based on Transformer\narchitecture have demonstrated superior performance. These methods utilize a\nbipartite graph matching algorithm to perform one-to-one optimal matching\nbetween predicted objects and actual objects. However, the instability of\nbipartite graph matching can lead to inconsistent optimization targets, thereby\naffecting the training performance of the model. Existing literature applies\ndenoising training to solve the problem of bipartite graph matching instability\nin object detection tasks. Unfortunately, this denoising training method cannot\nbe directly applied to text spotting tasks, as these tasks need to perform\nirregular shape detection tasks and more complex text recognition tasks than\nclassification. To address this issue, we propose a novel denoising training\nmethod (DNTextSpotter) for arbitrary-shaped text spotting. Specifically, we\ndecompose the queries of the denoising part into noised positional queries and\nnoised content queries. We use the four Bezier control points of the Bezier\ncenter curve to generate the noised positional queries. For the noised content\nqueries, considering that the output of the text in a fixed positional order is\nnot conducive to aligning position with content, we employ a masked character\nsliding method to initialize noised content queries, thereby assisting in the\nalignment of text content and position. To improve the model's perception of\nthe background, we further utilize an additional loss function for background\ncharacters classification in the denoising training part.Although DNTextSpotter\nis conceptually simple, it outperforms the state-of-the-art methods on four\nbenchmarks (Total-Text, SCUT-CTW1500, ICDAR15, and Inverse-Text), especially\nyielding an improvement of 11.3% against the best approach in Inverse-Text\ndataset.", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00355"}
{"title": "High-Precision Self-Supervised Monocular Depth Estimation with\n  Rich-Resource Prior", "authors": [], "abstract": "In the area of self-supervised monocular depth estimation, models that\nutilize rich-resource inputs, such as high-resolution and multi-frame inputs,\ntypically achieve better performance than models that use ordinary single image\ninput. However, these rich-resource inputs may not always be available,\nlimiting the applicability of these methods in general scenarios. In this\npaper, we propose Rich-resource Prior Depth estimator (RPrDepth), which only\nrequires single input image during the inference phase but can still produce\nhighly accurate depth estimations comparable to rich resource based methods.\nSpecifically, we treat rich-resource data as prior information and extract\nfeatures from it as reference features in an offline manner. When estimating\nthe depth for a single-image image, we search for similar pixels from the\nrich-resource features and use them as prior information to estimate the depth.\nExperimental results demonstrate that our model outperform other single-image\nmodel and can achieve comparable or even better performance than models with\nrich-resource inputs, only using low-resolution single-image input.", "categories": ["cs.CV"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00361"}
{"title": "Multimodal Fusion and Coherence Modeling for Video Topic Segmentation", "authors": [], "abstract": "The video topic segmentation (VTS) task segments videos into intelligible,\nnon-overlapping topics, facilitating efficient comprehension of video content\nand quick access to specific content. VTS is also critical to various\ndownstream video understanding tasks. Traditional VTS methods using shallow\nfeatures or unsupervised approaches struggle to accurately discern the nuances\nof topical transitions. Recently, supervised approaches have achieved superior\nperformance on video action or scene segmentation over unsupervised approaches.\nIn this work, we improve supervised VTS by thoroughly exploring multimodal\nfusion and multimodal coherence modeling. Specifically, (1) we enhance\nmultimodal fusion by exploring different architectures using cross-attention\nand mixture of experts. (2) To generally strengthen multimodality alignment and\nfusion, we pre-train and fine-tune the model with multimodal contrastive\nlearning. (3) We propose a new pre-training task tailored for the VTS task, and\na novel fine-tuning task for enhancing multimodal coherence modeling for VTS.\nWe evaluate the proposed approaches on educational videos, in the form of\nlectures, due to the vital role of topic segmentation of educational videos in\nboosting learning experiences. Additionally, we introduce a large-scale Chinese\nlecture video dataset to augment the existing English corpus, promoting further\nresearch in VTS. Experiments on both English and Chinese lecture datasets\ndemonstrate that our model achieves superior VTS performance compared to\ncompetitive unsupervised and supervised baselines.", "categories": ["cs.AI", "cs.CV", "eess.IV"], "primary_category": "cs.AI", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00365"}
{"title": "DiM-Gesture: Co-Speech Gesture Generation with Adaptive Layer\n  Normalization Mamba-2 framework", "authors": [], "abstract": "Speech-driven gesture generation is an emerging domain within virtual human\ncreation, where current methods predominantly utilize Transformer-based\narchitectures that necessitate extensive memory and are characterized by slow\ninference speeds. In response to these limitations, we propose\n\\textit{DiM-Gestures}, a novel end-to-end generative model crafted to create\nhighly personalized 3D full-body gestures solely from raw speech audio,\nemploying Mamba-based architectures. This model integrates a Mamba-based fuzzy\nfeature extractor with a non-autoregressive Adaptive Layer Normalization\n(AdaLN) Mamba-2 diffusion architecture. The extractor, leveraging a Mamba\nframework and a WavLM pre-trained model, autonomously derives implicit,\ncontinuous fuzzy features, which are then unified into a singular latent\nfeature. This feature is processed by the AdaLN Mamba-2, which implements a\nuniform conditional mechanism across all tokens to robustly model the interplay\nbetween the fuzzy features and the resultant gesture sequence. This innovative\napproach guarantees high fidelity in gesture-speech synchronization while\nmaintaining the naturalness of the gestures. Employing a diffusion model for\ntraining and inference, our framework has undergone extensive subjective and\nobjective evaluations on the ZEGGS and BEAT datasets. These assessments\nsubstantiate our model's enhanced performance relative to contemporary\nstate-of-the-art methods, demonstrating competitive outcomes with the DiTs\narchitecture (Persona-Gestors) while optimizing memory usage and accelerating\ninference speed.", "categories": ["cs.GR", "cs.AI", "cs.RO", "cs.SD"], "primary_category": "cs.GR", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00370"}
{"title": "Few-shot Defect Image Generation based on Consistency Modeling", "authors": [], "abstract": "Image generation can solve insufficient labeled data issues in defect\ndetection. Most defect generation methods are only trained on a single product\nwithout considering the consistencies among multiple products, leading to poor\nquality and diversity of generated results. To address these issues, we propose\nDefectDiffu, a novel text-guided diffusion method to model both intra-product\nbackground consistency and inter-product defect consistency across multiple\nproducts and modulate the consistency perturbation directions to control\nproduct type and defect strength, achieving diversified defect image\ngeneration. Firstly, we leverage a text encoder to separately provide\nconsistency prompts for background, defect, and fusion parts of the\ndisentangled integrated architecture, thereby disentangling defects and normal\nbackgrounds. Secondly, we propose the double-free strategy to generate defect\nimages through two-stage perturbation of consistency direction, thereby\ncontrolling product type and defect strength by adjusting the perturbation\nscale. Besides, DefectDiffu can generate defect mask annotations utilizing\ncross-attention maps from the defect part. Finally, to improve the generation\nquality of small defects and masks, we propose the adaptive attention-enhance\nloss to increase the attention to defects. Experimental results demonstrate\nthat DefectDiffu surpasses state-of-the-art methods in terms of generation\nquality and diversity, thus effectively improving downstream defection\nperformance. Moreover, defect perturbation directions can be transferred among\nvarious products to achieve zero-shot defect generation, which is highly\nbeneficial for addressing insufficient data issues. The code are available at\nhttps://github.com/FFDD-diffusion/DefectDiffu.", "categories": ["cs.CV"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00372"}
{"title": "On the Limitations and Prospects of Machine Unlearning for Generative AI", "authors": [], "abstract": "Generative AI (GenAI), which aims to synthesize realistic and diverse data\nsamples from latent variables or other data modalities, has achieved remarkable\nresults in various domains, such as natural language, images, audio, and\ngraphs. However, they also pose challenges and risks to data privacy, security,\nand ethics. Machine unlearning is the process of removing or weakening the\ninfluence of specific data samples or features from a trained model, without\naffecting its performance on other data or tasks. While machine unlearning has\nshown significant efficacy in traditional machine learning tasks, it is still\nunclear if it could help GenAI become safer and aligned with human desire. To\nthis end, this position paper provides an in-depth discussion of the machine\nunlearning approaches for GenAI. Firstly, we formulate the problem of machine\nunlearning tasks on GenAI and introduce the background. Subsequently, we\nsystematically examine the limitations of machine unlearning on GenAI models by\nfocusing on the two representative branches: LLMs and image generative\n(diffusion) models. Finally, we provide our prospects mainly from three\naspects: benchmark, evaluation metrics, and utility-unlearning trade-off, and\nconscientiously advocate for the future development of this field.", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00376"}
{"title": "Deepfake Media Forensics: State of the Art and Challenges Ahead", "authors": [], "abstract": "AI-generated synthetic media, also called Deepfakes, have significantly\ninfluenced so many domains, from entertainment to cybersecurity. Generative\nAdversarial Networks (GANs) and Diffusion Models (DMs) are the main frameworks\nused to create Deepfakes, producing highly realistic yet fabricated content.\nWhile these technologies open up new creative possibilities, they also bring\nsubstantial ethical and security risks due to their potential misuse. The rise\nof such advanced media has led to the development of a cognitive bias known as\nImpostor Bias, where individuals doubt the authenticity of multimedia due to\nthe awareness of AI's capabilities. As a result, Deepfake detection has become\na vital area of research, focusing on identifying subtle inconsistencies and\nartifacts with machine learning techniques, especially Convolutional Neural\nNetworks (CNNs). Research in forensic Deepfake technology encompasses five main\nareas: detection, attribution and recognition, passive authentication,\ndetection in realistic scenarios, and active authentication. Each area tackles\nspecific challenges, from tracing the origins of synthetic media and examining\nits inherent characteristics for authenticity. This paper reviews the primary\nalgorithms that address these challenges, examining their advantages,\nlimitations, and future prospects.", "categories": ["cs.CV"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00388"}
{"title": "Unsupervised Pairwise Causal Discovery on Heterogeneous Data using\n  Mutual Information Measures", "authors": [], "abstract": "A fundamental task in science is to determine the underlying causal relations\nbecause it is the knowledge of this functional structure what leads to the\ncorrect interpretation of an effect given the apparent associations in the\nobserved data. In this sense, Causal Discovery is a technique that tackles this\nchallenge by analyzing the statistical properties of the constituent variables.\nIn this work, we target the generalizability of the discovery method by\nfollowing a reductionist approach that only involves two variables, i.e., the\npairwise or bi-variate setting. We question the current (possibly misleading)\nbaseline results on the basis that they were obtained through supervised\nlearning, which is arguably contrary to this genuinely exploratory endeavor. In\nconsequence, we approach this problem in an unsupervised way, using robust\nMutual Information measures, and observing the impact of the different variable\ntypes, which is oftentimes ignored in the design of solutions. Thus, we provide\na novel set of standard unbiased results that can serve as a reference to guide\nfuture discovery tasks in completely unknown environments.", "categories": ["cs.AI", "cs.LG", "stat.ME"], "primary_category": "cs.AI", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00399"}
{"title": "DriveArena: A Closed-loop Generative Simulation Platform for Autonomous\n  Driving", "authors": [], "abstract": "This paper presented DriveArena, the first high-fidelity closed-loop\nsimulation system designed for driving agents navigating in real scenarios.\nDriveArena features a flexible, modular architecture, allowing for the seamless\ninterchange of its core components: Traffic Manager, a traffic simulator\ncapable of generating realistic traffic flow on any worldwide street map, and\nWorld Dreamer, a high-fidelity conditional generative model with infinite\nautoregression. This powerful synergy empowers any driving agent capable of\nprocessing real-world images to navigate in DriveArena's simulated environment.\nThe agent perceives its surroundings through images generated by World Dreamer\nand output trajectories. These trajectories are fed into Traffic Manager,\nachieving realistic interactions with other vehicles and producing a new scene\nlayout. Finally, the latest scene layout is relayed back into World Dreamer,\nperpetuating the simulation cycle. This iterative process fosters closed-loop\nexploration within a highly realistic environment, providing a valuable\nplatform for developing and evaluating driving agents across diverse and\nchallenging scenarios. DriveArena signifies a substantial leap forward in\nleveraging generative image data for the driving simulation platform, opening\ninsights for closed-loop autonomous driving. Code will be available soon on\nGitHub: https://github.com/PJLab-ADG/DriveArena", "categories": ["cs.RO", "cs.AI", "cs.CV"], "primary_category": "cs.RO", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00415"}
{"title": "Towards Reliable Advertising Image Generation Using Human Feedback", "authors": [], "abstract": "In the e-commerce realm, compelling advertising images are pivotal for\nattracting customer attention. While generative models automate image\ngeneration, they often produce substandard images that may mislead customers\nand require significant labor costs to inspect. This paper delves into\nincreasing the rate of available generated images. We first introduce a\nmulti-modal Reliable Feedback Network (RFNet) to automatically inspect the\ngenerated images. Combining the RFNet into a recurrent process, Recurrent\nGeneration, results in a higher number of available advertising images. To\nfurther enhance production efficiency, we fine-tune diffusion models with an\ninnovative Consistent Condition regularization utilizing the feedback from\nRFNet (RFFT). This results in a remarkable increase in the available rate of\ngenerated images, reducing the number of attempts in Recurrent Generation, and\nproviding a highly efficient production process without sacrificing visual\nappeal. We also construct a Reliable Feedback 1 Million (RF1M) dataset which\ncomprises over one million generated advertising images annotated by human,\nwhich helps to train RFNet to accurately assess the availability of generated\nimages and faithfully reflect the human feedback. Generally speaking, our\napproach offers a reliable solution for advertising image generation.", "categories": ["cs.CV"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00418"}
{"title": "MPT-PAR:Mix-Parameters Transformer for Panoramic Activity Recognition", "authors": [], "abstract": "The objective of the panoramic activity recognition task is to identify\nbehaviors at various granularities within crowded and complex environments,\nencompassing individual actions, social group activities, and global\nactivities. Existing methods generally use either parameter-independent modules\nto capture task-specific features or parameter-sharing modules to obtain common\nfeatures across all tasks. However, there is often a strong interrelatedness\nand complementary effect between tasks of different granularities that previous\nmethods have yet to notice. In this paper, we propose a model called MPT-PAR\nthat considers both the unique characteristics of each task and the synergies\nbetween different tasks simultaneously, thereby maximizing the utilization of\nfeatures across multi-granularity activity recognition. Furthermore, we\nemphasize the significance of temporal and spatial information by introducing a\nspatio-temporal relation-enhanced module and a scene representation learning\nmodule, which integrate the the spatio-temporal context of action and global\nscene into the feature map of each granularity. Our method achieved an overall\nF1 score of 47.5\\% on the JRDB-PAR dataset, significantly outperforming all the\nstate-of-the-art methods.", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00420"}
{"title": "Towards Evolutionary-based Automated Machine Learning for Small Molecule\n  Pharmacokinetic Prediction", "authors": [], "abstract": "Machine learning (ML) is revolutionising drug discovery by expediting the\nprediction of small molecule properties essential for developing new drugs.\nThese properties -- including absorption, distribution, metabolism and\nexcretion (ADME)-- are crucial in the early stages of drug development since\nthey provide an understanding of the course of the drug in the organism, i.e.,\nthe drug's pharmacokinetics. However, existing methods lack personalisation and\nrely on manually crafted ML algorithms or pipelines, which can introduce\ninefficiencies and biases into the process. To address these challenges, we\npropose a novel evolutionary-based automated ML method (AutoML) specifically\ndesigned for predicting small molecule properties, with a particular focus on\npharmacokinetics. Leveraging the advantages of grammar-based genetic\nprogramming, our AutoML method streamlines the process by automatically\nselecting algorithms and designing predictive pipelines tailored to the\nparticular characteristics of input molecular data. Results demonstrate\nAutoML's effectiveness in selecting diverse ML algorithms, resulting in\ncomparable or even improved predictive performances compared to conventional\napproaches. By offering personalised ML-driven pipelines, our method promises\nto enhance small molecule research in drug discovery, providing researchers\nwith a valuable tool for accelerating the development of novel therapeutic\ndrugs.", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00421"}
{"title": "Augmenting Channel Simulator and Semi- Supervised Learning for Efficient\n  Indoor Positioning", "authors": [], "abstract": "This work aims to tackle the labor-intensive and resource-consuming task of\nindoor positioning by proposing an efficient approach. The proposed approach\ninvolves the introduction of a semi-supervised learning (SSL) with a biased\nteacher (SSLB) algorithm, which effectively utilizes both labeled and unlabeled\nchannel data. To reduce measurement expenses, unlabeled data is generated using\nan updated channel simulator (UCHS), and then weighted by adaptive confidence\nvalues to simplify the tuning of hyperparameters. Simulation results\ndemonstrate that the proposed strategy achieves superior performance while\nminimizing measurement overhead and training expense compared to existing\nbenchmarks, offering a valuable and practical solution for indoor positioning.", "categories": ["eess.SP", "cs.AI"], "primary_category": "eess.SP", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00429"}
{"title": "A Qualitative Study on Using ChatGPT for Software Security: Perception\n  vs. Practicality", "authors": [], "abstract": "Artificial Intelligence (AI) advancements have enabled the development of\nLarge Language Models (LLMs) that can perform a variety of tasks with\nremarkable semantic understanding and accuracy. ChatGPT is one such LLM that\nhas gained significant attention due to its impressive capabilities for\nassisting in various knowledge-intensive tasks. Due to the knowledge-intensive\nnature of engineering secure software, ChatGPT's assistance is expected to be\nexplored for security-related tasks during the development/evolution of\nsoftware. To gain an understanding of the potential of ChatGPT as an emerging\ntechnology for supporting software security, we adopted a two-fold approach.\nInitially, we performed an empirical study to analyse the perceptions of those\nwho had explored the use of ChatGPT for security tasks and shared their views\non Twitter. It was determined that security practitioners view ChatGPT as\nbeneficial for various software security tasks, including vulnerability\ndetection, information retrieval, and penetration testing. Secondly, we\ndesigned an experiment aimed at investigating the practicality of this\ntechnology when deployed as an oracle in real-world settings. In particular, we\nfocused on vulnerability detection and qualitatively examined ChatGPT outputs\nfor given prompts within this prominent software security task. Based on our\nanalysis, responses from ChatGPT in this task are largely filled with generic\nsecurity information and may not be appropriate for industry use. To prevent\ndata leakage, we performed this analysis on a vulnerability dataset compiled\nafter the OpenAI data cut-off date from real-world projects covering 40\ndistinct vulnerability types and 12 programming languages. We assert that the\nfindings from this study would contribute to future research aimed at\ndeveloping and evaluating LLMs dedicated to software security.", "categories": ["cs.SE", "cs.AI", "cs.CR"], "primary_category": "cs.SE", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00435"}
{"title": "MonoMM: A Multi-scale Mamba-Enhanced Network for Real-time Monocular 3D\n  Object Detection", "authors": [], "abstract": "Recent advancements in transformer-based monocular 3D object detection\ntechniques have exhibited exceptional performance in inferring 3D attributes\nfrom single 2D images. However, most existing methods rely on\nresource-intensive transformer architectures, which often lead to significant\ndrops in computational efficiency and performance when handling long sequence\ndata. To address these challenges and advance monocular 3D object detection\ntechnology, we propose an innovative network architecture, MonoMM, a\nMulti-scale \\textbf{M}amba-Enhanced network for real-time Monocular 3D object\ndetection. This well-designed architecture primarily includes the following two\ncore modules: Focused Multi-Scale Fusion (FMF) Module, which focuses on\neffectively preserving and fusing image information from different scales with\nlower computational resource consumption. By precisely regulating the\ninformation flow, the FMF module enhances the model adaptability and robustness\nto scale variations while maintaining image details. Depth-Aware Feature\nEnhancement Mamba (DMB) Module: It utilizes the fused features from image\ncharacteristics as input and employs a novel adaptive strategy to globally\nintegrate depth information and visual information. This depth fusion strategy\nnot only improves the accuracy of depth estimation but also enhances the model\nperformance under different viewing angles and environmental conditions.\nMoreover, the modular design of MonoMM provides high flexibility and\nscalability, facilitating adjustments and optimizations according to specific\napplication needs. Extensive experiments conducted on the KITTI dataset show\nthat our method outperforms previous monocular methods and achieves real-time\ndetection.", "categories": ["cs.CV"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00438"}
{"title": "Focus, Distinguish, and Prompt: Unleashing CLIP for Efficient and\n  Flexible Scene Text Retrieval", "authors": [], "abstract": "Scene text retrieval aims to find all images containing the query text from\nan image gallery. Current efforts tend to adopt an Optical Character\nRecognition (OCR) pipeline, which requires complicated text detection and/or\nrecognition processes, resulting in inefficient and inflexible retrieval.\nDifferent from them, in this work we propose to explore the intrinsic potential\nof Contrastive Language-Image Pre-training (CLIP) for OCR-free scene text\nretrieval. Through empirical analysis, we observe that the main challenges of\nCLIP as a text retriever are: 1) limited text perceptual scale, and 2)\nentangled visual-semantic concepts. To this end, a novel model termed FDP\n(Focus, Distinguish, and Prompt) is developed. FDP first focuses on scene text\nvia shifting the attention to the text area and probing the hidden text\nknowledge, and then divides the query text into content word and function word\nfor processing, in which a semantic-aware prompting scheme and a distracted\nqueries assistance module are utilized. Extensive experiments show that FDP\nsignificantly enhances the inference speed while achieving better or\ncompetitive retrieval accuracy compared to existing methods. Notably, on the\nIIIT-STR benchmark, FDP surpasses the state-of-the-art model by 4.37% with a 4\ntimes faster speed. Furthermore, additional experiments under phrase-level and\nattribute-aware scene text retrieval settings validate FDP's particular\nadvantages in handling diverse forms of query text. The source code will be\npublicly available at https://github.com/Gyann-z/FDP.", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00441"}
{"title": "Ontological Relations from Word Embeddings", "authors": [], "abstract": "It has been reliably shown that the similarity of word embeddings obtained\nfrom popular neural models such as BERT approximates effectively a form of\nsemantic similarity of the meaning of those words. It is therefore natural to\nwonder if those embeddings contain enough information to be able to connect\nthose meanings through ontological relationships such as the one of\nsubsumption. If so, large knowledge models could be built that are capable of\nsemantically relating terms based on the information encapsulated in word\nembeddings produced by pre-trained models, with implications not only for\nontologies (ontology matching, ontology evolution, etc.) but also on the\nability to integrate ontological knowledge in neural models. In this paper, we\ntest how embeddings produced by several pre-trained models can be used to\npredict relations existing between classes and properties of popular\nupper-level and general ontologies. We show that even a simple feed-forward\narchitecture on top of those embeddings can achieve promising accuracies, with\nvarying generalisation abilities depending on the input data. To achieve that,\nwe produce a dataset that can be used to further enhance those models, opening\nnew possibilities for applications integrating knowledge from web ontologies.", "categories": ["cs.AI"], "primary_category": "cs.AI", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00444"}
{"title": "DiscipLink: Unfolding Interdisciplinary Information Seeking Process via\n  Human-AI Co-Exploration", "authors": [], "abstract": "Interdisciplinary studies often require researchers to explore literature in\ndiverse branches of knowledge. Yet, navigating through the highly scattered\nknowledge from unfamiliar disciplines poses a significant challenge. In this\npaper, we introduce DiscipLink, a novel interactive system that facilitates\ncollaboration between researchers and large language models (LLMs) in\ninterdisciplinary information seeking (IIS). Based on users' topics of\ninterest, DiscipLink initiates exploratory questions from the perspectives of\npossible relevant fields of study, and users can further tailor these\nquestions. DiscipLink then supports users in searching and screening papers\nunder selected questions by automatically expanding queries with\ndisciplinary-specific terminologies, extracting themes from retrieved papers,\nand highlighting the connections between papers and questions. Our evaluation,\ncomprising a within-subject comparative experiment and an open-ended\nexploratory study, reveals that DiscipLink can effectively support researchers\nin breaking down disciplinary boundaries and integrating scattered knowledge in\ndiverse fields. The findings underscore the potential of LLM-powered tools in\nfostering information-seeking practices and bolstering interdisciplinary\nresearch.", "categories": ["cs.HC", "cs.AI", "cs.IR"], "primary_category": "cs.HC", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00447"}
{"title": "Reenact Anything: Semantic Video Motion Transfer Using Motion-Textual\n  Inversion", "authors": [], "abstract": "Recent years have seen a tremendous improvement in the quality of video\ngeneration and editing approaches. While several techniques focus on editing\nappearance, few address motion. Current approaches using text, trajectories, or\nbounding boxes are limited to simple motions, so we specify motions with a\nsingle motion reference video instead. We further propose to use a pre-trained\nimage-to-video model rather than a text-to-video model. This approach allows us\nto preserve the exact appearance and position of a target object or scene and\nhelps disentangle appearance from motion. Our method, called motion-textual\ninversion, leverages our observation that image-to-video models extract\nappearance mainly from the (latent) image input, while the text/image embedding\ninjected via cross-attention predominantly controls motion. We thus represent\nmotion using text/image embedding tokens. By operating on an inflated\nmotion-text embedding containing multiple text/image embedding tokens per\nframe, we achieve a high temporal motion granularity. Once optimized on the\nmotion reference video, this embedding can be applied to various target images\nto generate videos with semantically similar motions. Our approach does not\nrequire spatial alignment between the motion reference video and target image,\ngeneralizes across various domains, and can be applied to various tasks such as\nfull-body and face reenactment, as well as controlling the motion of inanimate\nobjects and the camera. We empirically demonstrate the effectiveness of our\nmethod in the semantic video motion transfer task, significantly outperforming\nexisting methods in this context.", "categories": ["cs.CV", "cs.GR", "cs.LG"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00458"}
{"title": "Image Super-Resolution with Taylor Expansion Approximation and Large\n  Field Reception", "authors": [], "abstract": "Self-similarity techniques are booming in blind super-resolution (SR) due to\naccurate estimation of the degradation types involved in low-resolution images.\nHowever, high-dimensional matrix multiplication within self-similarity\ncomputation prohibitively consumes massive computational costs. We find that\nthe high-dimensional attention map is derived from the matrix multiplication\nbetween Query and Key, followed by a softmax function. This softmax makes the\nmatrix multiplication between Query and Key inseparable, posing a great\nchallenge in simplifying computational complexity. To address this issue, we\nfirst propose a second-order Taylor expansion approximation (STEA) to separate\nthe matrix multiplication of Query and Key, resulting in the complexity\nreduction from $\\mathcal{O}(N^2)$ to $\\mathcal{O}(N)$. Then, we design a\nmulti-scale large field reception (MLFR) to compensate for the performance\ndegradation caused by STEA. Finally, we apply these two core designs to\nlaboratory and real-world scenarios by constructing LabNet and RealNet,\nrespectively. Extensive experimental results tested on five synthetic datasets\ndemonstrate that our LabNet sets a new benchmark in qualitative and\nquantitative evaluations. Tested on the RealWorld38 dataset, our RealNet\nachieves superior visual quality over existing methods. Ablation studies\nfurther verify the contributions of STEA and MLFR towards both LabNet and\nRealNet frameworks.", "categories": ["cs.CV", "cs.AI", "eess.IV"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00470"}
{"title": "Towards Explainable and Interpretable Musical Difficulty Estimation: A\n  Parameter-efficient Approach", "authors": [], "abstract": "Estimating music piece difficulty is important for organizing educational\nmusic collections. This process could be partially automatized to facilitate\nthe educator's role. Nevertheless, the decisions performed by prevalent\ndeep-learning models are hardly understandable, which may impair the acceptance\nof such a technology in music education curricula. Our work employs explainable\ndescriptors for difficulty estimation in symbolic music representations.\nFurthermore, through a novel parameter-efficient white-box model, we outperform\nprevious efforts while delivering interpretable results. These comprehensible\noutcomes emulate the functionality of a rubric, a tool widely used in music\neducation. Our approach, evaluated in piano repertoire categorized in 9\nclasses, achieved 41.4% accuracy independently, with a mean squared error (MSE)\nof 1.7, showing precise difficulty estimation. Through our baseline, we\nillustrate how building on top of past research can offer alternatives for\nmusic difficulty assessment which are explainable and interpretable. With this,\nwe aim to promote a more effective communication between the Music Information\nRetrieval (MIR) community and the music education one.", "categories": ["cs.SD", "cs.AI", "cs.IR", "eess.AS"], "primary_category": "cs.SD", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00473"}
{"title": "HBot: A Chatbot for Healthcare Applications in Traditional Chinese\n  Medicine Based on Human Body 3D Visualization", "authors": [], "abstract": "The unique diagnosis and treatment techniques and remarkable clinical\nefficacy of traditional Chinese medicine (TCM) make it play an important role\nin the field of elderly care and healthcare, especially in the rehabilitation\nof some common chronic diseases of the elderly. Therefore, building a TCM\nchatbot for healthcare application will help users obtain consultation services\nin a direct and natural way. However, concepts such as acupuncture points\n(acupoints) and meridians involved in TCM always appear in the consultation,\nwhich cannot be displayed intuitively. To this end, we develop a\n\\textbf{h}ealthcare chat\\textbf{bot} (HBot) based on a human body model in 3D\nand knowledge graph, which provides conversational services such as knowledge\nQ\\&A, prescription recommendation, moxibustion therapy recommendation, and\nacupoint search. When specific acupoints are involved in the conversations\nbetween user and HBot, the 3D body will jump to the corresponding acupoints and\nhighlight them. Moreover, Hbot can also be used in training scenarios to\naccelerate the teaching process of TCM by intuitively displaying acupuncture\npoints and knowledge cards. The demonstration video is available at\nhttps://www.youtube.com/watch?v=UhQhutSKkTU . Our code and dataset are publicly\navailable at Gitee: https://gitee.com/plabrolin/interactive-3d-acup.git", "categories": ["cs.AI"], "primary_category": "cs.AI", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00481"}
{"title": "A Systematic Review on Long-Tailed Learning", "authors": [], "abstract": "Long-tailed data is a special type of multi-class imbalanced data with a very\nlarge amount of minority/tail classes that have a very significant combined\ninfluence. Long-tailed learning aims to build high-performance models on\ndatasets with long-tailed distributions, which can identify all the classes\nwith high accuracy, in particular the minority/tail classes. It is a\ncutting-edge research direction that has attracted a remarkable amount of\nresearch effort in the past few years. In this paper, we present a\ncomprehensive survey of latest advances in long-tailed visual learning. We\nfirst propose a new taxonomy for long-tailed learning, which consists of eight\ndifferent dimensions, including data balancing, neural architecture, feature\nenrichment, logits adjustment, loss function, bells and whistles, network\noptimization, and post hoc processing techniques. Based on our proposed\ntaxonomy, we present a systematic review of long-tailed learning methods,\ndiscussing their commonalities and alignable differences. We also analyze the\ndifferences between imbalance learning and long-tailed learning approaches.\nFinally, we discuss prospects and future directions in this field.", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MM"], "primary_category": "cs.LG", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00483"}
{"title": "Multi-label Sewer Pipe Defect Recognition with Mask Attention Feature\n  Enhancement and Label Correlation Learning", "authors": [], "abstract": "The coexistence of multiple defect categories as well as the substantial\nclass imbalance problem significantly impair the detection of sewer pipeline\ndefects. To solve this problem, a multi-label pipe defect recognition method is\nproposed based on mask attention guided feature enhancement and label\ncorrelation learning. The proposed method can achieve current approximate\nstate-of-the-art classification performance using just 1/16 of the Sewer-ML\ntraining dataset and exceeds the current best method by 11.87\\% in terms of F2\nmetric on the full dataset, while also proving the superiority of the model.\nThe major contribution of this study is the development of a more efficient\nmodel for identifying and locating multiple defects in sewer pipe images for a\nmore accurate sewer pipeline condition assessment. Moreover, by employing class\nactivation maps, our method can accurately pinpoint multiple defect categories\nin the image which demonstrates a strong model interpretability. Our code is\navailable at\n\\href{https://github.com/shengyu27/MA-Q2L}{\\textcolor{black}{https://github.com/shengyu27/MA-Q2L.}", "categories": ["cs.CV"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00489"}
{"title": "Graph Representation Learning via Causal Diffusion for\n  Out-of-Distribution Recommendation", "authors": [], "abstract": "Graph Neural Networks (GNNs)-based recommendation algorithms typically assume\nthat training and testing data are drawn from independent and identically\ndistributed (IID) spaces. However, this assumption often fails in the presence\nof out-of-distribution (OOD) data, resulting in significant performance\ndegradation. In this study, we construct a Structural Causal Model (SCM) to\nanalyze interaction data, revealing that environmental confounders (e.g., the\nCOVID-19 pandemic) lead to unstable correlations in GNN-based models, thus\nimpairing their generalization to OOD data. To address this issue, we propose a\nnovel approach, graph representation learning via causal diffusion\n(CausalDiffRec) for OOD recommendation. This method enhances the model's\ngeneralization on OOD data by eliminating environmental confounding factors and\nlearning invariant graph representations. Specifically, we use backdoor\nadjustment and variational inference to infer the real environmental\ndistribution, thereby eliminating the impact of environmental confounders. This\ninferred distribution is then used as prior knowledge to guide the\nrepresentation learning in the reverse phase of the diffusion process to learn\nthe invariant representation. In addition, we provide a theoretical derivation\nthat proves optimizing the objective function of CausalDiffRec can encourage\nthe model to learn environment-invariant graph representations, thereby\nachieving excellent generalization performance in recommendations under\ndistribution shifts. Our extensive experiments validate the effectiveness of\nCausalDiffRec in improving the generalization of OOD data, and the average\nimprovement is up to 10.69% on Food, 18.83% on KuaiRec, 22.41% on Yelp2018, and\n11.65% on Douban datasets.", "categories": ["cs.LG", "cs.AI", "cs.IR", "cs.SI"], "primary_category": "cs.LG", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00490"}
{"title": "GalleryGPT: Analyzing Paintings with Large Multimodal Models", "authors": [], "abstract": "Artwork analysis is important and fundamental skill for art appreciation,\nwhich could enrich personal aesthetic sensibility and facilitate the critical\nthinking ability. Understanding artworks is challenging due to its subjective\nnature, diverse interpretations, and complex visual elements, requiring\nexpertise in art history, cultural background, and aesthetic theory. However,\nlimited by the data collection and model ability, previous works for\nautomatically analyzing artworks mainly focus on classification, retrieval, and\nother simple tasks, which is far from the goal of AI. To facilitate the\nresearch progress, in this paper, we step further to compose comprehensive\nanalysis inspired by the remarkable perception and generation ability of large\nmultimodal models. Specifically, we first propose a task of composing paragraph\nanalysis for artworks, i.e., painting in this paper, only focusing on visual\ncharacteristics to formulate more comprehensive understanding of artworks. To\nsupport the research on formal analysis, we collect a large dataset\nPaintingForm, with about 19k painting images and 50k analysis paragraphs. We\nfurther introduce a superior large multimodal model for painting analysis\ncomposing, dubbed GalleryGPT, which is slightly modified and fine-tuned based\non LLaVA architecture leveraging our collected data. We conduct formal analysis\ngeneration and zero-shot experiments across several datasets to assess the\ncapacity of our model. The results show remarkable performance improvements\ncomparing with powerful baseline LMMs, demonstrating its superb ability of art\nanalysis and generalization. \\textcolor{blue}{The codes and model are available\nat: https://github.com/steven640pixel/GalleryGPT.", "categories": ["cs.CL", "cs.CV", "cs.MM"], "primary_category": "cs.CL", "created": "2024-08-01", "doi": "10.1145/3664647.3681656", "arxiv_id": "2408.00491"}
{"title": "Explainable Emotion Decoding for Human and Computer Vision", "authors": [], "abstract": "Modern Machine Learning (ML) has significantly advanced various research\nfields, but the opaque nature of ML models hinders their adoption in several\ndomains. Explainable AI (XAI) addresses this challenge by providing additional\ninformation to help users understand the internal decision-making process of ML\nmodels. In the field of neuroscience, enriching a ML model for brain decoding\nwith attribution-based XAI techniques means being able to highlight which brain\nareas correlate with the task at hand, thus offering valuable insights to\ndomain experts. In this paper, we analyze human and Computer Vision (CV)\nsystems in parallel, training and explaining two ML models based respectively\non functional Magnetic Resonance Imaging (fMRI) and movie frames. We do so by\nleveraging the \"StudyForrest\" dataset, which includes functional Magnetic\nResonance Imaging (fMRI) scans of subjects watching the \"Forrest Gump\" movie,\nemotion annotations, and eye-tracking data. For human vision the ML task is to\nlink fMRI data with emotional annotations, and the explanations highlight the\nbrain regions strongly correlated with the label. On the other hand, for\ncomputer vision, the input data is movie frames, and the explanations are\npixel-level heatmaps. We cross-analyzed our results, linking human attention\n(obtained through eye-tracking) with XAI saliency on CV models and brain region\nactivations. We show how a parallel analysis of human and computer vision can\nprovide useful information for both the neuroscience community (allocation\ntheory) and the ML community (biological plausibility of convolutional models).", "categories": ["cs.CV", "eess.IV", "q-bio.NC"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00493"}
{"title": "SegStitch: Multidimensional Transformer for Robust and Efficient Medical\n  Imaging Segmentation", "authors": [], "abstract": "Medical imaging segmentation plays a significant role in the automatic\nrecognition and analysis of lesions. State-of-the-art methods, particularly\nthose utilizing transformers, have been prominently adopted in 3D semantic\nsegmentation due to their superior performance in scalability and\ngeneralizability. However, plain vision transformers encounter challenges due\nto their neglect of local features and their high computational complexity. To\naddress these challenges, we introduce three key contributions: Firstly, we\nproposed SegStitch, an innovative architecture that integrates transformers\nwith denoising ODE blocks. Instead of taking whole 3D volumes as inputs, we\nadapt axial patches and customize patch-wise queries to ensure semantic\nconsistency. Additionally, we conducted extensive experiments on the BTCV and\nACDC datasets, achieving improvements up to 11.48% and 6.71% respectively in\nmDSC, compared to state-of-the-art methods. Lastly, our proposed method\ndemonstrates outstanding efficiency, reducing the number of parameters by 36.7%\nand the number of FLOPS by 10.7% compared to UNETR. This advancement holds\npromising potential for adapting our method to real-world clinical practice.\nThe code will be available at https://github.com/goblin327/SegStitch", "categories": ["cs.CV"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00496"}
{"title": "How Effective are Self-Supervised Models for Contact Identification in\n  Videos", "authors": [], "abstract": "The exploration of video content via Self-Supervised Learning (SSL) models\nhas unveiled a dynamic field of study, emphasizing both the complex challenges\nand unique opportunities inherent in this area. Despite the growing body of\nresearch, the ability of SSL models to detect physical contacts in videos\nremains largely unexplored, particularly the effectiveness of methods such as\ndownstream supervision with linear probing or full fine-tuning. This work aims\nto bridge this gap by employing eight different convolutional neural networks\n(CNNs) based video SSL models to identify instances of physical contact within\nvideo sequences specifically. The Something-Something v2 (SSv2) and\nEpic-Kitchen (EK-100) datasets were chosen for evaluating these approaches due\nto the promising results on UCF101 and HMDB51, coupled with their limited prior\nassessment on SSv2 and EK-100. Additionally, these datasets feature diverse\nenvironments and scenarios, essential for testing the robustness and accuracy\nof video-based models. This approach not only examines the effectiveness of\neach model in recognizing physical contacts but also explores the performance\nin the action recognition downstream task. By doing so, valuable insights into\nthe adaptability of SSL models in interpreting complex, dynamic visual\ninformation are contributed.", "categories": ["cs.CV"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00498"}
{"title": "A new approach for encoding code and assisting code understanding", "authors": [], "abstract": "Some companies(e.g., Microsoft Research and Google DeepMind) have discovered\nsome of the limitations of GPTs autoregressive paradigm next-word prediction,\nmanifested in the model lack of planning, working memory, backtracking, and\nreasoning skills. GPTs rely on a local and greedy process of generating the\nnext word, without a global understanding of the task or the output.We have\nconfirmed the above limitations through specialized empirical studies of code\ncomprehension. Although GPT4 is good at producing fluent and coherent text, it\ncannot handle complex logic and generate new code that haven not been seen, and\nit relies too much on the formatting of the prompt to generate the correct\ncode.We propose a new paradigm for code understanding that goes beyond the\nnext-word prediction paradigm, inspired by the successful application of\ndiffusion techniques to image generation(Dalle2, Sora) and protein structure\ngeneration(AlphaFold3), which have no autoregressive constraints.Instead of\nencoding the code in a form that mimics natural language, we encode the code as\na heterogeneous image paradigm with a memory of global information that mimics\nboth images and protein structures.We then refer to Sora's CLIP upstream\ntext-to-image encoder model to design a text-to-code encoder model that can be\napplied to various downstream code understanding tasks.The model learns the\nglobal understanding of code under the new paradigm heterogeneous image,\nconnects the encoding space of text and code, and encodes the input of text\ninto the vector of code most similar to it.Using self-supervised comparative\nlearning on 456,360 text-code pairs, the model achieved a zero-shot prediction\nof new data. This work is the basis for future work on code generation using\ndiffusion techniques under a new paradigm to avoid autoregressive limitations.", "categories": ["cs.AI"], "primary_category": "cs.AI", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00521"}
{"title": "Jailbreaking Text-to-Image Models with LLM-Based Agents", "authors": [], "abstract": "Recent advancements have significantly improved automated task-solving\ncapabilities using autonomous agents powered by large language models (LLMs).\nHowever, most LLM-based agents focus on dialogue, programming, or specialized\ndomains, leaving gaps in addressing generative AI safety tasks. These gaps are\nprimarily due to the challenges posed by LLM hallucinations and the lack of\nclear guidelines. In this paper, we propose Atlas, an advanced LLM-based\nmulti-agent framework that integrates an efficient fuzzing workflow to target\ngenerative AI models, specifically focusing on jailbreak attacks against\ntext-to-image (T2I) models with safety filters. Atlas utilizes a\nvision-language model (VLM) to assess whether a prompt triggers the T2I model's\nsafety filter. It then iteratively collaborates with both LLM and VLM to\ngenerate an alternative prompt that bypasses the filter. Atlas also enhances\nthe reasoning abilities of LLMs in attack scenarios by leveraging multi-agent\ncommunication, in-context learning (ICL) memory mechanisms, and the\nchain-of-thought (COT) approach. Our evaluation demonstrates that Atlas\nsuccessfully jailbreaks several state-of-the-art T2I models in a black-box\nsetting, which are equipped with multi-modal safety filters. In addition, Atlas\noutperforms existing methods in both query efficiency and the quality of the\ngenerated images.", "categories": ["cs.CR", "cs.AI", "cs.LG"], "primary_category": "cs.CR", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00523"}
{"title": "Hilbert curves for efficient exploratory landscape analysis\n  neighbourhood sampling", "authors": [], "abstract": "Landscape analysis aims to characterise optimisation problems based on their\nobjective (or fitness) function landscape properties. The problem search space\nis typically sampled, and various landscape features are estimated based on the\nsamples. One particularly salient set of features is information content, which\nrequires the samples to be sequences of neighbouring solutions, such that the\nlocal relationships between consecutive sample points are preserved. Generating\nsuch spatially correlated samples that also provide good search space coverage\nis challenging. It is therefore common to first obtain an unordered sample with\ngood search space coverage, and then apply an ordering algorithm such as the\nnearest neighbour to minimise the distance between consecutive points in the\nsample. However, the nearest neighbour algorithm becomes computationally\nprohibitive in higher dimensions, thus there is a need for more efficient\nalternatives. In this study, Hilbert space-filling curves are proposed as a\nmethod to efficiently obtain high-quality ordered samples. Hilbert curves are a\nspecial case of fractal curves, and guarantee uniform coverage of a bounded\nsearch space while providing a spatially correlated sample. We study the\neffectiveness of Hilbert curves as samplers, and discover that they are capable\nof extracting salient features at a fraction of the computational cost compared\nto Latin hypercube sampling with post-factum ordering. Further, we investigate\nthe use of Hilbert curves as an ordering strategy, and find that they order the\nsample significantly faster than the nearest neighbour ordering, without\nsacrificing the saliency of the extracted features.", "categories": ["cs.LG", "cs.AI", "cs.NE"], "primary_category": "cs.LG", "created": "2024-08-01", "doi": "10.1007/978-3-031-56855-8_18", "arxiv_id": "2408.00526"}
{"title": "High-Quality, ROS Compatible Video Encoding and Decoding for\n  High-Definition Datasets", "authors": [], "abstract": "Robotic datasets are important for scientific benchmarking and developing\nalgorithms, for example for Simultaneous Localization and Mapping (SLAM).\nModern robotic datasets feature video data of high resolution and high\nframerates. Storing and sharing those datasets becomes thus very costly,\nespecially if more than one camera is used for the datasets. It is thus\nessential to store this video data in a compressed format. This paper\ninvestigates the use of modern video encoders for robotic datasets. We provide\na software that can replay mp4 videos within ROS 1 and ROS 2 frameworks,\nsupporting the synchronized playback in simulated time. Furthermore, the paper\nevaluates different encoders and their settings to find optimal configurations\nin terms of resulting size, quality and encoding time. Through this work we\nshow that it is possible to store and share even highest quality video datasets\nwithin reasonable storage constraints.", "categories": ["cs.RO", "cs.CV"], "primary_category": "cs.RO", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00538"}
{"title": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs", "authors": [], "abstract": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance.", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00539"}
{"title": "The Energy Cost of Artificial Intelligence of Things Lifecycle", "authors": [], "abstract": "Artificial intelligence (AI)coupled with existing Internet of Things (IoT)\nenables more streamlined and autonomous operations across various economic\nsectors. Consequently, the paradigm of Artificial Intelligence of Things (AIoT)\nhaving AI techniques at its core implies additional energy and carbon costs\nthat may become significant with more complex neural architectures. To better\nunderstand the energy and Carbon Footprint (CF) of some AIoT components, very\nrecent studies employ conventional metrics. However, these metrics are not\ndesigned to capture energy efficiency aspects of inference. In this paper, we\npropose a new metric, the Energy Cost of AIoT Lifecycle (eCAL) to capture the\noverall energy cost of inference over the lifecycle of an AIoT system. We\ndevise a new methodology for determining eCAL of an AIoT system by analyzing\nthe complexity of data manipulation in individual components involved in the\nAIoT lifecycle and derive the overall and per bit energy consumption. With eCAL\nwe show that the better a model is and the more it is used, the more energy\nefficient an inference is. For an example AIoT configuration, eCAL for making\n$100$ inferences is $1.43$ times higher than for $1000$ inferences. We also\nevaluate the CF of the AIoT system by calculating the equivalent CO$_{2}$\nemissions based on the energy consumption and the Carbon Intensity (CI) across\ndifferent countries. Using 2023 renewable data, our analysis reveals that\ndeploying an AIoT system in Germany results in emitting $4.62$ times higher\nCO$_2$ than in Finland, due to latter using more low-CI energy sources.", "categories": ["cs.ET", "cs.AI", "cs.LG"], "primary_category": "cs.ET", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00540"}
{"title": "Illustrating Classic Brazilian Books using a Text-To-Image Diffusion\n  Model", "authors": [], "abstract": "In recent years, Generative Artificial Intelligence (GenAI) has undergone a\nprofound transformation in addressing intricate tasks involving diverse\nmodalities such as textual, auditory, visual, and pictorial generation. Within\nthis spectrum, text-to-image (TTI) models have emerged as a formidable approach\nto generating varied and aesthetically appealing compositions, spanning\napplications from artistic creation to realistic facial synthesis, and\ndemonstrating significant advancements in computer vision, image processing,\nand multimodal tasks. The advent of Latent Diffusion Models (LDMs) signifies a\nparadigm shift in the domain of AI capabilities. This article delves into the\nfeasibility of employing the Stable Diffusion LDM to illustrate literary works.\nFor this exploration, seven classic Brazilian books have been selected as case\nstudies. The objective is to ascertain the practicality of this endeavor and to\nevaluate the potential of Stable Diffusion in producing illustrations that\naugment and enrich the reader's experience. We will outline the beneficial\naspects, such as the capacity to generate distinctive and contextually\npertinent images, as well as the drawbacks, including any shortcomings in\nfaithfully capturing the essence of intricate literary depictions. Through this\nstudy, we aim to provide a comprehensive assessment of the viability and\nefficacy of utilizing AI-generated illustrations in literary contexts,\nelucidating both the prospects and challenges encountered in this pioneering\napplication of technology.", "categories": ["cs.AI"], "primary_category": "cs.AI", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00544"}
{"title": "Learning to Embed Distributions via Maximum Kernel Entropy", "authors": [], "abstract": "Empirical data can often be considered as samples from a set of probability\ndistributions. Kernel methods have emerged as a natural approach for learning\nto classify these distributions. Although numerous kernels between\ndistributions have been proposed, applying kernel methods to distribution\nregression tasks remains challenging, primarily because selecting a suitable\nkernel is not straightforward. Surprisingly, the question of learning a\ndata-dependent distribution kernel has received little attention. In this\npaper, we propose a novel objective for the unsupervised learning of\ndata-dependent distribution kernel, based on the principle of entropy\nmaximization in the space of probability measure embeddings. We examine the\ntheoretical properties of the latent embedding space induced by our objective,\ndemonstrating that its geometric structure is well-suited for solving\ndownstream discriminative tasks. Finally, we demonstrate the performance of the\nlearned kernel across different modalities.", "categories": ["cs.LG", "cs.AI", "eess.SP", "stat.ML"], "primary_category": "cs.LG", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00549"}
{"title": "Mitigating Multilingual Hallucination in Large Vision-Language Models", "authors": [], "abstract": "While Large Vision-Language Models (LVLMs) have exhibited remarkable\ncapabilities across a wide range of tasks, they suffer from hallucination\nproblems, where models generate plausible yet incorrect answers given the input\nimage-query pair. This hallucination phenomenon is even more severe when\nquerying the image in non-English languages, while existing methods for\nmitigating hallucinations in LVLMs only consider the English scenarios. In this\npaper, we make the first attempt to mitigate this important multilingual\nhallucination in LVLMs. With thorough experiment analysis, we found that\nmultilingual hallucination in LVLMs is a systemic problem that could arise from\ndeficiencies in multilingual capabilities or inadequate multimodal abilities.\nTo this end, we propose a two-stage Multilingual Hallucination Removal (MHR)\nframework for LVLMs, aiming to improve resistance to hallucination for both\nhigh-resource and low-resource languages. Instead of relying on the intricate\nmanual annotations of multilingual resources, we fully leverage the inherent\ncapabilities of the LVLM and propose a novel cross-lingual alignment method,\nwhich generates multiple responses for each image-query input and then\nidentifies the hallucination-aware pairs for each language. These data pairs\nare finally used for direct preference optimization to prompt the LVLMs to\nfavor non-hallucinating responses. Experimental results show that our MHR\nachieves a substantial reduction in hallucination generation for LVLMs.\nNotably, on our extended multilingual POPE benchmark, our framework delivers an\naverage increase of 19.0% in accuracy across 13 different languages. Our code\nand model weights are available at https://github.com/ssmisya/MHR", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00550"}
{"title": "Alleviating Hallucination in Large Vision-Language Models with Active\n  Retrieval Augmentation", "authors": [], "abstract": "Despite the remarkable ability of large vision-language models (LVLMs) in\nimage comprehension, these models frequently generate plausible yet factually\nincorrect responses, a phenomenon known as hallucination.Recently, in large\nlanguage models (LLMs), augmenting LLMs by retrieving information from external\nknowledge resources has been proven as a promising solution to mitigate\nhallucinations.However, the retrieval augmentation in LVLM significantly lags\nbehind the widespread applications of LVLM. Moreover, when transferred to\naugmenting LVLMs, sometimes the hallucination degree of the model is even\nexacerbated.Motivated by the research gap and counter-intuitive phenomenon, we\nintroduce a novel framework, the Active Retrieval-Augmented large\nvision-language model (ARA), specifically designed to address hallucinations by\nincorporating three critical dimensions: (i) dissecting the retrieval targets\nbased on the inherent hierarchical structures of images. (ii) pinpointing the\nmost effective retrieval methods and filtering out the reliable retrieval\nresults. (iii) timing the retrieval process to coincide with episodes of low\ncertainty, while circumventing unnecessary retrieval during periods of high\ncertainty. To assess the capability of our proposed ARA model in reducing\nhallucination, we employ three widely used LVLM models (LLaVA-1.5, Qwen-VL, and\nmPLUG-Owl2) across four benchmarks. Our empirical observations suggest that by\nutilizing fitting retrieval mechanisms and timing the retrieval judiciously, we\ncan effectively mitigate the hallucination problem. We hope that this study can\nprovide deeper insights into how to adapt the retrieval augmentation to LVLMs\nfor reducing hallucinations with more effective retrieval and minimal retrieval\noccurrences.", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00555"}
{"title": "MUFASA: Multi-View Fusion and Adaptation Network with Spatial Awareness\n  for Radar Object Detection", "authors": [], "abstract": "In recent years, approaches based on radar object detection have made\nsignificant progress in autonomous driving systems due to their robustness\nunder adverse weather compared to LiDAR. However, the sparsity of radar point\nclouds poses challenges in achieving precise object detection, highlighting the\nimportance of effective and comprehensive feature extraction technologies. To\naddress this challenge, this paper introduces a comprehensive feature\nextraction method for radar point clouds. This study first enhances the\ncapability of detection networks by using a plug-and-play module, GeoSPA. It\nleverages the Lalonde features to explore local geometric patterns.\nAdditionally, a distributed multi-view attention mechanism, DEMVA, is designed\nto integrate the shared information across the entire dataset with the global\ninformation of each individual frame. By employing the two modules, we present\nour method, MUFASA, which enhances object detection performance through\nimproved feature extraction. The approach is evaluated on the VoD and\nTJ4DRaDSet datasets to demonstrate its effectiveness. In particular, we achieve\nstate-of-the-art results among radar-based methods on the VoD dataset with the\nmAP of 50.24%.", "categories": ["cs.CV"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00565"}
{"title": "Non Verbis, Sed Rebus: Large Language Models are Weak Solvers of Italian\n  Rebuses", "authors": [], "abstract": "Rebuses are puzzles requiring constrained multi-step reasoning to identify a\nhidden phrase from a set of images and letters. In this work, we introduce a\nlarge collection of verbalized rebuses for the Italian language and use it to\nassess the rebus-solving capabilities of state-of-the-art large language\nmodels. While general-purpose systems such as LLaMA-3 and GPT-4o perform poorly\non this task, ad-hoc fine-tuning seems to improve models' performance. However,\nwe find that performance gains from training are largely motivated by\nmemorization. Our results suggest that rebus solving remains a challenging test\nbed to evaluate large language models' linguistic proficiency and sequential\ninstruction-following skills.", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00584"}
{"title": "Regional quality estimation for echocardiography using deep learning", "authors": [], "abstract": "Automatic estimation of cardiac ultrasound image quality can be beneficial\nfor guiding operators and ensuring the accuracy of clinical measurements.\nPrevious work often fails to distinguish the view correctness of the\nechocardiogram from the image quality. Additionally, previous studies only\nprovide a global image quality value, which limits their practical utility. In\nthis work, we developed and compared three methods to estimate image quality:\n1) classic pixel-based metrics like the generalized contrast-to-noise ratio\n(gCNR) on myocardial segments as region of interest and left ventricle lumen as\nbackground, obtained using a U-Net segmentation 2) local image coherence\nderived from a U-Net model that predicts coherence from B-Mode images 3) a deep\nconvolutional network that predicts the quality of each region directly in an\nend-to-end fashion. We evaluate each method against manual regional image\nquality annotations by three experienced cardiologists. The results indicate\npoor performance of the gCNR metric, with Spearman correlation to the\nannotations of \\r{ho} = 0.24. The end-to-end learning model obtains the best\nresult, \\r{ho} = 0.69, comparable to the inter-observer correlation, \\r{ho} =\n0.63. Finally, the coherence-based method, with \\r{ho} = 0.58, outperformed the\nclassical metrics and is more generic than the end-to-end approach.", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00591"}
{"title": "Learned Compression of Point Cloud Geometry and Attributes in a Single\n  Model through Multimodal Rate-Control", "authors": [], "abstract": "Point cloud compression is essential to experience volumetric multimedia as\nit drastically reduces the required streaming data rates. Point attributes,\nspecifically colors, extend the challenge of lossy compression beyond geometric\nrepresentation to achieving joint reconstruction of texture and geometry.\nState-of-the-art methods separate geometry and attributes to compress them\nindividually. This comes at a computational cost, requiring an encoder and a\ndecoder for each modality. Additionally, as attribute compression methods\nrequire the same geometry for encoding and decoding, the encoder emulates the\ndecoder-side geometry reconstruction as an input step to project and compress\nthe attributes. In this work, we propose to learn joint compression of geometry\nand attributes using a single, adaptive autoencoder model, embedding both\nmodalities into a unified latent space which is then entropy encoded. Key to\nthe technique is to replace the search for trade-offs between rate, attribute\nquality and geometry quality, through conditioning the model on the desired\nqualities of both modalities, bypassing the need for training model ensembles.\nTo differentiate important point cloud regions during encoding or to allow\nview-dependent compression for user-centered streaming, conditioning is\npointwise, which allows for local quality and rate variation. Our evaluation\nshows comparable performance to state-of-the-art compression methods for\ngeometry and attributes, while reducing complexity compared to related\ncompression methods.", "categories": ["cs.CV", "cs.MM", "eess.IV"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00599"}
{"title": "Unlocking Fair Use in the Generative AI Supply Chain: A Systematized\n  Literature Review", "authors": [], "abstract": "Through a systematization of generative AI (GenAI) stakeholder goals and\nexpectations, this work seeks to uncover what value different stakeholders see\nin their contributions to the GenAI supply line. This valuation enables us to\nunderstand whether fair use advocated by GenAI companies to train model\nprogresses the copyright law objective of promoting science and arts. While\nassessing the validity and efficacy of the fair use argument, we uncover\nresearch gaps and potential avenues for future works for researchers and\npolicymakers to address.", "categories": ["cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.AI", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00613"}
{"title": "Harnessing Uncertainty-aware Bounding Boxes for Unsupervised 3D Object\n  Detection", "authors": [], "abstract": "Unsupervised 3D object detection aims to identify objects of interest from\nunlabeled raw data, such as LiDAR points. Recent approaches usually adopt\npseudo 3D bounding boxes (3D bboxes) from clustering algorithm to initialize\nthe model training, and then iteratively updating both pseudo labels and the\ntrained model. However, pseudo bboxes inevitably contain noises, and such\ninaccurate annotation accumulates to the final model, compromising the\nperformance. Therefore, in an attempt to mitigate the negative impact of pseudo\nbboxes, we introduce a new uncertainty-aware framework. In particular, Our\nmethod consists of two primary components: uncertainty estimation and\nuncertainty regularization. (1) In the uncertainty estimation phase, we\nincorporate an extra auxiliary detection branch alongside the primary detector.\nThe prediction disparity between the primary and auxiliary detectors is\nleveraged to estimate uncertainty at the box coordinate level, including\nposition, shape, orientation. (2) Based on the assessed uncertainty, we\nregularize the model training via adaptively adjusting every 3D bboxes\ncoordinates. For pseudo bbox coordinates with high uncertainty, we assign a\nrelatively low loss weight. Experiment verifies that the proposed method is\nrobust against the noisy pseudo bboxes, yielding substantial improvements on\nnuScenes and Lyft compared to existing techniques, with increases of 6.9% in\nAP$_{BEV}$ and 2.5% in AP$_{3D}$ on nuScenes, and 2.2% in AP$_{BEV}$ and 1.0%\nin AP$_{3D}$ on Lyft.", "categories": ["cs.CV"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00619"}
{"title": "Are Bigger Encoders Always Better in Vision Large Models?", "authors": [], "abstract": "In recent years, multimodal large language models (MLLMs) have shown strong\npotential in real-world applications. They are developing rapidly due to their\nremarkable ability to comprehend multimodal information and their inherent\npowerful cognitive and reasoning capabilities. Among MLLMs, vision language\nmodels (VLM) stand out for their ability to understand vision information.\nHowever, the scaling trend of VLMs under the current mainstream paradigm has\nnot been extensively studied. Whether we can achieve better performance by\ntraining even larger models is still unclear. To address this issue, we\nconducted experiments on the pretraining stage of MLLMs. We conduct our\nexperiment using different encoder sizes and large language model (LLM) sizes.\nOur findings indicate that merely increasing the size of encoders does not\nnecessarily enhance the performance of VLMs. Moreover, we analyzed the effects\nof LLM backbone parameter size and data quality on the pretraining outcomes.\nAdditionally, we explored the differences in scaling laws between LLMs and\nVLMs.", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00620"}
{"title": "SynesLM: A Unified Approach for Audio-visual Speech Recognition and\n  Translation via Language Model and Synthetic Data", "authors": [], "abstract": "In this work, we present SynesLM, an unified model which can perform three\nmultimodal language understanding tasks: audio-visual automatic speech\nrecognition(AV-ASR) and visual-aided speech/machine translation(VST/VMT).\nUnlike previous research that focused on lip motion as visual cues for speech\nsignals, our work explores more general visual information within entire\nframes, such as objects and actions. Additionally, we use synthetic image data\nto enhance the correlation between image and speech data. We benchmark SynesLM\nagainst the How2 dataset, demonstrating performance on par with\nstate-of-the-art (SOTA) models dedicated to AV-ASR while maintaining our\nmultitasking framework. Remarkably, for zero-shot AV-ASR, SynesLM achieved SOTA\nperformance by lowering the Word Error Rate (WER) from 43.4% to 39.4% on the\nVisSpeech Dataset. Furthermore, our results in VST and VMT outperform the\nprevious results, improving the BLEU score to 43.5 from 37.2 for VST, and to\n54.8 from 54.4 for VMT.", "categories": ["eess.AS", "cs.CL", "cs.CV"], "primary_category": "eess.AS", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00624"}
{"title": "Empowering Snapshot Compressive Imaging: Spatial-Spectral State Space\n  Model with Across-Scanning and Local Enhancement", "authors": [], "abstract": "Snapshot Compressive Imaging (SCI) relies on decoding algorithms such as CNN\nor Transformer to reconstruct the hyperspectral image (HSI) from its compressed\nmeasurement. Although existing CNN and Transformer-based methods have proven\neffective, CNNs are limited by their inadequate modeling of long-range\ndependencies, while Transformer ones face high computational costs due to\nquadratic complexity. Recent Mamba models have demonstrated superior\nperformance over CNN and Transformer-based architectures in some visual tasks,\nbut these models have not fully utilized the local similarities in both spatial\nand spectral dimensions. Moreover, the long-sequence modeling capability of SSM\nmay offer an advantage in processing the numerous spectral bands for HSI\nreconstruction, which has not yet been explored. In this paper, we introduce a\nState Space Model with Across-Scanning and Local Enhancement, named ASLE-SSM,\nthat employs a Spatial-Spectral SSM for global-local balanced context encoding\nand cross-channel interaction promoting. Specifically, we introduce local\nscanning in the spatial dimension to balance the global and local receptive\nfields, and then propose our across-scanning method based on spatial-spectral\nlocal cubes to leverage local similarities between adjacent spectral bands and\npixels to guide the reconstruction process. These two scanning mechanisms\nextract the HSI's local features while balancing the global perspective without\nany additional costs. Experimental results illustrate ASLE-SSM's superiority\nover existing state-of-the-art methods, with an inference speed 2.4 times\nfaster than Transformer-based MST and saving 0.12 (M) of parameters, achieving\nthe lowest computational cost and parameter count.", "categories": ["cs.CV", "eess.IV"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00629"}
{"title": "DisTrack: a new Tool for Semi-automatic Misinformation Tracking in\n  Online Social Networks", "authors": [], "abstract": "Introduction: This article introduces DisTrack, a methodology and a tool\ndeveloped for tracking and analyzing misinformation within Online Social\nNetworks (OSNs). DisTrack is designed to combat the spread of misinformation\nthrough a combination of Natural Language Processing (NLP) Social Network\nAnalysis (SNA) and graph visualization. The primary goal is to detect\nmisinformation, track its propagation, identify its sources, and assess the\ninfluence of various actors within the network.\n  Methods: DisTrack's architecture incorporates a variety of methodologies\nincluding keyword search, semantic similarity assessments, and graph generation\ntechniques. These methods collectively facilitate the monitoring of\nmisinformation, the categorization of content based on alignment with known\nfalse claims, and the visualization of dissemination cascades through detailed\ngraphs. The tool is tailored to capture and analyze the dynamic nature of\nmisinformation spread in digital environments.\n  Results: The effectiveness of DisTrack is demonstrated through three case\nstudies focused on different themes: discredit/hate speech, anti-vaccine\nmisinformation, and false narratives about the Russia-Ukraine conflict. These\nstudies show DisTrack's capabilities in distinguishing posts that propagate\nfalsehoods from those that counteract them, and tracing the evolution of\nmisinformation from its inception.\n  Conclusions: The research confirms that DisTrack is a valuable tool in the\nfield of misinformation analysis. It effectively distinguishes between\ndifferent types of misinformation and traces their development over time. By\nproviding a comprehensive approach to understanding and combating\nmisinformation in digital spaces, DisTrack proves to be an essential asset for\nresearchers and practitioners working to mitigate the impact of false\ninformation in online social environments.", "categories": ["cs.SI", "cs.AI"], "primary_category": "cs.SI", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00633"}
{"title": "Deep Learning in Medical Image Classification from MRI-based Brain Tumor\n  Images", "authors": [], "abstract": "Brain tumors are among the deadliest diseases in the world. Magnetic\nResonance Imaging (MRI) is one of the most effective ways to detect brain\ntumors. Accurate detection of brain tumors based on MRI scans is critical, as\nit can potentially save many lives and facilitate better decision-making at the\nearly stages of the disease. Within our paper, four different types of\nMRI-based images have been collected from the database: glioma tumor, no tumor,\npituitary tumor, and meningioma tumor. Our study focuses on making predictions\nfor brain tumor classification. Five models, including four pre-trained models\n(MobileNet, EfficientNet-B0, ResNet-18, and VGG16) and one new model,\nMobileNet-BT, have been proposed for this study.", "categories": ["cs.CV"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00636"}
{"title": "Privacy-preserving datasets by capturing feature distributions with\n  Conditional VAEs", "authors": [], "abstract": "Large and well-annotated datasets are essential for advancing deep learning\napplications, however often costly or impossible to obtain by a single entity.\nIn many areas, including the medical domain, approaches relying on data sharing\nhave become critical to address those challenges. While effective in increasing\ndataset size and diversity, data sharing raises significant privacy concerns.\nCommonly employed anonymization methods based on the k-anonymity paradigm often\nfail to preserve data diversity, affecting model robustness. This work\nintroduces a novel approach using Conditional Variational Autoencoders (CVAEs)\ntrained on feature vectors extracted from large pre-trained vision foundation\nmodels. Foundation models effectively detect and represent complex patterns\nacross diverse domains, allowing the CVAE to faithfully capture the embedding\nspace of a given data distribution to generate (sample) a diverse,\nprivacy-respecting, and potentially unbounded set of synthetic feature vectors.\nOur method notably outperforms traditional approaches in both medical and\nnatural image domains, exhibiting greater dataset diversity and higher\nrobustness against perturbations while preserving sample privacy. These results\nunderscore the potential of generative models to significantly impact deep\nlearning applications in data-scarce and privacy-sensitive environments. The\nsource code is available at\nhttps://github.com/francescodisalvo05/cvae-anonymization .", "categories": ["cs.LG", "cs.CV", "eess.IV"], "primary_category": "cs.LG", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00639"}
{"title": "AMAES: Augmented Masked Autoencoder Pretraining on Public Brain MRI Data\n  for 3D-Native Segmentation", "authors": [], "abstract": "This study investigates the impact of self-supervised pretraining of 3D\nsemantic segmentation models on a large-scale, domain-specific dataset. We\nintroduce BRAINS-45K, a dataset of 44,756 brain MRI volumes from public\nsources, the largest public dataset available, and revisit a number of design\nchoices for pretraining modern segmentation architectures by simplifying and\noptimizing state-of-the-art methods, and combining them with a novel\naugmentation strategy. The resulting AMAES framework is based on\nmasked-image-modeling and intensity-based augmentation reversal and balances\nmemory usage, runtime, and finetuning performance. Using the popular U-Net and\nthe recent MedNeXt architecture as backbones, we evaluate the effect of\npretraining on three challenging downstream tasks, covering single-sequence,\nlow-resource settings, and out-of-domain generalization. The results highlight\nthat pretraining on the proposed dataset with AMAES significantly improves\nsegmentation performance in the majority of evaluated cases, and that it is\nbeneficial to pretrain the model with augmentations, despite pretraing on a\nlarge-scale dataset. Code and model checkpoints for reproducing results, as\nwell as the BRAINS-45K dataset are available at\n\\url{https://github.com/asbjrnmunk/amaes}.", "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "eess.IV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00640"}
{"title": "Towards End-to-End Explainable Facial Action Unit Recognition via\n  Vision-Language Joint Learning", "authors": [], "abstract": "Facial action units (AUs), as defined in the Facial Action Coding System\n(FACS), have received significant research interest owing to their diverse\nrange of applications in facial state analysis. Current mainstream FAU\nrecognition models have a notable limitation, i.e., focusing only on the\naccuracy of AU recognition and overlooking explanations of corresponding AU\nstates. In this paper, we propose an end-to-end Vision-Language joint learning\nnetwork for explainable FAU recognition (termed VL-FAU), which aims to\nreinforce AU representation capability and language interpretability through\nthe integration of joint multimodal tasks. Specifically, VL-FAU brings together\nlanguage models to generate fine-grained local muscle descriptions and\ndistinguishable global face description when optimising FAU recognition.\nThrough this, the global facial representation and its local AU representations\nwill achieve higher distinguishability among different AUs and different\nsubjects. In addition, multi-level AU representation learning is utilised to\nimprove AU individual attention-aware representation capabilities based on\nmulti-scale combined facial stem feature. Extensive experiments on DISFA and\nBP4D AU datasets show that the proposed approach achieves superior performance\nover the state-of-the-art methods on most of the metrics. In addition, compared\nwith mainstream FAU recognition methods, VL-FAU can provide local- and\nglobal-level interpretability language descriptions with the AUs' predictions.", "categories": ["cs.CV"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": "10.1145/3664647.3681443", "arxiv_id": "2408.00644"}
{"title": "SF3D: Stable Fast 3D Mesh Reconstruction with UV-unwrapping and\n  Illumination Disentanglement", "authors": [], "abstract": "We present SF3D, a novel method for rapid and high-quality textured object\nmesh reconstruction from a single image in just 0.5 seconds. Unlike most\nexisting approaches, SF3D is explicitly trained for mesh generation,\nincorporating a fast UV unwrapping technique that enables swift texture\ngeneration rather than relying on vertex colors. The method also learns to\npredict material parameters and normal maps to enhance the visual quality of\nthe reconstructed 3D meshes. Furthermore, SF3D integrates a delighting step to\neffectively remove low-frequency illumination effects, ensuring that the\nreconstructed meshes can be easily used in novel illumination conditions.\nExperiments demonstrate the superior performance of SF3D over the existing\ntechniques. Project page: https://stable-fast-3d.github.io", "categories": ["cs.CV", "cs.GR"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00653"}
{"title": "ExpertAF: Expert Actionable Feedback from Video", "authors": [], "abstract": "Feedback is essential for learning a new skill or improving one's current\nskill-level. However, current methods for skill-assessment from video only\nprovide scores or compare demonstrations, leaving the burden of knowing what to\ndo differently on the user. We introduce a novel method to generate actionable\nfeedback from video of a person doing a physical activity, such as basketball\nor soccer. Our method takes a video demonstration and its accompanying 3D body\npose and generates (1) free-form expert commentary describing what the person\nis doing well and what they could improve, and (2) a visual expert\ndemonstration that incorporates the required corrections. We show how to\nleverage Ego-Exo4D's videos of skilled activity and expert commentary together\nwith a strong language model to create a weakly-supervised training dataset for\nthis task, and we devise a multimodal video-language model to infer coaching\nfeedback. Our method is able to reason across multi-modal input combinations to\noutput full-spectrum, actionable coaching -- expert commentary, expert video\nretrieval, and the first-of-its-kind expert pose generation -- outperforming\nstrong vision-language models on both established metrics and human preference\nstudies.", "categories": ["cs.CV"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00672"}
{"title": "Learning in Multi-Objective Public Goods Games with Non-Linear Utilities", "authors": [], "abstract": "Addressing the question of how to achieve optimal decision-making under risk\nand uncertainty is crucial for enhancing the capabilities of artificial agents\nthat collaborate with or support humans. In this work, we address this question\nin the context of Public Goods Games. We study learning in a novel\nmulti-objective version of the Public Goods Game where agents have different\nrisk preferences, by means of multi-objective reinforcement learning. We\nintroduce a parametric non-linear utility function to model risk preferences at\nthe level of individual agents, over the collective and individual reward\ncomponents of the game. We study the interplay between such preference\nmodelling and environmental uncertainty on the incentive alignment level in the\ngame. We demonstrate how different combinations of individual preferences and\nenvironmental uncertainties sustain the emergence of cooperative patterns in\nnon-cooperative environments (i.e., where competitive strategies are dominant),\nwhile others sustain competitive patterns in cooperative environments (i.e.,\nwhere cooperative strategies are dominant).", "categories": ["cs.MA", "cs.AI", "cs.GT"], "primary_category": "cs.MA", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00682"}
{"title": "Can Developers Prompt? A Controlled Experiment for Code Documentation\n  Generation", "authors": [], "abstract": "Large language models (LLMs) bear great potential for automating tedious\ndevelopment tasks such as creating and maintaining code documentation. However,\nit is unclear to what extent developers can effectively prompt LLMs to create\nconcise and useful documentation. We report on a controlled experiment with 20\nprofessionals and 30 computer science students tasked with code documentation\ngeneration for two Python functions. The experimental group freely entered\nad-hoc prompts in a ChatGPT-like extension of Visual Studio Code, while the\ncontrol group executed a predefined few-shot prompt. Our results reveal that\nprofessionals and students were unaware of or unable to apply prompt\nengineering techniques. Especially students perceived the documentation\nproduced from ad-hoc prompts as significantly less readable, less concise, and\nless helpful than documentation from prepared prompts. Some professionals\nproduced higher quality documentation by just including the keyword Docstring\nin their ad-hoc prompts. While students desired more support in formulating\nprompts, professionals appreciated the flexibility of ad-hoc prompting.\nParticipants in both groups rarely assessed the output as perfect. Instead,\nthey understood the tools as support to iteratively refine the documentation.\nFurther research is needed to understand which prompting skills and preferences\ndevelopers have and which support they need for certain tasks.", "categories": ["cs.AI", "cs.HC", "cs.SE"], "primary_category": "cs.AI", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00686"}
{"title": "Accelerating Full Waveform Inversion By Transfer Learning", "authors": [], "abstract": "Full waveform inversion (FWI) is a powerful tool for reconstructing material\nfields based on sparsely measured data obtained by wave propagation. For\nspecific problems, discretizing the material field with a neural network (NN)\nimproves the robustness and reconstruction quality of the corresponding\noptimization problem. We call this method NN-based FWI. Starting from an\ninitial guess, the weights of the NN are iteratively updated to fit the\nsimulated wave signals to the sparsely measured data set. For gradient-based\noptimization, a suitable choice of the initial guess, i.e., a suitable NN\nweight initialization, is crucial for fast and robust convergence.\n  In this paper, we introduce a novel transfer learning approach to further\nimprove NN-based FWI. This approach leverages supervised pretraining to provide\na better NN weight initialization, leading to faster convergence of the\nsubsequent optimization problem. Moreover, the inversions yield physically more\nmeaningful local minima. The network is pretrained to predict the unknown\nmaterial field using the gradient information from the first iteration of\nconventional FWI. In our computational experiments on two-dimensional domains,\nthe training data set consists of reference simulations with arbitrarily\npositioned elliptical voids of different shapes and orientations. We compare\nthe performance of the proposed transfer learning NN-based FWI with three other\nmethods: conventional FWI, NN-based FWI without pretraining and conventional\nFWI with an initial guess predicted from the pretrained NN. Our results show\nthat transfer learning NN-based FWI outperforms the other methods in terms of\nconvergence speed and reconstruction quality.", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00695"}
{"title": "Joint Neural Networks for One-shot Object Recognition and Detection", "authors": [], "abstract": "This paper presents a novel joint neural networks approach to address the\nchallenging one-shot object recognition and detection tasks. Inspired by\nSiamese neural networks and state-of-art multi-box detection approaches, the\njoint neural networks are able to perform object recognition and detection for\ncategories that remain unseen during the training process. Following the\none-shot object recognition/detection constraints, the training and testing\ndatasets do not contain overlapped classes, in other words, all the test\nclasses remain unseen during training. The joint networks architecture is able\nto effectively compare pairs of images via stacked convolutional layers of the\nquery and target inputs, recognising patterns of the same input query category\nwithout relying on previous training around this category. The proposed\napproach achieves 61.41% accuracy for one-shot object recognition on the\nMiniImageNet dataset and 47.1% mAP for one-shot object detection when trained\non the COCO dataset and tested using the Pascal VOC dataset. Code available at\nhttps://github.com/cjvargasc/JNN recog and https://github.com/cjvargasc/JNN\ndetection/", "categories": ["cs.CV"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00701"}
{"title": "Future of Artificial Intelligence in Agile Software Development", "authors": [], "abstract": "The advent of Artificial intelligence has promising advantages that can be\nutilized to transform the landscape of software project development. The\nSoftware process framework consists of activities that constantly require\nroutine human interaction, leading to the possibility of errors and\nuncertainties. AI can assist software development managers, software testers,\nand other team members by leveraging LLMs, GenAI models, and AI agents to\nperform routine tasks, risk analysis and prediction, strategy recommendations,\nand support decision making. AI has the potential to increase efficiency and\nreduce the risks encountered by the project management team while increasing\nthe project success rates. Additionally, it can also break down complex notions\nand development processes for stakeholders to make informed decisions. In this\npaper, we propose an approach in which AI tools and technologies can be\nutilized to bestow maximum assistance for agile software projects, which have\nbecome increasingly favored in the industry in recent years.", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00703"}
{"title": "Point-supervised Brain Tumor Segmentation with Box-prompted MedSAM", "authors": [], "abstract": "Delineating lesions and anatomical structure is important for image-guided\ninterventions. Point-supervised medical image segmentation (PSS) has great\npotential to alleviate costly expert delineation labeling. However, due to the\nlack of precise size and boundary guidance, the effectiveness of PSS often\nfalls short of expectations. Although recent vision foundational models, such\nas the medical segment anything model (MedSAM), have made significant\nadvancements in bounding-box-prompted segmentation, it is not straightforward\nto utilize point annotation, and is prone to semantic ambiguity. In this\npreliminary study, we introduce an iterative framework to facilitate\nsemantic-aware point-supervised MedSAM. Specifically, the semantic box-prompt\ngenerator (SBPG) module has the capacity to convert the point input into\npotential pseudo bounding box suggestions, which are explicitly refined by the\nprototype-based semantic similarity. This is then succeeded by a prompt-guided\nspatial refinement (PGSR) module that harnesses the exceptional\ngeneralizability of MedSAM to infer the segmentation mask, which also updates\nthe box proposal seed in SBPG. Performance can be progressively improved with\nadequate iterations. We conducted an evaluation on BraTS2018 for the\nsegmentation of whole brain tumors and demonstrated its superior performance\ncompared to traditional PSS methods and on par with box-supervised methods.", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV", "physics.med-ph"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00706"}
{"title": "Synthetic dual image generation for reduction of labeling efforts in\n  semantic segmentation of micrographs with a customized metric function", "authors": [], "abstract": "Training of semantic segmentation models for material analysis requires\nmicrographs and their corresponding masks. It is quite unlikely that perfect\nmasks will be drawn, especially at the edges of objects, and sometimes the\namount of data that can be obtained is small, since only a few samples are\navailable. These aspects make it very problematic to train a robust model. We\ndemonstrate a workflow for the improvement of semantic segmentation models of\nmicrographs through the generation of synthetic microstructural images in\nconjunction with masks. The workflow only requires joining a few micrographs\nwith their respective masks to create the input for a Vector\nQuantised-Variational AutoEncoder model that includes an embedding space, which\nis trained such that a generative model (PixelCNN) learns the distribution of\neach input, transformed into discrete codes, and can be used to sample new\ncodes. The latter will eventually be decoded by VQ-VAE to generate images\nalongside corresponding masks for semantic segmentation. To evaluate the\nsynthetic data, we have trained U-Net models with different amounts of these\nsynthetic data in conjunction with real data. These models were then evaluated\nusing non-synthetic images only. Additionally, we introduce a customized metric\nderived from the mean Intersection over Union (mIoU). The proposed metric\nprevents a few falsely predicted pixels from greatly reducing the value of the\nmIoU. We have achieved a reduction in sample preparation and acquisition times,\nas well as the efforts, needed for image processing and labeling tasks, are\nless when it comes to training semantic segmentation model. The approach could\nbe generalized to various types of image data such that it serves as a\nuser-friendly solution for training models with a small number of real images.", "categories": ["cs.CV", "cs.CE", "cs.LG"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00707"}
{"title": "Investigating Brain Connectivity and Regional Statistics from EEG for\n  early stage Parkinson's Classification", "authors": [], "abstract": "We evaluate the effectiveness of combining brain connectivity metrics with\nsignal statistics for early stage Parkinson's Disease (PD) classification using\nelectroencephalogram data (EEG). The data is from 5 arousal states - wakeful\nand four sleep stages (N1, N2, N3 and REM). Our pipeline uses an Ada Boost\nmodel for classification on a challenging early stage PD classification task\nwith with only 30 participants (11 PD , 19 Healthy Control). Evaluating 9 brain\nconnectivity metrics we find the best connectivity metric to be different for\neach arousal state with Phase Lag Index achieving the highest individual\nclassification accuracy of 86\\% on N1 data. Further to this our pipeline using\nregional signal statistics achieves an accuracy of 78\\%, using brain\nconnectivity only achieves an accuracy of 86\\% whereas combining the two\nachieves a best accuracy of 91\\%. This best performance is achieved on N1 data\nusing Phase Lag Index (PLI) combined with statistics derived from the frequency\ncharacteristics of the EEG signal. This model also achieves a recall of 80 \\%\nand precision of 96\\%. Furthermore we find that on data from each arousal\nstate, combining PLI with regional signal statistics improves classification\naccuracy versus using signal statistics or brain connectivity alone. Thus we\nconclude that combining brain connectivity statistics with regional EEG\nstatistics is optimal for classifier performance on early stage Parkinson's.\nAdditionally, we find outperformance of N1 EEG for classification of\nParkinson's and expect this could be due to disrupted N1 sleep in PD. This\nshould be explored in future work.", "categories": ["q-bio.NC", "cs.AI", "eess.SP"], "primary_category": "q-bio.NC", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00711"}
{"title": "MotionFix: Text-Driven 3D Human Motion Editing", "authors": [], "abstract": "The focus of this paper is 3D motion editing. Given a 3D human motion and a\ntextual description of the desired modification, our goal is to generate an\nedited motion as described by the text. The challenges include the lack of\ntraining data and the design of a model that faithfully edits the source\nmotion. In this paper, we address both these challenges. We build a methodology\nto semi-automatically collect a dataset of triplets in the form of (i) a source\nmotion, (ii) a target motion, and (iii) an edit text, and create the new\nMotionFix dataset. Having access to such data allows us to train a conditional\ndiffusion model, TMED, that takes both the source motion and the edit text as\ninput. We further build various baselines trained only on text-motion pairs\ndatasets, and show superior performance of our model trained on triplets. We\nintroduce new retrieval-based metrics for motion editing and establish a new\nbenchmark on the evaluation set of MotionFix. Our results are encouraging,\npaving the way for further research on finegrained motion generation. Code and\nmodels will be made publicly available.", "categories": ["cs.CV", "cs.GR"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00712"}
{"title": "SAM 2: Segment Anything in Images and Videos", "authors": [], "abstract": "We present Segment Anything Model 2 (SAM 2), a foundation model towards\nsolving promptable visual segmentation in images and videos. We build a data\nengine, which improves model and data via user interaction, to collect the\nlargest video segmentation dataset to date. Our model is a simple transformer\narchitecture with streaming memory for real-time video processing. SAM 2\ntrained on our data provides strong performance across a wide range of tasks.\nIn video segmentation, we observe better accuracy, using 3x fewer interactions\nthan prior approaches. In image segmentation, our model is more accurate and 6x\nfaster than the Segment Anything Model (SAM). We believe that our data, model,\nand insights will serve as a significant milestone for video segmentation and\nrelated perception tasks. We are releasing a version of our model, the dataset\nand an interactive demo.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00714"}
{"title": "Pathway to Secure and Trustworthy 6G for LLMs: Attacks, Defense, and\n  Opportunities", "authors": [], "abstract": "Recently, large language models (LLMs) have been gaining a lot of interest\ndue to their adaptability and extensibility in emerging applications, including\ncommunication networks. It is anticipated that 6G mobile edge computing\nnetworks will be able to support LLMs as a service, as they provide ultra\nreliable low-latency communications and closed loop massive connectivity.\nHowever, LLMs are vulnerable to data and model privacy issues that affect the\ntrustworthiness of LLMs to be deployed for user-based services. In this paper,\nwe explore the security vulnerabilities associated with fine-tuning LLMs in 6G\nnetworks, in particular the membership inference attack. We define the\ncharacteristics of an attack network that can perform a membership inference\nattack if the attacker has access to the fine-tuned model for the downstream\ntask. We show that the membership inference attacks are effective for any\ndownstream task, which can lead to a personal data breach when using LLM as a\nservice. The experimental results show that the attack success rate of maximum\n92% can be achieved on named entity recognition task. Based on the experimental\nanalysis, we discuss possible defense mechanisms and present possible research\ndirections to make the LLMs more trustworthy in the context of 6G networks.", "categories": ["cs.CR", "cs.AI", "cs.DC"], "primary_category": "cs.CR", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00722"}
{"title": "An Empirical Analysis of Compute-Optimal Inference for Problem-Solving\n  with Language Models", "authors": [], "abstract": "The optimal training configurations of large language models (LLMs) with\nrespect to model sizes and compute budgets have been extensively studied. But\nhow to optimally configure LLMs during inference has not been explored in\nsufficient depth. We study compute-optimal inference: designing models and\ninference strategies that optimally trade off additional inference-time compute\nfor improved performance. As a first step towards understanding and designing\ncompute-optimal inference methods, we assessed the effectiveness and\ncomputational efficiency of multiple inference strategies such as Greedy\nSearch, Majority Voting, Best-of-N, Weighted Voting, and their variants on two\ndifferent Tree Search algorithms, involving different model sizes and\ncomputational budgets. We found that a smaller language model with a novel tree\nsearch algorithm typically achieves a Pareto-optimal trade-off. These results\nhighlight the potential benefits of deploying smaller models equipped with more\nsophisticated decoding algorithms in budget-constrained scenarios, e.g., on\nend-devices, to enhance problem-solving accuracy. For instance, we show that\nthe Llemma-7B model can achieve competitive accuracy to a Llemma-34B model on\nMATH500 while using $2\\times$ less FLOPs. Our findings could potentially apply\nto any generation task with a well-defined measure of success.", "categories": ["cs.AI"], "primary_category": "cs.AI", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00724"}
{"title": "Improving Retrieval-Augmented Generation in Medicine with Iterative\n  Follow-up Questions", "authors": [], "abstract": "The emergent abilities of large language models (LLMs) have demonstrated\ngreat potential in solving medical questions. They can possess considerable\nmedical knowledge, but may still hallucinate and are inflexible in the\nknowledge updates. While Retrieval-Augmented Generation (RAG) has been proposed\nto enhance the medical question-answering capabilities of LLMs with external\nknowledge bases, it may still fail in complex cases where multiple rounds of\ninformation-seeking are required. To address such an issue, we propose\niterative RAG for medicine (i-MedRAG), where LLMs can iteratively ask follow-up\nqueries based on previous information-seeking attempts. In each iteration of\ni-MedRAG, the follow-up queries will be answered by a vanilla RAG system and\nthey will be further used to guide the query generation in the next iteration.\nOur experiments show the improved performance of various LLMs brought by\ni-MedRAG compared with vanilla RAG on complex questions from clinical vignettes\nin the United States Medical Licensing Examination (USMLE), as well as various\nknowledge tests in the Massive Multitask Language Understanding (MMLU) dataset.\nNotably, our zero-shot i-MedRAG outperforms all existing prompt engineering and\nfine-tuning methods on GPT-3.5, achieving an accuracy of 69.68\\% on the MedQA\ndataset. In addition, we characterize the scaling properties of i-MedRAG with\ndifferent iterations of follow-up queries and different numbers of queries per\niteration. Our case studies show that i-MedRAG can flexibly ask follow-up\nqueries to form reasoning chains, providing an in-depth analysis of medical\nquestions. To the best of our knowledge, this is the first-of-its-kind study on\nincorporating follow-up queries into medical RAG.", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00727"}
{"title": "TurboEdit: Text-Based Image Editing Using Few-Step Diffusion Models", "authors": [], "abstract": "Diffusion models have opened the path to a wide range of text-based image\nediting frameworks. However, these typically build on the multi-step nature of\nthe diffusion backwards process, and adapting them to distilled, fast-sampling\nmethods has proven surprisingly challenging. Here, we focus on a popular line\nof text-based editing frameworks - the ``edit-friendly'' DDPM-noise inversion\napproach. We analyze its application to fast sampling methods and categorize\nits failures into two classes: the appearance of visual artifacts, and\ninsufficient editing strength. We trace the artifacts to mismatched noise\nstatistics between inverted noises and the expected noise schedule, and suggest\na shifted noise schedule which corrects for this offset. To increase editing\nstrength, we propose a pseudo-guidance approach that efficiently increases the\nmagnitude of edits without introducing new artifacts. All in all, our method\nenables text-based image editing with as few as three diffusion steps, while\nproviding novel insights into the mechanisms behind popular text-based editing\napproaches.", "categories": ["cs.CV", "cs.GR"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00735"}
{"title": "Virchow 2: Scaling Self-Supervised Mixed Magnification Models in\n  Pathology", "authors": [], "abstract": "Foundation models are rapidly being developed for computational pathology\napplications. However, it remains an open question which factors are most\nimportant for downstream performance with data scale and diversity, model size,\nand training algorithm all playing a role. In this work, we present the result\nof scaling both data and model size, surpassing previous studies in both\ndimensions, and introduce two new models: Virchow 2, a 632M parameter vision\ntransformer, and Virchow 2G, a 1.85B parameter vision transformer, each trained\nwith 3.1M histopathology whole slide images. To support this scale, we propose\ndomain-inspired adaptations to the DINOv2 training algorithm, which is quickly\nbecoming the default method in self-supervised learning for computational\npathology. We achieve state of the art performance on twelve tile-level tasks,\nas compared to the top performing competing models. Our results suggest that\ndata diversity and domain-specific training can outperform models that only\nscale in the number of parameters, but, on average, performance benefits from\ndomain-tailoring, data scale, and model scale.", "categories": ["cs.CV"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00738"}
{"title": "DynamoLLM: Designing LLM Inference Clusters for Performance and Energy\n  Efficiency", "authors": [], "abstract": "The rapid evolution and widespread adoption of generative large language\nmodels (LLMs) have made them a pivotal workload in various applications. Today,\nLLM inference clusters receive a large number of queries with strict Service\nLevel Objectives (SLOs). To achieve the desired performance, these models\nexecute on power-hungry GPUs causing the inference clusters to consume large\namount of energy and, consequently, result in excessive carbon emissions.\nFortunately, we find that there is a great opportunity to exploit the\nheterogeneity in inference compute properties and fluctuations in inference\nworkloads, to significantly improve energy-efficiency. However, such a diverse\nand dynamic environment creates a large search-space where different system\nconfigurations (e.g., number of instances, model parallelism, and GPU\nfrequency) translate into different energy-performance trade-offs. To address\nthese challenges, we propose DynamoLLM, the first energy-management framework\nfor LLM inference environments. DynamoLLM automatically and dynamically\nreconfigures the inference cluster to optimize for energy and cost of LLM\nserving under the service's performance SLOs. We show that at a service-level,\nDynamoLLM conserves 53% energy and 38% operational carbon emissions, and\nreduces 61% cost to the customer, while meeting the latency SLOs.", "categories": ["cs.AI", "cs.AR", "cs.DC"], "primary_category": "cs.AI", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00741"}
{"title": "Collaborative Vision-Text Representation Optimizing for Open-Vocabulary\n  Segmentation", "authors": [], "abstract": "Pre-trained vision-language models, e.g. CLIP, have been increasingly used to\naddress the challenging Open-Vocabulary Segmentation (OVS) task, benefiting\nfrom their well-aligned vision-text embedding space. Typical solutions involve\neither freezing CLIP during training to unilaterally maintain its zero-shot\ncapability, or fine-tuning CLIP vision encoder to achieve perceptual\nsensitivity to local regions. However, few of them incorporate vision-text\ncollaborative optimization. Based on this, we propose the Content-Dependent\nTransfer to adaptively enhance each text embedding by interacting with the\ninput image, which presents a parameter-efficient way to optimize the text\nrepresentation. Besides, we additionally introduce a Representation\nCompensation strategy, reviewing the original CLIP-V representation as\ncompensation to maintain the zero-shot capability of CLIP. In this way, the\nvision and text representation of CLIP are optimized collaboratively, enhancing\nthe alignment of the vision-text feature space. To the best of our knowledge,\nwe are the first to establish the collaborative vision-text optimizing\nmechanism within the OVS field. Extensive experiments demonstrate our method\nachieves superior performance on popular OVS benchmarks. In open-vocabulary\nsemantic segmentation, our method outperforms the previous state-of-the-art\napproaches by +0.5, +2.3, +3.4, +0.4 and +1.1 mIoU, respectively on A-847,\nA-150, PC-459, PC-59 and PAS-20. Furthermore, in a panoptic setting on ADE20K,\nwe achieve the performance of 27.1 PQ, 73.5 SQ, and 32.9 RQ. Code will be\navailable at https://github.com/jiaosiyu1999/MAFT-Plus.git .", "categories": ["cs.CV"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00744"}
{"title": "Leaf Angle Estimation using Mask R-CNN and LETR Vision Transformer", "authors": [], "abstract": "Modern day studies show a high degree of correlation between high yielding\ncrop varieties and plants with upright leaf angles. It is observed that plants\nwith upright leaf angles intercept more light than those without upright leaf\nangles, leading to a higher rate of photosynthesis. Plant scientists and\nbreeders benefit from tools that can directly measure plant parameters in the\nfield i.e. on-site phenotyping. The estimation of leaf angles by manual means\nin a field setting is tedious and cumbersome. We mitigate the tedium using a\ncombination of the Mask R-CNN instance segmentation neural network, and Line\nSegment Transformer (LETR), a vision transformer. The proposed Computer Vision\n(CV) pipeline is applied on two image datasets, Summer 2015-Ames ULA and Summer\n2015- Ames MLA, with a combined total of 1,827 plant images collected in the\nfield using FieldBook, an Android application aimed at on-site phenotyping. The\nleaf angles estimated by the proposed pipeline on the image datasets are\ncompared to two independent manual measurements using ImageJ, a Java-based\nimage processing program developed at the National Institutes of Health and the\nLaboratory for Optical and Computational Instrumentation. The results, when\ncompared for similarity using the Cosine Similarity measure, exhibit 0.98\nsimilarity scores on both independent measurements of Summer 2015-Ames ULA and\nSummer 2015-Ames MLA image datasets, demonstrating the feasibility of the\nproposed pipeline for on-site measurement of leaf angles.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00749"}
{"title": "A Policy-Gradient Approach to Solving Imperfect-Information Games with\n  Iterate Convergence", "authors": [], "abstract": "Policy gradient methods have become a staple of any single-agent\nreinforcement learning toolbox, due to their combination of desirable\nproperties: iterate convergence, efficient use of stochastic trajectory\nfeedback, and theoretically-sound avoidance of importance sampling corrections.\nIn multi-agent imperfect-information settings (extensive-form games), however,\nit is still unknown whether the same desiderata can be guaranteed while\nretaining theoretical guarantees. Instead, sound methods for extensive-form\ngames rely on approximating counterfactual values (as opposed to Q values),\nwhich are incompatible with policy gradient methodologies. In this paper, we\ninvestigate whether policy gradient can be safely used in two-player zero-sum\nimperfect-information extensive-form games (EFGs). We establish positive\nresults, showing for the first time that a policy gradient method leads to\nprovable best-iterate convergence to a regularized Nash equilibrium in\nself-play.", "categories": ["cs.GT", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.GT", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00751"}
{"title": "A deep learning-enabled smart garment for versatile sleep behaviour\n  monitoring", "authors": [], "abstract": "Continuous monitoring and accurate detection of complex sleep patterns\nassociated to different sleep-related conditions is essential, not only for\nenhancing sleep quality but also for preventing the risk of developing chronic\nillnesses associated to unhealthy sleep. Despite significant advances in\nresearch, achieving versatile recognition of various unhealthy and sub-healthy\nsleep patterns with simple wearable devices at home remains a significant\nchallenge. Here, we report a robust and durable ultrasensitive strain sensor\narray printed on a smart garment, in its collar region. This solution allows\ndetecting subtle vibrations associated with multiple sleep patterns at the\nextrinsic laryngeal muscles. Equipped with a deep learning neural network, it\ncan precisely identify six sleep states-nasal breathing, mouth breathing,\nsnoring, bruxism, central sleep apnea (CSA), and obstructive sleep apnea\n(OSA)-with an impressive accuracy of 98.6%, all without requiring specific\npositioning. We further demonstrate its explainability and generalization\ncapabilities in practical applications. Explainable artificial intelligence\n(XAI) visualizations reflect comprehensive signal pattern analysis with low\nbias. Transfer learning tests show that the system can achieve high accuracy\n(overall accuracy of 95%) on new users with very few-shot learning (less than\n15 samples per class). The scalable manufacturing process, robustness, high\naccuracy, and excellent generalization of the smart garment make it a promising\ntool for next-generation continuous sleep monitoring.", "categories": ["eess.SP", "cs.AI"], "primary_category": "eess.SP", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00753"}
{"title": "Coarse Correspondence Elicit 3D Spacetime Understanding in Multimodal\n  Language Model", "authors": [], "abstract": "Multimodal language models (MLLMs) are increasingly being implemented in\nreal-world environments, necessitating their ability to interpret 3D spaces and\ncomprehend temporal dynamics. Despite their potential, current top models\nwithin our community still fall short in adequately understanding spatial and\ntemporal dimensions. We introduce Coarse Correspondence, a simple,\ntraining-free, effective, and general-purpose visual prompting method to elicit\n3D and temporal understanding in multimodal LLMs. Our method uses a lightweight\ntracking model to find object correspondences between frames in a video or\nbetween sets of image viewpoints. It selects the most frequent object instances\nand visualizes them with markers with unique IDs in the image. With this simple\napproach, we achieve state-of-the-art results on 3D understanding benchmarks\nincluding ScanQA (+20.5\\%) and a subset of OpenEQA (+9.7\\%), and on long-form\nvideo benchmarks such as EgoSchema (+6.0\\%). We also curate a small diagnostic\ndataset to evaluate whether MLLMs can reason about space from a described\nviewpoint other than the camera viewpoint. Again, Coarse Correspondence\nimproves spatial perspective-taking abilities but we highlight that MLLMs\nstruggle with this task. Together, we demonstrate that our simple prompting\nmethod can significantly aid downstream tasks that require 3D or temporal\nreasoning.", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00754"}
{"title": "Text-Guided Video Masked Autoencoder", "authors": [], "abstract": "Recent video masked autoencoder (MAE) works have designed improved masking\nalgorithms focused on saliency. These works leverage visual cues such as motion\nto mask the most salient regions. However, the robustness of such visual cues\ndepends on how often input videos match underlying assumptions. On the other\nhand, natural language description is an information dense representation of\nvideo that implicitly captures saliency without requiring modality-specific\nassumptions, and has not been explored yet for video MAE. To this end, we\nintroduce a novel text-guided masking algorithm (TGM) that masks the video\nregions with highest correspondence to paired captions. Without leveraging any\nexplicit visual cues for saliency, our TGM is competitive with state-of-the-art\nmasking algorithms such as motion-guided masking. To further benefit from the\nsemantics of natural language for masked reconstruction, we next introduce a\nunified framework for joint MAE and masked video-text contrastive learning. We\nshow that across existing masking algorithms, unifying MAE and masked\nvideo-text contrastive learning improves downstream performance compared to\npure MAE on a variety of video recognition tasks, especially for linear probe.\nWithin this unified framework, our TGM achieves the best relative performance\non five action recognition and one egocentric datasets, highlighting the\ncomplementary nature of natural language for masked video modeling.", "categories": ["cs.CV"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00759"}
{"title": "Smoothed Energy Guidance: Guiding Diffusion Models with Reduced Energy\n  Curvature of Attention", "authors": [], "abstract": "Conditional diffusion models have shown remarkable success in visual content\ngeneration, producing high-quality samples across various domains, largely due\nto classifier-free guidance (CFG). Recent attempts to extend guidance to\nunconditional models have relied on heuristic techniques, resulting in\nsuboptimal generation quality and unintended effects. In this work, we propose\nSmoothed Energy Guidance (SEG), a novel training- and condition-free approach\nthat leverages the energy-based perspective of the self-attention mechanism to\nenhance image generation. By defining the energy of self-attention, we\nintroduce a method to reduce the curvature of the energy landscape of attention\nand use the output as the unconditional prediction. Practically, we control the\ncurvature of the energy landscape by adjusting the Gaussian kernel parameter\nwhile keeping the guidance scale parameter fixed. Additionally, we present a\nquery blurring method that is equivalent to blurring the entire attention\nweights without incurring quadratic complexity in the number of tokens. In our\nexperiments, SEG achieves a Pareto improvement in both quality and the\nreduction of side effects. The code is available at\n\\url{https://github.com/SusungHong/SEG-SDXL}.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00760"}
{"title": "UniTalker: Scaling up Audio-Driven 3D Facial Animation through A Unified\n  Model", "authors": [], "abstract": "Audio-driven 3D facial animation aims to map input audio to realistic facial\nmotion. Despite significant progress, limitations arise from inconsistent 3D\nannotations, restricting previous models to training on specific annotations\nand thereby constraining the training scale. In this work, we present\nUniTalker, a unified model featuring a multi-head architecture designed to\neffectively leverage datasets with varied annotations. To enhance training\nstability and ensure consistency among multi-head outputs, we employ three\ntraining strategies, namely, PCA, model warm-up, and pivot identity embedding.\nTo expand the training scale and diversity, we assemble A2F-Bench, comprising\nfive publicly available datasets and three newly curated datasets. These\ndatasets contain a wide range of audio domains, covering multilingual speech\nvoices and songs, thereby scaling the training data from commonly employed\ndatasets, typically less than 1 hour, to 18.5 hours. With a single trained\nUniTalker model, we achieve substantial lip vertex error reductions of 9.2% for\nBIWI dataset and 13.7% for Vocaset. Additionally, the pre-trained UniTalker\nexhibits promise as the foundation model for audio-driven facial animation\ntasks. Fine-tuning the pre-trained UniTalker on seen datasets further enhances\nperformance on each dataset, with an average error reduction of 6.3% on\nA2F-Bench. Moreover, fine-tuning UniTalker on an unseen dataset with only half\nthe data surpasses prior state-of-the-art models trained on the full dataset.\nThe code and dataset are available at the project page\nhttps://github.com/X-niper/UniTalker.", "categories": ["cs.CV"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00762"}
{"title": "AgentGen: Enhancing Planning Abilities for Large Language Model based\n  Agent via Environment and Task Generation", "authors": [], "abstract": "Large Language Model (LLM) based agents have garnered significant attention\nand are becoming increasingly popular. Furthermore, planning ability is a\ncrucial component of an LLM-based agent, involving interaction with the\nenvironment and executing actions to complete a planning task, which generally\nentails achieving a desired goal from an initial state. This paper investigates\nenhancing the planning abilities of LLMs through instruction tuning, referred\nto as agent training. Recent studies have demonstrated that utilizing\nexpert-level trajectory for instruction-tuning LLMs effectively enhances their\nplanning capabilities. However, existing work primarily focuses on synthesizing\ntrajectories from manually designed planning tasks and environments. The\nlabor-intensive nature of creating these environments and tasks impedes the\ngeneration of sufficiently varied and extensive trajectories. To address this\nlimitation, this paper explores the automated synthesis of diverse environments\nand a gradual range of planning tasks, from easy to difficult. We introduce a\nframework, AgentGen, that leverages LLMs first to generate environments and\nsubsequently generate planning tasks conditioned on these environments.\nSpecifically, to improve environmental diversity, we propose using an\ninspiration corpus composed of various domain-specific text segments as the\ncontext for synthesizing environments. Moreover, to increase the difficulty\ndiversity of generated planning tasks, we propose a bidirectional evolution\nmethod, Bi-Evol, that evolves planning tasks from easier and harder directions\nto synthesize a task set with a smoother difficulty curve. The evaluation\nresults derived from AgentBoard show that AgentGen greatly improves LLMs'\nplanning ability, e.g., the AgentGen instruction-tuned Llama-3 8B surpasses\nGPT-3.5 in overall performance. Moreover, in certain tasks, it even outperforms\nGPT-4.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00764"}
{"title": "MM-Vet v2: A Challenging Benchmark to Evaluate Large Multimodal Models\n  for Integrated Capabilities", "authors": [], "abstract": "MM-Vet, with open-ended vision-language questions targeting at evaluating\nintegrated capabilities, has become one of the most popular benchmarks for\nlarge multimodal model evaluation. MM-Vet assesses six core vision-language\n(VL) capabilities: recognition, knowledge, spatial awareness, language\ngeneration, OCR, and math. However, its question format is restricted to single\nimage-text pairs, lacking the interleaved image and text sequences prevalent in\nreal-world scenarios. To address this limitation, we introduce MM-Vet v2, which\nincludes a new VL capability called \"image-text sequence understanding\",\nevaluating models' ability to process VL sequences. Furthermore, we maintain\nthe high quality of evaluation samples while further expanding the evaluation\nset size. Using MM-Vet v2 to benchmark large multimodal models, we found that\nClaude 3.5 Sonnet is the best model with a score of 71.8, slightly\noutperforming GPT-4o which scored 71.0. Among open-weight models,\nInternVL2-Llama3-76B leads with a score of 68.4.", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00765"}
{"title": "Optimizing Diffusion Models for Joint Trajectory Prediction and\n  Controllable Generation", "authors": [], "abstract": "Diffusion models are promising for joint trajectory prediction and\ncontrollable generation in autonomous driving, but they face challenges of\ninefficient inference steps and high computational demands. To tackle these\nchallenges, we introduce Optimal Gaussian Diffusion (OGD) and Estimated Clean\nManifold (ECM) Guidance. OGD optimizes the prior distribution for a small\ndiffusion time $T$ and starts the reverse diffusion process from it. ECM\ndirectly injects guidance gradients to the estimated clean manifold,\neliminating extensive gradient backpropagation throughout the network. Our\nmethodology streamlines the generative process, enabling practical applications\nwith reduced computational overhead. Experimental validation on the large-scale\nArgoverse 2 dataset demonstrates our approach's superior performance, offering\na viable solution for computationally efficient, high-quality joint trajectory\nprediction and controllable generation for autonomous driving. Our project\nwebpage is at https://yixiaowang7.github.io/OptTrajDiff_Page/.", "categories": ["cs.CV"], "primary_category": "cs.CV", "created": "2024-08-01", "doi": null, "arxiv_id": "2408.00766"}