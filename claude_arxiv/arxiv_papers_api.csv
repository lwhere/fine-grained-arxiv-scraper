title,authors,abstract,categories,created
Endogenous Coalition Formation in Policy Debates,,"Political actors form coalitions around their joint normative beliefs in
order to influence the policy process on contentious issues such as climate
change or population ageing. Policy process theory maintains that learning
within and across coalitions is a central predictor of coalition formation and
policy change but has yet to explain how policy learning works. The present
article explains the formation and maintenance of coalitions by focusing on the
ways actors adopt policy beliefs from other actors in policy debates. A policy
debate is a complex social system in which temporal network dependence guides
how actors contribute ideological statements to the debate. Belief adoption
matters in three complementary ways: bonding, which exploits cues within
coalitions; bridging, which explores new beliefs outside one's perimeter in the
debate; and repulsion, which reinforces polarization between coalitions and
cements their belief systems. We formalize this theory of endogenous coalition
formation in policy debates and test it on a micro-level empirical dataset
using statistical network analysis and event history analysis.",cs.SI stat.AP,2019-03-25
"Interpretable Feature Learning in Multivariate Big Data Analysis for
  Network Monitoring",,"There is an increasing interest in the development of new data-driven models
useful to assess the performance of communication networks. For many
applications, like network monitoring and troubleshooting, a data model is of
little use if it cannot be interpreted by a human operator. In this paper, we
present an extension of the Multivariate Big Data Analysis (MBDA) methodology,
a recently proposed interpretable data analysis tool. In this extension, we
propose a solution to the automatic derivation of features, a cornerstone step
for the application of MBDA when the amount of data is massive. The resulting
network monitoring approach allows us to detect and diagnose disparate network
anomalies, with a data-analysis workflow that combines the advantages of
interpretable and interactive models with the power of parallel processing. We
apply the extended MBDA to two case studies: UGR'16, a benchmark flow-based
real-traffic dataset for anomaly detection, and Dartmouth'18, the longest and
largest Wi-Fi trace known to date.",cs.NI cs.LG stat.ML,2019-07-05
"A Simulative Study on Active Disturbance Rejection Control (ADRC) as a
  Control Tool for Practitioners",,"As an alternative to both classical PID-type and modern model-based
approaches to solving control problems, active disturbance rejection control
(ADRC) has gained significant traction in recent years. With its simple tuning
method and robustness against process parameter variations, it puts itself
forward as a valuable addition to the toolbox of control engineering
practitioners. This article aims at providing a single-source introduction and
reference to linear ADRC with this audience in mind. A simulative study is
carried out using generic first- and second-order plants to enable a quick
visual assessment of the abilities of ADRC. Finally, a modified form of the
discrete-time case is introduced to speed up real-time implementations as
necessary in applications with high dynamic requirements.",eess.SY cs.SY,2019-08-13
"Computer-assisted proofs for Lyapunov stability via Sums of Squares
  certificates and Constructive Analysis",,"We provide a computer-assisted approach to ensure that a given continuous or
discrete-time polynomial system is (asymptotically) stable. Our framework
relies on constructive analysis together with formally certified sums of
squares Lyapunov functions. The crucial steps are formalized within of the
proof assistant Minlog. We illustrate our approach with various examples issued
from the control system literature.",math.OC cs.LO,2020-06-17
Generic Analysis of Model Product Lines via Constraint Lifting,,"Engineering a product-line is more than just describing a product-line: to be
correct, every variant that can be generated must satisfy some constraints. To
ensure that all such variants will be correct (e.g. well-typed) there are only
two ways: either to check the variants of interest individually or to come up
with a complex product-line analysis algorithm, specific to every constraint.
In this paper, we address a generalization of this problem: we propose a
mechanism that allows to check whether a constraint holds simultaneously for
all variants which might be generated. The main contribution of this paper is a
function that assumes constraints that shall be fulfilled by all variants and
generates (""lifts"") out of them constraints for the product-line. These lifted
constraints can then be checked directly on a model product-line, thus
simultaneously be verified for all variants. The lifting is formulated in a
very general manner, which allows to make use of generic algorithms like SMT
solving or theorem proving in a modular way. We show how to verify lifted
constraints using SMT solving by automatically translating model product-lines
and constraints. The applicability of the approach is demonstrated with an
industrial case study, in which we apply our lifting to a domain specific
modelling language for manufacturing planning. Finally, a runtime analysis
shows scalability by analyzing different model product-lines with production
planning data from the BMW Group and Miele.",cs.SE,2020-08-26
"A Double Auction for Charging Scheduling among Vehicles Using
  DAG-Blockchains",,"Electric Vehicles (EVs) are becoming more and more popular in our daily life,
which replaces traditional fuel vehicles to reduce carbon emissions and protect
the environment. EVs need to be charged, but the number of charging piles in a
Charging Station (CS) is limited and charging is usually more time-consuming
than fueling. According to this scenario, we propose a secure and efficient
charging scheduling system based on a Directed Acyclic Graph (DAG)-blockchain
and double auction mechanism. In a smart area, it attempts to assign EVs to the
available CSs in the light of their submitted charging requests and status
information. First, we design a lightweight charging scheduling framework that
integrates DAG-blockchain and modern cryptography technology to ensure security
and scalability during performing scheduling and completing tradings. In this
process, a constrained multi-item double auction problem is formulated because
of the limited charging resources in a CS, which motivates EVs and CSs in this
area to participate in the market based on their preferences and statuses. Due
to this constraint, our problem is more complicated and harder to achieve
truthfulness as well as system efficiency compared to the existing double
auction model. To adapt to it, we propose two algorithms, namely Truthful
Mechanism for Charging (TMC) and Efficient Mechanism for Charging (EMC), to
determine an assignment between EVs and CSs and pricing strategies. Then, both
theoretical analysis and numerical simulations show the correctness and
effectiveness of our proposed algorithms.",cs.NI cs.GT,2020-10-03
"Transfer Function Analysis and Implementation of Active Disturbance
  Rejection Control",,"To support the adoption of active disturbance rejection control (ADRC) in
industrial practice, this article aims at improving both understanding and
implementation of ADRC using traditional means, in particular via transfer
functions and a frequency-domain view. Firstly, to enable an immediate
comparability with existing classical control solutions, a realizable transfer
function implementation of continous-time linear ADRC is introduced. Secondly,
a frequency-domain analysis of ADRC components, performance, parameter
sensitivity, and tuning method is performed. Finally, an exact implementation
of discrete-time ADRC using transfer functions is introduced for the first
time, with special emphasis on practical aspects such as computational
efficiency, low parameter footprint, and windup protection.",eess.SY cs.SY,2020-11-02
K-Deep Simplex: Deep Manifold Learning via Local Dictionaries,,"We propose K-Deep Simplex(KDS) which, given a set of data points, learns a
dictionary comprising synthetic landmarks, along with representation
coefficients supported on a simplex. KDS employs a local weighted $\ell_1$
penalty that encourages each data point to represent itself as a convex
combination of nearby landmarks. We solve the proposed optimization program
using alternating minimization and design an efficient, interpretable
autoencoder using algorithm unrolling. We theoretically analyze the proposed
program by relating the weighted $\ell_1$ penalty in KDS to a weighted $\ell_0$
program. Assuming that the data are generated from a Delaunay triangulation, we
prove the equivalence of the weighted $\ell_1$ and weighted $\ell_0$ programs.
We further show the stability of the representation coefficients under mild
geometrical assumptions. If the representation coefficients are fixed, we prove
that the sub-problem of minimizing over the dictionary yields a unique
solution. Further, we show that low-dimensional representations can be
efficiently obtained from the covariance of the coefficient matrix. Experiments
show that the algorithm is highly efficient and performs competitively on
synthetic and real data sets.",cs.LG cs.IT eess.SP math.IT math.OC,2020-12-03
"Nonparametric Bayesian inference for reversible multi-dimensional
  diffusions",,"We study nonparametric Bayesian models for reversible multi-dimensional
diffusions with periodic drift. For continuous observation paths, reversibility
is exploited to prove a general posterior contraction rate theorem for the
drift gradient vector field under approximation-theoretic conditions on the
induced prior for the invariant measure. The general theorem is applied to
Gaussian priors and $p$-exponential priors, which are shown to converge to the
truth at the minimax optimal rate over Sobolev smoothness classes in any
dimension.",math.ST cs.NA math.NA math.PR stat.TH,2020-12-22
Finite Automata Encoding Piecewise Polynomials,,"Finite automata are used to encode geometric figures, functions and can be
used for image compression and processing. The original approach is to
represent each point of a figure in $\mathbb{R}^n$ as a convolution of its $n$
coordinates written in some base. Then a figure is said to be encoded as a
finite automaton if the set of convolutions corresponding to the points in this
figure is accepted by a finite automaton. The only differentiable functions
which can be encoded as a finite automaton in this way are linear. In this
paper we propose a representation which enables to encode piecewise polynomial
functions with arbitrary degrees of smoothness that substantially extends a
family of functions which can be encoded as finite automata. Such
representation naturally comes from the framework of hierarchical tensor
product B-splines, which are piecewise polynomials widely utilized in numerical
computational geometry. We show that finite automata provide a suitable tool
for solving computational problems arising in this framework when the support
of a function is unbounded.",cs.CG cs.FL,2021-04-06
Understanding Prediction Discrepancies in Machine Learning Classifiers,,"A multitude of classifiers can be trained on the same data to achieve similar
performances during test time, while having learned significantly different
classification patterns. This phenomenon, which we call prediction
discrepancies, is often associated with the blind selection of one model
instead of another with similar performances. When making a choice, the machine
learning practitioner has no understanding on the differences between models,
their limits, where they agree and where they don't. But his/her choice will
result in concrete consequences for instances to be classified in the
discrepancy zone, since the final decision will be based on the selected
classification pattern. Besides the arbitrary nature of the result, a bad
choice could have further negative consequences such as loss of opportunity or
lack of fairness. This paper proposes to address this question by analyzing the
prediction discrepancies in a pool of best-performing models trained on the
same data. A model-agnostic algorithm, DIG, is proposed to capture and explain
discrepancies locally, to enable the practitioner to make the best educated
decision when selecting a model by anticipating its potential undesired
consequences. All the code to reproduce the experiments is available.",cs.LG cs.AI,2021-04-12
"A Generalized Projected Bellman Error for Off-policy Value Estimation in
  Reinforcement Learning",,"Many reinforcement learning algorithms rely on value estimation, however, the
most widely used algorithms -- namely temporal difference algorithms -- can
diverge under both off-policy sampling and nonlinear function approximation.
Many algorithms have been developed for off-policy value estimation based on
the linear mean squared projected Bellman error (MSPBE) and are sound under
linear function approximation. Extending these methods to the nonlinear case
has been largely unsuccessful. Recently, several methods have been introduced
that approximate a different objective -- the mean-squared Bellman error (MSBE)
-- which naturally facilitate nonlinear approximation. In this work, we build
on these insights and introduce a new generalized MSPBE that extends the linear
MSPBE to the nonlinear setting. We show how this generalized objective unifies
previous work and obtain new bounds for the value error of the solutions of the
generalized objective. We derive an easy-to-use, but sound, algorithm to
minimize the generalized objective, and show that it is more stable across
runs, is less sensitive to hyperparameters, and performs favorably across four
control domains with neural network function approximation.",cs.LG cs.AI,2021-04-28
Generalized Kernel Thinning,,"The kernel thinning (KT) algorithm of Dwivedi and Mackey (2021) compresses a
probability distribution more effectively than independent sampling by
targeting a reproducing kernel Hilbert space (RKHS) and leveraging a less
smooth square-root kernel. Here we provide four improvements. First, we show
that KT applied directly to the target RKHS yields tighter, dimension-free
guarantees for any kernel, any distribution, and any fixed function in the
RKHS. Second, we show that, for analytic kernels like Gaussian, inverse
multiquadric, and sinc, target KT admits maximum mean discrepancy (MMD)
guarantees comparable to or better than those of square-root KT without making
explicit use of a square-root kernel. Third, we prove that KT with a fractional
power kernel yields better-than-Monte-Carlo MMD guarantees for non-smooth
kernels, like Laplace and Mat\'ern, that do not have square-roots. Fourth, we
establish that KT applied to a sum of the target and power kernels (a procedure
we call KT+) simultaneously inherits the improved MMD guarantees of power KT
and the tighter individual function guarantees of target KT. In our experiments
with target KT and KT+, we witness significant improvements in integration
error even in $100$ dimensions and when compressing challenging differential
equation posteriors.",stat.ML cs.LG math.ST stat.ME stat.TH,2021-10-04
Log-concave poset inequalities,,"We study combinatorial inequalities for various classes of set systems:
matroids, polymatroids, poset antimatroids, and interval greedoids. We prove
log-concavity inequalities for counting certain weighted feasible words, which
generalize and extend several previous results establishing Mason conjectures
for the numbers of independent sets of matroids. Notably, we prove matching
equality conditions for both earlier inequalities and our extensions.
  In contrast with much of the previous work, our proofs are combinatorial and
employ nothing but linear algebra. We use the language formulation of greedoids
which allows a linear algebraic setup, which in turn can be analyzed
recursively. The underlying non-commutative nature of matrices associated with
greedoids allows us to proceed beyond polymatroids and prove the equality
conditions. As further application of our tools, we rederive both Stanley's
inequality on the number of certain linear extensions, and its equality
conditions, which we then also extend to the weighted case.",math.CO cs.DM,2021-10-20
"U-Net-based Lung Thickness Map for Pixel-level Lung Volume Estimation of
  Chest X-rays",,"Purpose: We aimed to estimate the total lung volume (TLV) from real and
synthetic frontal X-ray radiographs on a pixel level using lung thickness maps
generated by a U-Net.
  Methods: 5,959 thorax X-ray computed tomography (CT) scans were retrieved
from two publicly available datasets of the lung nodule analysis 2016 (n=656)
and the RSNA pulmonary embolism detection challenge 2020 (n=5,303).
Additionally, thorax CT scans from 72 subjects (33 healthy: 20 men, mean age
[range] = 62.4 [34, 80]; 39 suffering from chronic obstructive pulmonary
disease: 25 men, mean age [range] = 69.0 [47, 91]) were retrospectively
selected (10.2018-12.2019) from our in-house dataset such that for each
subject, a frontal chest X-ray radiograph no older than seven days was
available. All CT scans and their corresponding lung segmentation were forward
projected using a simulated X-ray spectrum to generate synthetic radiographs
and lung thickness maps, respectively. A U-Net model was trained and tested on
synthetic radiographs from the public datasets to predict lung thickness maps
and consequently estimate TLV. Model performance was further assessed by
evaluating the TLV estimations for the in-house synthetic and real radiograph
pairs using Pearson correlation coefficient (r) and significance testing.
  Results: Strong correlations were measured between the predicted and
CT-derived ground truth TLV values for test data from synthetic
($n_{Public}$=1,191, r=0.987, P < 0.001; $n_{In-house}$=72, r=0.973, P < 0.001)
and real radiographs (n=72, r=0.908, P < 0.001).
  Conclusion: TLV from U-Net-generated pixel-level lung thickness maps were
successfully estimated for synthetic and real radiographs.",eess.IV cs.AI cs.CV,2021-10-24
Neyman-Pearson Multi-class Classification via Cost-sensitive Learning,,"Most existing classification methods aim to minimize the overall
misclassification error rate. However, in applications such as loan default
prediction, different types of errors can have varying consequences. To address
this asymmetry issue, two popular paradigms have been developed: the
Neyman-Pearson (NP) paradigm and the cost-sensitive (CS) paradigm. Previous
studies on the NP paradigm have primarily focused on the binary case, while the
multi-class NP problem poses a greater challenge due to its unknown
feasibility. In this work, we tackle the multi-class NP problem by establishing
a connection with the CS problem via strong duality and propose two algorithms.
We extend the concept of NP oracle inequalities, crucial in binary
classifications, to NP oracle properties in the multi-class context. Our
algorithms satisfy these NP oracle properties under certain conditions.
Furthermore, we develop practical algorithms to assess the feasibility and
strong duality in multi-class NP problems, which can offer practitioners the
landscape of a multi-class NP problem with various target error levels.
Simulations and real data studies validate the effectiveness of our algorithms.
To our knowledge, this is the first study to address the multi-class NP problem
with theoretical guarantees. The proposed algorithms have been implemented in
the R package \texttt{npcs}, which is available on CRAN.",stat.ML cs.LG stat.ME,2021-11-08
Reputation Gaming in Stack Overflow,,"Stack Overflow incentive system awards users with reputation scores to ensure
quality. The decentralized nature of the forum may make the incentive system
prone to manipulation. This paper offers, for the first time, a comprehensive
study of the reported types of reputation manipulation scenarios that might be
exercised in Stack Overflow and the prevalence of such reputation gamers by a
qualitative study of 1,697 posts from meta Stack Exchange sites. We found four
different types of reputation fraud scenarios, such as voting rings where
communities form to upvote each other repeatedly on similar posts. We developed
algorithms that enable platform managers to automatically identify these
suspicious reputation gaming scenarios for review. The first algorithm
identifies isolated/semi-isolated communities where probable reputation frauds
may occur mostly by collaborating with each other. The second algorithm looks
for sudden unusual big jumps in the reputation scores of users. We evaluated
the performance of our algorithms by examining the reputation history dashboard
of Stack Overflow users from the Stack Overflow website. We observed that
around 60-80% of users flagged as suspicious by our algorithms experienced
reductions in their reputation scores by Stack Overflow.",cs.SE,2021-11-13
"From Unsupervised to Few-shot Graph Anomaly Detection: A Multi-scale
  Contrastive Learning Approach",,"Anomaly detection from graph data is an important data mining task in many
applications such as social networks, finance, and e-commerce. Existing efforts
in graph anomaly detection typically only consider the information in a single
scale (view), thus inevitably limiting their capability in capturing anomalous
patterns in complex graph data. To address this limitation, we propose a novel
framework, graph ANomaly dEtection framework with Multi-scale cONtrastive
lEarning (ANEMONE in short). By using a graph neural network as a backbone to
encode the information from multiple graph scales (views), we learn better
representation for nodes in a graph. In maximizing the agreements between
instances at both the patch and context levels concurrently, we estimate the
anomaly score of each node with a statistical anomaly estimator according to
the degree of agreement from multiple perspectives. To further exploit a
handful of ground-truth anomalies (few-shot anomalies) that may be collected in
real-life applications, we further propose an extended algorithm, ANEMONE-FS,
to integrate valuable information in our method. We conduct extensive
experiments under purely unsupervised settings and few-shot anomaly detection
settings, and we demonstrate that the proposed method ANEMONE and its variant
ANEMONE-FS consistently outperform state-of-the-art algorithms on six benchmark
datasets.",cs.LG,2022-02-11
Firefighting with a Distance-Based Restriction,,"In the classic version of the game of firefighter, on the first turn a fire
breaks out on a vertex in a graph $G$ and then $k$ firefighters protect $k$
vertices. On each subsequent turn, the fire spreads to the collective unburnt
neighbourhood of all the burning vertices and the firefighters again protect
$k$ vertices. Once a vertex has been burnt or protected it remains that way for
the rest of the game. A common objective with respect to some infinite graph
$G$ is to determine how many firefighters are necessary to stop the fire from
spreading after a finite number of turns, commonly referred to as containing
the fire. We introduce the concept of distance-restricted firefighting where
the firefighters' movement is restricted so they can only move up to some fixed
distance $d$ per turn rather than being able to move without restriction. We
establish some general properties of this new game in contrast to properties of
the original game, and we investigate specific cases of the distance-restricted
game on the infinite square, strong, and hexagonal grids. We conjecture that
two firefighters are insufficient on the square grid when $d = 2$, and we pose
some questions about how many firefighters are required in general when $d =
1$.",math.CO cs.DM,2022-04-04
Isoperimetric Inequalities Made Simpler,,"We give an alternative, simple method to prove isoperimetric inequalities
over the hypercube. In particular, we show:
  1. An elementary proof of classical isoperimetric inequalities of Talagrand,
as well as a stronger isoperimetric result conjectured by Talagrand and
recently proved by Eldan and Gross.
  2. A strengthening of the Friedgut junta theorem, asserting that if the
$p$-moment of the sensitivity of a function is constant for some $1/2 +
\varepsilon\leq p\leq 1$, then the function is close to a junta. In this
language, Friedgut's theorem is the special case that $p=1$.",math.CO cs.DM,2022-04-13
What You See is What You Classify: Black Box Attributions,,"An important step towards explaining deep image classifiers lies in the
identification of image regions that contribute to individual class scores in
the model's output. However, doing this accurately is a difficult task due to
the black-box nature of such networks. Most existing approaches find such
attributions either using activations and gradients or by repeatedly perturbing
the input. We instead address this challenge by training a second deep network,
the Explainer, to predict attributions for a pre-trained black-box classifier,
the Explanandum. These attributions are provided in the form of masks that only
show the classifier-relevant parts of an image, masking out the rest. Our
approach produces sharper and more boundary-precise masks when compared to the
saliency maps generated by other methods. Moreover, unlike most existing
approaches, ours is capable of directly generating very distinct class-specific
masks in a single forward pass. This makes the proposed method very efficient
during inference. We show that our attributions are superior to established
methods both visually and quantitatively with respect to the PASCAL VOC-2007
and Microsoft COCO-2014 datasets.",cs.CV cs.AI cs.LG,2022-05-23
The virtual element method on polygonal pixel-based tessellations,,"We analyze and validate the virtual element method combined with a boundary
correction similar to the one in [1,2], to solve problems on two dimensional
domains with curved boundaries approximated by polygonal domains. We focus on
the case of approximating domains obtained as the union of squared elements out
of a uniform structured mesh, such as the one that naturally arises when the
domain is issued from an image. We show, both theoretically and numerically,
that resorting to polygonal elements allows the assumptions required for
stability to be satisfied for any polynomial order. This allows us to fully
exploit the potential of higher order methods. Efficiency is ensured by a novel
static condensation strategy acting on the edges of the decomposition.",math.NA cs.NA,2022-06-07
"Parallel Compositing of Volumetric Depth Images for Interactive
  Visualization of Distributed Volumes at High Frame Rates",,"We present a parallel compositing algorithm for Volumetric Depth Images
(VDIs) of large three-dimensional volume data. Large distributed volume data
are routinely produced in both numerical simulations and experiments, yet it
remains challenging to visualize them at smooth, interactive frame rates. VDIs
are view-dependent piecewise constant representations of volume data that offer
a potential solution. They are more compact and less expensive to render than
the original data. So far, however, there is no method for generating VDIs from
distributed data. We propose an algorithm that enables this by sort-last
parallel generation and compositing of VDIs with automatically chosen
content-adaptive parameters. The resulting composited VDI can then be streamed
for remote display, providing responsive visualization of large, distributed
volume data.",cs.GR,2022-06-29
One-Time Certificates for Reliable and Secure Document Signing,,"Electronic documents are signed using private keys and verified using the
corresponding digital certificates through the well-known public key
infrastructure model. Private keys must be kept in a safe container so they can
be reused. This makes private key management a critical component of public key
infrastructures with no failproof answer. Therefore, existing solutions must
employ cumbersome and often expensive revocation methods to handle private key
compromises. We propose a new cryptographic key management model built with
long-term, irrevocable digital certificates, each bound to a single document.
Our model issues a unique digital certificate for each new document to be
signed. We demonstrate that private keys associated with these certificates
should be deleted after each signature, eliminating the need to store those
keys. Furthermore, we show that these certificates do not require any
revocation mechanism to be trusted. We analyze the overhead caused by the
frequent generation of new key pairs for each document, provide a security
overview and show the advantages over the traditional model.",cs.CR,2022-08-08
Generalization in Neural Networks: A Broad Survey,,"This paper reviews concepts, modeling approaches, and recent findings along a
spectrum of different levels of abstraction of neural network models including
generalization across (1) Samples, (2) Distributions, (3) Domains, (4) Tasks,
(5) Modalities, and (6) Scopes. Strategies for (1) sample generalization from
training to test data are discussed, with suggestive evidence presented that,
at least for the ImageNet dataset, popular classification models show
substantial overfitting. An empirical example and perspectives from statistics
highlight how models' (2) distribution generalization can benefit from
consideration of causal relationships and counterfactual scenarios. Transfer
learning approaches and results for (3) domain generalization are summarized,
as is the wealth of domain generalization benchmark datasets available. Recent
breakthroughs surveyed in (4) task generalization include few-shot
meta-learning approaches and the emergence of transformer-based foundation
models such as those used for language processing. Studies performing (5)
modality generalization are reviewed, including those that integrate image and
text data and that apply a biologically-inspired network across olfactory,
visual, and auditory modalities. Higher-level (6) scope generalization results
are surveyed, including graph-based approaches to represent symbolic knowledge
in networks and attribution strategies for improving networks' explainability.
Additionally, concepts from neuroscience are discussed on the modular
architecture of brains and the steps by which dopamine-driven conditioning
leads to abstract thinking.",cs.LG cs.AI,2022-09-04
A learning theory for quantum photonic processors and beyond,,"We consider the tasks of learning quantum states, measurements and channels
generated by continuous-variable (CV) quantum circuits. This family of circuits
is suited to describe optical quantum technologies and in particular it
includes state-of-the-art photonic processors capable of showing quantum
advantage. We define classes of functions that map classical variables, encoded
into the CV circuit parameters, to outcome probabilities evaluated on those
circuits. We then establish efficient learnability guarantees for such classes,
by computing bounds on their pseudo-dimension or covering numbers, showing that
CV quantum circuits can be learned with a sample complexity that scales
polynomially with the circuit's size, i.e., the number of modes. Our results
show that CV circuits can be trained efficiently using a number of training
samples that, unlike their finite-dimensional counterpart, does not scale with
the circuit depth.",quant-ph cs.CC cs.IT cs.LG math-ph math.IT math.MP,2022-09-07
"SleePyCo: Automatic Sleep Scoring with Feature Pyramid and Contrastive
  Learning",,"Automatic sleep scoring is essential for the diagnosis and treatment of sleep
disorders and enables longitudinal sleep tracking in home environments.
Conventionally, learning-based automatic sleep scoring on single-channel
electroencephalogram (EEG) is actively studied because obtaining multi-channel
signals during sleep is difficult. However, learning representation from raw
EEG signals is challenging owing to the following issues: 1) sleep-related EEG
patterns occur on different temporal and frequency scales and 2) sleep stages
share similar EEG patterns. To address these issues, we propose a deep learning
framework named SleePyCo that incorporates 1) a feature pyramid and 2)
supervised contrastive learning for automatic sleep scoring. For the feature
pyramid, we propose a backbone network named SleePyCo-backbone to consider
multiple feature sequences on different temporal and frequency scales.
Supervised contrastive learning allows the network to extract class
discriminative features by minimizing the distance between intra-class features
and simultaneously maximizing that between inter-class features. Comparative
analyses on four public datasets demonstrate that SleePyCo consistently
outperforms existing frameworks based on single-channel EEG. Extensive ablation
experiments show that SleePyCo exhibits enhanced overall performance, with
significant improvements in discrimination between the N1 and rapid eye
movement (REM) stages.",cs.LG cs.AI eess.SP,2022-09-20
"Enhancing convolutional neural network generalizability via low-rank
  weight approximation",,"Noise is ubiquitous during image acquisition. Sufficient denoising is often
an important first step for image processing. In recent decades, deep neural
networks (DNNs) have been widely used for image denoising. Most DNN-based image
denoising methods require a large-scale dataset or focus on supervised
settings, in which single/pairs of clean images or a set of noisy images are
required. This poses a significant burden on the image acquisition process.
Moreover, denoisers trained on datasets of limited scale may incur
over-fitting. To mitigate these issues, we introduce a new self-supervised
framework for image denoising based on the Tucker low-rank tensor
approximation. With the proposed design, we are able to characterize our
denoiser with fewer parameters and train it based on a single image, which
considerably improves the model's generalizability and reduces the cost of data
acquisition. Extensive experiments on both synthetic and real-world noisy
images have been conducted. Empirical results show that our proposed method
outperforms existing non-learning-based methods (e.g., low-pass filter,
non-local mean), single-image unsupervised denoisers (e.g., DIP, NN+BM3D)
evaluated on both in-sample and out-sample datasets. The proposed method even
achieves comparable performances with some supervised methods (e.g., DnCNN).",cs.CV cs.LG stat.AP stat.ML,2022-09-26
KRF: Keypoint Refinement with Fusion Network for 6D Pose Estimation,,"Some robust point cloud registration approaches with controllable pose
refinement magnitude, such as ICP and its variants, are commonly used to
improve 6D pose estimation accuracy. However, the effectiveness of these
methods gradually diminishes with the advancement of deep learning techniques
and the enhancement of initial pose accuracy, primarily due to their lack of
specific design for pose refinement. In this paper, we propose Point Cloud
Completion and Keypoint Refinement with Fusion Data (PCKRF), a new pose
refinement pipeline for 6D pose estimation. The pipeline consists of two steps.
First, it completes the input point clouds via a novel pose-sensitive point
completion network. The network uses both local and global features with pose
information during point completion. Then, it registers the completed object
point cloud with the corresponding target point cloud by our proposed Color
supported Iterative KeyPoint (CIKP) method. The CIKP method introduces color
information into registration and registers a point cloud around each keypoint
to increase stability. The PCKRF pipeline can be integrated with existing
popular 6D pose estimation methods, such as the full flow bidirectional fusion
network, to further improve their pose estimation accuracy. Experiments
demonstrate that our method exhibits superior stability compared to existing
approaches when optimizing initial poses with relatively high precision.
Notably, the results indicate that our method effectively complements most
existing pose estimation techniques, leading to improved performance in most
cases. Furthermore, our method achieves promising results even in challenging
scenarios involving textureless and symmetrical objects. Our source code is
available at https://github.com/zhanhz/KRF.",cs.CV cs.RO,2022-10-07
"DPIS: An Enhanced Mechanism for Differentially Private SGD with
  Importance Sampling",,"Nowadays, differential privacy (DP) has become a well-accepted standard for
privacy protection, and deep neural networks (DNN) have been immensely
successful in machine learning. The combination of these two techniques, i.e.,
deep learning with differential privacy, promises the privacy-preserving
release of high-utility models trained with sensitive data such as medical
records. A classic mechanism for this purpose is DP-SGD, which is a
differentially private version of the stochastic gradient descent (SGD)
optimizer commonly used for DNN training. Subsequent approaches have improved
various aspects of the model training process, including noise decay schedule,
model architecture, feature engineering, and hyperparameter tuning. However,
the core mechanism for enforcing DP in the SGD optimizer remains unchanged ever
since the original DP-SGD algorithm, which has increasingly become a
fundamental barrier limiting the performance of DP-compliant machine learning
solutions.
  Motivated by this, we propose DPIS, a novel mechanism for differentially
private SGD training that can be used as a drop-in replacement of the core
optimizer of DP-SGD, with consistent and significant accuracy gains over the
latter. The main idea is to employ importance sampling (IS) in each SGD
iteration for mini-batch selection, which reduces both sampling variance and
the amount of random noise injected to the gradients that is required to
satisfy DP. Integrating IS into the complex mathematical machinery of DP-SGD is
highly non-trivial. DPIS addresses the challenge through novel mechanism
designs, fine-grained privacy analysis, efficiency enhancements, and an
adaptive gradient clipping optimization. Extensive experiments on four
benchmark datasets, namely MNIST, FMNIST, CIFAR-10 and IMDb, demonstrate the
superior effectiveness of DPIS over existing solutions for deep learning with
differential privacy.",cs.CR cs.LG,2022-10-18
RepGhost: A Hardware-Efficient Ghost Module via Re-parameterization,,"Feature reuse has been a key technique in light-weight convolutional neural
networks (CNNs) architecture design. Current methods usually utilize a
concatenation operator to keep large channel numbers cheaply (thus large
network capacity) by reusing feature maps from other layers. Although
concatenation is parameters- and FLOPs-free, its computational cost on hardware
devices is non-negligible. To address this, this paper provides a new
perspective to realize feature reuse implicitly and more efficiently instead of
concatenation. A novel hardware-efficient RepGhost module is proposed for
implicit feature reuse via reparameterization, instead of using concatenation
operator. Based on the RepGhost module, we develop our efficient RepGhost
bottleneck and RepGhostNet. Experiments on ImageNet and COCO benchmarks
demonstrate that our RepGhostNet is much more effective and efficient than
GhostNet and MobileNetV3 on mobile devices. Specially, our RepGhostNet
surpasses GhostNet 0.5x by 2.5% Top-1 accuracy on ImageNet dataset with less
parameters and comparable latency on an ARM-based mobile device. Code and model
weights are available at https://github.com/ChengpengChen/RepGhost.",cs.CV,2022-11-11
"Contextual Bandits with Packing and Covering Constraints: A Modular
  Lagrangian Approach via Regression",,"We consider contextual bandits with linear constraints (CBwLC), a variant of
contextual bandits in which the algorithm consumes multiple resources subject
to linear constraints on total consumption. This problem generalizes contextual
bandits with knapsacks (CBwK), allowing for packing and covering constraints,
as well as positive and negative resource consumption. We provide the first
algorithm for CBwLC (or CBwK) that is based on regression oracles. The
algorithm is simple, computationally efficient, and statistically optimal under
mild assumptions. Further, we provide the first vanishing-regret guarantees for
CBwLC (or CBwK) that extend beyond the stochastic environment. We side-step
strong impossibility results from prior work by identifying a weaker (and,
arguably, fairer) benchmark to compare against. Our algorithm builds on
LagrangeBwK (Immorlica et al., FOCS 2019), a Lagrangian-based technique for
CBwK, and SquareCB (Foster and Rakhlin, ICML 2020), a regression-based
technique for contextual bandits. Our analysis leverages the inherent
modularity of both techniques.",cs.LG stat.ML,2022-11-14
Implicit frictional dynamics with soft constraints,,"Dynamics simulation with frictional contacts is important for a wide range of
applications, from cloth simulation to object manipulation. Recent methods
using smoothed lagged friction forces have enabled robust and differentiable
simulation of elastodynamics with friction. However, the resulting frictional
behavior can be inaccurate and may not converge to analytic solutions. Here we
evaluate the accuracy of lagged friction models in comparison with implicit
frictional contact systems. We show that major inaccuracies near the stick-slip
threshold in such systems are caused by lagging of friction forces rather than
by smoothing the Coulomb friction curve. Furthermore, we demonstrate how
systems involving implicit or lagged friction can be correctly used with
higher-order time integration and highlight limitations in earlier attempts. We
demonstrate how to exploit forward-mode automatic differentiation to simplify
and, in some cases, improve the performance of the inexact Newton method.
Finally, we show that other complex phenomena can also be simulated effectively
while maintaining smoothness of the entire system. We extend our method to
exhibit stick-slip frictional behavior and preserve volume on compressible and
nearly-incompressible media using soft constraints.",cs.GR,2022-11-19
Hidden Poison: Machine Unlearning Enables Camouflaged Poisoning Attacks,,"We introduce camouflaged data poisoning attacks, a new attack vector that
arises in the context of machine unlearning and other settings when model
retraining may be induced. An adversary first adds a few carefully crafted
points to the training dataset such that the impact on the model's predictions
is minimal. The adversary subsequently triggers a request to remove a subset of
the introduced points at which point the attack is unleashed and the model's
predictions are negatively affected. In particular, we consider clean-label
targeted attacks (in which the goal is to cause the model to misclassify a
specific test point) on datasets including CIFAR-10, Imagenette, and Imagewoof.
This attack is realized by constructing camouflage datapoints that mask the
effect of a poisoned dataset.",cs.LG cs.AI cs.CR cs.CY,2022-12-20
"Posterior-Variance-Based Error Quantification for Inverse Problems in
  Imaging",,"In this work, a method for obtaining pixel-wise error bounds in Bayesian
regularization of inverse imaging problems is introduced. The proposed method
employs estimates of the posterior variance together with techniques from
conformal prediction in order to obtain coverage guarantees for the error
bounds, without making any assumption on the underlying data distribution. It
is generally applicable to Bayesian regularization approaches, independent,
e.g., of the concrete choice of the prior. Furthermore, the coverage guarantees
can also be obtained in case only approximate sampling from the posterior is
possible. With this in particular, the proposed framework is able to
incorporate any learned prior in a black-box manner. Guaranteed coverage
without assumptions on the underlying distributions is only achievable since
the magnitude of the error bounds is, in general, unknown in advance.
Nevertheless, experiments with multiple regularization approaches presented in
the paper confirm that in practice, the obtained error bounds are rather tight.
For realizing the numerical experiments, also a novel primal-dual Langevin
algorithm for sampling from non-smooth distributions is introduced in this
work.",cs.CV math.PR,2022-12-23
"Exact Fractional Inference via Re-Parametrization & Interpolation
  between Tree-Re-Weighted- and Belief Propagation- Algorithms",,"The computational complexity of inference -- required to compute the
partition function, $Z$, of an Ising model over a graph of $N$''spins"" -- is
most likely exponential in $N$. Efficient variational methods, such as Belief
Propagation (BP) and Tree Re-Weighted (TRW) algorithms, compute $Z$
approximately by minimizing the respective (BP- or TRW-) free energy. We
generalize the variational scheme by building a $\lambda$-fractional
interpolation, $Z^{(\lambda)}$, where $\lambda=0$ and $\lambda=1$ correspond to
TRW- and BP-approximations, respectively. This fractional scheme -- coined
Fractional Belief Propagation (FBP) -- guarantees that in the attractive
(ferromagnetic) case $Z^{(TRW)} \geq Z^{(\lambda)} \geq Z^{(BP)}$, and there
exists a unique (``exact"") $\lambda_*$ such that $Z=Z^{(\lambda_*)}$.
Generalizing the re-parametrization approach of
\citep{wainwright_tree-based_2002} and the loop series approach of
\citep{chertkov_loop_2006}, we show how to express $Z$ as a product, $\forall
\lambda:\ Z=Z^{(\lambda)}{\tilde Z}^{(\lambda)}$, where the multiplicative
correction, ${\tilde Z}^{(\lambda)}$, is an expectation over a node-independent
probability distribution built from node-wise fractional marginals. Our
theoretical analysis is complemented by extensive experiments with models from
Ising ensembles over planar and random graphs of medium- and large-sizes. The
empirical study yields a number of interesting observations, such as the
ability to estimate ${\tilde Z}^{(\lambda)}$ with $O(N^{2::4})$ fractional
samples and suppression of $\lambda_*$ fluctuations with an increase in $N$ for
instances from a particular random Ising ensemble. We also verify and discuss
the applicability of this approach to the problem of image de-noising.",cs.LG cond-mat.stat-mech,2023-01-24
"NU-AIR -- A Neuromorphic Urban Aerial Dataset for Detection and
  Localization of Pedestrians and Vehicles",,"This paper presents an open-source aerial neuromorphic dataset that captures
pedestrians and vehicles moving in an urban environment. The dataset, titled
NU-AIR, features 70.75 minutes of event footage acquired with a 640 x 480
resolution neuromorphic sensor mounted on a quadrotor operating in an urban
environment. Crowds of pedestrians, different types of vehicles, and street
scenes featuring busy urban environments are captured at different elevations
and illumination conditions. Manual bounding box annotations of vehicles and
pedestrians contained in the recordings are provided at a frequency of 30 Hz,
yielding 93,204 labels in total. Evaluation of the dataset's fidelity is
performed through comprehensive ablation study for three Spiking Neural
Networks (SNNs) and training ten Deep Neural Networks (DNNs) to validate the
quality and reliability of both the dataset and corresponding annotations. All
data and Python code to voxelize the data and subsequently train SNNs/DNNs has
been open-sourced.",cs.CV,2023-02-18
A Finite Sample Complexity Bound for Distributionally Robust Q-learning,,"We consider a reinforcement learning setting in which the deployment
environment is different from the training environment. Applying a robust
Markov decision processes formulation, we extend the distributionally robust
$Q$-learning framework studied in Liu et al. [2022]. Further, we improve the
design and analysis of their multi-level Monte Carlo estimator. Assuming access
to a simulator, we prove that the worst-case expected sample complexity of our
algorithm to learn the optimal robust $Q$-function within an $\epsilon$ error
in the sup norm is upper bounded by $\tilde
O(|S||A|(1-\gamma)^{-5}\epsilon^{-2}p_{\wedge}^{-6}\delta^{-4})$, where
$\gamma$ is the discount rate, $p_{\wedge}$ is the non-zero minimal support
probability of the transition kernels and $\delta$ is the uncertainty size.
This is the first sample complexity result for the model-free robust RL
problem. Simulation studies further validate our theoretical results.",cs.LG stat.ML,2023-02-25
Centroid-centered Modeling for Efficient Vision Transformer Pre-training,,"Masked Image Modeling (MIM) is a new self-supervised vision pre-training
paradigm using a Vision Transformer (ViT). Previous works can be pixel-based or
token-based, using original pixels or discrete visual tokens from parametric
tokenizer models, respectively. Our proposed centroid-based approach, CCViT,
leverages k-means clustering to obtain centroids for image modeling without
supervised training of the tokenizer model, which only takes seconds to create.
This non-parametric centroid tokenizer only takes seconds to create and is
faster for token inference. The centroids can represent both patch pixels and
index tokens with the property of local invariance. Specifically, we adopt
patch masking and centroid replacing strategies to construct corrupted inputs,
and two stacked encoder blocks to predict corrupted patch tokens and
reconstruct original patch pixels. Experiments show that our CCViT achieves
84.4% top-1 accuracy on ImageNet-1K classification with ViT-B and 86.0% with
ViT-L. We also transfer our pre-trained model to other downstream tasks. Our
approach achieves competitive results with recent baselines without external
supervision and distillation training from other models.",cs.CV,2023-03-08
A dual basis approach to multidimensional scaling,,"Classical multidimensional scaling (CMDS) is a technique that embeds a set of
objects in a Euclidean space given their pairwise Euclidean distances. The main
part of CMDS involves double centering a squared distance matrix and using a
truncated eigendecomposition to recover the point coordinates. In this paper,
motivated by a study in Euclidean distance geometry, we explore a dual basis
approach to CMDS. We give an explicit formula for the dual basis vectors and
fully characterize the spectrum of an essential matrix in the dual basis
framework. We make connections to a related problem in metric nearness.",math.SP cs.IT cs.LG math.IT,2023-03-09
"Full State Estimation of Continuum Robots from Tip Velocities: A
  Cosserat-Theoretic Boundary Observer",,"State estimation of robotic systems is essential to implementing feedback
controllers, which usually provide better robustness to modeling uncertainties
than open-loop controllers. However, state estimation of soft robots is very
challenging because soft robots have theoretically infinite degrees of freedom
while existing sensors only provide a limited number of discrete measurements.
This work focuses on soft robotic manipulators, also known as continuum robots.
We design an observer algorithm based on the well-known Cosserat rod theory,
which models continuum robots by nonlinear partial differential equations
(PDEs) evolving in geometric Lie groups. The observer can estimate all
infinite-dimensional continuum robot states, including poses, strains, and
velocities, by only sensing the tip velocity of the continuum robot, and hence
it is called a ``boundary'' observer. More importantly, the estimation error
dynamics is formally proven to be locally input-to-state stable. The key idea
is to inject sequential tip velocity measurements into the observer in a way
that dissipates the energy of the estimation errors through the boundary. The
distinct advantage of this PDE-based design is that it can be implemented using
any existing numerical implementation for Cosserat rod models. All theoretical
convergence guarantees will be preserved, regardless of the discretization
method. We call this property ``one design for any discretization''. Extensive
numerical studies are included and suggest that the domain of attraction is
large and the observer is robust to uncertainties of tip velocity measurements
and model parameters.",cs.RO cs.SY eess.SY,2023-03-10
"An unconditionally stable space-time isogeometric method for the
  acoustic wave equation",,"We study space--time isogeometric discretizations of the linear acoustic wave
equation that use splines of arbitrary degree p, both in space and time. We
propose a space--time variational formulation that is obtained by adding a
non-consistent penalty term of order 2p+2 to the bilinear form coming from
integration by parts. This formulation, when discretized with tensor-product
spline spaces with maximal regularity in time, is unconditionally stable: the
mesh size in time is not constrained by the mesh size in space. We give
extensive numerical evidence for the good stability, approximation, dissipation
and dispersion properties of the stabilized isogeometric formulation, comparing
against stabilized finite element schemes, for a range of wave propagation
problems with constant and variable wave speed.",math.NA cs.NA,2023-03-13
"Predicting the Geolocation of Tweets Using transformer models on
  Customized Data",,"This research is aimed to solve the tweet/user geolocation prediction task
and provide a flexible methodology for the geotagging of textual big data. The
suggested approach implements neural networks for natural language processing
(NLP) to estimate the location as coordinate pairs (longitude, latitude) and
two-dimensional Gaussian Mixture Models (GMMs). The scope of proposed models
has been finetuned on a Twitter dataset using pretrained Bidirectional Encoder
Representations from Transformers (BERT) as base models. Performance metrics
show a median error of fewer than 30 km on a worldwide-level, and fewer than 15
km on the US-level datasets for the models trained and evaluated on text
features of tweets' content and metadata context. Our source code and data are
available at https://github.com/K4TEL/geo-twitter.git",cs.CL cs.AI,2023-03-14
"Ultra-High-Resolution Detector Simulation with Intra-Event Aware GAN and
  Self-Supervised Relational Reasoning",,"Simulating high-resolution detector responses is a computationally intensive
process that has long been challenging in Particle Physics. Despite the ability
of generative models to streamline it, full ultra-high-granularity detector
simulation still proves to be difficult as it contains correlated and
fine-grained information. To overcome these limitations, we propose Intra-Event
Aware Generative Adversarial Network (IEA-GAN). IEA-GAN presents a Relational
Reasoning Module that approximates an event in detector simulation, generating
contextualized high-resolution full detector responses with a proper relational
inductive bias. IEA-GAN also introduces a Self-Supervised intra-event aware
loss and Uniformity loss, significantly enhancing sample fidelity and
diversity. We demonstrate IEA-GAN's application in generating sensor-dependent
images for the ultra-high-granularity Pixel Vertex Detector (PXD), with more
than 7.5 M information channels at the Belle II Experiment. Applications of
this work span from Foundation Models for high-granularity detector simulation,
such as at the HL-LHC (High Luminosity LHC), to simulation-based inference and
fine-grained density estimation. To our knowledge, IEA-GAN is the first
algorithm for faithful ultra-high-granularity full detector simulation with
event-based reasoning.",physics.ins-det cs.AI cs.CV hep-ph physics.data-an,2023-03-07
EVA-02: A Visual Representation for Neon Genesis,,"We launch EVA-02, a next-generation Transformer-based visual representation
pre-trained to reconstruct strong and robust language-aligned vision features
via masked image modeling. With an updated plain Transformer architecture as
well as extensive pre-training from an open & accessible giant CLIP vision
encoder, EVA-02 demonstrates superior performance compared to prior
state-of-the-art approaches across various representative vision tasks, while
utilizing significantly fewer parameters and compute budgets. Notably, using
exclusively publicly accessible training data, EVA-02 with only 304M parameters
achieves a phenomenal 90.0 fine-tuning top-1 accuracy on ImageNet-1K val set.
Additionally, our EVA-02-CLIP can reach up to 80.4 zero-shot top-1 on
ImageNet-1K, outperforming the previous largest & best open-sourced CLIP with
only ~1/6 parameters and ~1/6 image-text training data. We offer four EVA-02
variants in various model sizes, ranging from 6M to 304M parameters, all with
impressive performance. To facilitate open access and open research, we release
the complete suite of EVA-02 to the community at
https://github.com/baaivision/EVA/tree/master/EVA-02.",cs.CV cs.CL,2023-03-20
The Subspace Flatness Conjecture and Faster Integer Programming,,"In a seminal paper, Kannan and Lov\'asz (1988) considered a quantity
$\mu_{KL}(\Lambda,K)$ which denotes the best volume-based lower bound on the
covering radius $\mu(\Lambda,K)$ of a convex body $K$ with respect to a lattice
$\Lambda$. Kannan and Lov\'asz proved that $\mu(\Lambda,K) \leq n \cdot
\mu_{KL}(\Lambda,K)$ and the Subspace Flatness Conjecture by Dadush (2012)
claims a $O(\log(2n))$ factor suffices, which would match the lower bound from
the work of Kannan and Lov\'asz.
  We settle this conjecture up to a constant in the exponent by proving that
$\mu(\Lambda,K) \leq O(\log^{3}(2n)) \cdot \mu_{KL} (\Lambda,K)$. Our proof is
based on the Reverse Minkowski Theorem due to Regev and Stephens-Davidowitz
(2017). Following the work of Dadush (2012, 2019), we obtain a
$(\log(2n))^{O(n)}$-time randomized algorithm to solve integer programs in $n$
variables. Another implication of our main result is a near-optimal flatness
constant of $O(n \log^{3}(2n))$.",math.OC cs.CC cs.DM cs.DS math.CO,2023-03-25
Dual Cross-Attention for Medical Image Segmentation,,"We propose Dual Cross-Attention (DCA), a simple yet effective attention
module that is able to enhance skip-connections in U-Net-based architectures
for medical image segmentation. DCA addresses the semantic gap between encoder
and decoder features by sequentially capturing channel and spatial dependencies
across multi-scale encoder features. First, the Channel Cross-Attention (CCA)
extracts global channel-wise dependencies by utilizing cross-attention across
channel tokens of multi-scale encoder features. Then, the Spatial
Cross-Attention (SCA) module performs cross-attention to capture spatial
dependencies across spatial tokens. Finally, these fine-grained encoder
features are up-sampled and connected to their corresponding decoder parts to
form the skip-connection scheme. Our proposed DCA module can be integrated into
any encoder-decoder architecture with skip-connections such as U-Net and its
variants. We test our DCA module by integrating it into six U-Net-based
architectures such as U-Net, V-Net, R2Unet, ResUnet++, DoubleUnet and
MultiResUnet. Our DCA module shows Dice Score improvements up to 2.05% on GlaS,
2.74% on MoNuSeg, 1.37% on CVC-ClinicDB, 1.12% on Kvasir-Seg and 1.44% on
Synapse datasets. Our codes are available at:
https://github.com/gorkemcanates/Dual-Cross-Attention",cs.CV cs.LG eess.IV,2023-03-30
Determinantal Sieving,,"We introduce determinantal sieving, a new, remarkably powerful tool in the
toolbox of algebraic FPT algorithms. Given a polynomial $P(X)$ on a set of
variables $X=\{x_1,\ldots,x_n\}$ and a linear matroid $M=(X,\mathcal{I})$ of
rank $k$, both over a field $\mathbb{F}$ of characteristic 2, in $2^k$
evaluations we can sieve for those terms in the monomial expansion of $P$ which
are multilinear and whose support is a basis for $M$. Alternatively, using
$2^k$ evaluations of $P$ we can sieve for those monomials whose odd support
spans $M$. Applying this framework, we improve on a range of algebraic FPT
algorithms, such as:
  1. Solving $q$-Matroid Intersection in time $O^*(2^{(q-2)k})$ and $q$-Matroid
Parity in time $O^*(2^{qk})$, improving on $O^*(4^{qk})$ over general fields
(Brand and Pratt, ICALP 2021)
  2. $T$-Cycle, Colourful $(s,t)$-Path, Colourful $(S,T)$-Linkage in undirected
graphs, and the more general Rank $k$ $(S,T)$-Linkage problem, all in
$O^*(2^k)$ time, improving on $O^*(2^{k+|S|})$ and $O^*(2^{|S|+O(k^2
\log(k+|\mathbb{F}|))})$ respectively (Fomin et al., SODA 2023)
  3. Many instances of the Diverse X paradigm, finding a collection of $r$
solutions to a problem with a minimum mutual distance of $d$ in time
$O^*(2^{r(r-1)d/2})$, improving solutions for $k$-Distinct Branchings from time
$2^{O(k \log k)}$ to $O^*(2^k)$ (Bang-Jensen et al., ESA 2021), and for Diverse
Perfect Matchings from $O^*(2^{2^{O(rd)}})$ to $O^*(2^{r^2d/2})$ (Fomin et al.,
STACS 2021)
  Here, all matroids are assumed to be represented over fields of
characteristic 2. Over general fields, we achieve similar results at the cost
of using exponential space by working over the exterior algebra. For a class of
arithmetic circuits we call strongly monotone, this is even achieved without
any loss of running time. However, the odd support sieving result appears to be
specific to working over characteristic 2.",cs.DS,2023-04-04
"Untangling the Effects of Down-Sampling and Selection in Genetic
  Programming",,"Genetic programming systems often use large training sets to evaluate the
quality of candidate solutions for selection, which is often computationally
expensive. Down-sampling training sets has long been used to decrease the
computational cost of evaluation in a wide range of application domains. More
specifically, recent studies have shown that both random and informed
down-sampling can substantially improve problem-solving success for GP systems
that use the lexicase parent selection algorithm. We test whether these
down-sampling techniques can also improve problem-solving success in the
context of three other commonly used selection methods, fitness-proportionate,
tournament, implicit fitness sharing plus tournament selection, across six
program synthesis GP problems. We verified that down-sampling can significantly
improve the problem-solving success for all three of these other selection
schemes, demonstrating its general efficacy. We discern that the selection
pressure imposed by the selection scheme does not interact with the
down-sampling method. However, we find that informed down-sampling can improve
problem solving success significantly over random down-sampling when the
selection scheme has a mechanism for diversity maintenance like lexicase or
implicit fitness sharing. Overall, our results suggest that down-sampling
should be considered more often when solving test-based problems, regardless of
the selection scheme in use.",cs.NE,2023-04-14
"Application of Transformers for Nonlinear Channel Compensation in
  Optical Systems",,"In this paper, we introduce a new nonlinear optical channel equalizer based
on Transformers. By leveraging parallel computation and attending directly to
the memory across a sequence of symbols, we show that Transformers can be used
effectively for nonlinear compensation (NLC) in coherent long-haul transmission
systems. For this application, we present an implementation of the encoder part
of the Transformer and analyze its performance over a wide range of different
hyper-parameters. It is shown that by proper embeddings and processing blocks
of symbols at each iteration and also carefully selecting subsets of the
encoder's output to be processed together, an efficient nonlinear equalization
can be achieved for different complexity constraints. To reduce the
computational complexity of the attention mechanism, we further propose the use
of a physic-informed mask inspired by nonlinear perturbation theory. We also
compare the Transformer-NLC with digital back-propagation (DBP) under different
transmission scenarios in order to demonstrate the flexibility and
generalizability of the proposed data-driven solution.",cs.IT cs.LG eess.SP math.IT,2023-04-25
"Technical Note: Defining and Quantifying AND-OR Interactions for
  Faithful and Concise Explanation of DNNs",,"In this technical note, we aim to explain a deep neural network (DNN) by
quantifying the encoded interactions between input variables, which reflects
the DNN's inference logic. Specifically, we first rethink the definition of
interactions, and then formally define faithfulness and conciseness for
interaction-based explanation. To this end, we propose two kinds of
interactions, i.e., the AND interaction and the OR interaction. For
faithfulness, we prove the uniqueness of the AND (OR) interaction in
quantifying the effect of the AND (OR) relationship between input variables.
Besides, based on AND-OR interactions, we design techniques to boost the
conciseness of the explanation, while not hurting the faithfulness. In this
way, the inference logic of a DNN can be faithfully and concisely explained by
a set of symbolic concepts.",cs.LG cs.AI cs.CV,2023-04-26
"Martian time-series unraveled: A multi-scale nested approach with
  factorial variational autoencoders",,"Unsupervised source separation involves unraveling an unknown set of source
signals recorded through a mixing operator, with limited prior knowledge about
the sources, and only access to a dataset of signal mixtures. This problem is
inherently ill-posed and is further challenged by the variety of timescales
exhibited by sources in time series data from planetary space missions. As
such, a systematic multi-scale unsupervised approach is needed to identify and
separate sources at different timescales. Existing methods typically rely on a
preselected window size that determines their operating timescale, limiting
their capacity to handle multi-scale sources. To address this issue, we propose
an unsupervised multi-scale clustering and source separation framework by
leveraging wavelet scattering spectra that provide a low-dimensional
representation of stochastic processes, capable of distinguishing between
different non-Gaussian stochastic processes. Nested within this representation
space, we develop a factorial variational autoencoder that is trained to
probabilistically cluster sources at different timescales. To perform source
separation, we use samples from clusters at multiple timescales obtained via
the factorial variational autoencoder as prior information and formulate an
optimization problem in the wavelet scattering spectra representation space.
When applied to the entire seismic dataset recorded during the NASA InSight
mission on Mars, containing sources varying greatly in timescale, our approach
disentangles such different sources, e.g., minute-long transient one-sided
pulses (known as ""glitches"") and structured ambient noises resulting from
atmospheric activities that typically last for tens of minutes, and provides an
opportunity to conduct further investigations into the isolated sources.",cs.LG astro-ph.EP stat.ML,2023-05-25
Orbit recovery for band-limited functions,,"We study the third moment for functions on arbitrary compact Lie groups. We
use techniques of representation theory to generalize the notion of
band-limited functions in classical Fourier theory to functions on the compact
groups $SU(n), SO(n), Sp(n)$. We then prove that for generic band-limited
functions the third moment or, its Fourier equivalent, the bispectrum
determines the function up to translation by a single unitary matrix. Moreover,
if $G=SU(n)$ or $G=SO(2n+1)$ we prove that the third moment determines the
$G$-orbit of a band-limited function. As a corollary we obtain a large class of
finite-dimensional representations of these groups for which the third moment
determines the orbit of a generic vector. When $G=SO(3)$ this gives a result
relevant to cryo-EM which was our original motivation for studying this
problem.",cs.IT math.IT math.RT,2023-05-31
"BetaZero: Belief-State Planning for Long-Horizon POMDPs using Learned
  Approximations",,"Real-world planning problems, including autonomous driving and sustainable
energy applications like carbon storage and resource exploration, have recently
been modeled as partially observable Markov decision processes (POMDPs) and
solved using approximate methods. To solve high-dimensional POMDPs in practice,
state-of-the-art methods use online planning with problem-specific heuristics
to reduce planning horizons and make the problems tractable. Algorithms that
learn approximations to replace heuristics have recently found success in
large-scale fully observable domains. The key insight is the combination of
online Monte Carlo tree search with offline neural network approximations of
the optimal policy and value function. In this work, we bring this insight to
partially observable domains and propose BetaZero, a belief-state planning
algorithm for high-dimensional POMDPs. BetaZero learns offline approximations
that replace heuristics to enable online decision making in long-horizon
problems. We address several challenges inherent in large-scale partially
observable domains; namely challenges of transitioning in stochastic
environments, prioritizing action branching with a limited search budget, and
representing beliefs as input to the network. To formalize the use of all
limited search information, we train against a novel $Q$-weighted visit counts
policy. We test BetaZero on various well-established POMDP benchmarks found in
the literature and a real-world problem of critical mineral exploration.
Experiments show that BetaZero outperforms state-of-the-art POMDP solvers on a
variety of tasks.",cs.AI,2023-05-31
"Sketched and truncated polynomial Krylov methods: Evaluation of matrix
  functions",,"Among randomized numerical linear algebra strategies, so-called sketching
procedures are emerging as effective reduction means to accelerate the
computation of Krylov subspace methods for, e.g., the solution of linear
systems, eigenvalue computations, and the approximation of matrix functions.
While there is plenty of experimental evidence showing that sketched Krylov
solvers may dramatically improve performance over standard Krylov methods, many
features of these schemes are still unexplored. We derive a new sketched
Arnoldi-type relation that allows us to obtain several different new
theoretical results. These lead to an improvement of our understanding of
sketched Krylov methods, in particular by explaining why the frequently
occurring sketched Ritz values far outside the spectral region of A do not
negatively influence the convergence of sketched Krylov methods for f (A)b. Our
findings also help to identify, among several possible equivalent formulations,
the most suitable sketched approximations according to their numerical
stability properties. These results are also employed to analyze the error of
sketched Krylov methods in the approximation of the action of matrix functions,
significantly contributing to the theory available in the current literature.",math.NA cs.NA,2023-06-10
"Stochastic Approach for Modeling a Soft Robotic Finger with Creep
  Behavior",,"Soft robots have high adaptability and safeness which are derived from their
softness, and therefore it is paid attention to use them in human society.
However, the controllability of soft robots is not enough to perform dexterous
behaviors when considering soft robots as alternative laborers for humans. The
model-based control is effective to achieve dexterous behaviors. When
considering building a model which is suitable for control, there are problems
based on their special properties such as the creep behavior or the variability
of motion. In this paper, the lumped parameterized model with viscoelastic
joints for a soft finger is established for the creep behavior. Parameters are
expressed as distributions, which makes it possible to take into account the
variability of motion. Furthermore, stochastic analyses are performed based on
the parameters' distribution. They show high adaptivity compared with
experimental results and also enable the investigation of the effects of
parameters for robots' variability.",cs.RO,2023-06-12
"Naeural AI OS -- Decentralized ubiquitous computing MLOps execution
  engine",,"Over the past few years, ubiquitous, or pervasive computing has gained
popularity as the primary approach for a wide range of applications, including
enterprise-grade systems, consumer applications, and gaming systems. Ubiquitous
computing refers to the integration of computing technologies into everyday
objects and environments, creating a network of interconnected devices that can
communicate with each other and with humans. By using ubiquitous computing
technologies, communities can become more connected and efficient, with members
able to communicate and collaborate more easily. This enabled
interconnectedness and collaboration can lead to a more successful and
sustainable community. The spread of ubiquitous computing, however, has
emphasized the importance of automated learning and smart applications in
general. Even though there have been significant strides in Artificial
Intelligence and Deep Learning, large scale adoption has been hesitant due to
mounting pressure on expensive and highly complex cloud numerical-compute
infrastructures. Adopting, and even developing, practical machine learning
systems can come with prohibitive costs, not only in terms of complex
infrastructures but also of solid expertise in Data Science and Machine
Learning. In this paper we present an innovative approach for low-code
development and deployment of end-to-end AI cooperative application pipelines.
We address infrastructure allocation, costs, and secure job distribution in a
fully decentralized global cooperative community based on tokenized economics.",cs.AI cs.DC cs.NI,2023-06-14
RAPGen: An Approach for Fixing Code Inefficiencies in Zero-Shot,,"Performance bugs are non-functional bugs that can even manifest in
well-tested commercial products. Fixing these performance bugs is an important
yet challenging problem. In this work, we address this challenge and present a
new approach called Retrieval-Augmented Prompt Generation (RAPGen). Given a
code snippet with a performance issue, RAPGen first retrieves a prompt
instruction from a pre-constructed knowledge-base of previous performance bug
fixes and then generates a prompt using the retrieved instruction. It then uses
this prompt on a Large Language Model (such as Codex) in zero-shot to generate
a fix. We compare our approach with the various prompt variations and state of
the art methods in the task of performance bug fixing. Our evaluation shows
that RAPGen can generate performance improvement suggestions equivalent or
better than a developer in ~60% of the cases, getting ~42% of them verbatim, in
an expert-verified dataset of past performance changes made by C# developers.",cs.SE cs.AI,2023-06-29
One-time Pad Encryption Model for Non-local Correlations,,"We present a cryptographic-inspired framework for modeling Bell nonlocal
correlations. Drawing inspiration from the renowned De Broglie-Bohm theory, we
conceptualize nonlocal boxes as realistic systems featuring instantaneous
signaling at the hidden variable level. By introducing randomness into the
distribution of the hidden variable the superluminal signaling model is made
compatible with the operational no-signalling condition. As our design mimics
the famous symmetric key encryption system called {\it One-time Pad} (OTP), we
call this the OTP model for nonlocal boxes. We illustrate the efficacy of this
model through various esoteric examples related to the non-classical nature of
nonlocal boxes. In particular, the breakdown of communication complexity using
nonlocal boxes can be better understood in this framework. Additionally, we
delve into the Van Dam protocol, revealing its connection to homomorphic
encryption studied in cryptography. Exploring potential avenues for
encapsulating quantum-realizable nonlocal correlations within our framework, we
highlight that the Information Causality principle imposes additional
constraints at the hidden variable level. Present work thus orchestrates the
results in classical cryptography to improve our understanding of nonlocal
correlations and welcomes further research to this connection.",quant-ph cs.CR,2023-07-07
"Evaluate Fine-tuning Strategies for Fetal Head Ultrasound Image
  Segmentation with U-Net",,"Fetal head segmentation is a crucial step in measuring the fetal head
circumference (HC) during gestation, an important biometric in obstetrics for
monitoring fetal growth. However, manual biometry generation is time-consuming
and results in inconsistent accuracy. To address this issue, convolutional
neural network (CNN) models have been utilized to improve the efficiency of
medical biometry. But training a CNN network from scratch is a challenging
task, we proposed a Transfer Learning (TL) method. Our approach involves
fine-tuning (FT) a U-Net network with a lightweight MobileNet as the encoder to
perform segmentation on a set of fetal head ultrasound (US) images with limited
effort. This method addresses the challenges associated with training a CNN
network from scratch. It suggests that our proposed FT strategy yields
segmentation performance that is comparable when trained with a reduced number
of parameters by 85.8%. And our proposed FT strategy outperforms other
strategies with smaller trainable parameter sizes below 4.4 million. Thus, we
contend that it can serve as a dependable FT approach for reducing the size of
models in medical image analysis. Our key findings highlight the importance of
the balance between model performance and size in developing Artificial
Intelligence (AI) applications by TL methods. Code is available at
https://github.com/13204942/FT_Methods_for_Fetal_Head_Segmentation.",eess.IV cs.CV cs.LG,2023-07-18
Conformal prediction for frequency-severity modeling,,"We present a model-agnostic framework for the construction of prediction
intervals of insurance claims, with finite sample statistical guarantees,
extending the technique of split conformal prediction to the domain of
two-stage frequency-severity modeling. The framework effectiveness is showcased
with simulated and real datasets using classical parametric models and
contemporary machine learning methods. When the underlying severity model is a
random forest, we extend the two-stage split conformal prediction algorithm,
showing how the out-of-bag mechanism can be leveraged to eliminate the need for
a calibration set in the conformal procedure.",stat.ME cs.LG stat.ML,2023-07-24
"Static Posterior Inference of Bayesian Probabilistic Programming via
  Polynomial Solving",,"In Bayesian probabilistic programming, a central problem is to estimate the
normalised posterior distribution (NPD) of a probabilistic program with
conditioning via score (a.k.a. observe) statements. Most previous approaches
address this problem by Markov Chain Monte Carlo and variational inference, and
therefore could not generate guaranteed outcomes within a finite time limit.
Moreover, existing methods for exact inference either impose syntactic
restrictions or cannot guarantee successful inference in general.
  In this work, we propose a novel automated approach to derive guaranteed
bounds for NPD via polynomial solving. We first establish a fixed-point theorem
for the wide class of score-at-end Bayesian probabilistic programs that
terminate almost-surely and have a single bounded score statement at program
termination. Then, we propose a multiplicative variant of Optional Stopping
Theorem (OST) to address score-recursive Bayesian programs where score
statements with weights greater than one could appear inside a loop. Finally,
we use polynomial solving to implement our fixed-point theorem and OST variant.
To improve the accuracy of the polynomial solving, we further propose a
truncation operation and the synthesis of multiple bounds over various program
inputs. Our approach can handle Bayesian probabilistic programs with unbounded
while loops and continuous distributions with infinite supports. Experiments
over a wide range of benchmarks show that compared with the most relevant
approach (Beutner et al., PLDI 2022) for guaranteed NPD analysis via recursion
unrolling, our approach is more time efficient and derives comparable or even
tighter NPD bounds. Furthermore, our approach can handle score-recursive
programs which previous approaches could not.",cs.PL,2023-07-24
On Approximability of Satisfiable k-CSPs: IV,,"We prove a stability result for general $3$-wise correlations over
distributions satisfying mild connectivity properties. More concretely, we show
that if $\Sigma,\Gamma$ and $\Phi$ are alphabets of constant size, and $\mu$ is
a pairwise connected distribution over $\Sigma\times\Gamma\times\Phi$ with no
$(\mathbb{Z},+)$ embeddings in which the probability of each atom is
$\Omega(1)$, then the following holds. Any triplets of $1$-bounded functions
$f\colon \Sigma^n\to\mathbb{C}$, $g\colon \Gamma^n\to\mathbb{C}$, $h\colon
\Phi^n\to\mathbb{C}$ satisfying
  \[
  \left|\mathbb{E}_{(x,y,z)\sim \mu^{\otimes
n}}\big[f(x)g(y)h(z)\big]\right|\geq \varepsilon
  \]
  must arise from an Abelian group associated with the distribution $\mu$. More
specifically, we show that there is an Abelian group $(H,+)$ of constant size
such that for any such $f,g$ and $h$, the function $f$ (and similarly $g$ and
$h$) is correlated with a function of the form $\tilde{f}(x) =
\chi(\sigma(x_1),\ldots,\sigma(x_n)) L (x)$, where $\sigma\colon \Sigma \to H$
is some map, $\chi\in \hat{H}^{\otimes n}$ is a character, and $L\colon
\Sigma^n\to\mathbb{C}$ is a low-degree function with bounded $2$-norm.
  En route we prove a few additional results that may be of independent
interest, such as an improved direct product theorem, as well as a result we
refer to as a ``restriction inverse theorem'' about the structure of functions
that, under random restrictions, with noticeable probability have significant
correlation with a product function. In companion papers, we show applications
of our results to the fields of Probabilistically Checkable Proofs, as well as
various areas in discrete mathematics such as extremal combinatorics and
additive combinatorics.",cs.CC math.CO,2023-07-30
Planar Friction Modelling with LuGre Dynamics and Limit Surfaces,,"During planar motion, contact surfaces exhibit a coupling between tangential
and rotational friction forces. This paper proposes planar friction models
grounded in the LuGre model and limit surface theory. First, distributed planar
extended state models are proposed and the Elasto-Plastic model is extended for
multi-dimensional friction. Subsequently, we derive a reduced planar friction
model, coupled with a pre-calculated limit surface, that offers reduced
computational cost. The limit surface approximation through an ellipsoid is
discussed. The properties of the planar friction models are assessed in various
simulations, demonstrating that the reduced planar friction model achieves
comparable performance to the distributed model while exhibiting ~80 times
lower computational cost.",eess.SY cs.RO cs.SY,2023-08-02
"A second-order structure-preserving discretization for the
  Cahn-Hilliard/Allen-Cahn system with cross-kinetic coupling",,"We study the numerical solution of a Cahn-Hilliard/Allen-Cahn system with
strong coupling through state and gradient dependent non-diagonal mobility
matrices. A fully discrete approximation scheme in space and time is proposed
which preserves the underlying gradient flow structure and leads to dissipation
of the free-energy on the discrete level. Existence and uniqueness of the
discrete solution is established and relative energy estimates are used to
prove optimal convergence rates in space and time under minimal smoothness
assumptions. Numerical tests are presented for illustration of the theoretical
results and to demonstrate the viability of the proposed methods.",math.NA cs.NA,2023-08-03
"End-to-End Reinforcement Learning of Koopman Models for Economic
  Nonlinear Model Predictive Control",,"(Economic) nonlinear model predictive control ((e)NMPC) requires dynamic
models that are sufficiently accurate and computationally tractable.
Data-driven surrogate models for mechanistic models can reduce the
computational burden of (e)NMPC; however, such models are typically trained by
system identification for maximum prediction accuracy on simulation samples and
perform suboptimally in (e)NMPC. We present a method for end-to-end
reinforcement learning of Koopman surrogate models for optimal performance as
part of (e)NMPC. We apply our method to two applications derived from an
established nonlinear continuous stirred-tank reactor model. The controller
performance is compared to that of (e)NMPCs utilizing models trained using
system identification, and model-free neural network controllers trained using
reinforcement learning. We show that the end-to-end trained models outperform
those trained using system identification in (e)NMPC, and that, in contrast to
the neural network controllers, the (e)NMPC controllers can react to changes in
the control setting without retraining.",cs.LG cs.SY eess.SY,2023-08-03
SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore,,"The legality of training language models (LMs) on copyrighted or otherwise
restricted data is under intense debate. However, as we show, model performance
significantly degrades if trained only on low-risk text (e.g., out-of-copyright
books or government documents), due to its limited size and domain coverage. We
present SILO, a new language model that manages this risk-performance tradeoff
during inference. SILO is built by (1) training a parametric LM on Open License
Corpus (OLC), a new corpus we curate with 228B tokens of public domain and
permissively licensed text and (2) augmenting it with a more general and easily
modifiable nonparametric datastore (e.g., containing copyrighted books or news)
that is only queried during inference. The datastore allows use of high-risk
data without training on it, supports sentence-level data attribution, and
enables data producers to opt out from the model by removing content from the
store. These capabilities can foster compliance with data-use regulations such
as the fair use doctrine in the United States and the GDPR in the European
Union. Our experiments show that the parametric LM struggles on domains not
covered by OLC. However, access to the datastore greatly improves out of domain
performance, closing 90% of the performance gap with an LM trained on the Pile,
a more diverse corpus with mostly high-risk text. We also analyze which
nonparametric approach works best, where the remaining errors lie, and how
performance scales with datastore size. Our results suggest that it is possible
to build high quality language models while mitigating their legal risk.",cs.CL cs.AI cs.LG,2023-08-08
"rCanary: Detecting Memory Leaks Across Semi-automated Memory Management
  Boundary in Rust",,"Rust is an effective system programming language that guarantees memory
safety via compile-time verifications. It employs a novel ownership-based
resource management model to facilitate automated deallocation. This model is
anticipated to eliminate memory leaks. However, we observed that user
intervention drives it into semi-automated memory management and makes it
error-prone to cause leaks. In contrast to violating memory-safety guarantees
restricted by the unsafe keyword, the boundary of leaking memory is implicit,
and the compiler would not emit any warnings for developers. In this paper, we
present rCanary, a static, non-intrusive, and fully automated model checker to
detect leaks across the semiautomated boundary. We design an encoder to
abstract data with heap allocation and formalize a refined leak-free memory
model based on boolean satisfiability. It can generate SMT-Lib2 format
constraints for Rust MIR and is implemented as a Cargo component. We evaluate
rCanary by using flawed package benchmarks collected from the pull requests of
open-source Rust projects. The results indicate that it is possible to recall
all these defects with acceptable false positives. We further apply our tool to
more than 1,200 real-world crates from crates.io and GitHub, identifying 19
crates having memory leaks. Our analyzer is also efficient, that costs 8.4
seconds per package.",cs.SE,2023-08-09
"Seed Kernel Counting using Domain Randomization and Object Tracking
  Neural Networks",,"High-throughput phenotyping (HTP) of seeds, also known as seed phenotyping,
is the comprehensive assessment of complex seed traits such as growth,
development, tolerance, resistance, ecology, yield, and the measurement of
parameters that form more complex traits. One of the key aspects of seed
phenotyping is cereal yield estimation that the seed production industry relies
upon to conduct their business. While mechanized seed kernel counters are
available in the market currently, they are often priced high and sometimes
outside the range of small scale seed production firms' affordability. The
development of object tracking neural network models such as You Only Look Once
(YOLO) enables computer scientists to design algorithms that can estimate
cereal yield inexpensively. The key bottleneck with neural network models is
that they require a plethora of labelled training data before they can be put
to task. We demonstrate that the use of synthetic imagery serves as a feasible
substitute to train neural networks for object tracking that includes the tasks
of object classification and detection. Furthermore, we propose a seed kernel
counter that uses a low-cost mechanical hopper, trained YOLOv8 neural network
model, and object tracking algorithms on StrongSORT and ByteTrack to estimate
cereal yield from videos. The experiment yields a seed kernel count with an
accuracy of 95.2\% and 93.2\% for Soy and Wheat respectively using the
StrongSORT algorithm, and an accuray of 96.8\% and 92.4\% for Soy and Wheat
respectively using the ByteTrack algorithm.",cs.CV cs.AI,2023-08-10
The Hard-Constraint PINNs for Interface Optimal Control Problems,,"We show that the physics-informed neural networks (PINNs), in combination
with some recently developed discontinuity capturing neural networks, can be
applied to solve optimal control problems subject to partial differential
equations (PDEs) with interfaces and some control constraints. The resulting
algorithm is mesh-free and scalable to different PDEs, and it ensures the
control constraints rigorously. Since the boundary and interface conditions, as
well as the PDEs, are all treated as soft constraints by lumping them into a
weighted loss function, it is necessary to learn them simultaneously and there
is no guarantee that the boundary and interface conditions can be satisfied
exactly. This immediately causes difficulties in tuning the weights in the
corresponding loss function and training the neural networks. To tackle these
difficulties and guarantee the numerical accuracy, we propose to impose the
boundary and interface conditions as hard constraints in PINNs by developing a
novel neural network architecture. The resulting hard-constraint PINNs approach
guarantees that both the boundary and interface conditions can be satisfied
exactly or with a high degree of accuracy, and they are decoupled from the
learning of the PDEs. Its efficiency is promisingly validated by some elliptic
and parabolic interface optimal control problems.",math.OC cs.LG,2023-08-13
Domain Adaptation for Code Model-based Unit Test Case Generation,,"Recently, deep learning-based test case generation approaches have been
proposed to automate the generation of unit test cases. In this study, we
leverage Transformer-based code models to generate unit tests with the help of
Domain Adaptation (DA) at a project level. Specifically, we use CodeT5, a
relatively small language model trained on source code data, and fine-tune it
on the test generation task. Then, we apply domain adaptation to each target
project data to learn project-specific knowledge (project-level DA). We use the
Methods2test dataset to fine-tune CodeT5 for the test generation task and the
Defects4j dataset for project-level domain adaptation and evaluation. We
compare our approach with (a) CodeT5 fine-tuned on the test generation without
DA, (b) the A3Test tool, and (c) GPT-4 on five projects from the Defects4j
dataset. The results show that tests generated using DA can increase the line
coverage by 18.62%, 19.88%, and 18.02% and mutation score by 16.45%, 16.01%,
and 12.99% compared to the above (a), (b), and (c) baselines, respectively. The
overall results show consistent improvements in metrics such as parse rate,
compile rate, BLEU, and CodeBLEU. In addition, we show that our approach can be
seen as a complementary solution alongside existing search-based test
generation tools such as EvoSuite, to increase the overall coverage and
mutation scores with an average of 34.42% and 6.8%, for line coverage and
mutation score, respectively.",cs.SE cs.AI,2023-08-15
Diversifying AI: Towards Creative Chess with AlphaZero,,"In recent years, Artificial Intelligence (AI) systems have surpassed human
intelligence in a variety of computational tasks. However, AI systems, like
humans, make mistakes, have blind spots, hallucinate, and struggle to
generalize to new situations. This work explores whether AI can benefit from
creative decision-making mechanisms when pushed to the limits of its
computational rationality. In particular, we investigate whether a team of
diverse AI systems can outperform a single AI in challenging tasks by
generating more ideas as a group and then selecting the best ones. We study
this question in the game of chess, the so-called drosophila of AI. We build on
AlphaZero (AZ) and extend it to represent a league of agents via a
latent-conditioned architecture, which we call AZ_db. We train AZ_db to
generate a wider range of ideas using behavioral diversity techniques and
select the most promising ones with sub-additive planning. Our experiments
suggest that AZ_db plays chess in diverse ways, solves more puzzles as a group
and outperforms a more homogeneous team. Notably, AZ_db solves twice as many
challenging puzzles as AZ, including the challenging Penrose positions. When
playing chess from different openings, we notice that players in AZ_db
specialize in different openings, and that selecting a player for each opening
using sub-additive planning results in a 50 Elo improvement over AZ. Our
findings suggest that diversity bonuses emerge in teams of AI agents, just as
they do in teams of humans and that diversity is a valuable asset in solving
computationally hard problems.",cs.AI cs.LG,2023-08-17
Optimized Deep Learning Models for Malware Detection under Concept Drift,,"Despite the promising results of machine learning models in malicious files
detection, they face the problem of concept drift due to their constant
evolution. This leads to declining performance over time, as the data
distribution of the new files differs from the training one, requiring frequent
model update. In this work, we propose a model-agnostic protocol to improve a
baseline neural network against drift. We show the importance of feature
reduction and training with the most recent validation set possible, and
propose a loss function named Drift-Resilient Binary Cross-Entropy, an
improvement to the classical Binary Cross-Entropy more effective against drift.
We train our model on the EMBER dataset, published in2018, and evaluate it on a
dataset of recent malicious files, collected between 2020 and 2023. Our
improved model shows promising results, detecting 15.2% more malware than a
baseline model.",cs.CR cs.AI cs.LG,2023-08-21
"Pyramid diffractive optical networks for unidirectional image
  magnification and demagnification",,"Diffractive deep neural networks (D2NNs) are composed of successive
transmissive layers optimized using supervised deep learning to all-optically
implement various computational tasks between an input and output field-of-view
(FOV). Here, we present a pyramid-structured diffractive optical network design
(which we term P-D2NN), optimized specifically for unidirectional image
magnification and demagnification. In this design, the diffractive layers are
pyramidally scaled in alignment with the direction of the image magnification
or demagnification. This P-D2NN design creates high-fidelity magnified or
demagnified images in only one direction, while inhibiting the image formation
in the opposite direction - achieving the desired unidirectional imaging
operation using a much smaller number of diffractive degrees of freedom within
the optical processor volume. Furthermore, P-D2NN design maintains its
unidirectional image magnification/demagnification functionality across a large
band of illumination wavelengths despite being trained with a single
wavelength. We also designed a wavelength-multiplexed P-D2NN, where a
unidirectional magnifier and a unidirectional demagnifier operate
simultaneously in opposite directions, at two distinct illumination
wavelengths. Furthermore, we demonstrate that by cascading multiple
unidirectional P-D2NN modules, we can achieve higher magnification factors. The
efficacy of the P-D2NN architecture was also validated experimentally using
terahertz illumination, successfully matching our numerical simulations. P-D2NN
offers a physics-inspired strategy for designing task-specific visual
processors.",physics.optics cs.CV cs.NE physics.app-ph,2023-08-29
"Characterizing Learning Curves During Language Model Pre-Training:
  Learning, Forgetting, and Stability",,"How do language models learn to make predictions during pre-training? To
study this, we extract learning curves from five autoregressive English
language model pre-training runs, for 1M unseen tokens in context. We observe
that the language models generate short repetitive phrases before learning to
generate longer and more coherent text. We also find that individual tokens
often exhibit sudden increases or decreases in loss that are surprisingly
consistent across pre-training runs. To better understand these fluctuations,
we quantify the final surprisal, within-run variability, age of acquisition,
forgettability, and cross-run variability of learning curves for individual
tokens in context. More frequent tokens reach lower final surprisals, exhibit
less variability within and across pre-training runs, are learned earlier, and
are less likely to be ""forgotten"" during pre-training. Higher n-gram
probabilities further accentuate these effects. Independent of the target
token, shorter and more frequent contexts correlate with marginally more stable
and quickly acquired predictions. Based on our results, we argue for the
existence of sequential learning dependencies between different model
capabilities, and we characterize language model learning as early n-gram
learning before gradual refinement of tail n-gram predictions.",cs.CL,2023-08-29
"In-class Data Analysis Replications: Teaching Students while Testing
  Science",,"Science is facing a reproducibility crisis. Previous work has proposed
incorporating data analysis replications into classrooms as a potential
solution. However, despite the potential benefits, it is unclear whether this
approach is feasible, and if so, what the involved stakeholders-students,
educators, and scientists-should expect from it. Can students perform a data
analysis replication over the course of a class? What are the costs and
benefits for educators? And how can this solution help benchmark and improve
the state of science?
  In the present study, we incorporated data analysis replications in the
project component of the Applied Data Analysis course (CS-401) taught at EPFL
(N=354 students). Here we report pre-registered findings based on surveys
administered throughout the course. First, we demonstrate that students can
replicate previously published scientific papers, most of them qualitatively
and some exactly. We find discrepancies between what students expect of data
analysis replications and what they experience by doing them along with changes
in expectations about reproducibility, which together serve as evidence of
attitude shifts to foster students' critical thinking. Second, we provide
information for educators about how much overhead is needed to incorporate
replications into the classroom and identify concerns that replications bring
as compared to more traditional assignments. Third, we identify tangible
benefits of the in-class data analysis replications for scientific communities,
such as a collection of replication reports and insights about replication
barriers in scientific work that should be avoided going forward.
  Overall, we demonstrate that incorporating replication tasks into a large
data science class can increase the reproducibility of scientific work as a
by-product of data science instruction, thus benefiting both science and
students.",cs.CY cs.AI cs.LG cs.SI,2023-08-31
"Understanding Vector-Valued Neural Networks and Their Relationship with
  Real and Hypercomplex-Valued Neural Networks",,"Despite the many successful applications of deep learning models for
multidimensional signal and image processing, most traditional neural networks
process data represented by (multidimensional) arrays of real numbers. The
intercorrelation between feature channels is usually expected to be learned
from the training data, requiring numerous parameters and careful training. In
contrast, vector-valued neural networks are conceived to process arrays of
vectors and naturally consider the intercorrelation between feature channels.
Consequently, they usually have fewer parameters and often undergo more robust
training than traditional neural networks. This paper aims to present a broad
framework for vector-valued neural networks, referred to as V-nets. In this
context, hypercomplex-valued neural networks are regarded as vector-valued
models with additional algebraic properties. Furthermore, this paper explains
the relationship between vector-valued and traditional neural networks.
Precisely, a vector-valued neural network can be obtained by placing
restrictions on a real-valued model to consider the intercorrelation between
feature channels. Finally, we show how V-nets, including hypercomplex-valued
neural networks, can be implemented in current deep-learning libraries as
real-valued networks.",cs.LG cs.NE,2023-09-14
Intention-Aware Planner for Robust and Safe Aerial Tracking,,"Autonomous target tracking with quadrotors has wide applications in many
scenarios, such as cinematographic follow-up shooting or suspect chasing.
Target motion prediction is necessary when designing the tracking planner.
However, the widely used constant velocity or constant rotation assumption can
not fully capture the dynamics of the target. The tracker may fail when the
target happens to move aggressively, such as sudden turn or deceleration. In
this paper, we propose an intention-aware planner by additionally considering
the intention of the target to enhance safety and robustness in aerial tracking
applications. Firstly, a designated intention prediction method is proposed,
which combines a user-defined potential assessment function and a state
observation function. A reachable region is generated to specifically evaluate
the turning intentions. Then we design an intention-driven hybrid A* method to
predict the future possible positions for the target. Finally, an
intention-aware optimization approach is designed to generate a
spatial-temporal optimal trajectory, allowing the tracker to perceive
unexpected situations from the target. Benchmark comparisons and real-world
experiments are conducted to validate the performance of our method.",cs.RO,2023-09-15
"Structure to Property: Chemical Element Embeddings and a Deep Learning
  Approach for Accurate Prediction of Chemical Properties",,"We introduce the elEmBERT model for chemical classification tasks. It is
based on deep learning techniques, such as a multilayer encoder architecture.
We demonstrate the opportunities offered by our approach on sets of organic,
inorganic and crystalline compounds. In particular, we developed and tested the
model using the Matbench and Moleculenet benchmarks, which include crystal
properties and drug design-related benchmarks. We also conduct an analysis of
vector representations of chemical compounds, shedding light on the underlying
patterns in structural data. Our model exhibits exceptional predictive
capabilities and proves universally applicable to molecular and material
datasets. For instance, on the Tox21 dataset, we achieved an average precision
of 96%, surpassing the previously best result by 10%.",physics.chem-ph cond-mat.mtrl-sci cs.LG physics.atm-clus q-bio.QM,2023-09-17
"Nonlinear dynamic analysis of shear- and torsion-free rods using
  isogeometric discretization and outlier removal",,"In this paper, we present a discrete formulation of nonlinear shear- and
torsion-free rods introduced by Gebhardt and Romero in [20] that uses
isogeometric discretization and robust time integration. Omitting the director
as an independent variable field, we reduce the number of degrees of freedom
and obtain discrete solutions in multiple copies of the Euclidean space (R^3),
which is larger than the corresponding multiple copies of the manifold (R^3 x
S^2) obtained with standard Hermite finite elements. For implicit time
integration, we choose the same integration scheme as Gebhardt and Romero in
[20] that is a hybrid form of the midpoint and the trapezoidal rules. In
addition, we apply a recently introduced approach for outlier removal by
Hiemstra et al. [26] that reduces high-frequency content in the response
without affecting the accuracy, ensuring robustness of our nonlinear discrete
formulation. We illustrate the efficiency of our nonlinear discrete formulation
for static and transient rods under different loading conditions, demonstrating
good accuracy in space, time and the frequency domain. Our numerical example
coincides with a relevant application case, the simulation of mooring lines.",cs.CE,2023-09-19
"GNSS/Multi-Sensor Fusion Using Continuous-Time Factor Graph Optimization
  for Robust Localization",,"Accurate and robust vehicle localization in highly urbanized areas is
challenging. Sensors are often corrupted in those complicated and large-scale
environments. This paper introduces GNSS-FGO, an online and global trajectory
estimator that fuses GNSS observations alongside multiple sensor measurements
for robust vehicle localization. In GNSS-FGO, we fuse asynchronous sensor
measurements into the graph with a continuous-time trajectory representation
using Gaussian process regression. This enables querying states at arbitrary
timestamps so that sensor observations are fused without requiring strict state
and measurement synchronization. Thus, the proposed method presents a
generalized factor graph for multi-sensor fusion. To evaluate and study
different GNSS fusion strategies, we fuse GNSS measurements in loose and tight
coupling with a speed sensor, IMU, and lidar-odometry. We employed datasets
from measurement campaigns in Aachen, Duesseldorf, and Cologne in experimental
studies and presented comprehensive discussions on sensor observations,
smoother types, and hyperparameter tuning. Our results show that the proposed
approach enables robust trajectory estimation in dense urban areas, where the
classic multi-sensor fusion method fails due to sensor degradation. In a test
sequence containing a 17km route through Aachen, the proposed method results in
a mean 2D positioning error of 0.48m while fusing raw GNSS observations with
lidar odometry in a tight coupling.",cs.RO,2023-09-20
"Self-supervised learning unveils change in urban housing from
  street-level images",,"Cities around the world face a critical shortage of affordable and decent
housing. Despite its critical importance for policy, our ability to effectively
monitor and track progress in urban housing is limited. Deep learning-based
computer vision methods applied to street-level images have been successful in
the measurement of socioeconomic and environmental inequalities but did not
fully utilize temporal images to track urban change as time-varying labels are
often unavailable. We used self-supervised methods to measure change in London
using 15 million street images taken between 2008 and 2021. Our novel
adaptation of Barlow Twins, Street2Vec, embeds urban structure while being
invariant to seasonal and daily changes without manual annotations. It
outperformed generic embeddings, successfully identified point-level change in
London's housing supply from street-level images, and distinguished between
major and minor change. This capability can provide timely information for
urban planning and policy decisions toward more liveable, equitable, and
sustainable cities.",cs.CV cs.LG,2023-09-20
"Prompt, Plan, Perform: LLM-based Humanoid Control via Quantized
  Imitation Learning",,"In recent years, reinforcement learning and imitation learning have shown
great potential for controlling humanoid robots' motion. However, these methods
typically create simulation environments and rewards for specific tasks,
resulting in the requirements of multiple policies and limited capabilities for
tackling complex and unknown tasks. To overcome these issues, we present a
novel approach that combines adversarial imitation learning with large language
models (LLMs). This innovative method enables the agent to learn reusable
skills with a single policy and solve zero-shot tasks under the guidance of
LLMs. In particular, we utilize the LLM as a strategic planner for applying
previously learned skills to novel tasks through the comprehension of
task-specific prompts. This empowers the robot to perform the specified actions
in a sequence. To improve our model, we incorporate codebook-based vector
quantization, allowing the agent to generate suitable actions in response to
unseen textual commands from LLMs. Furthermore, we design general reward
functions that consider the distinct motion features of humanoid robots,
ensuring the agent imitates the motion data while maintaining goal orientation
without additional guiding direction approaches or policies. To the best of our
knowledge, this is the first framework that controls humanoid robots using a
single learning policy network and LLM as a planner. Extensive experiments
demonstrate that our method exhibits efficient and adaptive ability in
complicated motion tasks.",cs.RO,2023-09-20
"Unlocking massively parallel spectral proper orthogonal decompositions
  in the PySPOD package",,"We propose a parallel (distributed) version of the spectral proper orthogonal
decomposition (SPOD) technique. The parallel SPOD algorithm distributes the
spatial dimension of the dataset preserving time. This approach is adopted to
preserve the non-distributed fast Fourier transform of the data in time,
thereby avoiding the associated bottlenecks. The parallel SPOD algorithm is
implemented in the PySPOD (https://github.com/MathEXLab/PySPOD) library and
makes use of the standard message passing interface (MPI) library, implemented
in Python via mpi4py (https://mpi4py.readthedocs.io/en/stable/). An extensive
performance evaluation of the parallel package is provided, including strong
and weak scalability analyses. The open-source library allows the analysis of
large datasets of interest across the scientific community. Here, we present
applications in fluid dynamics and geophysics, that are extremely difficult (if
not impossible) to achieve without a parallel algorithm. This work opens the
path toward modal analyses of big quasi-stationary data, helping to uncover new
unexplored spatio-temporal patterns.",physics.comp-ph cs.DC cs.MS,2023-09-21
"Penalty Ensembles for Navier-Stokes with Random Initial Conditions and
  Forcing",,"In many applications, uncertainty in problem data leads to the need for
numerous computationally expensive simulations. This report addresses this
challenge by developing a penalty-based ensemble algorithm. Building upon Jiang
and Layton's work on ensemble algorithms that use a shared coefficient matrix,
this report introduces the combination of penalty methods to enhance its
capabilities. Penalty methods uncouple velocity and pressure by relaxing the
incompressibility condition. Eliminating the pressure results in a system that
requires less memory. The reduction in memory allows for larger ensemble sizes,
which give more information about the flow and can be used to extend the
predictability horizon.",math.NA cs.NA,2023-09-22
"SurrogatePrompt: Bypassing the Safety Filter of Text-To-Image Models via
  Substitution",,"Advanced text-to-image models such as DALL-E 2 and Midjourney possess the
capacity to generate highly realistic images, raising significant concerns
regarding the potential proliferation of unsafe content. This includes adult,
violent, or deceptive imagery of political figures. Despite claims of rigorous
safety mechanisms implemented in these models to restrict the generation of
not-safe-for-work (NSFW) content, we successfully devise and exhibit the first
prompt attacks on Midjourney, resulting in the production of abundant
photorealistic NSFW images. We reveal the fundamental principles of such prompt
attacks and suggest strategically substituting high-risk sections within a
suspect prompt to evade closed-source safety measures. Our novel framework,
SurrogatePrompt, systematically generates attack prompts, utilizing large
language models, image-to-text, and image-to-image modules to automate attack
prompt creation at scale. Evaluation results disclose an 88% success rate in
bypassing Midjourney's proprietary safety filter with our attack prompts,
leading to the generation of counterfeit images depicting political figures in
violent scenarios. Both subjective and objective assessments validate that the
images generated from our attack prompts present considerable safety hazards.",cs.CV cs.CR,2023-09-25
"Multi-Source Domain Adaptation for Object Detection with Prototype-based
  Mean-teacher",,"Adapting visual object detectors to operational target domains is a
challenging task, commonly achieved using unsupervised domain adaptation (UDA)
methods. Recent studies have shown that when the labeled dataset comes from
multiple source domains, treating them as separate domains and performing a
multi-source domain adaptation (MSDA) improves the accuracy and robustness over
blending these source domains and performing a UDA. For adaptation, existing
MSDA methods learn domain-invariant and domain-specific parameters (for each
source domain). However, unlike single-source UDA methods, learning
domain-specific parameters makes them grow significantly in proportion to the
number of source domains. This paper proposes a novel MSDA method called
Prototype-based Mean Teacher (PMT), which uses class prototypes instead of
domain-specific subnets to encode domain-specific information. These prototypes
are learned using a contrastive loss, aligning the same categories across
domains and separating different categories far apart. Given the use of
prototypes, the number of parameters required for our PMT method does not
increase significantly with the number of source domains, thus reducing memory
issues and possible overfitting. Empirical studies indicate that PMT
outperforms state-of-the-art MSDA methods on several challenging object
detection datasets. Our code is available at
https://github.com/imatif17/Prototype-Mean-Teacher.",cs.CV cs.AI,2023-09-26
"A comparison between black-, grey- and white-box modeling for the
  bidirectional Raman amplifier optimization",,"Designing and optimizing optical amplifiers to maximize system performance is
becoming increasingly important as optical communication systems strive to
increase throughput. Offline optimization of optical amplifiers relies on
models ranging from white-box models deeply rooted in physics to black-box
data-driven and physics-agnostic models. Here, we compare the capabilities of
white-, grey- and black-box models on the challenging test case of optimizing a
bidirectional distributed Raman amplifier to achieve a target
frequency-distance signal power profile. We show that any of the studied
methods can achieve similar frequency and distance flatness of between 1 and
3.6 dB (depending on the definition of flatness) over the C-band in an 80-km
span. Then, we discuss the models' applicability, advantages, and drawbacks
based on the target application scenario, in particular in terms of
flexibility, optimization speed, and access to training data.",physics.app-ph cs.CE cs.LG physics.optics,2023-09-11
Psychoacoustic Challenges Of Speech Enhancement On VoIP Platforms,,"Within the ambit of VoIP (Voice over Internet Protocol) telecommunications,
the complexities introduced by acoustic transformations merit rigorous
analysis. This research, rooted in the exploration of proprietary sender-side
denoising effects, meticulously evaluates platforms such as Google Meets and
Zoom. The study draws upon the Deep Noise Suppression (DNS) 2020 dataset,
ensuring a structured examination tailored to various denoising settings and
receiver interfaces. A methodological novelty is introduced via Blinder-Oaxaca
decomposition, traditionally an econometric tool, repurposed herein to analyze
acoustic-phonetic perturbations within VoIP systems. To further ground the
implications of these transformations, psychoacoustic metrics, specifically
PESQ and STOI, were used to explain of perceptual quality and intelligibility.
Cumulatively, the insights garnered underscore the intricate landscape of
VoIP-influenced acoustic dynamics. In addition to the primary findings, a
multitude of metrics are reported, extending the research purview. Moreover,
out-of-domain benchmarking for both time and time-frequency domain speech
enhancement models is included, thereby enhancing the depth and applicability
of this inquiry.",cs.SD cs.CL eess.AS,2023-10-10
"Deep ReLU networks and high-order finite element methods II: Chebyshev
  emulation",,"We show expression rates and stability in Sobolev norms of deep feedforward
ReLU neural networks (NNs) in terms of the number of parameters defining the NN
for continuous, piecewise polynomial functions, on arbitrary, finite partitions
$\mathcal{T}$ of a bounded interval $(a,b)$. Novel constructions of ReLU NN
surrogates encoding function approximations in terms of Chebyshev polynomial
expansion coefficients are developed which require fewer neurons than previous
constructions. Chebyshev coefficients can be computed easily from the values of
the function in the Clenshaw--Curtis points using the inverse fast Fourier
transform. Bounds on expression rates and stability are obtained that are
superior to those of constructions based on ReLU NN emulations of monomials as
considered in [Opschoor, Petersen and Schwab, 2020] and [Montanelli, Yang and
Du, 2021]. All emulation bounds are explicit in terms of the (arbitrary)
partition of the interval, the target emulation accuracy and the polynomial
degree in each element of the partition. ReLU NN emulation error estimates are
provided for various classes of functions and norms, commonly encountered in
numerical analysis. In particular, we show exponential ReLU emulation rate
bounds for analytic functions with point singularities and develop an interface
between Chebfun approximations and constructive ReLU NN emulations.",math.NA cs.LG cs.NA,2023-10-11
"CompA: Addressing the Gap in Compositional Reasoning in Audio-Language
  Models",,"A fundamental characteristic of audio is its compositional nature.
Audio-language models (ALMs) trained using a contrastive approach (e.g., CLAP)
that learns a shared representation between audio and language modalities have
improved performance in many downstream applications, including zero-shot audio
classification, audio retrieval, etc. However, the ability of these models to
effectively perform compositional reasoning remains largely unexplored and
necessitates additional research. In this paper, we propose CompA, a collection
of two expert-annotated benchmarks with a majority of real-world audio samples,
to evaluate compositional reasoning in ALMs. Our proposed CompA-order evaluates
how well an ALM understands the order or occurrence of acoustic events in
audio, and CompA-attribute evaluates attribute-binding of acoustic events. An
instance from either benchmark consists of two audio-caption pairs, where both
audios have the same acoustic events but with different compositions. An ALM is
evaluated on how well it matches the right audio to the right caption. Using
this benchmark, we first show that current ALMs perform only marginally better
than random chance, thereby struggling with compositional reasoning. Next, we
propose CompA-CLAP, where we fine-tune CLAP using a novel learning method to
improve its compositional reasoning abilities. To train CompA-CLAP, we first
propose improvements to contrastive training with composition-aware hard
negatives, allowing for more focused training. Next, we propose a novel modular
contrastive loss that helps the model learn fine-grained compositional
understanding and overcomes the acute scarcity of openly available
compositional audios. CompA-CLAP significantly improves over all our baseline
models on the CompA benchmark, indicating its superior compositional reasoning
capabilities.",cs.SD cs.AI cs.CL eess.AS,2023-10-12
Instructive Dialogue Summarization with Query Aggregations,,"Conventional dialogue summarization methods directly generate summaries and
do not consider user's specific interests. This poses challenges in cases where
the users are more focused on particular topics or aspects. With the
advancement of instruction-finetuned language models, we introduce
instruction-tuning to dialogues to expand the capability set of dialogue
summarization models. To overcome the scarcity of instructive dialogue
summarization data, we propose a three-step approach to synthesize high-quality
query-based summarization triples. This process involves summary-anchored query
generation, query filtering, and query-based summary generation. By training a
unified model called InstructDS (Instructive Dialogue Summarization) on three
summarization datasets with multi-purpose instructive triples, we expand the
capability of dialogue summarization models. We evaluate our method on four
datasets, including dialogue summarization and dialogue reading comprehension.
Experimental results show that our approach outperforms the state-of-the-art
models and even models with larger sizes. Additionally, our model exhibits
higher generalizability and faithfulness, as confirmed by human subjective
evaluations.",cs.CL,2023-10-17
LLMs as Hackers: Autonomous Linux Privilege Escalation Attacks,,"Penetration testing, an essential component of software security testing,
allows organizations to identify and remediate vulnerabilities in their
systems, thus bolstering their defense mechanisms against cyberattacks. One
recent advancement in the realm of penetration testing is the utilization of
Language Models (LLMs). We explore the intersection of LLMs and penetration
testing to gain insight into their capabilities and challenges in the context
of privilege escalation. We introduce a fully automated privilege-escalation
tool designed for evaluating the efficacy of LLMs for (ethical) hacking,
executing benchmarks using multiple LLMs, and investigating their respective
results.
  Our results show that GPT-4-turbo is well suited to exploit vulnerabilities
(33-83% of vulnerabilities). GPT-3.5-turbo can abuse 16-50% of vulnerabilities,
while local models, such as Llama3, can only exploit between 0 and 33% of the
vulnerabilities.
  We analyze the impact of different context sizes, in-context learning,
optional high-level guidance mechanisms, and memory management techniques. We
discuss challenging areas for LLMs, including maintaining focus during testing,
coping with errors, and finally comparing LLMs with human hackers.
  The current version of the LLM-guided privilege-escalation prototype can be
found at https://github.com/ipa-labs/hackingBuddyGPT.",cs.CR cs.AI,2023-10-17
Resolving social dilemmas with minimal reward transfer,,"Social dilemmas present a significant challenge in multi-agent cooperation
because individuals are incentivised to behave in ways that undermine socially
optimal outcomes. Consequently, self-interested agents often avoid collective
behaviour. In response, we formalise social dilemmas and introduce a novel
metric, the general self-interest level, to quantify the disparity between
individual and group rationality in such scenarios. This metric represents the
maximum proportion of their individual rewards that agents can retain while
ensuring that a social welfare optimum becomes a dominant strategy. Our
approach diverges from traditional concepts of altruism, instead focusing on
strategic reward redistribution. By transferring rewards among agents in a
manner that aligns individual and group incentives, rational agents will
maximise collective welfare while pursuing their own interests. We provide an
algorithm to compute efficient transfer structures for an arbitrary number of
agents, and introduce novel multi-player social dilemma games to illustrate the
effectiveness of our method. This work provides both a descriptive tool for
analysing social dilemmas and a prescriptive solution for resolving them via
efficient reward transfer contracts. Applications include mechanism design,
where we can assess the impact on collaborative behaviour of modifications to
models of environments.",cs.GT,2023-10-19
"Genetic Algorithms with Neural Cost Predictor for Solving Hierarchical
  Vehicle Routing Problems",,"When vehicle routing decisions are intertwined with higher-level decisions,
the resulting optimization problems pose significant challenges for
computation. Examples are the multi-depot vehicle routing problem (MDVRP),
where customers are assigned to depots before delivery, and the capacitated
location routing problem (CLRP), where the locations of depots should be
determined first. A simple and straightforward approach for such hierarchical
problems would be to separate the higher-level decisions from the complicated
vehicle routing decisions. For each higher-level decision candidate, we may
evaluate the underlying vehicle routing problems to assess the candidate. As
this approach requires solving vehicle routing problems multiple times, it has
been regarded as impractical in most cases. We propose a novel
deep-learning-based approach called Genetic Algorithm with Neural Cost
Predictor (GANCP) to tackle the challenge and simplify algorithm developments.
For each higher-level decision candidate, we predict the objective function
values of the underlying vehicle routing problems using a pre-trained graph
neural network without actually solving the routing problems. In particular,
our proposed neural network learns the objective values of the HGS-CVRP
open-source package that solves capacitated vehicle routing problems. Our
numerical experiments show that this simplified approach is effective and
efficient in generating high-quality solutions for both MDVRP and CLRP and has
the potential to expedite algorithm developments for complicated hierarchical
problems. We provide computational results evaluated in the standard benchmark
instances used in the literature.",cs.NE,2023-10-21
"The Janus Interface: How Fine-Tuning in Large Language Models Amplifies
  the Privacy Risks",,"The rapid advancements of large language models (LLMs) have raised public
concerns about the privacy leakage of personally identifiable information (PII)
within their extensive training datasets. Recent studies have demonstrated that
an adversary could extract highly sensitive privacy data from the training data
of LLMs with carefully designed prompts. However, these attacks suffer from the
model's tendency to hallucinate and catastrophic forgetting (CF) in the
pre-training stage, rendering the veracity of divulged PIIs negligible. In our
research, we propose a novel attack, Janus, which exploits the fine-tuning
interface to recover forgotten PIIs from the pre-training data in LLMs. We
formalize the privacy leakage problem in LLMs and explain why forgotten PIIs
can be recovered through empirical analysis on open-source language models.
Based upon these insights, we evaluate the performance of Janus on both
open-source language models and two latest LLMs, i.e., GPT-3.5-Turbo and
LLaMA-2-7b. Our experiment results show that Janus amplifies the privacy risks
by over 10 times in comparison with the baseline and significantly outperforms
the state-of-the-art privacy extraction attacks including prefix attacks and
in-context learning (ICL). Furthermore, our analysis validates that existing
fine-tuning APIs provided by OpenAI and Azure AI Studio are susceptible to our
Janus attack, allowing an adversary to conduct such an attack at a low cost.",cs.CR cs.AI cs.CL cs.LG,2023-10-23
"Temporally Disentangled Representation Learning under Unknown
  Nonstationarity",,"In unsupervised causal representation learning for sequential data with
time-delayed latent causal influences, strong identifiability results for the
disentanglement of causally-related latent variables have been established in
stationary settings by leveraging temporal structure. However, in nonstationary
setting, existing work only partially addressed the problem by either utilizing
observed auxiliary variables (e.g., class labels and/or domain indexes) as side
information or assuming simplified latent causal dynamics. Both constrain the
method to a limited range of scenarios. In this study, we further explored the
Markov Assumption under time-delayed causally related process in nonstationary
setting and showed that under mild conditions, the independent latent
components can be recovered from their nonlinear mixture up to a permutation
and a component-wise transformation, without the observation of auxiliary
variables. We then introduce NCTRL, a principled estimation framework, to
reconstruct time-delayed latent causal variables and identify their relations
from measured sequential data only. Empirical evaluations demonstrated the
reliable identification of time-delayed latent causal influences, with our
methodology substantially outperforming existing baselines that fail to exploit
the nonstationarity adequately and then, consequently, cannot distinguish
distribution shifts.",cs.LG stat.ML,2023-10-28
"Early detection of inflammatory arthritis to improve referrals using
  multimodal machine learning from blood testing, semi-structured and
  unstructured patient records",,"Early detection of inflammatory arthritis (IA) is critical to efficient and
accurate hospital referral triage for timely treatment and preventing the
deterioration of the IA disease course, especially under limited healthcare
resources. The manual assessment process is the most common approach in
practice for the early detection of IA, but it is extremely labor-intensive and
inefficient. A large amount of clinical information needs to be assessed for
every referral from General Practice (GP) to the hospitals. Machine learning
shows great potential in automating repetitive assessment tasks and providing
decision support for the early detection of IA. However, most machine
learning-based methods for IA detection rely on blood testing results. But in
practice, blood testing data is not always available at the point of referrals,
so we need methods to leverage multimodal data such as semi-structured and
unstructured data for early detection of IA. In this research, we present
fusion and ensemble learning-based methods using multimodal data to assist
decision-making in the early detection of IA, and a conformal prediction-based
method to quantify the uncertainty of the prediction and detect any unreliable
predictions. To the best of our knowledge, our study is the first attempt to
utilize multimodal data to support the early detection of IA from GP referrals.",cs.LG,2023-10-30
"Ontologies for Models and Algorithms in Applied Mathematics and Related
  Disciplines",,"In applied mathematics and related disciplines, the
modeling-simulation-optimization workflow is a prominent scheme, with
mathematical models and numerical algorithms playing a crucial role. For these
types of mathematical research data, the Mathematical Research Data Initiative
has developed, merged and implemented ontologies and knowledge graphs. This
contributes to making mathematical research data FAIR by introducing semantic
technology and documenting the mathematical foundations accordingly. Using the
concrete example of microfracture analysis of porous media, it is shown how the
knowledge of the underlying mathematical model and the corresponding numerical
algorithms for its solution can be represented by the ontologies.",cs.AI cs.DB cs.DL cs.IR,2023-10-31
Neural Retrievers are Biased Towards LLM-Generated Content,,"Recently, the emergence of large language models (LLMs) has revolutionized
the paradigm of information retrieval (IR) applications, especially in web
search, by generating vast amounts of human-like texts on the Internet. As a
result, IR systems in the LLM era are facing a new challenge: the indexed
documents are now not only written by human beings but also automatically
generated by the LLMs. How these LLM-generated documents influence the IR
systems is a pressing and still unexplored question. In this work, we conduct a
quantitative evaluation of IR models in scenarios where both human-written and
LLM-generated texts are involved. Surprisingly, our findings indicate that
neural retrieval models tend to rank LLM-generated documents higher. We refer
to this category of biases in neural retrievers towards the LLM-generated
content as the \textbf{source bias}. Moreover, we discover that this bias is
not confined to the first-stage neural retrievers, but extends to the
second-stage neural re-rankers. Then, in-depth analyses from the perspective of
text compression indicate that LLM-generated texts exhibit more focused
semantics with less noise, making it easier for neural retrieval models to
semantic match. To mitigate the source bias, we also propose a plug-and-play
debiased constraint for the optimization objective, and experimental results
show its effectiveness. Finally, we discuss the potential severe concerns
stemming from the observed source bias and hope our findings can serve as a
critical wake-up call to the IR community and beyond. To facilitate future
explorations of IR in the LLM era, the constructed two new benchmarks are
available at https://github.com/KID-22/Source-Bias.",cs.IR cs.AI cs.CL,2023-10-31
"SC-MIL: Sparsely Coded Multiple Instance Learning for Whole Slide Image
  Classification",,"Multiple Instance Learning (MIL) has been widely used in weakly supervised
whole slide image (WSI) classification. Typical MIL methods include a feature
embedding part, which embeds the instances into features via a pre-trained
feature extractor, and an MIL aggregator that combines instance embeddings into
predictions. Most efforts have typically focused on improving these parts. This
involves refining the feature embeddings through self-supervised pre-training
as well as modeling the correlations between instances separately.
  In this paper, we proposed a sparsely coding MIL (SC-MIL) method that
addresses those two aspects at the same time by leveraging sparse dictionary
learning. The sparse dictionary learning captures the similarities of instances
by expressing them as sparse linear combinations of atoms in an over-complete
dictionary. In addition, imposing sparsity improves instance feature embeddings
by suppressing irrelevant instances while retaining the most relevant ones. To
make the conventional sparse coding algorithm compatible with deep learning, we
unrolled it into a sparsely coded module leveraging deep unrolling. The
proposed SC module can be incorporated into any existing MIL framework in a
plug-and-play manner with an acceptable computational cost. The experimental
results on multiple datasets demonstrated that the proposed SC module could
substantially boost the performance of state-of-the-art MIL methods. The codes
are available at
\href{https://github.com/sotiraslab/SCMIL.git}{https://github.com/sotiraslab/SCMIL.git}.",cs.CV cs.AI cs.LG,2023-10-31
"Exploring the Problems, their Causes and Solutions of AI Pair
  Programming: A Study on GitHub and Stack Overflow",,"With the recent advancement of Artificial Intelligence (AI) and Large
Language Models (LLMs), AI-based code generation tools become a practical
solution for software development. GitHub Copilot, the AI pair programmer,
utilizes machine learning models trained on a large corpus of code snippets to
generate code suggestions using natural language processing. Despite its
popularity in software development, there is limited empirical evidence on the
actual experiences of practitioners who work with Copilot. To this end, we
conducted an empirical study to understand the problems that practitioners face
when using Copilot, as well as their underlying causes and potential solutions.
We collected data from 473 GitHub issues, 706 GitHub discussions, and 142 Stack
Overflow posts. Our results reveal that (1) Operation Issue and Compatibility
Issue are the most common problems faced by Copilot users, (2) Copilot Internal
Error, Network Connection Error, and Editor/IDE Compatibility Issue are
identified as the most frequent causes, and (3) Bug Fixed by Copilot, Modify
Configuration/Setting, and Use Suitable Version are the predominant solutions.
Based on the results, we discuss the potential areas of Copilot for
enhancement, and provide the implications for the Copilot users, the Copilot
team, and researchers.",cs.SE,2023-11-02
Survey of Simulators for Aerial Robots,,"Uncrewed Aerial Vehicle (UAV) research faces challenges with safety,
scalability, costs, and ecological impact when conducting hardware testing.
High-fidelity simulators offer a vital solution by replicating real-world
conditions to enable the development and evaluation of novel perception and
control algorithms. However, the large number of available simulators poses a
significant challenge for researchers to determine which simulator best suits
their specific use-case, based on each simulator's limitations and
customization readiness. In this paper we present an overview of 44 UAV
simulators, including in-depth, systematic comparisons for 14 of the
simulators. Additionally, we present a set of decision factors for selection of
simulators, aiming to enhance the efficiency and safety of research endeavors.",cs.RO,2023-11-03
"Goal-Oriented Wireless Communication Resource Allocation for
  Cyber-Physical Systems",,"The proliferation of novel industrial applications at the wireless edge, such
as smart grids and vehicle networks, demands the advancement of cyber-physical
systems. The performance of CPSs is closely linked to the last-mile wireless
communication networks, which often become bottlenecks due to their inherent
limited resources. Current CPS operations often treat wireless communication
networks as unpredictable and uncontrollable variables, ignoring the potential
adaptability of wireless networks, which results in inefficient and overly
conservative CPS operations. Meanwhile, current wireless communications often
focus more on throughput and other transmission-related metrics instead of CPS
goals. In this study, we introduce the framework of goal-oriented wireless
communication resource allocations, accounting for the semantics and
significance of data for CPS operation goals. This guarantees optimal CPS
performance from a cybernetic standpoint. We formulate a bandwidth allocation
problem aimed at maximizing the information utility gain of transmitted data
brought to CPS operation goals. Since the goal-oriented bandwidth allocation
problem is a large-scale combinational problem, we propose a divide-and-conquer
and greedy solution algorithm. The information utility gain is first
approximately decomposed into marginal utility information gains and computed
in a parallel manner. Subsequently, the bandwidth allocation problem is
reformulated as a knapsack problem, which can be further solved greedily with a
guaranteed sub-optimality gap. We further demonstrate how our proposed
goal-oriented bandwidth allocation algorithm can be applied in four potential
CPS applications, including data-driven decision-making, edge learning,
federated learning, and distributed optimization.",eess.SP cs.SY eess.SY,2023-11-06
Parikh's Theorem Made Symbolic,,"Parikh's Theorem is a fundamental result in automata theory with numerous
applications in computer science: software verification (e.g. infinite-state
verification, string constraints, and theory of arrays), verification of
cryptographic protocols (e.g. using Horn clauses modulo equational theories)
and database querying (e.g. evaluating path-queries in graph databases).
Parikh's Theorem states that the letter-counting abstraction of a language
recognized by finite automata or context-free grammars is definable in
Presburger Arithmetic. Unfortunately, real-world applications typically require
large alphabets - which are well-known to be not amenable to explicit treatment
of the alphabets.
  Symbolic automata have proven in the last decade to be an effective
algorithmic framework for handling large finite or even infinite alphabets. A
symbolic automaton employs an effective boolean algebra, which offers a
symbolic representation of character sets and often lends itself to an
exponentially more succinct representation of a language. Instead of
letter-counting, Parikh's Theorem for symbolic automata amounts to counting the
number of times different predicates are satisfied by an input sequence.
Unfortunately, naively applying Parikh's Theorem from classical automata theory
to symbolic automata yields existential Presburger formulas of exponential
size. We provide a new construction for Parikh's Theorem for symbolic automata
and grammars, which avoids this exponential blowup: our algorithm computes an
existential formula in polynomial-time over (quantifier-free) Presburger and
the base theory. In fact, our algorithm extends to the model of parametric
symbolic grammars, which are one of the most expressive models of languages
over infinite alphabets. We have implemented our algorithm and show it can be
used to solve string constraints that are difficult to solve by existing
solvers.",cs.FL cs.LO,2023-11-07
"Asymmetric Contrastive Multimodal Learning for Advancing Chemical
  Understanding",,"The versatility of multimodal deep learning holds tremendous promise for
advancing scientific research and practical applications. As this field
continues to evolve, the collective power of cross-modal analysis promises to
drive transformative innovations, leading us to new frontiers in chemical
understanding and discovery. Hence, we introduce Asymmetric Contrastive
Multimodal Learning (ACML) as a novel approach tailored for molecules,
showcasing its potential to advance the field of chemistry. ACML harnesses the
power of effective asymmetric contrastive learning to seamlessly transfer
information from various chemical modalities to molecular graph
representations. By combining pre-trained chemical unimodal encoders and a
shallow-designed graph encoder, ACML facilitates the assimilation of
coordinated chemical semantics from different modalities, leading to
comprehensive representation learning with efficient training. We demonstrate
the effectiveness of this framework through large-scale cross-modality
retrieval and isomer discrimination tasks. Additionally, ACML enhances
interpretability by revealing chemical semantics in graph presentations and
bolsters the expressive power of graph neural networks, as evidenced by
improved performance in molecular property prediction tasks from MoleculeNet.
ACML exhibits its capability to revolutionize chemical research and
applications, providing a deeper understanding of the chemical semantics of
different modalities.",cs.LG,2023-11-10
Managing Large Enclaves in a Data Center,,"Live migration of applications and VMs in data centers is an old and
quintessential problem. In this large body of work, an important open problem
still remains, which is the migration of secure enclaves (sandboxes) running on
trusted execution environments (TEEs) like Intel SGX. Here, the decade-old
stop-and-copy-based method is used, in which the entire application`s execution
is stopped and the state is collected and transferred. This method has an
exceedingly long downtime when we consider enclaves with large memory
footprints. Better solutions have eluded us because of some design limitations
posed by TEEs like Intel SGX, such as the opacity of data within enclaves (not
visible to the OS/hypervisor) and the lack of mechanisms to track writes on
secure pages. We propose a new technique, OptMig, to circumvent these
limitations and implement secure enclave migration with a near-zero downtime.
We rely on a short compiler pass and propose a novel migration mechanism. Our
optimizations reduce the total downtime by 77-96% for a suite of Intel SGX
applications that have multi-GB memory footprints. We show results for our
system on a real cloud and in settings that use containers, VMs, and microVMs",cs.CR,2023-11-12
"$\Pi_{2}$-Rule Systems and Inductive Classes of G\""{o}del Algebras",,"In this paper we present a general theory of $\Pi_{2}$-rules for systems of
intuitionistic and modal logic. We introduce the notions of $\Pi_{2}$-rule
system and of an Inductive Class, and provide model-theoretic and algebraic
completeness theorems, which serve as our basic tools. As an illustration of
the general theory, we analyse the structure of inductive classes of G\""{o}del
algebras, from a structure theoretic and logical point of view. We show that
unlike other well-studied settings (such as logics, or single-conclusion rule
systems), there are continuum many $\Pi_{2}$-rule systems extending
$\mathsf{LC}=\mathsf{IPC}+(p\rightarrow q)\vee (q\rightarrow p)$, and show how
our methods allow easy proofs of the admissibility of the well-known
Takeuti-Titani rule. Our final results concern general questions admissibility
in $\mathsf{LC}$: (1) we present a full classification of those inductive
classes which are inductively complete, i.e., where all $\Pi_{2}$-rules which
are admissible are derivable, and (2) show that the problem of admissibility of
$\Pi_{2}$-rules over $\mathsf{LC}$ is decidable.",math.LO cs.LO,2023-11-13
An introduction to reinforcement learning for neuroscience,,"Reinforcement learning has a rich history in neuroscience, from early work on
dopamine as a reward prediction error signal for temporal difference learning
(Schultz et al., 1997) to recent work suggesting that dopamine could implement
a form of 'distributional reinforcement learning' popularized in deep learning
(Dabney et al., 2020). Throughout this literature, there has been a tight link
between theoretical advances in reinforcement learning and neuroscientific
experiments and findings. As a result, the theories describing our experimental
data have become increasingly complex and difficult to navigate. In this
review, we cover the basic theory underlying classical work in reinforcement
learning and build up to an introductory overview of methods in modern deep
reinforcement learning that have found applications in systems neuroscience. We
start with an overview of the reinforcement learning problem and classical
temporal difference algorithms, followed by a discussion of 'model-free' and
'model-based' reinforcement learning together with methods such as DYNA and
successor representations that fall in between these two extremes. Throughout
these sections, we highlight the close parallels between such machine learning
methods and related work in both experimental and theoretical neuroscience. We
then provide an introduction to deep reinforcement learning with examples of
how these methods have been used to model different learning phenomena in
systems neuroscience, such as meta-reinforcement learning (Wang et al., 2018)
and distributional reinforcement learning (Dabney et al., 2020). Code that
implements the methods discussed in this work and generates the figures is also
provided.",q-bio.NC cs.LG,2023-11-13
"SkelVIT: Consensus of Vision Transformers for a Lightweight
  Skeleton-Based Action Recognition System",,"Skeleton-based action recognition receives the attention of many researchers
as it is robust to viewpoint and illumination changes, and its processing is
much more efficient than the processing of video frames. With the emergence of
deep learning models, it has become very popular to represent the skeleton data
in pseudo-image form and apply CNN for action recognition. Thereafter, studies
concentrated on finding effective methods for forming pseudo-images. Recently,
attention networks, more specifically transformers have provided promising
results in various vision problems. In this study, the effectiveness of VIT for
skeleton-based action recognition is examined and its robustness on the
pseudo-image representation scheme is investigated. To this end, a three-level
architecture, SkelVit is proposed, which forms a set of pseudo images, applies
a classifier on each of the representations, and combines their results to find
the final action class. The performance of SkelVit is examined thoroughly via a
set of experiments. First, the sensitivity of the system to representation is
investigated by comparing it with two of the state-of-the-art pseudo-image
representation methods. Then, the classifiers of SkelVit are realized in two
experimental setups by CNNs and VITs, and their performances are compared. In
the final experimental setup, the contribution of combining classifiers is
examined by applying the model with a different number of classifiers.
Experimental studies reveal that the proposed system with its lightweight
representation scheme achieves better results than the state-of-the-art
methods. It is also observed that the vision transformer is less sensitive to
the initial pseudo-image representation compared to CNN. Nevertheless, even
with the vision transformer, the recognition performance can be further
improved by the consensus of classifiers.",cs.CV cs.AI cs.LG cs.RO,2023-11-14
Manifold learning in Wasserstein space,,"This paper aims at building the theoretical foundations for manifold learning
algorithms in the space of absolutely continuous probability measures on a
compact and convex subset of $\mathbb{R}^d$, metrized with the Wasserstein-2
distance $\mathrm{W}$. We begin by introducing a construction of submanifolds
$\Lambda$ of probability measures equipped with metric $\mathrm{W}_\Lambda$,
the geodesic restriction of $W$ to $\Lambda$. In contrast to other
constructions, these submanifolds are not necessarily flat, but still allow for
local linearizations in a similar fashion to Riemannian submanifolds of
$\mathbb{R}^d$. We then show how the latent manifold structure of
$(\Lambda,\mathrm{W}_{\Lambda})$ can be learned from samples
$\{\lambda_i\}_{i=1}^N$ of $\Lambda$ and pairwise extrinsic Wasserstein
distances $\mathrm{W}$ only. In particular, we show that the metric space
$(\Lambda,\mathrm{W}_{\Lambda})$ can be asymptotically recovered in the sense
of Gromov--Wasserstein from a graph with nodes $\{\lambda_i\}_{i=1}^N$ and edge
weights $W(\lambda_i,\lambda_j)$. In addition, we demonstrate how the tangent
space at a sample $\lambda$ can be asymptotically recovered via spectral
analysis of a suitable ""covariance operator"" using optimal transport maps from
$\lambda$ to sufficiently close and diverse samples $\{\lambda_i\}_{i=1}^N$.
The paper closes with some explicit constructions of submanifolds $\Lambda$ and
numerical examples on the recovery of tangent spaces through spectral analysis.",stat.ML cs.LG math.DG,2023-11-14
Frozen Set Design for Precoded Polar Codes,,"This paper focuses on the frozen set design for precoded polar codes decoded
by the successive cancellation list (SCL) algorithm. We propose a novel frozen
set design method, whose computational complexity is low due to the use of
analytical bounds and constrained frozen set structure. We derive new bounds
based on the recently published complexity analysis of SCL decoding with near
maximum-likelihood (ML) performance. To predict the ML performance, we employ
the state-of-the-art bounds relying on the code weight distribution. The bounds
and constrained frozen set structure are incorporated into the genetic
algorithm to generate optimized frozen sets with low complexity. Our simulation
results show that the constructed precoded polar codes of length 512 have a
superior frame error rate (FER) performance compared to the state-of-the-art
codes under SCL decoding with various list sizes.",cs.IT math.IT,2023-11-16
Evolutionary game selection creates cooperative environments,,"The emergence of collective cooperation in competitive environments is a
well-known phenomenon in biology, economics, and social systems. While most
evolutionary game models focus on the evolution of strategies for a fixed game,
how strategic decisions coevolve with the environment has so far mostly been
overlooked. Here, we consider a game selection model where not only the
strategies but also the game can change over time following evolutionary
principles. Our results show that coevolutionary dynamics of games and
strategies can induce novel collective phenomena, fostering the emergence of
cooperative environments. When the model is taken on structured populations the
architecture of the interaction network can significantly amplify pro-social
behavior, with a critical role played by network heterogeneity and the presence
of clustered groups of similar players, distinctive features observed in
real-world populations. By unveiling the link between the evolution of
strategies and games for different structured populations, our model sheds new
light on the origin of social dilemmas ubiquitously observed in real-world
social systems.",physics.soc-ph cs.GT cs.SI math.DS q-bio.PE,2023-11-18
"A causal intervention framework for synthesizing mobility data and
  evaluating predictive neural networks",,"Deep neural networks are increasingly utilized in mobility prediction tasks,
yet their intricate internal workings pose challenges for interpretability,
especially in comprehending how various aspects of mobility behavior affect
predictions. This study introduces a causal intervention framework to assess
the impact of mobility-related factors on neural networks designed for next
location prediction -- a task focusing on predicting the immediate next
location of an individual. To achieve this, we employ individual mobility
models to synthesize location visit sequences and control behavior dynamics by
intervening in their data generation process. We evaluate the interventional
location sequences using mobility metrics and input them into well-trained
networks to analyze performance variations. The results demonstrate the
effectiveness in producing location sequences with distinct mobility behaviors,
thereby facilitating the simulation of diverse yet realistic spatial and
temporal changes. These changes result in performance fluctuations in next
location prediction networks, revealing impacts of critical mobility behavior
factors, including sequential patterns in location transitions, proclivity for
exploring new locations, and preferences in location choices at population and
individual levels. The gained insights hold value for the real-world
application of mobility prediction networks, and the framework is expected to
promote the use of causal inference to enhance the interpretability and
robustness of neural networks in mobility applications.",physics.soc-ph cs.LG cs.SI,2023-11-20
"Explainable Time Series Anomaly Detection using Masked Latent Generative
  Modeling",,"We present a novel time series anomaly detection method that achieves
excellent detection accuracy while offering a superior level of explainability.
Our proposed method, TimeVQVAE-AD, leverages masked generative modeling adapted
from the cutting-edge time series generation method known as TimeVQVAE. The
prior model is trained on the discrete latent space of a time-frequency domain.
Notably, the dimensional semantics of the time-frequency domain are preserved
in the latent space, enabling us to compute anomaly scores across different
frequency bands, which provides a better insight into the detected anomalies.
Additionally, the generative nature of the prior model allows for sampling
likely normal states for detected anomalies, enhancing the explainability of
the detected anomalies through counterfactuals. Our experimental evaluation on
the UCR Time Series Anomaly archive demonstrates that TimeVQVAE-AD
significantly surpasses the existing methods in terms of detection accuracy and
explainability. We provide our implementation on GitHub:
https://github.com/ML4ITS/TimeVQVAE-AnomalyDetection.",cs.LG stat.ML,2023-11-21
VALUED -- Vision and Logical Understanding Evaluation Dataset,,"Starting with early successes in computer vision tasks, deep learning based
techniques have since overtaken state of the art approaches in a multitude of
domains. However, it has been demonstrated time and again that these techniques
fail to capture semantic context and logical constraints, instead often relying
on spurious correlations to arrive at the answer. Since application of deep
learning techniques to critical scenarios are dependent on adherence to domain
specific constraints, several attempts have been made to address this issue.
One limitation holding back a thorough exploration of this area, is a lack of
suitable datasets which feature a rich set of rules. In order to address this,
we present the VALUE (Vision And Logical Understanding Evaluation) Dataset,
consisting of 200,000$+$ annotated images and an associated rule set, based on
the popular board game - chess. The curated rule set considerably constrains
the set of allowable predictions, and are designed to probe key semantic
abilities like localization and enumeration. Alongside standard metrics,
additional metrics to measure performance with regards to logical consistency
is presented. We analyze several popular and state of the art vision models on
this task, and show that, although their performance on standard metrics are
laudable, they produce a plethora of incoherent results, indicating that this
dataset presents a significant challenge for future works.",cs.CV cs.AI,2023-11-21
"Transferring to Real-World Layouts: A Depth-aware Framework for Scene
  Adaptation",,"Scene segmentation via unsupervised domain adaptation (UDA) enables the
transfer of knowledge acquired from source synthetic data to real-world target
data, which largely reduces the need for manual pixel-level annotations in the
target domain. To facilitate domain-invariant feature learning, existing
methods typically mix data from both the source domain and target domain by
simply copying and pasting the pixels. Such vanilla methods are usually
sub-optimal since they do not take into account how well the mixed layouts
correspond to real-world scenarios. Real-world scenarios are with an inherent
layout. We observe that semantic categories, such as sidewalks, buildings, and
sky, display relatively consistent depth distributions, and could be clearly
distinguished in a depth map. Based on such observation, we propose a
depth-aware framework to explicitly leverage depth estimation to mix the
categories and facilitate the two complementary tasks, i.e., segmentation and
depth learning in an end-to-end manner. In particular, the framework contains a
Depth-guided Contextual Filter (DCF) forndata augmentation and a cross-task
encoder for contextual learning. DCF simulates the real-world layouts, while
the cross-task encoder further adaptively fuses the complementing features
between two tasks. Besides, it is worth noting that several public datasets do
not provide depth annotation. Therefore, we leverage the off-the-shelf depth
estimation network to generate the pseudo depth. Extensive experiments show
that our proposed methods, even with pseudo depth, achieve competitive
performance on two widely-used bench-marks, i.e. 77.7 mIoU on GTA to Cityscapes
and 69.3 mIoU on Synthia to Cityscapes.",cs.CV,2023-11-21
"LeFi: Learn to Incentivize Federated Learning in Automotive Edge
  Computing",,"Federated learning (FL) is the promising privacy-preserve approach to
continually update the central machine learning (ML) model (e.g., object
detectors in edge servers) by aggregating the gradients obtained from local
observation data in distributed connected and automated vehicles (CAVs). The
incentive mechanism is to incentivize individual selfish CAVs to participate in
FL towards the improvement of overall model accuracy. It is, however,
challenging to design the incentive mechanism, due to the complex correlation
between the overall model accuracy and unknown incentive sensitivity of CAVs,
especially under the non-independent and identically distributed (Non-IID) data
of individual CAVs. In this paper, we propose a new learn-to-incentivize
algorithm to adaptively allocate rewards to individual CAVs under unknown
sensitivity functions. First, we gradually learn the unknown sensitivity
function of individual CAVs with accumulative observations, by using
compute-efficient Gaussian process regression (GPR). Second, we iteratively
update the reward allocation to individual CAVs with new sampled gradients,
derived from GPR. Third, we project the updated reward allocations to comply
with the total budget. We evaluate the performance of extensive simulations,
where the simulation parameters are obtained from realistic profiling of the
CIFAR-10 dataset and NVIDIA RTX 3080 GPU. The results show that our proposed
algorithm substantially outperforms existing solutions, in terms of accuracy,
scalability, and adaptability.",cs.NI,2023-11-21
"Towards Natural Language-Guided Drones: GeoText-1652 Benchmark with
  Spatial Relation Matching",,"Navigating drones through natural language commands remains challenging due
to the dearth of accessible multi-modal datasets and the stringent precision
requirements for aligning visual and textual data. To address this pressing
need, we introduce GeoText-1652, a new natural language-guided geo-localization
benchmark. This dataset is systematically constructed through an interactive
human-computer process leveraging Large Language Model (LLM) driven annotation
techniques in conjunction with pre-trained vision models. GeoText-1652 extends
the established University-1652 image dataset with spatial-aware text
annotations, thereby establishing one-to-one correspondences between image,
text, and bounding box elements. We further introduce a new optimization
objective to leverage fine-grained spatial associations, called blending
spatial matching, for region-level spatial relation matching. Extensive
experiments reveal that our approach maintains a competitive recall rate
comparing other prevailing cross-modality methods. This underscores the
promising potential of our approach in elevating drone control and navigation
through the seamless integration of natural language commands in real-world
scenarios.",cs.CV cs.MM,2023-11-21
"Localization from structured distance matrices via low-rank matrix
  recovery",,"We study the problem of determining the configuration of $n$ points by using
their distances to $m$ nodes, referred to as anchor nodes. One sampling scheme
is Nystrom sampling, which assumes known distances between the anchors and
between the anchors and the $n$ points, while the distances among the $n$
points are unknown. For this scheme, a simple adaptation of the Nystrom method,
which is often used for kernel approximation, is a viable technique to estimate
the configuration of the anchors and the $n$ points. In this manuscript, we
propose a modified version of Nystrom sampling, where the distances from every
node to one central node are known, but all other distances are incomplete. In
this setting, the standard Nystrom approach is not applicable, necessitating an
alternative technique to estimate the configuration of the anchors and the $n$
points. We show that this problem can be framed as the recovery of a low-rank
submatrix of a Gram matrix. Using synthetic and real data, we demonstrate that
the proposed approach can exactly recover configurations of points given
sufficient distance samples. This underscores that, in contrast to methods that
rely on global sampling of distance matrices, the task of estimating the
configuration of points can be done efficiently via structured sampling with
well-chosen reliable anchors. Finally, our main analysis is grounded in a
specific centering of the points. With this in mind, we extend previous work in
Euclidean distance geometry by providing a general dual basis approach for
points centered anywhere.",cs.IT cs.LG cs.RO eess.SP math.IT,2023-11-29
"NumCalc: An open source BEM code for solving acoustic scattering
  problems",,"The calculation of the acoustic field in or around objects is an important
task in acoustic engineering. To numerically solve this task, the boundary
element method (BEM) is a commonly used method especially for infinite domains.
The open source tool Mesh2HRTF and its BEM core NumCalc provide users with a
collection of free software for acoustic simulations without the need of having
an in-depth knowledge into numerical methods. However, we feel that users
should have a basic understanding with respect to the methods behind the
software they are using. We are convinced that this basic understanding helps
in avoiding common mistakes and also helps to understand the requirements to
use the software. To provide this background is the first motivation for this
paper. A second motivation for this paper is to demonstrate the accuracy of
NumCalc when solving benchmark problems. Thus, users can get an idea about the
accuracy they can expect when using NumCalc as well as the memory and CPU
requirements of NumCalc. A third motivation for this paper is to give users
detailed information about some parts of the actual implementation that are
usually not mentioned in literature, e.g., the specific version of the fast
multipole method and its clustering process or how to use frequency-dependent
admittance boundary conditions.",math.NA cs.NA,2023-09-25
"Scalable Bayesian uncertainty quantification with data-driven priors for
  radio interferometric imaging",,"Next-generation radio interferometers like the Square Kilometer Array have
the potential to unlock scientific discoveries thanks to their unprecedented
angular resolution and sensitivity. One key to unlocking their potential
resides in handling the deluge and complexity of incoming data. This challenge
requires building radio interferometric imaging methods that can cope with the
massive data sizes and provide high-quality image reconstructions with
uncertainty quantification (UQ). This work proposes a method coined QuantifAI
to address UQ in radio-interferometric imaging with data-driven (learned)
priors for high-dimensional settings. Our model, rooted in the Bayesian
framework, uses a physically motivated model for the likelihood. The model
exploits a data-driven convex prior, which can encode complex information
learned implicitly from simulations and guarantee the log-concavity of the
posterior. We leverage probability concentration phenomena of high-dimensional
log-concave posteriors that let us obtain information about the posterior,
avoiding MCMC sampling techniques. We rely on convex optimisation methods to
compute the MAP estimation, which is known to be faster and better scale with
dimension than MCMC sampling strategies. Our method allows us to compute local
credible intervals, i.e., Bayesian error bars, and perform hypothesis testing
of structure on the reconstructed image. In addition, we propose a novel
blazing-fast method to compute pixel-wise uncertainties at different scales. We
demonstrate our method by reconstructing radio-interferometric images in a
simulated setting and carrying out fast and scalable UQ, which we validate with
MCMC sampling. Our method shows an improved image quality and more meaningful
uncertainties than the benchmark method based on a sparsity-promoting prior.
QuantifAI's source code: https://github.com/astro-informatics/QuantifAI.",astro-ph.IM cs.LG,2023-11-30
Green Edge AI: A Contemporary Survey,,"Artificial intelligence (AI) technologies have emerged as pivotal enablers
across a multitude of industries largely due to their significant resurgence
over the past decade. The transformative power of AI is primarily derived from
the utilization of deep neural networks (DNNs), which require extensive data
for training and substantial computational resources for processing.
Consequently, DNN models are typically trained and deployed on resource-rich
cloud servers. However, due to potential latency issues associated with cloud
communications, deep learning (DL) workflows are increasingly being
transitioned to wireless edge networks in proximity to end-user devices (EUDs).
This shift is designed to support latency-sensitive applications and has given
rise to a new paradigm of edge AI, which will play a critical role in upcoming
sixth-generation (6G) networks to support ubiquitous AI applications. Despite
its considerable potential, edge AI faces substantial challenges, mostly due to
the dichotomy between the resource limitations of wireless edge networks and
the resource-intensive nature of DL. Specifically, the acquisition of
large-scale data, as well as the training and inference processes of DNNs, can
rapidly deplete the battery energy of EUDs. This necessitates an
energy-conscious approach to edge AI to ensure both optimal and sustainable
performance. In this paper, we present a contemporary survey on green edge AI.
We commence by analyzing the principal energy consumption components of edge AI
systems to identify the fundamental design principles of green edge AI. Guided
by these principles, we then explore energy-efficient design methodologies for
the three critical tasks in edge AI systems, including training data
acquisition, edge training, and edge inference. Finally, we underscore
potential future research directions to further enhance the energy efficiency
of edge AI.",cs.AI cs.IT cs.NI math.IT,2023-11-30
Robot Synesthesia: In-Hand Manipulation with Visuotactile Sensing,,"Executing contact-rich manipulation tasks necessitates the fusion of tactile
and visual feedback. However, the distinct nature of these modalities poses
significant challenges. In this paper, we introduce a system that leverages
visual and tactile sensory inputs to enable dexterous in-hand manipulation.
Specifically, we propose Robot Synesthesia, a novel point cloud-based tactile
representation inspired by human tactile-visual synesthesia. This approach
allows for the simultaneous and seamless integration of both sensory inputs,
offering richer spatial information and facilitating better reasoning about
robot actions. The method, trained in a simulated environment and then deployed
to a real robot, is applicable to various in-hand object rotation tasks.
Comprehensive ablations are performed on how the integration of vision and
touch can improve reinforcement learning and Sim2Real performance. Our project
page is available at https://yingyuan0414.github.io/visuotactile/ .",cs.RO cs.CV cs.LG,2023-12-04
"TriDeNT: Triple Deep Network Training for Privileged Knowledge
  Distillation in Histopathology",,"Computational pathology models rarely utilise data that will not be available
for inference. This means most models cannot learn from highly informative data
such as additional immunohistochemical (IHC) stains and spatial
transcriptomics. We present TriDeNT, a novel self-supervised method for
utilising privileged data that is not available during inference to improve
performance. We demonstrate the efficacy of this method for a range of
different paired data including immunohistochemistry, spatial transcriptomics
and expert nuclei annotations. In all settings, TriDeNT outperforms other
state-of-the-art methods in downstream tasks, with observed improvements of up
to 101%. Furthermore, we provide qualitative and quantitative measurements of
the features learned by these models and how they differ from baselines.
TriDeNT offers a novel method to distil knowledge from scarce or costly data
during training, to create significantly better models for routine inputs.",cs.CV cs.AI cs.LG q-bio.TO,2023-12-04
iMatching: Imperative Correspondence Learning,,"Learning feature correspondence is a foundational task in computer vision,
holding immense importance for downstream applications such as visual odometry
and 3D reconstruction. Despite recent progress in data-driven models, feature
correspondence learning is still limited by the lack of accurate per-pixel
correspondence labels. To overcome this difficulty, we introduce a new
self-supervised scheme, imperative learning (IL), for training feature
correspondence. It enables correspondence learning on arbitrary uninterrupted
videos without any camera pose or depth labels, heralding a new era for
self-supervised correspondence learning. Specifically, we formulated the
problem of correspondence learning as a bilevel optimization, which takes the
reprojection error from bundle adjustment as a supervisory signal for the
model. To avoid large memory and computation overhead, we leverage the
stationary point to effectively back-propagate the implicit gradients through
bundle adjustment. Through extensive experiments, we demonstrate superior
performance on tasks including feature matching and pose estimation, in which
we obtained an average of 30% accuracy gain over the state-of-the-art matching
models. This preprint corresponds to the Accepted Manuscript in European
Conference on Computer Vision (ECCV) 2024.",cs.CV,2023-12-04
"DGInStyle: Domain-Generalizable Semantic Segmentation with Image
  Diffusion Models and Stylized Semantic Control",,"Large, pretrained latent diffusion models (LDMs) have demonstrated an
extraordinary ability to generate creative content, specialize to user data
through few-shot fine-tuning, and condition their output on other modalities,
such as semantic maps. However, are they usable as large-scale data generators,
e.g., to improve tasks in the perception stack, like semantic segmentation? We
investigate this question in the context of autonomous driving, and answer it
with a resounding ""yes"". We propose an efficient data generation pipeline
termed DGInStyle. First, we examine the problem of specializing a pretrained
LDM to semantically-controlled generation within a narrow domain. Second, we
propose a Style Swap technique to endow the rich generative prior with the
learned semantic control. Third, we design a Multi-resolution Latent Fusion
technique to overcome the bias of LDMs towards dominant objects. Using
DGInStyle, we generate a diverse dataset of street scenes, train a
domain-agnostic semantic segmentation model on it, and evaluate the model on
multiple popular autonomous driving datasets. Our approach consistently
increases the performance of several domain generalization methods compared to
the previous state-of-the-art methods. The source code and the generated
dataset are available at https://dginstyle.github.io.",cs.CV,2023-12-05
Lite-Mind: Towards Efficient and Robust Brain Representation Network,,"The limited data availability and the low signal-to-noise ratio of fMRI
signals lead to the challenging task of fMRI-to-image retrieval.
State-of-the-art MindEye remarkably improves fMRI-to-image retrieval
performance by leveraging a large model, i.e., a 996M MLP Backbone per subject,
to align fMRI embeddings to the final hidden layer of CLIP's Vision Transformer
(ViT). However, significant individual variations exist among subjects, even
under identical experimental setups, mandating the training of large
subject-specific models. The substantial parameters pose significant challenges
in deploying fMRI decoding on practical devices. To this end, we propose
Lite-Mind, a lightweight, efficient, and robust brain representation learning
paradigm based on Discrete Fourier Transform (DFT), which efficiently aligns
fMRI voxels to fine-grained information of CLIP. We elaborately design a DFT
backbone with Spectrum Compression and Frequency Projector modules to learn
informative and robust voxel embeddings. Our experiments demonstrate that
Lite-Mind achieves an impressive 94.6% fMRI-to-image retrieval accuracy on the
NSD dataset for Subject 1, with 98.7% fewer parameters than MindEye. Lite-Mind
is also proven to be able to be migrated to smaller fMRI datasets and
establishes a new state-of-the-art for zero-shot classification on the GOD
dataset.",cs.CV cs.AI,2023-12-06
"A Balanced Positional Control Architecture for a 12-DoF Quadruped Robot
  through Simulation-validation and Hardware Testing",,"A multi-joint enabled robot requires extensive mathematical calculations to
determine the end effector's position with respect to the other connective
joints involved and their corresponding frames in a specific coordinate system.
If a control architecture employs fewer positional constraints which cannot
precisely determine the end effector's position in all quadrants of a 2D
Cartesian plane then the robot is generally under-constrained, leading to
challenges in accurate positioning to the end-effector across the entire plane.
Consequently, only a subset of the end effector's degree of freedom (DoF) can
be assigned for the robot's leg position for pose and trajectory estimation
purposes. This paper introduces a novel approach and proposes an algorithm to
consider a balanced control of the robot's leg position in a coordinate system
so the robot's leg can be precisely determined and the DoF is not limited.
Mathematical derivation of the joint angles is derived with forward and inverse
kinematics, and Python-based simulation has been done to verify and simulate
the robot's locomotion. Using Python-based code for serial communication with a
micro-controller unit makes this approach more effective for demonstrating its
application on a prototype leg its movement has been realized. The experimental
prototype leg exhibits a commendable 78.9% accuracy with the simulated result,
validating the robustness of our algorithm in practical scenarios. A
comprehensive assessment of the control algorithm with random and continuous
data point test has been conducted to ensure performance, so the algorithm can
as well be deployed in a physical robot.",cs.RO cs.SY eess.SY,2023-12-11
"AM-RADIO: Agglomerative Vision Foundation Model -- Reduce All Domains
  Into One",,"A handful of visual foundation models (VFMs) have recently emerged as the
backbones for numerous downstream tasks. VFMs like CLIP, DINOv2, SAM are
trained with distinct objectives, exhibiting unique characteristics for various
downstream tasks. We find that despite their conceptual differences, these
models can be effectively merged into a unified model through multi-teacher
distillation. We name this approach AM-RADIO (Agglomerative Model -- Reduce All
Domains Into One). This integrative approach not only surpasses the performance
of individual teacher models but also amalgamates their distinctive features,
such as zero-shot vision-language comprehension, detailed pixel-level
understanding, and open vocabulary segmentation capabilities. In pursuit of the
most hardware-efficient backbone, we evaluated numerous architectures in our
multi-teacher distillation pipeline using the same training recipe. This led to
the development of a novel architecture (E-RADIO) that exceeds the performance
of its predecessors and is at least 7x faster than the teacher models. Our
comprehensive benchmarking process covers downstream tasks including ImageNet
classification, ADE20k semantic segmentation, COCO object detection and
LLaVa-1.5 framework.
  Code: https://github.com/NVlabs/RADIO",cs.CV,2023-12-10
Semantics-Division Duplexing: A Novel Full-Duplex Paradigm,,"In-band full-duplex (IBFD) is a theoretically effective solution to increase
the overall throughput for the future wireless communications system by
enabling transmission and reception over the same time-frequency resources.
However, reliable source reconstruction remains a great challenge in the
practical IBFD systems due to the non-ideal elimination of the
self-interference and the inherent limitations of the separate source and
channel coding methods. On the other hand, artificial intelligence-enabled
semantic communication can provide a viable direction for the optimization of
the IBFD system. This article introduces a novel IBFD paradigm with the
guidance of semantic communication called semantics-division duplexing (SDD).
It utilizes semantic domain processing to further suppress self-interference,
distinguish the expected semantic information, and recover the desired sources.
Further integration of the digital and semantic domain processing can be
implemented so as to achieve intelligent and concise communications. We present
the advantages of the SDD paradigm with theoretical explanations and provide
some visualized results to verify its effectiveness.",cs.IT eess.SP math.IT,2023-12-14
"The DSA Transparency Database: Auditing Self-reported Moderation Actions
  by Social Media",,"Since September 2023, the Digital Services Act (DSA) obliges large online
platforms to submit detailed data on each moderation action they take within
the European Union (EU) to the DSA Transparency Database. From its inception,
this centralized database has sparked scholarly interest as an unprecedented
and potentially unique trove of data on real-world online moderation. Here, we
thoroughly analyze all 353.12M records submitted by the eight largest social
media platforms in the EU during the first 100 days of the database.
Specifically, we conduct a platform-wise comparative study of their: volume of
moderation actions, grounds for decision, types of applied restrictions, types
of moderated content, timeliness in undertaking and submitting moderation
actions, and use of automation. Furthermore, we systematically cross-check the
contents of the database with the platforms' own transparency reports. Our
analyses reveal that (i) the platforms adhered only in part to the philosophy
and structure of the database, (ii) the structure of the database is partially
inadequate for the platforms' reporting needs, (iii) the platforms exhibited
substantial differences in their moderation actions, (iv) a remarkable fraction
of the database data is inconsistent, (v) the platform X (formerly Twitter)
presents the most inconsistencies. Our findings have far-reaching implications
for policymakers and scholars across diverse disciplines. They offer guidance
for future regulations that cater to the reporting needs of online platforms in
general, but also highlight opportunities to improve and refine the database
itself.",cs.SI cs.AI cs.CY cs.HC,2023-12-15
"Unstructured Moving Least Squares Material Point Methods: A Stable
  Kernel Approach With Continuous Gradient Reconstruction on General
  Unstructured Tessellations",,"The Material Point Method (MPM) is a hybrid Eulerian Lagrangian simulation
technique for solid mechanics with significant deformation. Structured
background grids are commonly employed in the standard MPM, but they may give
rise to several accuracy problems in handling complex geometries. When using
(2D) unstructured triangular or (3D) tetrahedral background elements, however,
significant challenges arise (\eg, cell-crossing error). Substantial numerical
errors develop due to the inherent $\mathcal{C}^0$ continuity property of the
interpolation function, which causes discontinuous gradients across element
boundaries. Prior efforts in constructing $\mathcal{C}^1$ continuous
interpolation functions have either not been adapted for unstructured grids or
have only been applied to 2D triangular meshes. In this study, an Unstructured
Moving Least Squares MPM (UMLS-MPM) is introduced to accommodate 2D and 3D
simplex tessellation. The central idea is to incorporate a diminishing function
into the sample weights of the MLS kernel, ensuring an analytically continuous
velocity gradient estimation. Numerical analyses confirm the method's
capability in mitigating cell crossing inaccuracies and realizing expected
convergence.",cs.CE,2023-12-16
"M^2ConceptBase: A Fine-Grained Aligned Concept-Centric Multimodal
  Knowledge Base",,"Multimodal knowledge bases (MMKBs) provide cross-modal aligned knowledge
crucial for multimodal tasks. However, the images in existing MMKBs are
generally collected for entities in encyclopedia knowledge graphs. Therefore,
detailed groundings of visual semantics with linguistic concepts are lacking,
which are essential for the visual concept cognition ability of multimodal
models. Addressing this gap, we introduce M^2ConceptBase, the first
concept-centric MMKB. M^2ConceptBase models concepts as nodes with associated
images and detailed textual descriptions. We propose a context-aware multimodal
symbol grounding approach to align concept-image and concept-description pairs
using context information from image-text datasets. Comprising 951K images and
152K concepts, M^2ConceptBase links each concept to an average of 6.27 images
and a single description, ensuring comprehensive visual and textual semantics.
Human studies confirm more than 95% alignment accuracy, underscoring its
quality. Additionally, our experiments demonstrate that M^2ConceptBase
significantly enhances VQA model performance on the OK-VQA task. M^2ConceptBase
also substantially improves the fine-grained concept understanding capabilities
of multimodal large language models through retrieval augmentation in two
concept-related tasks, highlighting its value.",cs.AI,2023-12-16
An Algebraic Approach to the Longest Path Problem,,"The Longest Path Problem is a question of finding the maximum length between
pairs of vertices of a graph. In the general case, the problem is NP-complete.
However, there is a small collection of graph classes for which there exists an
efficient solution. Current approaches involve either approximation or
computational enumeration. For Tree-like classes of graphs, there are
approximation and enumeration algorithms which solves the problem efficiently.
Despite this, we propose a new method of approaching the longest path problem
with exact algebraic solutions that give rise to polynomial-time algorithms.
Our method provides algorithms that are proven correct by their underlying
algebraic operations unlike existing purely algorithmic solutions to this
problem. We introduce a `booleanize' mapping on the adjacency matrix of a graph
which we prove identifies the solution for trees, uniform block graphs, block
graphs, and directed acyclic graphs with exact conditions and associated
polynomial-time algorithms. In addition, we display additional algorithms that
can generate every possible longest path of acyclic graphs in efficient time,
as well as for block graphs.",cs.DS cs.DM math.CO,2023-11-14
"LLM in a flash: Efficient Large Language Model Inference with Limited
  Memory",,"Large language models (LLMs) are central to modern natural language
processing, delivering exceptional performance in various tasks. However, their
substantial computational and memory requirements present challenges,
especially for devices with limited DRAM capacity. This paper tackles the
challenge of efficiently running LLMs that exceed the available DRAM capacity
by storing the model parameters in flash memory, but bringing them on demand to
DRAM. Our method involves constructing an inference cost model that takes into
account the characteristics of flash memory, guiding us to optimize in two
critical areas: reducing the volume of data transferred from flash and reading
data in larger, more contiguous chunks. Within this hardware-informed
framework, we introduce two principal techniques. First, ""windowing""
strategically reduces data transfer by reusing previously activated neurons,
and second, ""row-column bundling"", tailored to the sequential data access
strengths of flash memory, increases the size of data chunks read from flash
memory. These methods collectively enable running models up to twice the size
of the available DRAM, with a 4-5x and 20-25x increase in inference speed
compared to naive loading approaches in CPU and GPU, respectively. Our
integration of sparsity awareness, context-adaptive loading, and a
hardware-oriented design paves the way for effective inference of LLMs on
devices with limited memory.",cs.CL cs.AI cs.LG,2023-12-12
"KGLens: Towards Efficient and Effective Knowledge Probing of Large
  Language Models with Knowledge Graphs",,"Large Language Models (LLMs) might hallucinate facts, while curated Knowledge
Graph (KGs) are typically factually reliable especially with domain-specific
knowledge. Measuring the alignment between KGs and LLMs can effectively probe
the factualness and identify the knowledge blind spots of LLMs. However,
verifying the LLMs over extensive KGs can be expensive. In this paper, we
present KGLens, a Thompson-sampling-inspired framework aimed at effectively and
efficiently measuring the alignment between KGs and LLMs. KGLens features a
graph-guided question generator for converting KGs into natural language, along
with a carefully designed importance sampling strategy based on parameterized
KG structure to expedite KG traversal. Our simulation experiment compares the
brute force method with KGLens under six different sampling methods,
demonstrating that our approach achieves superior probing efficiency.
Leveraging KGLens, we conducted in-depth analyses of the factual accuracy of
ten LLMs across three large domain-specific KGs from Wikidata, composing over
19K edges, 700 relations, and 21K entities. Human evaluation results indicate
that KGLens can assess LLMs with a level of accuracy nearly equivalent to that
of human annotators, achieving 95.7% of the accuracy rate.",cs.AI cs.CL cs.LG,2023-12-15
"A Dual-way Enhanced Framework from Text Matching Point of View for
  Multimodal Entity Linking",,"Multimodal Entity Linking (MEL) aims at linking ambiguous mentions with
multimodal information to entity in Knowledge Graph (KG) such as Wikipedia,
which plays a key role in many applications. However, existing methods suffer
from shortcomings, including modality impurity such as noise in raw image and
ambiguous textual entity representation, which puts obstacles to MEL. We
formulate multimodal entity linking as a neural text matching problem where
each multimodal information (text and image) is treated as a query, and the
model learns the mapping from each query to the relevant entity from candidate
entities. This paper introduces a dual-way enhanced (DWE) framework for MEL:
(1) our model refines queries with multimodal data and addresses semantic gaps
using cross-modal enhancers between text and image information. Besides, DWE
innovatively leverages fine-grained image attributes, including facial
characteristic and scene feature, to enhance and refine visual features. (2)By
using Wikipedia descriptions, DWE enriches entity semantics and obtains more
comprehensive textual representation, which reduces between textual
representation and the entities in KG. Extensive experiments on three public
benchmarks demonstrate that our method achieves state-of-the-art (SOTA)
performance, indicating the superiority of our model. The code is released on
https://github.com/season1blue/DWE",cs.AI cs.CV,2023-12-18
Meili: Enabling SmartNIC as a Service in the Cloud,,"SmartNICs are touted as an attractive substrate for network application
offloading, offering benefits in programmability, host resource saving, and
energy efficiency. The current usage restricts offloading to local hosts and
confines SmartNIC ownership to individual application teams, resulting in poor
resource efficiency and scalability. This paper presents Meili, a novel system
that realizes SmartNIC as a service to address these issues. Meili organizes
heterogeneous SmartNIC resources as a pool and offers a unified one-NIC
abstraction to application developers. This allows developers to focus solely
on the application logic while dynamically optimizing their performance needs.
Our evaluation on NVIDIA BlueField series and AMD Pensando SmartNICs
demonstrates that Meili achieves scalable single-flow throughput with a maximum
8 {\mu}s latency overhead and enhances resource efficiency by 3.07$\times$
compared to standalone deployments and 1.44$\times$ compared to
state-of-the-art microservice deployments.",cs.NI cs.DC,2023-12-19
"Moving a Derivation Along a Derivation Preserves the Spine in Adhesive
  Categories",,"In this paper, we investigate the relationship between two elementary
operations on derivations in adhesive high-level replacement systems that are
well-known in the context of graph transformation: moving a derivation along a
derivation based on parallel and sequential independence on one hand and
restriction of a derivation with respect to a monomorphism into the start
object on the other hand. Intuitively, a restriction clips off parts of the
start object that are never matched by a rule application throughout the
derivation on the other hand. As main result, it is shown that moving a
derivation preserves its spine being the minimal restriction.",cs.DM,2023-12-20
"Ponymation: Learning Articulated 3D Animal Motions from Unlabeled Online
  Videos",,"We introduce a new method for learning a generative model of articulated 3D
animal motions from raw, unlabeled online videos. Unlike existing approaches
for 3D motion synthesis, our model requires no pose annotations or parametric
shape models for training; it learns purely from a collection of unlabeled web
video clips, leveraging semantic correspondences distilled from self-supervised
image features. At the core of our method is a video Photo-Geometric
Auto-Encoding framework that decomposes each training video clip into a set of
explicit geometric and photometric representations, including a rest-pose 3D
shape, an articulated pose sequence, and texture, with the objective of
re-rendering the input video via a differentiable renderer. This decomposition
allows us to learn a generative model over the underlying articulated pose
sequences akin to a Variational Auto-Encoding (VAE) formulation, but without
requiring any external pose annotations. At inference time, we can generate new
motion sequences by sampling from the learned motion VAE, and create plausible
4D animations of an animal automatically within seconds given a single input
image.",cs.CV,2023-12-21
"Neural Point Cloud Diffusion for Disentangled 3D Shape and Appearance
  Generation",,"Controllable generation of 3D assets is important for many practical
applications like content creation in movies, games and engineering, as well as
in AR/VR. Recently, diffusion models have shown remarkable results in
generation quality of 3D objects. However, none of the existing models enable
disentangled generation to control the shape and appearance separately. For the
first time, we present a suitable representation for 3D diffusion models to
enable such disentanglement by introducing a hybrid point cloud and neural
radiance field approach. We model a diffusion process over point positions
jointly with a high-dimensional feature space for a local density and radiance
decoder. While the point positions represent the coarse shape of the object,
the point features allow modeling the geometry and appearance details. This
disentanglement enables us to sample both independently and therefore to
control both separately. Our approach sets a new state of the art in generation
compared to previous disentanglement-capable methods by reduced FID scores of
30-90% and is on-par with other non disentanglement-capable state-of-the art
methods.",cs.CV,2023-12-21
"Attribute-driven Disentangled Representation Learning for Multimodal
  Recommendation",,"Recommendation algorithms forecast user preferences by correlating user and
item representations derived from historical interaction patterns. In pursuit
of enhanced performance, many methods focus on learning robust and independent
representations by disentangling the intricate factors within interaction data
across various modalities in an unsupervised manner. However, such an approach
obfuscates the discernment of how specific factors (e.g., category or brand)
influence the outcomes, making it challenging to regulate their effects. In
response to this challenge, we introduce a novel method called Attribute-Driven
Disentangled Representation Learning (short for AD-DRL), which explicitly
incorporates attributes from different modalities into the disentangled
representation learning process. By assigning a specific attribute to each
factor in multimodal features, AD-DRL can disentangle the factors at both
attribute and attribute-value levels. To obtain robust and independent
representations for each factor associated with a specific attribute, we first
disentangle the representations of features both within and across different
modalities. Moreover, we further enhance the robustness of the representations
by fusing the multimodal features of the same factor. Empirical evaluations
conducted on three public real-world datasets substantiate the effectiveness of
AD-DRL, as well as its interpretability and controllability.",cs.IR cs.MM,2023-12-21
"Quantum-Assisted Joint Caching and Power Allocation for Integrated
  Satellite-Terrestrial Networks",,"Low earth orbit (LEO) satellite network can complement terrestrial networks
for achieving global wireless coverage and improving delay-sensitive Internet
services. This paper proposes an integrated satellite-terrestrial network
(ISTN) architecture to provide ground users with seamless and reliable content
delivery services. For optimal service provisioning in this architecture, we
formulate an optimization model to maximize the network throughput by jointly
optimizing content delivery policy, cache placement, and transmission power
allocation. The resulting optimization model is a large-scale mixed-integer
nonlinear program (MINLP) that is intractable for classical computer solvers.
Inspired by quantum computing techniques, we propose a hybrid quantum-classical
generalized Benders' decomposition (HQCGBD) algorithm to address this
challenge. Specifically, we first exploit the generalized Benders'
decomposition (GBD) to decompose the problem into a master problem and a
subproblem and then leverage the state-of-art quantum annealer to solve the
challenging master problem.",cs.NI eess.SP,2023-12-22
"Variational approximation for a non-isothermal coupled phase-field
  system: Structure-preservation & Nonlinear stability",,"A Cahn-Hilliard-Allen-Cahn phase-field model coupled with a heat transfer
equation, particularly with full non-diagonal mobility matrices, is studied.
After reformulating the problem w.r.t. the inverse of temperature, we proposed
and analysed a structure-preserving approximation for the semi-discretisation
in space and then a fully discrete approximation using conforming finite
elements and time-stepping methods. We prove structure-preserving property and
discrete stability using relative entropy methods for the semi-discrete and
fully discrete case. The theoretical results are illustrated by numerical
experiments.",math.NA cs.NA math.AP,2023-12-22
On the expressive power of inquisitive epistemic logic,,"Inquisitive modal logic, InqML, in its epistemic incarnation, extends
standard epistemic logic to capture not just the information that agents have,
but also the questions that they are interested in. We use the natural notion
of bisimulation equivalence in the setting of InqML, as introduced in
[Ciardelli/Otto: JSL 2021], to characterise the expressiveness of InqML as the
bisimulation invariant fragment of first-order logic over natural classes of
two-sorted first-order structures that arise as relational encodings of
inquisitive epistemic (S5-like) models. The non-elementary nature of these
classes crucially requires non-classical model-theoretic methods for the
analysis of first-order expressiveness, irrespective of whether we aim for
characterisations in the sense of classical or of finite model theory.",math.LO cs.LO,2023-12-22
"Error-Correction Performance of Regular Ring-Linear LDPC Codes over Lee
  Channels",,"Most low-density parity-check (LDPC) code constructions are considered over
finite fields. In this work, we focus on regular LDPC codes over integer
residue rings and analyze their performance with respect to the Lee metric.
Their error-correction performance is studied over two channel models, in the
Lee metric. The first channel model is a discrete memoryless channel, whereas
in the second channel model an error vector is drawn uniformly at random from
all vectors of a fixed Lee weight. It is known that the two channel laws
coincide in the asymptotic regime, meaning that their marginal distributions
match. For both channel models, we derive upper bounds on the block error
probability in terms of a random coding union bound as well as sphere packing
bounds that make use of the marginal distribution of the considered channels.
We estimate the decoding error probability of regular LDPC code ensembles over
the channels using the marginal distribution and determining the expected Lee
weight distribution of a random LDPC code over a finite integer ring. By means
of density evolution and finite-length simulations, we estimate the
error-correction performance of selected LDPC code ensembles under belief
propagation decoding and a low-complexity symbol message passing decoding
algorithm and compare the performances. The analysis developed in this paper
may serve to design regular LDPC codes over integer residue rings for storage
and cryptographic application.",cs.IT math.IT,2023-12-22
Stochastic models of memristive behavior,,"Under normal operations, memristive devices undergo variability in time and
space and have internal dynamics. Interplay of memory and stochastic signal
processing in memristive devices makes them candidates for performing
bio-inspired tasks of information transduction and transformation, where
intrinsic random behavior can be harnessed for high performance of circuits
built up of individual memory storing elements. The paper discusses models of
single memristive devices exhibiting both - dynamic hysteresis and Stochastic
Resonance, addressing also the cooperative effect of correlated noises acting
on the system and occurrence of dirty hysteretic rounding.",cs.ET physics.class-ph,2023-12-23
"Optimal Decision Tree and Adaptive Submodular Ranking with Noisy
  Outcomes",,"In pool-based active learning, the learner is given an unlabeled data set and
aims to efficiently learn the unknown hypothesis by querying the labels of the
data points. This can be formulated as the classical Optimal Decision Tree
(ODT) problem: Given a set of tests, a set of hypotheses, and an outcome for
each pair of test and hypothesis, our objective is to find a low-cost testing
procedure (i.e., decision tree) that identifies the true hypothesis. This
optimization problem has been extensively studied under the assumption that
each test generates a deterministic outcome. However, in numerous applications,
for example, clinical trials, the outcomes may be uncertain, which renders the
ideas from the deterministic setting invalid. In this work, we study a
fundamental variant of the ODT problem in which some test outcomes are noisy,
even in the more general case where the noise is persistent, i.e., repeating a
test gives the same noisy output. Our approximation algorithms provide
guarantees that are nearly best possible and hold for the general case of a
large number of noisy outcomes per test or per hypothesis where the performance
degrades continuously with this number. We numerically evaluated our algorithms
for identifying toxic chemicals and learning linear classifiers, and observed
that our algorithms have costs very close to the information-theoretic minimum.",cs.LG stat.ML,2023-12-23
Can ChatGPT Read Who You Are?,,"The interplay between artificial intelligence (AI) and psychology,
particularly in personality assessment, represents an important emerging area
of research. Accurate personality trait estimation is crucial not only for
enhancing personalization in human-computer interaction but also for a wide
variety of applications ranging from mental health to education. This paper
analyzes the capability of a generic chatbot, ChatGPT, to effectively infer
personality traits from short texts. We report the results of a comprehensive
user study featuring texts written in Czech by a representative population
sample of 155 participants. Their self-assessments based on the Big Five
Inventory (BFI) questionnaire serve as the ground truth. We compare the
personality trait estimations made by ChatGPT against those by human raters and
report ChatGPT's competitive performance in inferring personality traits from
text. We also uncover a 'positivity bias' in ChatGPT's assessments across all
personality dimensions and explore the impact of prompt composition on
accuracy. This work contributes to the understanding of AI capabilities in
psychological assessment, highlighting both the potential and limitations of
using large language models for personality inference. Our research underscores
the importance of responsible AI development, considering ethical implications
such as privacy, consent, autonomy, and bias in AI applications.",cs.CY cs.CL cs.HC,2023-12-26
Citizen science for social physics: Digital tools and participation,,"Social physics is an active and diverse field in which many scientists with
formal training in physics study a broad class of complex social phenomena.
Social physics investigates societal problems but most often does not count on
the active and conscious participation of the citizens. We here want to support
the idea that citizen science, and more particularly citizen social science,
can contribute to the broad field of social physics. We do so by sharing some
of our own experiences during the last decade. We first describe several human
mobility experiments in urban contexts with the participation of concerned
young students, old women or other different groups of neighbours. We second
share how we have studied community mental health care provision in
collaboration with a civil society organisation and with the intense
involvement of persons with lived experience in mental health. In both cases,
we narrow down the discussion to digital tools being used and the involved
participatory dynamics. In this way, we share key learnings to enhance a
synergistic relationship between social physics and citizen science and with
the aim increase the societal impact of the research on complex social
phenomena.",physics.soc-ph cond-mat.stat-mech cs.HC,2023-12-27
On-Demand JSON: A Better Way to Parse Documents?,,"JSON is a popular standard for data interchange on the Internet. Ingesting
JSON documents can be a performance bottleneck. A popular parsing strategy
consists in converting the input text into a tree-based data structure --
sometimes called a Document Object Model or DOM. We designed and implemented a
novel JSON parsing interface -- called On-Demand -- that appears to the
programmer like a conventional DOM-based approach. However, the underlying
implementation is a pointer iterating through the content, only materializing
the results (objects, arrays, strings, numbers) lazily.On recent commodity
processors, an implementation of our approach provides superior performance in
multiple benchmarks. To ensure reproducibility, our work is freely available as
open source software. Several systems use On-Demand: e.g., Apache Doris, the
Node.js JavaScript runtime, Milvus, and Velox.",cs.DB cs.PF,2023-12-28
On Completely Edge-Independent Spanning Trees in Locally Twisted Cubes,,"A network can contain numerous spanning trees. If two spanning trees
$T_i,T_j$ do not share any common edges, $T_i$ and $T_j$ are said to be
pairwisely edge-disjoint. For spanning trees $T_1, T_2, ..., T_m$, if every two
of them are pairwisely edge-disjoint, they are called completely
edge-independent spanning trees (CEISTs for short). CEISTs can facilitate many
network functionalities, and constructing CEISTs as maximally allowed as
possible in a given network is a worthy undertaking. In this paper, we
establish the maximal number of CEISTs in the locally twisted cube network, and
propose an algorithm to construct $\lfloor \frac{n}{2} \rfloor$ CEISTs in
$LTQ_n$, the $n$-dimensional locally twisted cube. The proposed algorithm has
been actually implemented, and we present the outputs. Network broadcasting in
the $LTQ_n$ was simulated using $\lfloor\frac{n}{2}\rfloor$ CEISTs, and the
performance compared with broadcasting using a single tree.",cs.DC cs.DM,2024-01-03
"Narrowing the Knowledge Evaluation Gap: Open-Domain Question Answering
  with Multi-Granularity Answers",,"Factual questions typically can be answered correctly at different levels of
granularity. For example, both ``August 4, 1961'' and ``1961'' are correct
answers to the question ``When was Barack Obama born?''. Standard question
answering (QA) evaluation protocols, however, do not explicitly take this into
account and compare a predicted answer against answers of a single granularity
level. In this work, we propose GRANOLA QA, a novel evaluation setting where a
predicted answer is evaluated in terms of accuracy and informativeness against
a set of multi-granularity answers. We present a simple methodology for
enriching existing datasets with multi-granularity answers, and create
GRANOLA-EQ, a multi-granularity version of the EntityQuestions dataset. We
evaluate a range of decoding methods on GRANOLA-EQ, including a new algorithm,
called Decoding with Response Aggregation (DRAG), that is geared towards
aligning the response granularity with the model's uncertainty. Our experiments
show that large language models with standard decoding tend to generate
specific answers, which are often incorrect. In contrast, when evaluated on
multi-granularity answers, DRAG yields a nearly 20 point increase in accuracy
on average, which further increases for rare entities. Overall, this reveals
that standard evaluation and decoding schemes may significantly underestimate
the knowledge encapsulated in LMs.",cs.CL,2024-01-09
"HiMTM: Hierarchical Multi-Scale Masked Time Series Modeling with
  Self-Distillation for Long-Term Forecasting",,"Time series forecasting is a critical and challenging task in practical
application. Recent advancements in pre-trained foundation models for time
series forecasting have gained significant interest. However, current methods
often overlook the multi-scale nature of time series, which is essential for
accurate forecasting. To address this, we propose HiMTM, a hierarchical
multi-scale masked time series modeling with self-distillation for long-term
forecasting. HiMTM integrates four key components: (1) hierarchical multi-scale
transformer (HMT) to capture temporal information at different scales; (2)
decoupled encoder-decoder (DED) that directs the encoder towards feature
extraction while the decoder focuses on pretext tasks; (3) hierarchical
self-distillation (HSD) for multi-stage feature-level supervision signals
during pre-training; and (4) cross-scale attention fine-tuning (CSA-FT) to
capture dependencies between different scales for downstream tasks. These
components collectively enhance multi-scale feature extraction in masked time
series modeling, improving forecasting accuracy. Extensive experiments on seven
mainstream datasets show that HiMTM surpasses state-of-the-art self-supervised
and end-to-end learning methods by a considerable margin of 3.16-68.54\%.
Additionally, HiMTM outperforms the latest robust self-supervised learning
method, PatchTST, in cross-domain forecasting by a significant margin of 2.3\%.
The effectiveness of HiMTM is further demonstrated through its application in
natural gas demand forecasting.",cs.LG,2024-01-10
"BOK-VQA: Bilingual outside Knowledge-Based Visual Question Answering via
  Graph Representation Pretraining",,"The current research direction in generative models, such as the recently
developed GPT4, aims to find relevant knowledge information for multimodal and
multilingual inputs to provide answers. Under these research circumstances, the
demand for multilingual evaluation of visual question answering (VQA) tasks, a
representative task of multimodal systems, has increased. Accordingly, we
propose a bilingual outside-knowledge VQA (BOK-VQA) dataset in this study that
can be extended to multilingualism. The proposed data include 17K images, 17K
question-answer pairs for both Korean and English and 280K instances of
knowledge information related to question-answer content. We also present a
framework that can effectively inject knowledge information into a VQA system
by pretraining the knowledge information of BOK-VQA data in the form of graph
embeddings. Finally, through in-depth analysis, we demonstrated the actual
effect of the knowledge information contained in the constructed training data
on VQA.",cs.CL,2024-01-12
Modeling Latent Selection with Structural Causal Models,,"Selection bias is ubiquitous in real-world data, and can lead to misleading
results if not dealt with properly. We introduce a conditioning operation on
Structural Causal Models (SCMs) to model latent selection from a causal
perspective. We show that the conditioning operation transforms an SCM with the
presence of an explicit latent selection mechanism into an SCM without such
selection mechanism, which partially encodes the causal semantics of the
selected subpopulation according to the original SCM. Furthermore, we show that
this conditioning operation preserves the simplicity, acyclicity, and linearity
of SCMs, and commutes with marginalization. Thanks to these properties,
combined with marginalization and intervention, the conditioning operation
offers a valuable tool for conducting causal reasoning tasks within causal
models where latent details have been abstracted away. We demonstrate by
example how classical results of causal inference can be generalized to include
selection bias and how the conditioning operation helps with modeling of
real-world problems.",cs.AI cs.LG math.ST stat.ME stat.ML stat.TH,2024-01-12
"Harnessing Deep Learning and Satellite Imagery for Post-Buyout Land
  Cover Mapping",,"Environmental disasters such as floods, hurricanes, and wildfires have
increasingly threatened communities worldwide, prompting various mitigation
strategies. Among these, property buyouts have emerged as a prominent approach
to reducing vulnerability to future disasters. This strategy involves
governments purchasing at-risk properties from willing sellers and converting
the land into open space, ostensibly reducing future disaster risk and impact.
However, the aftermath of these buyouts, particularly concerning land-use
patterns and community impacts, remains under-explored. This research aims to
fill this gap by employing innovative techniques like satellite imagery
analysis and deep learning to study these patterns. To achieve this goal, we
employed FEMA's Hazard Mitigation Grant Program (HMGP) buyout dataset,
encompassing over 41,004 addresses of these buyout properties from 1989 to
2017. Leveraging Google's Maps Static API, we gathered 40,053 satellite images
corresponding to these buyout lands. Subsequently, we implemented five
cutting-edge machine learning models to evaluate their performance in
classifying land cover types. Notably, this task involved multi-class
classification, and our model achieved an outstanding ROC-AUC score of 98.86%",cs.CV cs.CY,2024-01-15
"On Inter-dataset Code Duplication and Data Leakage in Large Language
  Models",,"Motivation. Large language models (LLMs) have exhibited remarkable
proficiency in diverse software engineering (SE) tasks. Handling such tasks
typically involves acquiring foundational coding knowledge on large,
general-purpose datasets during a pre-training phase, and subsequently refining
on smaller, task-specific datasets as part of a fine-tuning phase.
  Problem statement. While intra-dataset code duplication examines the
intersection between the training and test splits within a given dataset and
has been addressed in prior research, inter-dataset code duplication, which
gauges the overlap between different datasets, remains largely unexplored. If
this phenomenon exists, it could compromise the integrity of LLM evaluations
because of the inclusion of fine-tuning test samples that were already
encountered during pre-training, resulting in inflated performance metrics.
  Contribution. This paper explores the phenomenon of inter-dataset code
duplication and its impact on evaluating LLMs across diverse SE tasks.
  Study design. We conduct an empirical study using the CodeSearchNet dataset
(CSN), a widely adopted pre-training dataset, and five fine-tuning datasets
used for various se tasks. We first identify the intersection between the
pre-training and fine-tuning datasets using a deduplication process. Next, we
pre-train two versions of LLMs using a subset of CSN: one leaky LLM and one
non-leaky LLM. Finally, we fine-tune both models and compare their performances
using leaky fine-tuning test samples.
  Results. Our findings reveal a potential threat to the evaluation of LLMs
across multiple SE tasks, stemming from the inter-dataset code duplication
phenomenon. We also demonstrate that this threat is accentuated by the chosen
fine-tuning technique. Furthermore, we provide evidence that open-source models
could be affected by inter-dataset duplication.",cs.SE,2024-01-15
Fairness Concerns in App Reviews: A Study on AI-based Mobile Apps,,"Fairness is one of the socio-technical concerns that must be addressed in
software systems. Considering the popularity of mobile software applications
(apps) among a wide range of individuals worldwide, mobile apps with unfair
behaviors and outcomes can affect a significant proportion of the global
population, potentially more than any other type of software system. Users
express a wide range of socio-technical concerns in mobile app reviews. This
research aims to investigate fairness concerns raised in mobile app reviews.
Our research focuses on AI-based mobile app reviews as the chance of unfair
behaviors and outcomes in AI-based mobile apps may be higher than in
non-AI-based apps. To this end, we first manually constructed a ground-truth
dataset, including 1,132 fairness and 1,473 non-fairness reviews. Leveraging
the ground-truth dataset, we developed and evaluated a set of machine learning
and deep learning models that distinguish fairness reviews from non-fairness
reviews. Our experiments show that our best-performing model can detect
fairness reviews with a precision of 94%. We then applied the best-performing
model on approximately 9.5M reviews collected from 108 AI-based apps and
identified around 92K fairness reviews. Next, applying the K-means clustering
technique to the 92K fairness reviews, followed by manual analysis, led to the
identification of six distinct types of fairness concerns (e.g., 'receiving
different quality of features and services in different platforms and devices'
and 'lack of transparency and fairness in dealing with user-generated
content'). Finally, the manual analysis of 2,248 app owners' responses to the
fairness reviews identified six root causes (e.g., 'copyright issues') that app
owners report to justify fairness concerns.",cs.SE cs.AI cs.CY,2024-01-15
"FMB: a Functional Manipulation Benchmark for Generalizable Robotic
  Learning",,"In this paper, we propose a real-world benchmark for studying robotic
learning in the context of functional manipulation: a robot needs to accomplish
complex long-horizon behaviors by composing individual manipulation skills in
functionally relevant ways. The core design principles of our Functional
Manipulation Benchmark (FMB) emphasize a harmonious balance between complexity
and accessibility. Tasks are deliberately scoped to be narrow, ensuring that
models and datasets of manageable scale can be utilized effectively to track
progress. Simultaneously, they are diverse enough to pose a significant
generalization challenge. Furthermore, the benchmark is designed to be easily
replicable, encompassing all essential hardware and software components. To
achieve this goal, FMB consists of a variety of 3D-printed objects designed for
easy and accurate replication by other researchers. The objects are
procedurally generated, providing a principled framework to study
generalization in a controlled fashion. We focus on fundamental manipulation
skills, including grasping, repositioning, and a range of assembly behaviors.
The FMB can be used to evaluate methods for acquiring individual skills, as
well as methods for combining and ordering such skills to solve complex,
multi-stage manipulation tasks. We also offer an imitation learning framework
that includes a suite of policies trained to solve the proposed tasks. This
enables researchers to utilize our tasks as a versatile toolkit for examining
various parts of the pipeline. For example, researchers could propose a better
design for a grasping controller and evaluate it in combination with our
baseline reorientation and assembly policies as part of a pipeline for solving
multi-stage tasks. Our dataset, object CAD files, code, and evaluation videos
can be found on our project website:
https://functional-manipulation-benchmark.github.io",cs.RO,2024-01-16
Blackout Mitigation via Physics-guided RL,,"This paper considers the sequential design of remedial control actions in
response to system anomalies for the ultimate objective of preventing
blackouts. A physics-guided reinforcement learning (RL) framework is designed
to identify effective sequences of real-time remedial look-ahead decisions
accounting for the long-term impact on the system's stability. The paper
considers a space of control actions that involve both discrete-valued
transmission line-switching decisions (line reconnections and removals) and
continuous-valued generator adjustments. To identify an effective blackout
mitigation policy, a physics-guided approach is designed that uses power-flow
sensitivity factors associated with the power transmission network to guide the
RL exploration during agent training. Comprehensive empirical evaluations using
the open-source Grid2Op platform demonstrate the notable advantages of
incorporating physical signals into RL decisions, establishing the gains of the
proposed physics-guided approach compared to its black box counterparts. One
important observation is that strategically~\emph{removing} transmission lines,
in conjunction with multiple real-time generator adjustments, often renders
effective long-term decisions that are likely to prevent or delay blackouts.",eess.SY cs.AI cs.SY,2024-01-17
The Manga Whisperer: Automatically Generating Transcriptions for Comics,,"In the past few decades, Japanese comics, commonly referred to as Manga, have
transcended both cultural and linguistic boundaries to become a true worldwide
sensation. Yet, the inherent reliance on visual cues and illustration within
manga renders it largely inaccessible to individuals with visual impairments.
In this work, we seek to address this substantial barrier, with the aim of
ensuring that manga can be appreciated and actively engaged by everyone.
Specifically, we tackle the problem of diarisation i.e. generating a
transcription of who said what and when, in a fully automatic way.
  To this end, we make the following contributions: (1) we present a unified
model, Magi, that is able to (a) detect panels, text boxes and character boxes,
(b) cluster characters by identity (without knowing the number of clusters
apriori), and (c) associate dialogues to their speakers; (2) we propose a novel
approach that is able to sort the detected text boxes in their reading order
and generate a dialogue transcript; (3) we annotate an evaluation benchmark for
this task using publicly available [English] manga pages. The code, evaluation
datasets and the pre-trained model can be found at:
https://github.com/ragavsachdeva/magi.",cs.CV,2024-01-18
"Learning Backdoors for Mixed Integer Linear Programs with Contrastive
  Learning",,"Many real-world problems can be efficiently modeled as Mixed Integer Linear
Programs (MILPs) and solved with the Branch-and-Bound method. Prior work has
shown the existence of MILP backdoors, small sets of variables such that
prioritizing branching on them when possible leads to faster running times.
However, finding high-quality backdoors that improve running times remains an
open question. Previous work learns to estimate the relative solver speed of
randomly sampled backdoors through ranking and then decide whether to use the
highest-ranked backdoor candidate. In this paper, we utilize the Monte-Carlo
tree search method to collect backdoors for training, rather than relying on
random sampling, and adapt a contrastive learning framework to train a Graph
Attention Network model to predict backdoors. Our method, evaluated on several
common MILP problem domains, demonstrates performance improvements over both
Gurobi and previous models.",cs.AI cs.LG math.OC,2024-01-18
Parametric Matrix Models,,"We present a general class of machine learning algorithms called parametric
matrix models. In contrast with most existing machine learning models that
imitate the biology of neurons, parametric matrix models use matrix equations
that emulate the physics of quantum systems. Similar to how physics problems
are usually solved, parametric matrix models learn the governing equations that
lead to the desired outputs. Parametric matrix models can be efficiently
trained from empirical data, and the equations may use algebraic, differential,
or integral relations. While originally designed for scientific computing, we
prove that parametric matrix models are universal function approximators that
can be applied to general machine learning problems. After introducing the
underlying theory, we apply parametric matrix models to a series of different
challenges that show their performance for a wide range of problems. For all
the challenges tested here, parametric matrix models produce accurate results
within an efficient and interpretable computational framework that allows for
input feature extrapolation.",cs.LG cond-mat.dis-nn nucl-th physics.comp-ph quant-ph,2024-01-22
"Distilling Mathematical Reasoning Capabilities into Small Language
  Models",,"This work addresses the challenge of democratizing advanced Large Language
Models (LLMs) by compressing their mathematical reasoning capabilities into
sub-billion parameter Small Language Models (SLMs) without compromising
performance. We introduce Equation-of-Thought Distillation (EoTD), a novel
technique that encapsulates the reasoning process into equation-based
representations to construct an EoTD dataset for fine-tuning SLMs.
Additionally, we propose the Ensemble Thoughts Distillation (ETD) framework to
enhance the reasoning performance of SLMs. This involves creating a reasoning
dataset with multiple thought processes, including Chain-of-Thought (CoT),
Program-of-Thought (PoT), and Equation-of-Thought (EoT), and using it for
fine-tuning. Our experimental performance demonstrates that EoTD significantly
boosts the reasoning abilities of SLMs, while ETD enables these models to
achieve state-of-the-art reasoning performance.",cs.CL cs.AI,2024-01-22
Disentangled Condensation for Large-scale Graphs,,"Graph condensation has emerged as an intriguing technique to save the
expensive training costs of Graph Neural Networks (GNNs) by substituting a
condensed small graph with the original graph. Despite the promising results
achieved, previous methods usually employ an entangled paradigm of redundant
parameters (nodes, edges, GNNs), which incurs complex joint optimization during
condensation. This paradigm has considerably impeded the scalability of graph
condensation, making it challenging to condense extremely large-scale graphs
and generate high-fidelity condensed graphs. Therefore, we propose to
disentangle the condensation process into a two-stage GNN-free paradigm,
independently condensing nodes and generating edges while eliminating the need
to optimize GNNs at the same time. The node condensation module avoids the
complexity of GNNs by focusing on node feature alignment with anchors of the
original graph, while the edge translation module constructs the edges of the
condensed nodes by transferring the original structure knowledge with
neighborhood anchors. This simple yet effective approach achieves at least 10
times faster than state-of-the-art methods with comparable accuracy on
medium-scale graphs. Moreover, the proposed DisCo can successfully scale up to
the Ogbn-papers100M graph with flexible reduction rates. Extensive downstream
tasks and ablation study on five common datasets further demonstrate the
effectiveness of the proposed DisCo framework. The source code will be made
publicly available.",cs.SI cs.LG,2024-01-18
Vivim: a Video Vision Mamba for Medical Video Segmentation,,"Medical video segmentation gains increasing attention in clinical practice
due to the redundant dynamic references in video frames. However, traditional
convolutional neural networks have a limited receptive field and
transformer-based networks are mediocre in constructing long-term dependency
from the perspective of computational complexity. This bottleneck poses a
significant challenge when processing longer sequences in medical video
analysis tasks using available devices with limited memory. Recently, state
space models (SSMs), famous by Mamba, have exhibited impressive achievements in
efficient long sequence modeling, which develops deep neural networks by
expanding the receptive field on many vision tasks significantly.
Unfortunately, vanilla SSMs failed to simultaneously capture causal temporal
cues and preserve non-casual spatial information. To this end, this paper
presents a Video Vision Mamba-based framework, dubbed as Vivim, for medical
video segmentation tasks. Our Vivim can effectively compress the long-term
spatiotemporal representation into sequences at varying scales with our
designed Temporal Mamba Block. We also introduce an improved boundary-aware
affine constraint across frames to enhance the discriminative ability of Vivim
on ambiguous lesions. Extensive experiments on thyroid segmentation, breast
lesion segmentation in ultrasound videos, and polyp segmentation in colonoscopy
videos demonstrate the effectiveness and efficiency of our Vivim, superior to
existing methods. The code is available at:
https://github.com/scott-yjyang/Vivim. The dataset will be released once
accepted.",cs.CV,2024-01-25
MoE-Infinity: Offloading-Efficient MoE Model Serving,,"This paper presents MoE-Infinity, an offloading-efficient serving system for
sparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity
achieves novel request-level tracing for expert activation, capturing MoE's
sparse execution patterns such as selective activation, group activation, and
skewed reuse. Leveraging the request-level trace, MoE-Infinity performs
effective expert prefetching and expert caching, achieving high efficiency in
transferring model parameters from host memory to GPU memory. Experimental
results demonstrate that MoE-Infinity achieves low latency comparable to
expensive full-GPU deployments, which require up to 4X more GPU resources than
MoE-Infinity. Compared to offloading-supporting LLM serving systems such as
DeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,
MoE-Infinity exhibits superior latency performance, providing 2-20X
improvements when serving various MoE models for a large collection of LLM
tasks. MoE-Infinity's source code is publicly available a
https://github.com/TorchMoE/MoE-Infinity",cs.LG cs.PF,2024-01-25
Text Image Inpainting via Global Structure-Guided Diffusion Models,,"Real-world text can be damaged by corrosion issues caused by environmental or
human factors, which hinder the preservation of the complete styles of texts,
e.g., texture and structure. These corrosion issues, such as graffiti signs and
incomplete signatures, bring difficulties in understanding the texts, thereby
posing significant challenges to downstream applications, e.g., scene text
recognition and signature identification. Notably, current inpainting
techniques often fail to adequately address this problem and have difficulties
restoring accurate text images along with reasonable and consistent styles.
Formulating this as an open problem of text image inpainting, this paper aims
to build a benchmark to facilitate its study. In doing so, we establish two
specific text inpainting datasets which contain scene text images and
handwritten text images, respectively. Each of them includes images revamped by
real-life and synthetic datasets, featuring pairs of original images, corrupted
images, and other assistant information. On top of the datasets, we further
develop a novel neural framework, Global Structure-guided Diffusion Model
(GSDM), as a potential solution. Leveraging the global structure of the text as
a prior, the proposed GSDM develops an efficient diffusion model to recover
clean texts. The efficacy of our approach is demonstrated by thorough empirical
study, including a substantial boost in both recognition accuracy and image
quality. These findings not only highlight the effectiveness of our method but
also underscore its potential to enhance the broader field of text image
understanding and processing. Code and datasets are available at:
https://github.com/blackprotoss/GSDM.",cs.CV,2024-01-26
Evaluation in Neural Style Transfer: A Review,,"The field of Neural Style Transfer (NST) has witnessed remarkable progress in
the past few years, with approaches being able to synthesize artistic and
photorealistic images and videos of exceptional quality. To evaluate such
results, a diverse landscape of evaluation methods and metrics is used,
including authors' opinions based on side-by-side comparisons, human evaluation
studies that quantify the subjective judgements of participants, and a
multitude of quantitative computational metrics which objectively assess the
different aspects of an algorithm's performance. However, there is no consensus
regarding the most suitable and effective evaluation procedure that can
guarantee the reliability of the results. In this review, we provide an
in-depth analysis of existing evaluation techniques, identify the
inconsistencies and limitations of current evaluation methods, and give
recommendations for standardized evaluation practices. We believe that the
development of a robust evaluation framework will not only enable more
meaningful and fairer comparisons among NST methods but will also enhance the
comprehension and interpretation of research findings in the field.",cs.CV cs.LG cs.NE,2024-01-30
"Convergence of the deep BSDE method for stochastic control problems
  formulated through the stochastic maximum principle",,"It is well-known that decision-making problems from stochastic control can be
formulated by means of a forward-backward stochastic differential equation
(FBSDE). Recently, the authors of Ji et al. 2022 proposed an efficient deep
learning algorithm based on the stochastic maximum principle (SMP). In this
paper, we provide a convergence result for this deep SMP-BSDE algorithm and
compare its performance with other existing methods. In particular, by adopting
a strategy as in Han and Long 2020, we derive a-posteriori estimate, and show
that the total approximation error can be bounded by the value of the loss
functional and the discretization error. We present numerical examples for
high-dimensional stochastic control problems, both in case of drift- and
diffusion control, which showcase superior performance compared to existing
algorithms.",math.OC cs.NA math.NA q-fin.CP,2024-01-30
"Assessment of Diagnostic Capabilities of Methods of Recreation of
  Voltage Fluctuations",,"Voltage fluctuations are one of the most common low-frequency disturbances of
power quality. Diagnostics of these disturbances are a complex process because
voltage fluctuations affect different loads in different ways. Therefore, there
is no measure of power quality that allows for the complementary assessment of
severity of this disturbance, allow for the identification of sources of
voltage fluctuations, and post-factum investigation of their effects. Among the
currently used measures of voltage fluctuations, voltage fluctuation indices
have the greatest diagnostic capabilities. Many preliminary studies also show
the potential possibility of recreation of voltage fluctuations, including:
based on voltage fluctuation indices. This paper presents the results of
research on methods of recreation of voltage fluctuations from voltage
fluctuation indices. The research carried out included a set of data obtained
in a real power grid. Moreover, the impact of the discrimination period on the
accuracy of recreation of voltage fluctuations has been assessed. The presented
research results show, on the one hand, the usefulness of voltage fluctuation
indices in the process of recreation of voltage fluctuations and, on the other
hand, further challenges in the recreation of voltage fluctuations.",eess.SP cs.NA math.NA,2024-01-31
Laboratory Setup for Testing Low-Frequency Disturbances of Power Quality,,"Low-frequency disturbances of power quality are one of the most common
disturbances in the power grid. These disturbances are most often the result of
the impact of power electronic and energy-saving devices, the number of which
is increasing significantly in the power grid. Due to the simultaneous
operation of various types of loads in the power grid, various types of
simultaneous disturbances of power quality occur, such as voltage fluctuations
and distortions. Therefore, there is a need to analyze this type of
simultaneous interaction. For this purpose, a special and complementary
laboratory setup has been prepared, which allows for the examination of actual
states occurring in modern power networks. Selected research results are
presented for this laboratory setup, which determine its basic properties.
Possible applications and possibilities of the laboratory setup are presented
from the point of view of current challenges.",eess.SP cs.SY eess.SY,2024-01-31
A Hybrid Strategy for Chat Transcript Summarization,,"Text summarization is the process of condensing a piece of text to fewer
sentences, while still preserving its content. Chat transcript, in this
context, is a textual copy of a digital or online conversation between a
customer (caller) and agent(s). This paper presents an indigenously (locally)
developed hybrid method that first combines extractive and abstractive
summarization techniques in compressing ill-punctuated or un-punctuated chat
transcripts to produce more readable punctuated summaries and then optimizes
the overall quality of summarization through reinforcement learning. Extensive
testing, evaluations, comparisons, and validation have demonstrated the
efficacy of this approach for large-scale deployment of chat transcript
summarization, in the absence of manually generated reference (annotated)
summaries.",cs.CL,2024-02-02
"Accelerating Look-ahead in Bayesian Optimization: Multilevel Monte Carlo
  is All you Need",,"We leverage multilevel Monte Carlo (MLMC) to improve the performance of
multi-step look-ahead Bayesian optimization (BO) methods that involve nested
expectations and maximizations. Often these expectations must be computed by
Monte Carlo (MC). The complexity rate of naive MC degrades for nested
operations, whereas MLMC is capable of achieving the canonical MC convergence
rate for this type of problem, independently of dimension and without any
smoothness assumptions. Our theoretical study focuses on the approximation
improvements for twoand three-step look-ahead acquisition functions, but, as we
discuss, the approach is generalizable in various ways, including beyond the
context of BO. Our findings are verified numerically and the benefits of MLMC
for BO are illustrated on several benchmark examples. Code is available at
https://github.com/Shangda-Yang/MLMCBO .",stat.ML cs.LG math.OC math.PR stat.CO stat.ME,2024-02-03
"DefInt: A Default-interventionist Framework for Efficient Reasoning with
  Hybrid Large Language Models",,"Large language models (LLMs) have shown impressive emergent abilities in a
wide range of tasks, but still face challenges in handling complex reasoning
problems. Previous works like chain-of-thought (CoT) and tree-of-thoughts (ToT)
have predominately focused on enhancing accuracy, but overlook the rapidly
increasing token cost, which could be particularly problematic for open-ended
real-world tasks with huge solution spaces. Motivated by the dual process
theory of human cognition, we propose a Default-Interventionist framework
(DefInt) to unleash the synergistic potential of hybrid LLMs. By default,
DefInt uses smaller-scale language models to generate low-cost reasoning
thoughts, which resembles the fast intuitions produced by System 1. If the
intuitions are considered with low confidence, DefInt will invoke the
reflective reasoning of scaled-up language models as the intervention of System
2, which can override the default thoughts and rectify the reasoning process.
Experiments on five representative reasoning tasks show that DefInt
consistently achieves state-of-the-art reasoning accuracy and solution
diversity. More importantly, it substantially reduces the token cost by 49%-79%
compared to the second accurate baselines. Specifically, the open-ended tasks
have an average 75% token cost reduction. Code repo with all prompts will be
released upon publication.",cs.CL cs.AI cs.LG,2024-02-04
Design And Implementation of A Scalable Financial Exchange in the Cloud,,"Financial exchanges are increasingly moving their operations to the public
cloud. This migration presents challenges because of the extreme networking
requirements of an exchange. Exchanges require a low latency multicast service
to disseminate market information to all market participants (MPs)
simultaneously. Fair or simultaneous delivery of data is necessary for fair
competition in the market as MPs often compete on how fast they can react to a
piece of information. Furthermore, exchanges need a mechanism for scalable
order submission by MPs, which at times show a bursty behavior. Bursty order
submission can lead to excessive packet losses, and naive remedies may increase
orders latency. The lack of switch multicast in the cloud, varying latency
between VMs, and limited control over the underlying network fabric make it
difficult for a cloud tenant to realize an exchange's requirements.
  We design and implement Jasper, a first of its kind scalable cloud-hosted
financial exchange. Jasper consists of two major components: (i) a low latency
overlay multicast service, and (ii) a scalable order submission mechanism. The
multicast service minimizes latency and its variance in the cloud and achieves
simultaneous delivery of data. For order submission, Jasper implements a
special priority queue, FancyPQ, that helps achieve high throughput for an
exchange and keeps the latency of orders low during periods of bursts. Further,
Jasper leverages confidential computing capabilities to define a security
boundary between an exchange and market participants. Jasper achieves better
scalability and 50% lower latency than the multicast service provided by AWS.
FancyPQ increases the throughput of exchange by up to 150% and lowers the
latency of orders by up to 99.9% during periods of bursts.",cs.NI,2024-02-14
"Non-Adaptive Multi-Stage Algorithm for Group Testing with Prior
  Statistics",,"In this paper, we propose an efficient multi-stage algorithm for non-adaptive
Group Testing (GT) with general correlated prior statistics. The proposed
solution can be applied to any correlated statistical prior represented in
trellis, e.g., finite state machines and Markov processes. We introduce a
variation of List Viterbi Algorithm (LVA) to enable accurate recovery using
much fewer tests than objectives, which efficiently gains from the correlated
prior statistics structure. Our numerical results demonstrate that the proposed
Multi-Stage GT (MSGT) algorithm can obtain the optimal Maximum A Posteriori
(MAP) performance with feasible complexity in practical regimes, such as with
COVID-19 and sparse signal recovery applications, and reduce in the scenarios
tested the number of pooled tests by at least $25\%$ compared to existing
classical low complexity GT algorithms. Moreover, we analytically characterize
the complexity of the proposed MSGT algorithm that guarantees its efficiency.",cs.IT math.IT q-bio.QM stat.AP,2024-02-15
"Hierarchical State Space Models for Continuous Sequence-to-Sequence
  Modeling",,"Reasoning from sequences of raw sensory data is a ubiquitous problem across
fields ranging from medical devices to robotics. These problems often involve
using long sequences of raw sensor data (e.g. magnetometers, piezoresistors) to
predict sequences of desirable physical quantities (e.g. force, inertial
measurements). While classical approaches are powerful for locally-linear
prediction problems, they often fall short when using real-world sensors. These
sensors are typically non-linear, are affected by extraneous variables (e.g.
vibration), and exhibit data-dependent drift. For many problems, the prediction
task is exacerbated by small labeled datasets since obtaining ground-truth
labels requires expensive equipment. In this work, we present Hierarchical
State-Space Models (HiSS), a conceptually simple, new technique for continuous
sequential prediction. HiSS stacks structured state-space models on top of each
other to create a temporal hierarchy. Across six real-world sensor datasets,
from tactile-based state prediction to accelerometer-based inertial
measurement, HiSS outperforms state-of-the-art sequence models such as causal
Transformers, LSTMs, S4, and Mamba by at least 23% on MSE. Our experiments
further indicate that HiSS demonstrates efficient scaling to smaller datasets
and is compatible with existing data-filtering techniques. Code, datasets and
videos can be found on https://hiss-csp.github.io.",cs.LG cs.RO eess.SP,2024-02-15
Memristor-Based MobileNetV3 Circuit Design for Image Classification,,"The increasing computational demands of deep learning models pose significant
challenges for edge devices. To address this, we propose a memristor-based
circuit design for MobileNetV3, specifically for image classification tasks.
Our design leverages the low power consumption and high integration density of
memristors, making it suitable for edge computing. The architecture includes
optimized memristive convolutional modules, batch normalization modules,
activation function modules, global average pooling modules, and fully
connected modules. Experimental results on the CIFAR-10 dataset show that our
memristor-based MobileNetV3 achieves over 90\% accuracy while significantly
reducing inference time and energy consumption compared to traditional
implementations. This work demonstrates the potential of memristor-based
designs for efficient deployment of deep learning models in
resource-constrained environments.",cs.AR,2024-02-16
"A Lattice Boltzmann Method for Non-Newtonian Blood Flow in Coiled
  Intracranial Aneurysms",,"Intracranial aneurysms are the leading cause of hemorrhagic stroke. One of
the established treatment approaches is the embolization induced by coil
insertion. However, the prediction of treatment and subsequent changed flow
characteristics in the aneurysm is still an open problem. In this work, we
present an approach based on a patient-specific geometry and parameters
including a coil representation as inhomogeneous porous medium. The model
consists of the volume-averaged Navier-Stokes equations for a non-Newtonian
blood rheology. We solve these equations using a problem-adapted lattice
Boltzmann method and present a comparison between fully-resolved and
volume-averaged simulations. The results indicate the validity of the model.
Overall, this workflow allows for patient specific assessment of the flow due
to potential treatment.",math.NA cs.NA physics.flu-dyn,2024-02-16
"LLM-Assisted Crisis Management: Building Advanced LLM Platforms for
  Effective Emergency Response and Public Collaboration",,"Emergencies and critical incidents often unfold rapidly, necessitating a
swift and effective response. In this research, we introduce a novel approach
to identify and classify emergency situations from social media posts and
direct emergency messages using an open source Large Language Model, LLAMA2.
The goal is to harness the power of natural language processing and machine
learning to assist public safety telecommunicators and huge crowds during
countrywide emergencies. Our research focuses on developing a language model
that can understand users describe their situation in the 911 call, enabling
LLAMA2 to analyze the content and offer relevant instructions to the
telecommunicator, while also creating workflows to notify government agencies
with the caller's information when necessary. Another benefit this language
model provides is its ability to assist people during a significant emergency
incident when the 911 system is overwhelmed, by assisting the users with simple
instructions and informing authorities with their location and emergency
information.",cs.CL cs.AI cs.HC cs.LG,2024-01-12
"Offline Training of Language Model Agents with Functions as Learnable
  Weights",,"Researchers and practitioners have recently reframed powerful Large Language
Models (LLMs) as agents, enabling them to automate complex tasks largely via
the use of specialized functions. To facilitate the development of LLM agents,
we present a novel paradigm of training LLM agents without modifying the LLM
weights, which is particularly useful when the LLMs are difficult or
inaccessible for modifications. Inspired by how humans continuously forge tools
to adapt to real-world tasks, rather than change our biological structure to
fit a static set of tools, we propose to progressively forge agent's functions
to better solve the downstream tasks instead of modifying the LLM weights. By
treating the functions as learnable `agent parameters' and leveraging the
fundamental idea of model training in artificial intelligence, we develop
AgentOptimizer that employs the LLM to update agents' functions and devise an
agent training algorithm with two strategies, roll-back, and early-stop, to
streamline the training process. With extensive experiments, we showcase that
the agent training paradigm could significantly improve the performance of
representative LLM agents in various downstream tasks. We also study the
behavior of the agent training regarding aspects like the learning curve and
domain transferability.",cs.AI cs.CL,2024-02-17
"Impact of data for forecasting on performance of model predictive
  control in buildings with smart energy storage",,"Data is required to develop forecasting models for use in Model Predictive
Control (MPC) schemes in building energy systems. However, data is costly to
both collect and exploit. Determining cost optimal data usage strategies
requires understanding of the forecast accuracy and resulting MPC operational
performance it enables. This study investigates the performance of both simple
and state-of-the-art machine learning prediction models for MPC in
multi-building energy systems using a simulated case study with historic
building energy data. The impact on forecast accuracy of measures to improve
model data efficiency are quantified, specifically for: reuse of prediction
models, reduction of training data duration, reduction of model data features,
and online model training. A simple linear multi-layer perceptron model is
shown to provide equivalent forecast accuracy to state-of-the-art models, with
greater data efficiency and generalisability. The use of more than 2 years of
training data for load prediction models provided no significant improvement in
forecast accuracy. Forecast accuracy and data efficiency were improved
simultaneously by using change-point analysis to screen training data. Reused
models and those trained with 3 months of data had on average 10% higher error
than baseline, indicating that deploying MPC systems without prior data
collection may be economic.",eess.SY cs.LG cs.SY,2024-02-19
Improved Space Bounds for Subset Sum,,"More than 40 years ago, Schroeppel and Shamir presented an algorithm that
solves the Subset Sum problem for $n$ integers in time $O^*(2^{0.5n})$ and
space $O^*(2^{0.25n})$. The time upper bound remains unbeaten, but the space
upper bound has been improved to $O^*(2^{0.249999n})$ in a recent breakthrough
paper by Nederlof and W\k{e}grzycki (STOC 2021). Their algorithm is a clever
combination of a number of previously known techniques with a new reduction and
a new algorithm for the Orthogonal Vectors problem. In this paper, we improve
the space bound by Nederlof and W\k{e}grzycki to $O^*(2^{0.246n})$ and also
simplify their algorithm and its analysis. We achieve this by using an idea,
due to Howgrave-Graham and Joux, of using a random prime number to filter the
family of subsets. We incorporate it into the algorithm by Schroeppel and
Shamir and then use this amalgam inside the representation technique. This
allows us to reduce an instance of Subset Sum to a larger number of instances
of weighted orthogonal vector.",cs.CC cs.DS,2024-02-20
FedADMM-InSa: An Inexact and Self-Adaptive ADMM for Federated Learning,,"Federated learning (FL) is a promising framework for learning from
distributed data while maintaining privacy. The development of efficient FL
algorithms encounters various challenges, including heterogeneous data and
systems, limited communication capacities, and constrained local computational
resources. Recently developed FedADMM methods show great resilience to both
data and system heterogeneity. However, they still suffer from performance
deterioration if the hyperparameters are not carefully tuned. To address this
issue, we propose an inexact and self-adaptive FedADMM algorithm, termed
FedADMM-InSa. First, we design an inexactness criterion for the clients' local
updates to eliminate the need for empirically setting the local training
accuracy. This inexactness criterion can be assessed by each client
independently based on its unique condition, thereby reducing the local
computational cost and mitigating the undesirable straggle effect. The
convergence of the resulting inexact ADMM is proved under the assumption of
strongly convex loss functions. Additionally, we present a self-adaptive scheme
that dynamically adjusts each client's penalty parameter, enhancing algorithm
robustness by mitigating the need for empirical penalty parameter choices for
each client. Extensive numerical experiments on both synthetic and real-world
datasets are conducted. As validated by some numerical tests, our proposed
algorithm can reduce the clients' local computational load significantly and
also accelerate the learning process compared to the vanilla FedADMM.",cs.LG cs.CR cs.DC math.OC,2024-02-21
"EXACT-Net:EHR-guided lung tumor auto-segmentation for non-small cell
  lung cancer radiotherapy",,"Lung cancer is a devastating disease with the highest mortality rate among
cancer types. Over 60% of non-small cell lung cancer (NSCLC) patients, which
accounts for 87% of diagnoses, require radiation therapy. Rapid treatment
initiation significantly increases the patient's survival rate and reduces the
mortality rate. Accurate tumor segmentation is a critical step in the diagnosis
and treatment of NSCLC. Manual segmentation is time and labor-consuming and
causes delays in treatment initiation. Although many lung nodule detection
methods, including deep learning-based models, have been proposed, there is
still a long-standing problem of high false positives (FPs) with most of these
methods. Here, we developed an electronic health record (EHR) guided lung tumor
auto-segmentation called EXACT-Net (EHR-enhanced eXACtitude in Tumor
segmentation), where the extracted information from EHRs using a pre-trained
large language model (LLM), was used to remove the FPs and keep the TP nodules
only. The auto-segmentation model was trained on NSCLC patients' computed
tomography (CT), and the pre-trained LLM was used with the zero-shot learning
approach. Our approach resulted in a 250% boost in successful nodule detection
using the data from ten NSCLC patients treated in our institution.",eess.IV cs.CV physics.med-ph,2024-02-21
Human-Centric Decision-Making in Cell-Less 6G Networks,,"In next-generation networks, cells will be replaced by a collection of
points-of-access (PoAs), with overlapping coverage areas and/or different
technologies. Along with a promise for greater performance and flexibility,
this creates further pressure on network management algorithms, which must make
joint decisions on (i) PoA-to-user association and (ii) PoA management. We
solve this challenging problem through an efficient and effective solution
concept called Cluster-then-Match (CtM). Importantly, CtM makes human-centric
decisions, where pure network performance is balanced against metrics like
energy consumption and electromagnetic field exposure, which concern all humans
in the network area -- including those who are not network users. Through our
performance evaluation, which leverages detailed models for EMF exposure
estimation and standard-specified signal propagation models, we show that CtM
outperforms state-of-the-art network management schemes, including those
utilizing machine learning, reducing energy consumption by over 80%.",cs.NI,2024-02-22
"ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and
  Two-Phase Partition",,"Self-attention is an essential component of large language models (LLM) but a
significant source of inference latency for long sequences. In multi-tenant LLM
serving scenarios, the compute and memory operation cost of self-attention can
be optimized by using the probability that multiple LLM requests have shared
system prompts in prefixes. In this paper, we introduce ChunkAttention, a
prefix-aware self-attention module that can detect matching prompt prefixes
across multiple requests and share their key/value tensors in memory at runtime
to improve the memory utilization of KV cache. This is achieved by breaking
monolithic key/value tensors into smaller chunks and structuring them into the
auxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,
we design an efficient self-attention kernel, where a two-phase partition
algorithm is implemented to improve the data locality during self-attention
computation in the presence of shared system prompts. Experiments show that
ChunkAttention can speed up the self-attention kernel by 3.2-4.8$\times$
compared to the state-of-the-art implementation, with the length of the system
prompt ranging from 1024 to 4096.",cs.LG cs.CL,2024-02-23
"Grasp, See and Place: Efficient Unknown Object Rearrangement with Policy
  Structure Prior",,"We focus on the task of unknown object rearrangement, where a robot is
supposed to re-configure the objects into a desired goal configuration
specified by an RGB-D image. Recent works explore unknown object rearrangement
systems by incorporating learning-based perception modules. However, they are
sensitive to perception error, and pay less attention to task-level
performance. In this paper, we aim to develop an effective system for unknown
object rearrangement amidst perception noise. We theoretically reveal the noisy
perception impacts grasp and place in a decoupled way, and show such a
decoupled structure is valuable to improve task optimality. We propose GSP, a
dual-loop system with the decoupled structure as prior. For the inner loop, we
learn a see policy for self-confident in-hand object matching. For the outer
loop, we learn a grasp policy aware of object matching and grasp capability
guided by task-level rewards. We leverage the foundation model CLIP for object
matching, policy learning and self-termination. A series of experiments
indicate that GSP can conduct unknown object rearrangement with higher
completion rates and fewer steps.",cs.RO cs.LG,2024-02-23
Reputational Algorithm Aversion,,"People are often reluctant to incorporate information produced by algorithms
into their decisions, a phenomenon called ``algorithm aversion''. This paper
shows how algorithm aversion arises when the choice to follow an algorithm
conveys information about a human's ability. I develop a model in which workers
make forecasts of an uncertain outcome based on their own private information
and an algorithm's signal. Low-skill workers receive worse information than the
algorithm and hence should always follow the algorithm's signal, while
high-skill workers receive better information than the algorithm and should
sometimes override it. However, due to reputational concerns, low-skill workers
inefficiently override the algorithm to increase the likelihood they are
perceived as high-skill. The model provides a fully rational microfoundation
for algorithm aversion that aligns with the broad concern that AI systems will
displace many types of workers.",econ.TH cs.AI cs.GT cs.HC,2024-02-23
"Sense-Then-Train: An Active-Sensing-Based Beam Training Design for
  Near-Field MIMO Systems",,"An active-sensing-based sense-then-train (STT) scheme is proposed for beam
training in near-field multiple-input multiple-output (MIMO) systems. Compared
to conventional codebook-based schemes, the proposed STT scheme is capable of
not only addressing the complex spherical-wave propagation but also effectively
exploiting the additional degrees-of-freedoms (DoFs). The STT scheme is
tailored for both single-beam and multi-beam cases. 1) For the single-beam
case, the STT scheme first utilizes a sensing phase to estimate a
low-dimensional representation of the near-field MIMO channel in the truncated
wavenumber domain. Then, in the subsequent training phase, the neural network
modules at transceivers are updated online to align beams, utilizing
sequentially received ping-pong pilots. This approach can efficiently obtain
the aligned beam pair without relying on predefined codebooks or training
datasets. 2) For the multi-beam case, based on the single-beam STT, a
Gram-Schmidt method is further utilized to guarantee the orthogonality between
beams in the training phase. Numerical results unveil that 1) the proposed STT
scheme can significantly enhance the beam training performance in the near
field compared to the conventional far-field codebook-based schemes, and 2) the
proposed STT scheme can perform fast and low-complexity beam training, while
achieving a near-optimal performance without full channel state information in
both cases.",cs.IT eess.SP math.IT,2024-02-23
Informed Meta-Learning,,"In noisy and low-data regimes prevalent in real-world applications, a key
challenge of machine learning lies in effectively incorporating inductive
biases that promote data efficiency and robustness. Meta-learning and informed
ML stand out as two approaches for incorporating prior knowledge into ML
pipelines. While the former relies on a purely data-driven source of priors,
the latter is guided by prior domain knowledge. In this paper, we formalise a
hybrid paradigm, informed meta-learning, facilitating the incorporation of
priors from unstructured knowledge representations, such as natural language;
thus, unlocking complementarity in cross-task knowledge sharing of humans and
machines. We establish the foundational components of informed meta-learning
and present a concrete instantiation of this framework--the Informed Neural
Process. Through a series of experiments, we demonstrate the potential benefits
of informed meta-learning in improving data efficiency, robustness to
observational noise and task distribution shifts.",cs.LG,2024-02-25
"Demonstrating and Reducing Shortcuts in Vision-Language Representation
  Learning",,"Vision-language models (VLMs) mainly rely on contrastive training to learn
general-purpose representations of images and captions. We focus on the
situation when one image is associated with several captions, each caption
containing both information shared among all captions and unique information
per caption about the scene depicted in the image. In such cases, it is unclear
whether contrastive losses are sufficient for learning task-optimal
representations that contain all the information provided by the captions or
whether the contrastive learning setup encourages the learning of a simple
shortcut that minimizes contrastive loss. We introduce synthetic shortcuts for
vision-language: a training and evaluation framework where we inject synthetic
shortcuts into image-text data. We show that contrastive VLMs trained from
scratch or fine-tuned with data containing these synthetic shortcuts mainly
learn features that represent the shortcut. Hence, contrastive losses are not
sufficient to learn task-optimal representations, i.e., representations that
contain all task-relevant information shared between the image and associated
captions. We examine two methods to reduce shortcut learning in our training
and evaluation framework: (i) latent target decoding and (ii) implicit feature
modification. We show empirically that both methods improve performance on the
evaluation task, but only partly reduce shortcut learning when training and
evaluating with our shortcut learning framework. Hence, we show the difficulty
and challenge of our shortcut learning framework for contrastive
vision-language representation learning.",cs.CV cs.AI,2024-02-27
"Inducing Generalization across Languages and Tasks using Featurized
  Low-Rank Mixtures",,"Adapting pretrained large language models (LLMs) to various downstream tasks
in tens or hundreds of human languages is computationally expensive.
Parameter-efficient fine-tuning (PEFT) significantly reduces the adaptation
cost, by tuning only a small amount of parameters. However, common PEFT methods
LoRA (Hu et al., 2022) suffer from suboptimal performance on diverse dataset
mixtures, due to aggressive parameter tying and negative interference among
different datasets. In this work, we propose Featurized Low-rank Mixtures
(FLix), a novel PEFT method designed for effective multitask multilingual
adaptation. FLix associates each unique dataset feature, such as the dataset's
language or task, with its own low-rank weight update parameters. By composing
feature-specific parameters for each dataset, FLix can accommodate diverse
dataset mixtures and generalize better to unseen datasets. Our experiments show
that FLix leads to significant improvements over a variety of tasks for both
supervised learning and zero-shot settings with gains of up to $14.2$ inexact
match points in zero-shot semantic parsing.",cs.CL cs.AI,2024-02-27
Exploring the space of graphs with fixed discrete curvatures,,"Discrete curvatures are quantities associated to the nodes and edges of a
graph that reflect the local geometry around them. These curvatures have a rich
mathematical theory and they have recently found success as a tool to analyze
networks across a wide range of domains. In this work, we consider the problem
of constructing graphs with a prescribed set of discrete edge curvatures, and
explore the space of such graphs. We address this problem in two ways: first,
we develop an evolutionary algorithm to sample graphs with discrete curvatures
close to a given set. We use this algorithm to explore how other network
statistics vary when constrained by the discrete curvatures in the network.
Second, we solve the exact reconstruction problem for the specific case of
Forman-Ricci curvature. By leveraging the theory of Markov bases, we obtain a
finite set of rewiring moves that connects the space of all graphs with a fixed
discrete curvature.",physics.soc-ph cs.DM math.CO,2024-02-28
Stable Reduced-Rank VAR Identification,,"The vector autoregression (VAR) has been widely used in system
identification, econometrics, natural science, and many other areas. However,
when the state dimension becomes large the parameter dimension explodes. So
rank reduced modelling is attractive and is well developed. But a fundamental
requirement in almost all applications is stability of the fitted model. And
this has not been addressed in the rank reduced case. Here, we develop, for the
first time, a closed-form formula for an estimator of a rank reduced transition
matrix which is guaranteed to be stable. We show that our estimator is
consistent and asymptotically statistically efficient and illustrate it in
comparative simulations.",stat.ME cs.SY eess.SY,2024-02-29
"Human vs. Machine: Behavioral Differences Between Expert Humans and
  Language Models in Wargame Simulations",,"To some, the advent of AI promises better decision-making and increased
military effectiveness while reducing the influence of human error and
emotions. However, there is still debate about how AI systems, especially large
language models (LLMs) that can be applied to many tasks, behave compared to
humans in high-stakes military decision-making scenarios with the potential for
increased risks towards escalation and unnecessary conflicts. To test this
potential and scrutinize the use of LLMs for such purposes, we use a new
wargame experiment with 107 national security experts designed to examine
crisis escalation in a fictional US-China scenario and compare the behavior of
human player teams to LLM-simulated team responses in separate simulations.
Here, we find that the LLM-simulated responses can be more aggressive and
significantly affected by changes in the scenario. We show a considerable
high-level agreement in the LLM and human responses and significant
quantitative and qualitative differences in individual actions and strategic
tendencies. These differences depend on intrinsic biases in LLMs regarding the
appropriate level of violence following strategic instructions, the choice of
LLM, and whether the LLMs are tasked to decide for a team of players directly
or first to simulate dialog between a team of players. When simulating the
dialog, the discussions lack quality and maintain a farcical harmony. The LLM
simulations cannot account for human player characteristics, showing no
significant difference even for extreme traits, such as ""pacifist"" or
""aggressive sociopath."" When probing behavioral consistency across individual
moves of the simulation, the tested LLMs deviated from each other but generally
showed somewhat consistent behavior. Our results motivate policymakers to be
cautious before granting autonomy or following AI-based strategy
recommendations.",cs.CY cs.AI cs.CL,2024-03-05
Designing Informative Metrics for Few-Shot Example Selection,,"Pretrained language models (PLMs) have shown remarkable few-shot learning
capabilities when provided with properly formatted examples. However, selecting
the ""best"" examples remains an open challenge. We propose a complexity-based
prompt selection approach for sequence tagging tasks. This approach avoids the
training of a dedicated model for selection of examples, and instead uses
certain metrics to align the syntactico-semantic complexity of test sentences
and examples. We use both sentence- and word-level metrics to match the
complexity of examples to the (test) sentence being considered. Our results
demonstrate that our approach extracts greater performance from PLMs: it
achieves state-of-the-art performance on few-shot NER, achieving a 5% absolute
improvement in F1 score on the CoNLL2003 dataset for GPT-4. We also see large
gains of upto 28.85 points (F1/Acc.) in smaller models like GPT-j-6B.",cs.CL cs.LG,2024-03-06
PIPsUS: Self-Supervised Point Tracking in Ultrasound,,"Finding point-level correspondences is a fundamental problem in ultrasound
(US), since it can enable US landmark tracking for intraoperative image
guidance in different surgeries, including head and neck. Most existing US
tracking methods, e.g., those based on optical flow or feature matching, were
initially designed for RGB images before being applied to US. Therefore domain
shift can impact their performance. Training could be supervised by
ground-truth correspondences, but these are expensive to acquire in US. To
solve these problems, we propose a self-supervised pixel-level tracking model
called PIPsUS. Our model can track an arbitrary number of points in one forward
pass and exploits temporal information by considering multiple, instead of just
consecutive, frames. We developed a new self-supervised training strategy that
utilizes a long-term point-tracking model trained for RGB images as a teacher
to guide the model to learn realistic motions and use data augmentation to
enforce tracking from US appearance. We evaluate our method on neck and oral US
and echocardiography, showing higher point tracking accuracy when compared with
fast normalized cross-correlation and tuned optical flow. Code will be
available once the paper is accepted.",cs.CV,2024-03-07
Multi-Tower Multi-Interest Recommendation with User Representation Repel,,"In the era of information overload, the value of recommender systems has been
profoundly recognized in academia and industry alike. Multi-interest sequential
recommendation, in particular, is a subfield that has been receiving increasing
attention in recent years. By generating multiple-user representations,
multi-interest learning models demonstrate superior expressiveness than
single-user representation models, both theoretically and empirically. Despite
major advancements in the field, three major issues continue to plague the
performance and adoptability of multi-interest learning methods, the difference
between training and deployment objectives, the inability to access item
information, and the difficulty of industrial adoption due to its single-tower
architecture. We address these challenges by proposing a novel multi-tower
multi-interest framework with user representation repel. Experimental results
across multiple large-scale industrial datasets proved the effectiveness and
generalizability of our proposed framework.",cs.IR cs.LG,2024-03-08
"Noise Level Adaptive Diffusion Model for Robust Reconstruction of
  Accelerated MRI",,"In general, diffusion model-based MRI reconstruction methods incrementally
remove artificially added noise while imposing data consistency to reconstruct
the underlying images. However, real-world MRI acquisitions already contain
inherent noise due to thermal fluctuations. This phenomenon is particularly
notable when using ultra-fast, high-resolution imaging sequences for advanced
research, or using low-field systems favored by low- and middle-income
countries. These common scenarios can lead to sub-optimal performance or
complete failure of existing diffusion model-based reconstruction techniques.
Specifically, as the artificially added noise is gradually removed, the
inherent MRI noise becomes increasingly pronounced, making the actual noise
level inconsistent with the predefined denoising schedule and consequently
inaccurate image reconstruction. To tackle this problem, we propose a posterior
sampling strategy with a novel NoIse Level Adaptive Data Consistency (Nila-DC)
operation. Extensive experiments are conducted on two public datasets and an
in-house clinical dataset with field strength ranging from 0.3T to 3T, showing
that our method surpasses the state-of-the-art MRI reconstruction methods, and
is highly robust against various noise levels. The code for Nila is available
at https://github.com/Solor-pikachu/Nila.",eess.IV cs.AI cs.CV,2024-03-08
"Switching the Loss Reduces the Cost in Batch (Offline) Reinforcement
  Learning",,"We propose training fitted Q-iteration with log-loss (FQI-log) for batch
reinforcement learning (RL). We show that the number of samples needed to learn
a near-optimal policy with FQI-log scales with the accumulated cost of the
optimal policy, which is zero in problems where acting optimally achieves the
goal and incurs no cost. In doing so, we provide a general framework for
proving small-cost bounds, i.e. bounds that scale with the optimal achievable
cost, in batch RL. Moreover, we empirically verify that FQI-log uses fewer
samples than FQI trained with squared loss on problems where the optimal policy
reliably achieves the goal.",cs.LG,2024-03-08
FrameQuant: Flexible Low-Bit Quantization for Transformers,,"Transformers are the backbone of powerful foundation models for many Vision
and Natural Language Processing tasks. But their compute and memory/storage
footprint is large, and so, serving such models is expensive often requiring
high-end hardware. To mitigate this difficulty, Post-Training Quantization
seeks to modify a pre-trained model and quantize it to eight bits or lower,
significantly boosting compute/memory/latency efficiency. Such models have been
successfully quantized to four bits with some performance loss. In this work,
we outline a simple scheme to quantize Transformer-based models to just two
bits (plus some overhead) with only a small drop in accuracy. Key to our
formulation is a concept borrowed from Harmonic analysis called Fusion Frames.
Our main finding is that the quantization must take place not in the original
weight space, but instead in the Fusion Frame representations. If quantization
is interpreted as the addition of noise, our casting of the problem allows
invoking an extensive body of known consistent recovery and noise robustness
guarantees. Further, if desired, de-noising filters are known in closed form.
We show empirically, via a variety of experiments, that (almost) two-bit
quantization for Transformer models promises sizable efficiency gains. The code
is available at https://github.com/vsingh-group/FrameQuant",cs.LG cs.CL,2024-03-09
"Analysis of Total Variation Minimization for Clustered Federated
  Learning",,"A key challenge in federated learning applications is the statistical
heterogeneity of local datasets. Clustered federated learning addresses this
challenge by identifying clusters of local datasets that are approximately
homogeneous. One recent approach to clustered federated learning is generalized
total variation minimization (GTVMin). This approach requires a similarity
graph which can be obtained by domain expertise or in a data-driven fashion via
graph learning techniques. Under a widely applicable clustering assumption, we
derive an upper bound the deviation between GTVMin solutions and their
cluster-wise averages. This bound provides valuable insights into the
effectiveness and robustness of GTVMin in addressing statistical heterogeneity
within federated learning environments.",cs.LG,2024-03-10
"UPS: Efficiently Building Foundation Models for PDE Solving via
  Cross-Modal Adaptation",,"We present Unified PDE Solvers (UPS), a data- and compute-efficient approach
to developing unified neural operators for diverse families of spatiotemporal
PDEs from various domains, dimensions, and resolutions. UPS embeds different
PDEs into a shared representation space and processes them using a
FNO-transformer architecture. Rather than training the network from scratch,
which is data-demanding and computationally expensive, we warm-start the
transformer from pretrained LLMs and perform explicit alignment to reduce the
modality gap while improving data and compute efficiency. The cross-modal UPS
achieves state-of-the-art results on a wide range of 1D and 2D PDE families
from PDEBench, outperforming existing unified models using 4 times less data
and 26 times less compute. Meanwhile, it is capable of few-shot transfer to
unseen PDE families and coefficients.",cs.LG,2024-03-11
"Empowering Robot Path Planning with Large Language Models: osmAG Map
  Topology & Hierarchy Comprehension with LLMs",,"Large Language Models (LLMs) have demonstrated great potential in robotic
applications by providing essential general knowledge. Mobile robots rely on
map comprehension for tasks like localization and navigation. In this paper, we
explore enabling LLMs to comprehend the topology and hierarchy of Area Graph, a
text-based hierarchical, topometric semantic map representation utilizing
polygons to demark areas such as rooms or buildings. Our experiments
demonstrate that with the right map representation, LLMs can effectively
comprehend Area Graph's topology and hierarchy. After straightforward
fine-tuning, the LLaMA2 models exceeded ChatGPT-3.5 in mastering these aspects.
Our dataset, dataset generation code, fine-tuned LoRA adapters can be accessed
at https://github.com/xiefujing/LLM-osmAG-Comprehension.",cs.RO,2024-03-13
"AFGI: Towards Accurate and Fast-convergent Gradient Inversion Attack in
  Federated Learning",,"Federated learning (FL) empowers privacypreservation in model training by
only exposing users' model gradients. Yet, FL users are susceptible to gradient
inversion attacks (GIAs) which can reconstruct ground-truth training data such
as images based on model gradients. However, reconstructing high-resolution
images by existing GIAs faces two challenges: inferior accuracy and
slow-convergence, especially when duplicating labels exist in the training
batch. To address these challenges, we present an Accurate and Fast-convergent
Gradient Inversion attack algorithm, called AFGI, with two components: Label
Recovery Block (LRB) which can accurately restore duplicating labels of private
images based on exposed gradients; VME Regularization Term, which includes the
total variance of reconstructed images, the discrepancy between three-channel
means and edges, between values from exposed gradients and reconstructed
images, respectively. The AFGI can be regarded as a white-box attack strategy
to reconstruct images by leveraging labels recovered by LRB. In particular,
AFGI is efficient that accurately reconstruct ground-truth images when users'
training batch size is up to 48. Our experimental results manifest that AFGI
can diminish 85% time costs while achieving superb inversion quality in the
ImageNet dataset. At last, our study unveils the shortcomings of FL in
privacy-preservation, prompting the development of more advanced countermeasure
strategies.",cs.CV,2024-03-13
Actor-Critic Physics-informed Neural Lyapunov Control,,"Designing control policies for stabilization tasks with provable guarantees
is a long-standing problem in nonlinear control. A crucial performance metric
is the size of the resulting region of attraction, which essentially serves as
a robustness ""margin"" of the closed-loop system against uncertainties. In this
paper, we propose a new method to train a stabilizing neural network controller
along with its corresponding Lyapunov certificate, aiming to maximize the
resulting region of attraction while respecting the actuation constraints.
Crucial to our approach is the use of Zubov's Partial Differential Equation
(PDE), which precisely characterizes the true region of attraction of a given
control policy. Our framework follows an actor-critic pattern where we
alternate between improving the control policy (actor) and learning a Zubov
function (critic). Finally, we compute the largest certifiable region of
attraction by invoking an SMT solver after the training procedure. Our
numerical experiments on several design problems show consistent and
significant improvements in the size of the resulting region of attraction.",cs.LG cs.RO cs.SY eess.SY,2024-03-13
"Language-Grounded Dynamic Scene Graphs for Interactive Object Search
  with Mobile Manipulation",,"To fully leverage the capabilities of mobile manipulation robots, it is
imperative that they are able to autonomously execute long-horizon tasks in
large unexplored environments. While large language models (LLMs) have shown
emergent reasoning skills on arbitrary tasks, existing work primarily
concentrates on explored environments, typically focusing on either navigation
or manipulation tasks in isolation. In this work, we propose MoMa-LLM, a novel
approach that grounds language models within structured representations derived
from open-vocabulary scene graphs, dynamically updated as the environment is
explored. We tightly interleave these representations with an object-centric
action space. Given object detections, the resulting approach is zero-shot,
open-vocabulary, and readily extendable to a spectrum of mobile manipulation
and household robotic tasks. We demonstrate the effectiveness of MoMa-LLM in a
novel semantic interactive search task in large realistic indoor environments.
In extensive experiments in both simulation and the real world, we show
substantially improved search efficiency compared to conventional baselines and
state-of-the-art approaches, as well as its applicability to more abstract
tasks. We make the code publicly available at
http://moma-llm.cs.uni-freiburg.de.",cs.RO,2024-03-13
"Improved Trade-offs Between Amortization and Download Bandwidth for
  Linear HSS",,"A Homomorphic Secret Sharing (HSS) scheme is a secret-sharing scheme that
shares a secret $x$ among $s$ servers, and additionally allows an output client
to reconstruct some function $f(x)$ using information that can be locally
computed by each server. A key parameter in HSS schemes is download rate, which
quantifies how much information the output client needs to download from the
servers. Often, download rate is improved by amortizing over $\ell$ instances
of the problem, making $\ell$ also a key parameter of interest.
  Recent work (Fosli, Ishai, Kolobov, and Wootters 2022) established a limit on
the download rate of linear HSS schemes for computing low-degree polynomials
and constructed schemes that achieve this optimal download rate; their schemes
required amortization over $\ell = \Omega(s \log(s))$ instances of the problem.
Subsequent work (Blackwell and Wootters, 2023) completely characterized linear
HSS schemes that achieve optimal download rate in terms of a coding-theoretic
notion termed optimal labelweight codes. A consequence of this characterization
was that $\ell = \Omega(s \log(s))$ is in fact necessary to achieve optimal
download rate.
  In this paper, we characterize all linear HSS schemes, showing that schemes
of any download rate are equivalent to a generalization of optimal labelweight
codes. This equivalence is constructive and provides a way to obtain an
explicit linear HSS scheme from any linear code. Using this characterization,
we present explicit linear HSS schemes with slightly sub-optimal rate but with
much improved amortization $\ell = O(s)$. Our constructions are based on
algebraic geometry codes (specifically Hermitian codes and Goppa codes).",cs.IT math.IT,2024-03-13
Caltech Aerial RGB-Thermal Dataset in the Wild,,"We present the first publicly-available RGB-thermal dataset designed for
aerial robotics operating in natural environments. Our dataset captures a
variety of terrain across the United States, including rivers, lakes,
coastlines, deserts, and forests, and consists of synchronized RGB, thermal,
global positioning, and inertial data. We provide semantic segmentation
annotations for 10 classes commonly encountered in natural settings in order to
drive the development of perception algorithms robust to adverse weather and
nighttime conditions. Using this dataset, we propose new and challenging
benchmarks for thermal and RGB-thermal (RGB-T) semantic segmentation, RGB-T
image translation, and motion tracking. We present extensive results using
state-of-the-art methods and highlight the challenges posed by temporal and
geographical domain shifts in our data. The dataset and accompanying code is
available at https://github.com/aerorobotics/caltech-aerial-rgbt-dataset.",cs.CV cs.RO,2024-03-13
A Hybrid Intelligence Method for Argument Mining,,"Large-scale survey tools enable the collection of citizen feedback in opinion
corpora. Extracting the key arguments from a large and noisy set of opinions
helps in understanding the opinions quickly and accurately. Fully automated
methods can extract arguments but (1) require large labeled datasets that
induce large annotation costs and (2) work well for known viewpoints, but not
for novel points of view. We propose HyEnA, a hybrid (human + AI) method for
extracting arguments from opinionated texts, combining the speed of automated
processing with the understanding and reasoning capabilities of humans. We
evaluate HyEnA on three citizen feedback corpora. We find that, on the one
hand, HyEnA achieves higher coverage and precision than a state-of-the-art
automated method when compared to a common set of diverse opinions, justifying
the need for human insight. On the other hand, HyEnA requires less human effort
and does not compromise quality compared to (fully manual) expert analysis,
demonstrating the benefit of combining human and artificial intelligence.",cs.AI cs.CL cs.HC,2024-03-11
"Attention-based Class-Conditioned Alignment for Multi-Source Domain
  Adaptation of Object Detectors",,"Domain adaptation methods for object detection (OD) strive to mitigate the
impact of distribution shifts by promoting feature alignment across source and
target domains. Multi-source domain adaptation (MSDA) allows leveraging
multiple annotated source datasets and unlabeled target data to improve the
accuracy and robustness of the detection model. Most state-of-the-art MSDA
methods for OD perform feature alignment in a class-agnostic manner. This is
challenging since the objects have unique modal information due to variations
in object appearance across domains. A recent prototype-based approach proposed
a class-wise alignment, yet it suffers from error accumulation due to noisy
pseudo-labels that can negatively affect adaptation with imbalanced data. To
overcome these limitations, we propose an attention-based class-conditioned
alignment method for MSDA that aligns instances of each object category across
domains. In particular, an attention module coupled with an adversarial domain
classifier allows learning domain-invariant and class-specific instance
representations. Experimental results on multiple benchmarking MSDA datasets
indicate that our method outperforms the state-of-the-art methods and is robust
to class imbalance using a conceptually simple class-conditioning method. Our
code is available at https://github.com/imatif17/ACIA.",cs.CV cs.LG,2024-03-14
"Inter-individual and inter-site neural code conversion without shared
  stimuli",,"Inter-individual variability in fine-grained functional brain organization
poses challenges for scalable data analysis and modeling. Functional alignment
techniques can help mitigate these individual differences but typically require
paired brain data with the same stimuli between individuals, which is often
unavailable. We present a neural code conversion method that overcomes this
constraint by optimizing conversion parameters based on the discrepancy between
the stimulus contents represented by original and converted brain activity
patterns. This approach, combined with hierarchical features of deep neural
networks (DNNs) as latent content representations, achieves conversion accuracy
comparable to methods using shared stimuli. The converted brain activity from a
source subject can be accurately decoded using the target's pre-trained
decoders, producing high-quality visual image reconstructions that rival
within-individual decoding, even with data across different sites and limited
training samples. Our approach offers a promising framework for scalable neural
data analysis and modeling and a foundation for brain-to-brain communication.",q-bio.NC cs.HC,2024-03-18
AlignRec: Aligning and Training in Multimodal Recommendations,,"With the development of multimedia systems, multimodal recommendations are
playing an essential role, as they can leverage rich contexts beyond
interactions. Existing methods mainly regard multimodal information as an
auxiliary, using them to help learn ID features; However, there exist semantic
gaps among multimodal content features and ID-based features, for which
directly using multimodal information as an auxiliary would lead to
misalignment in representations of users and items. In this paper, we first
systematically investigate the misalignment issue in multimodal
recommendations, and propose a solution named AlignRec. In AlignRec, the
recommendation objective is decomposed into three alignments, namely alignment
within contents, alignment between content and categorical ID, and alignment
between users and items. Each alignment is characterized by a specific
objective function and is integrated into our multimodal recommendation
framework. To effectively train AlignRec, we propose starting from pre-training
the first alignment to obtain unified multimodal features and subsequently
training the following two alignments together with these features as input. As
it is essential to analyze whether each multimodal feature helps in training
and accelerate the iteration cycle of recommendation models, we design three
new classes of metrics to evaluate intermediate performance. Our extensive
experiments on three real-world datasets consistently verify the superiority of
AlignRec compared to nine baselines. We also find that the multimodal features
generated by AlignRec are better than currently used ones, which are to be
open-sourced in our repository https://github.com/sjtulyf123/AlignRec_CIKM24.",cs.IR cs.LG,2024-03-18
Saliency Guided Image Warping for Unsupervised Domain Adaptation,,"Driving is challenging in conditions like night, rain, and snow. The lack of
good labeled datasets has hampered progress in scene understanding under such
conditions. Unsupervised domain adaptation (UDA) using large labeled clear-day
datasets is a promising research direction in such cases. Current UDA methods,
however, treat all image pixels uniformly, leading to over-reliance on the
dominant scene backgrounds (e.g., roads, sky, sidewalks) that appear
dramatically different across domains. As a result, they struggle to learn
effective features of smaller and often sparse foreground objects (e.g.,
people, vehicles, signs).
  In this work, we improve UDA training by using in-place image warping to
focus on salient object regions. Our insight is that while backgrounds vary
significantly across domains (e.g., snowy night vs. clear day), object
appearances vary to a lesser extent. Therefore, we design instance-level
saliency guidance to adaptively oversample object regions, which reduces
adverse effects from background context and enhances backbone feature learning.
We then unwarp the better learned features while adapting from source to
target. Our approach improves adaptation across geographies, lighting, and
weather conditions, and is agnostic to the task (segmentation, detection),
domain adaptation algorithm, saliency guidance, and underlying model
architecture. Result highlights include +6.1 mAP50 for BDD100K Clear
$\rightarrow$ DENSE Foggy, +3.7 mAP50 for BDD100K Day $\rightarrow$ Night, +3.0
mAP50 for BDD100K Clear $\rightarrow$ Rainy, and +6.3 mIoU for Cityscapes
$\rightarrow$ ACDC. Our method adds minimal training memory and incurs no
additional inference latency. Please see Appendix for more results and
analysis.",cs.CV cs.LG,2024-03-19
The opportunities and risks of large language models in mental health,,"Global rates of mental health concerns are rising, and there is increasing
realization that existing models of mental health care will not adequately
expand to meet the demand. With the emergence of large language models (LLMs)
has come great optimism regarding their promise to create novel, large-scale
solutions to support mental health. Despite their nascence, LLMs have already
been applied to mental health related tasks. In this paper, we summarize the
extant literature on efforts to use LLMs to provide mental health education,
assessment, and intervention and highlight key opportunities for positive
impact in each area. We then highlight risks associated with LLMs' application
to mental health and encourage the adoption of strategies to mitigate these
risks. The urgent need for mental health support must be balanced with
responsible development, testing, and deployment of mental health LLMs. It is
especially critical to ensure that mental health LLMs are fine-tuned for mental
health, enhance mental health equity, and adhere to ethical standards and that
people, including those with lived experience with mental health concerns, are
involved in all stages from development through deployment. Prioritizing these
efforts will minimize potential harms to mental health and maximize the
likelihood that LLMs will positively impact mental health globally.",cs.CL cs.AI cs.CY cs.HC cs.LG,2024-03-21
"Navigating Fairness: Practitioners' Understanding, Challenges, and
  Strategies in AI/ML Development",,"The rise in the use of AI/ML applications across industries has sparked more
discussions about the fairness of AI/ML in recent times. While prior research
on the fairness of AI/ML exists, there is a lack of empirical studies focused
on understanding the perspectives and experiences of AI practitioners in
developing a fair AI/ML system. Understanding AI practitioners' perspectives
and experiences on the fairness of AI/ML systems are important because they are
directly involved in its development and deployment and their insights can
offer valuable real-world perspectives on the challenges associated with
ensuring fairness in AI/ML systems. We conducted semi-structured interviews
with 22 AI practitioners to investigate their understanding of what a 'fair
AI/ML' is, the challenges they face in developing a fair AI/ML system, the
consequences of developing an unfair AI/ML system, and the strategies they
employ to ensure AI/ML system fairness. We developed a framework showcasing the
relationship between AI practitioners' understanding of 'fair AI/ML' system and
(i) their challenges in its development, (ii) the consequences of developing an
unfair AI/ML system, and (iii) strategies used to ensure AI/ML system fairness.
By exploring AI practitioners' perspectives and experiences, this study
provides actionable insights to enhance AI/ML fairness, which may promote
fairer systems, reduce bias, and foster public trust in AI technologies.
Additionally, we also identify areas for further investigation and offer
recommendations to aid AI practitioners and AI companies in navigating
fairness.",cs.CY cs.AI cs.SE,2024-03-20
"A Survey on Self-Supervised Graph Foundation Models: Knowledge-Based
  Perspective",,"Graph self-supervised learning (SSL) is now a go-to method for pre-training
graph foundation models (GFMs). There is a wide variety of knowledge patterns
embedded in the graph data, such as node properties and clusters, which are
crucial to learning generalized representations for GFMs. However, existing
surveys of GFMs have several shortcomings: they lack comprehensiveness
regarding the most recent progress, have unclear categorization of
self-supervised methods, and take a limited architecture-based perspective that
is restricted to only certain types of graph models. As the ultimate goal of
GFMs is to learn generalized graph knowledge, we provide a comprehensive survey
of self-supervised GFMs from a novel knowledge-based perspective. We propose a
knowledge-based taxonomy, which categorizes self-supervised graph models by the
specific graph knowledge utilized. Our taxonomy consists of microscopic (nodes,
links, etc.), mesoscopic (context, clusters, etc.), and macroscopic knowledge
(global structure, manifolds, etc.). It covers a total of 9 knowledge
categories and more than 25 pretext tasks for pre-training GFMs, as well as
various downstream task generalization strategies. Such a knowledge-based
taxonomy allows us to re-examine graph models based on new architectures more
clearly, such as graph language models, as well as provide more in-depth
insights for constructing GFMs.",cs.LG cs.SI,2024-03-24
"Towards a FAIR Documentation of Workflows and Models in Applied
  Mathematics",,"Modeling-Simulation-Optimization workflows play a fundamental role in applied
mathematics. The Mathematical Research Data Initiative, MaRDI, responded to
this by developing a FAIR and machine-interpretable template for a
comprehensive documentation of such workflows. MaRDMO, a Plugin for the
Research Data Management Organiser, enables scientists from diverse fields to
document and publish their workflows on the MaRDI Portal seamlessly using the
MaRDI template. Central to these workflows are mathematical models. MaRDI
addresses them with the MathModDB ontology, offering a structured formal model
description. Here, we showcase the interaction between MaRDMO and the MathModDB
Knowledge Graph through an algebraic modeling workflow from the Digital
Humanities. This demonstration underscores the versatility of both services
beyond their original numerical domain.",cs.AI cs.DB cs.DL,2024-03-26
Grappa -- A Machine Learned Molecular Mechanics Force Field,,"Simulating large molecular systems over long timescales requires force fields
that are both accurate and efficient. In recent years, E(3) equivariant neural
networks have lifted the tension between computational efficiency and accuracy
of force fields, but they are still several orders of magnitude more expensive
than established molecular mechanics (MM) force fields. Here, we propose
Grappa, a machine learning framework to predict MM parameters from the
molecular graph, employing a graph attentional neural network and a transformer
with symmetry-preserving positional encoding. The resulting Grappa force field
outperformstabulated and machine-learned MM force fields in terms of accuracy
at the same computational efficiency and can be used in existing Molecular
Dynamics (MD) engines like GROMACS and OpenMM. It predicts energies and forces
of small molecules, peptides, RNA and - showcasing its extensibility to
uncharted regions of chemical space - radicals at state-of-the-art MM accuracy.
We demonstrate Grappa's transferability to macromolecules in MD simulations
from a small fast folding protein up to a whole virus particle. Our force field
sets the stage for biomolecular simulations closer to chemical accuracy, but
with the same computational cost as established protein force fields.",physics.chem-ph cs.LG physics.comp-ph,2024-03-25
"Advancing Multimodal Data Fusion in Pain Recognition: A Strategy
  Leveraging Statistical Correlation and Human-Centered Perspectives",,"This research presents a novel multimodal data fusion methodology for pain
behavior recognition, integrating statistical correlation analysis with
human-centered insights. Our approach introduces two key innovations: 1)
integrating data-driven statistical relevance weights into the fusion strategy
to effectively utilize complementary information from heterogeneous modalities,
and 2) incorporating human-centric movement characteristics into multimodal
representation learning for detailed modeling of pain behaviors. Validated
across various deep learning architectures, our method demonstrates superior
performance and broad applicability. We propose a customizable framework that
aligns each modality with a suitable classifier based on statistical
significance, advancing personalized and effective multimodal fusion.
Furthermore, our methodology provides explainable analysis of multimodal data,
contributing to interpretable and explainable AI in healthcare. By highlighting
the importance of data diversity and modality-specific representations, we
enhance traditional fusion techniques and set new standards for recognizing
complex pain behaviors. Our findings have significant implications for
promoting patient-centered healthcare interventions and supporting explainable
clinical decision-making.",cs.AI,2024-03-30
Learning to Plan for Language Modeling from Unlabeled Data,,"By training to predict the next token in an unlabeled corpus, large language
models learn to perform many tasks without any labeled data. However, their
next-token-prediction objective arguably limits their performance in scenarios
that require planning, such as writing a coherent article. In this paper, we
train a module for planning the future writing process via a self-supervised
learning objective. Given the textual context, this planning module learns to
predict future abstract writing actions, which correspond to centroids in a
clustered text embedding space. By conditioning on these actions, our model
extends the successful language model formula to more abstract planning in an
unsupervised way. Empirically, we demonstrate that our method improves language
modeling performance in general, particularly with respect to the text
structure. Because our framework uses a planner module that is unsupervised and
external to the language model, new planner modules can be trained at large
scale and easily be shared with the community.",cs.CL cs.AI,2024-03-31
"T-Mamba: A unified framework with Long-Range Dependency in dual-domain
  for 2D & 3D Tooth Segmentation",,"Tooth segmentation is a pivotal step in modern digital dentistry, essential
for applications across orthodontic diagnosis and treatment planning. Despite
its importance, this process is fraught with challenges due to the high noise
and low contrast inherent in 2D and 3D tooth data. Both Convolutional Neural
Networks (CNNs) and Transformers has shown promise in medical image
segmentation, yet each method has limitations in handling long-range
dependencies and computational complexity. To address this issue, this paper
introduces T-Mamba, integrating frequency-based features and shared
bi-positional encoding into vision mamba to address limitations in efficient
global feature modeling. Besides, we design a gate selection unit to integrate
two features in spatial domain and one feature in frequency domain adaptively.
T-Mamba is the first work to introduce frequency-based features into vision
mamba, and its flexibility allows it to process both 2D and 3D tooth data
without the need for separate modules. Also, the TED3, a large-scale public
tooth 2D dental X-ray dataset, has been presented in this paper. Extensive
experiments demonstrate that T-Mamba achieves new SOTA results on a public
tooth CBCT dataset and outperforms previous SOTA methods on TED3 dataset. The
code and models are publicly available at: https://github.com/isbrycee/T-Mamba.",cs.CV,2024-04-01
"Variable-Length Stop-Feedback Coding for Minimum Age of Incorrect
  Information",,"The Age of Incorrect Information (AoII) is studied within the context of
remote monitoring a Markov source using variable-length stop-feedback (VLSF)
coding. Leveraging recent results on the non-asymptotic channel coding rate, we
consider sources with small cardinality, where feedback is non-instantaneous as
the transmitted information and feedback message have comparable lengths. We
focus on the feedback sequence, i.e. the times of feedback transmissions, and
derive AoII-optimal and delay-optimal feedback sequences. Our results showcase
the impact of the feedback sequence on the AoII, revealing that a lower average
delay does not necessarily correspond to a lower average AoII. We discuss the
implications of our findings and suggest directions for coding scheme design.",cs.IT math.IT,2024-04-01
"Modality Translation for Object Detection Adaptation Without Forgetting
  Prior Knowledge",,"A common practice in deep learning involves training large neural networks on
massive datasets to achieve high accuracy across various domains and tasks.
While this approach works well in many application areas, it often fails
drastically when processing data from a new modality with a significant
distribution shift from the data used to pre-train the model. This paper
focuses on adapting a large object detection model trained on RGB images to new
data extracted from IR images with a substantial modality shift. We propose
Modality Translator (ModTr) as an alternative to the common approach of
fine-tuning a large model to the new modality. ModTr adapts the IR input image
with a small transformation network trained to directly minimize the detection
loss. The original RGB model can then work on the translated inputs without any
further changes or fine-tuning to its parameters. Experimental results on
translating from IR to RGB images on two well-known datasets show that our
simple approach provides detectors that perform comparably or better than
standard fine-tuning, without forgetting the knowledge of the original model.
This opens the door to a more flexible and efficient service-based detection
pipeline, where a unique and unaltered server, such as an RGB detector, runs
constantly while being queried by different modalities, such as IR with the
corresponding translations model. Our code is available at:
https://github.com/heitorrapela/ModTr.",cs.CV cs.AI,2024-04-01
"Proper Implicit Discretization of Arbitrary-Order Robust Exact
  Differentiators",,"This paper considers the implicit Euler discretization of Levant's arbitrary
order robust exact differentiator in presence of sampled measurements. Existing
implicit discretizations of that differentiator are shown to exhibit either
unbounded bias errors or, surprisingly, discretization chattering despite the
use of the implicit discretization. A new, proper implicit discretization that
exhibits neither of these two detrimental effects is proposed by computing the
differentiator's outputs as appropriately designed linear combinations of its
state variables. A numerical differentiator implementation is discussed and
closed-form stability conditions for arbitrary differentiation orders are
given. The influence of bounded measurement noise and numerical approximation
errors is formally analyzed. Numerical simulations confirm the obtained
results.",math.NA cs.NA cs.SY eess.SY,2024-04-03
"Enhancing Interpretability of Vertebrae Fracture Grading using
  Human-interpretable Prototypes",,"Vertebral fracture grading classifies the severity of vertebral fractures,
which is a challenging task in medical imaging and has recently attracted Deep
Learning (DL) models. Only a few works attempted to make such models
human-interpretable despite the need for transparency and trustworthiness in
critical use cases like DL-assisted medical diagnosis. Moreover, such models
either rely on post-hoc methods or additional annotations. In this work, we
propose a novel interpretable-by-design method, ProtoVerse, to find relevant
sub-parts of vertebral fractures (prototypes) that reliably explain the model's
decision in a human-understandable way. Specifically, we introduce a novel
diversity-promoting loss to mitigate prototype repetitions in small datasets
with intricate semantics. We have experimented with the VerSe'19 dataset and
outperformed the existing prototype-based method. Further, our model provides
superior interpretability against the post-hoc method. Importantly, expert
radiologists validated the visual interpretability of our results, showing
clinical applicability.",cs.CV cs.AI,2024-04-03
"Site-specific Deterministic Temperature and Humidity Forecasts with
  Explainable and Reliable Machine Learning",,"Site-specific weather forecasts are essential to accurate prediction of power
demand and are consequently of great interest to energy operators. However,
weather forecasts from current numerical weather prediction (NWP) models lack
the fine-scale detail to capture all important characteristics of localised
real-world sites. Instead they provide weather information representing a
rectangular gridbox (usually kilometres in size). Even after post-processing
and bias correction, area-averaged information is usually not optimal for
specific sites. Prior work on site optimised forecasts has focused on linear
methods, weighted consensus averaging, time-series methods, and others. Recent
developments in machine learning (ML) have prompted increasing interest in
applying ML as a novel approach towards this problem. In this study, we
investigate the feasibility of optimising forecasts at sites by adopting the
popular machine learning model gradient boosting decision tree, supported by
the Python version of the XGBoost package. Regression trees have been trained
with historical NWP and site observations as training data, aimed at predicting
temperature and dew point at multiple site locations across Australia. We
developed a working ML framework, named 'Multi-SiteBoost' and initial testing
results show a significant improvement compared with gridded values from
bias-corrected NWP models. The improvement from XGBoost is found to be
comparable with non-ML methods reported in literature. With the insights
provided by SHapley Additive exPlanations (SHAP), this study also tests various
approaches to understand the ML predictions and increase the reliability of the
forecasts generated by ML.",physics.ao-ph cs.LG,2024-04-04
"A High-Fidelity Simulation Framework for Grasping Stability Analysis in
  Human Casualty Manipulation",,"Recently, there has been a growing interest in rescue robots due to their
vital role in addressing emergency scenarios and providing crucial support in
challenging or hazardous situations where human intervention is difficult.
However, very few of these robots are capable of actively engaging with humans
and undertaking physical manipulation tasks. This limitation is largely
attributed to the absence of tools that can realistically simulate physical
interactions, especially the contact mechanisms between a robotic gripper and a
human body. In this letter, we aim to address key limitations in current
developments towards robotic casualty manipulation. Firstly, we present an
integrative simulation framework for casualty manipulation. We adapt a finite
element method (FEM) tool into the grasping and manipulation scenario, and the
developed framework can provide accurate biomechanical reactions resulting from
manipulation. Secondly, we conduct a detailed assessment of grasping stability
during casualty grasping and manipulation simulations. To validate the
necessity and superior performance of the proposed high-fidelity simulation
framework, we conducted a qualitative and quantitative comparison of grasping
stability analyses between the proposed framework and the state-of-the-art
multi-body physics simulations. Through these efforts, we have taken the first
step towards a feasible solution for robotic casualty manipulation.",cs.RO,2024-04-04
DATENeRF: Depth-Aware Text-based Editing of NeRFs,,"Recent advancements in diffusion models have shown remarkable proficiency in
editing 2D images based on text prompts. However, extending these techniques to
edit scenes in Neural Radiance Fields (NeRF) is complex, as editing individual
2D frames can result in inconsistencies across multiple views. Our crucial
insight is that a NeRF scene's geometry can serve as a bridge to integrate
these 2D edits. Utilizing this geometry, we employ a depth-conditioned
ControlNet to enhance the coherence of each 2D image modification. Moreover, we
introduce an inpainting approach that leverages the depth information of NeRF
scenes to distribute 2D edits across different images, ensuring robustness
against errors and resampling challenges. Our results reveal that this
methodology achieves more consistent, lifelike, and detailed edits than
existing leading methods for text-driven NeRF scene editing.",cs.CV,2024-04-06
"Optimizing Parameters of the LinDistFlow Power Flow Approximation for
  Distribution Systems",,"The DistFlow model accurately represents power flows in distribution systems,
but the model's nonlinearities result in computational challenges for many
applications. Accordingly, a linear approximation known as \mbox{LinDistFlow}
(and its three-phase extension LinDist3Flow) is commonly employed. This paper
introduces a parameter optimization algorithm for enhancing the accuracy of
this approximation for both balanced single-phase equivalent and unbalanced
three-phase distribution network models, with the goal of aligning the outputs
more closely with those from the nonlinear DistFlow model. Using sensitivity
information, our algorithm optimizes the LinDistFlow approximation's
coefficient and bias parameters to minimize discrepancies in predictions of
voltage magnitudes relative to the nonlinear DistFlow model. The parameter
optimization algorithm employs the Truncated Newton Conjugate-Gradient (TNC)
method to fine-tune coefficients and bias parameters during an offline training
phase to improve the LinDistFlow approximation's accuracy. % in optimization
applications. Numerical results underscore the algorithm's efficacy, showcasing
accuracy improvements in $L_{1}$-norm and $L_{\infty}$-norm losses of up to
$92\%$ and $88\%$, respectively, relative to the traditional LinDistFlow model.
We also assess how the optimized parameters perform under changes in the
network topology and demonstrate the optimized LinDistFlow approximation's
efficacy in a hosting capacity optimization problem.",eess.SY cs.SY,2024-04-07
"Neural Cellular Automata for Lightweight, Robust and Explainable
  Classification of White Blood Cell Images",,"Diagnosis of hematological malignancies depends on accurate identification of
white blood cells in peripheral blood smears. Deep learning techniques are
emerging as a viable solution to scale and optimize this process by automatic
cell classification. However, these techniques face several challenges such as
limited generalizability, sensitivity to domain shifts, and lack of
explainability. Here, we introduce a novel approach for white blood cell
classification based on neural cellular automata (NCA). We test our approach on
three datasets of white blood cell images and show that we achieve competitive
performance compared to conventional methods. Our NCA-based method is
significantly smaller in terms of parameters and exhibits robustness to domain
shifts. Furthermore, the architecture is inherently explainable, providing
insights into the decision process for each classification, which helps to
understand and validate model predictions. Our results demonstrate that NCA can
be used for image classification, and that they address key challenges of
conventional methods, indicating a high potential for applicability in clinical
practice.",cs.CV eess.IV,2024-04-08
"Hierarchical Insights: Exploiting Structural Similarities for Reliable
  3D Semantic Segmentation",,"Safety-critical applications such as autonomous driving require robust 3D
environment perception algorithms capable of handling diverse and ambiguous
surroundings. The predictive performance of classification models is heavily
influenced by the dataset and the prior knowledge provided by the annotated
labels. While labels guide the learning process, they often fail to capture the
inherent relationships between classes that are naturally understood by humans.
We propose a training strategy for a 3D LiDAR semantic segmentation model that
learns structural relationships between classes through abstraction. This is
achieved by implicitly modeling these relationships using a learning rule for
hierarchical multi-label classification (HMC). Our detailed analysis
demonstrates that this training strategy not only improves the model's
confidence calibration but also retains additional information useful for
downstream tasks such as fusion, prediction, and planning.",cs.CV cs.AI cs.RO,2024-04-09
"Improving Retrieval for RAG based Question Answering Models on Financial
  Documents",,"The effectiveness of Large Language Models (LLMs) in generating accurate
responses relies heavily on the quality of input provided, particularly when
employing Retrieval Augmented Generation (RAG) techniques. RAG enhances LLMs by
sourcing the most relevant text chunk(s) to base queries upon. Despite the
significant advancements in LLMs' response quality in recent years, users may
still encounter inaccuracies or irrelevant answers; these issues often stem
from suboptimal text chunk retrieval by RAG rather than the inherent
capabilities of LLMs. To augment the efficacy of LLMs, it is crucial to refine
the RAG process. This paper explores the existing constraints of RAG pipelines
and introduces methodologies for enhancing text retrieval. It delves into
strategies such as sophisticated chunking techniques, query expansion, the
incorporation of metadata annotations, the application of re-ranking
algorithms, and the fine-tuning of embedding algorithms. Implementing these
approaches can substantially improve the retrieval quality, thereby elevating
the overall performance and reliability of LLMs in processing and responding to
queries.",cs.IR cs.CL cs.LG q-fin.GN,2024-03-22
Obstructions to return preservation for episturmian morphisms,,"This paper studies obstructions to preservation of return sets by episturmian
morphisms. We show, by way of an explicit construction, that infinitely many
obstructions exist. This generalizes and improves an earlier result about
Sturmian morphisms.",math.CO cs.DM,2024-04-11
Tight Bounds for Sorting Under Partial Information,,"Sorting has a natural generalization where the input consists of: (1) a
ground set $X$ of size $n$, (2) a partial oracle $O_P$ specifying some fixed
partial order $P$ on $X$ and (3) a linear oracle $O_L$ specifying a linear
order $L$ that extends $P$. The goal is to recover the linear order $L$ on $X$
using the fewest number of linear oracle queries.
  In this problem, we measure algorithmic complexity through three metrics:
oracle queries to $O_L$, oracle queries to $O_P$, and the time spent. Any
algorithm requires worst-case $\log_2 e(P)$ linear oracle queries to recover
the linear order on $X$.
  Kahn and Saks presented the first algorithm that uses $\Theta(\log e(P))$
linear oracle queries (using $O(n^2)$ partial oracle queries and exponential
time). The state-of-the-art for the general problem is by Cardinal, Fiorini,
Joret, Jungers and Munro who at STOC'10 manage to separate the linear and
partial oracle queries into a preprocessing and query phase. They can
preprocess $P$ using $O(n^2)$ partial oracle queries and $O(n^{2.5})$ time.
Then, given $O_L$, they uncover the linear order on $X$ in $\Theta(\log e(P))$
linear oracle queries and $O(n + \log e(P))$ time -- which is worst-case
optimal in the number of linear oracle queries but not in the time spent.
  For $c \geq 1$, our algorithm can preprocess $O_P$ using $O(n^{1 +
\frac{1}{c}})$ queries and time. Given $O_L$, we uncover $L$ using $\Theta(c
\log e(P))$ queries and time. We show a matching lower bound, as there exist
positive constants $(\alpha, \beta)$ where for any constant $c \geq 1$, any
algorithm that uses at most $\alpha \cdot n^{1 + \frac{1}{c}}$ preprocessing
must use worst-case at least $\beta \cdot c \log e(P)$ linear oracle queries.
Thus, we solve the problem of sorting under partial information through an
algorithm that is asymptotically tight across all three metrics.",cs.DS,2024-04-12
Swing-Up of a Weakly Actuated Double Pendulum via Nonlinear Normal Modes,,"We identify the nonlinear normal modes spawning from the stable equilibrium
of a double pendulum under gravity, and we establish their connection to
homoclinic orbits through the unstable upright position as energy increases.
This result is exploited to devise an efficient swing-up strategy for a double
pendulum with weak, saturating actuators. Our approach involves stabilizing the
system onto periodic orbits associated with the nonlinear modes while gradually
injecting energy. Since these modes are autonomous system evolutions, the
required control effort for stabilization is minimal. Even with actuator
limitations of less than 1% of the maximum gravitational torque, the proposed
method accomplishes the swing-up of the double pendulum by allowing sufficient
time.",eess.SY cs.RO cs.SY,2024-04-12
"Explainable Light-Weight Deep Learning Pipeline for Improved Drought
  Stress Identification",,"Early identification of drought stress in crops is vital for implementing
effective mitigation measures and reducing yield loss. Non-invasive imaging
techniques hold immense potential by capturing subtle physiological changes in
plants under water deficit. Sensor based imaging data serves as a rich source
of information for machine learning and deep learning algorithms, facilitating
further analysis aimed at identifying drought stress. While these approaches
yield favorable results, real-time field applications requires algorithms
specifically designed for the complexities of natural agricultural conditions.
Our work proposes a novel deep learning framework for classifying drought
stress in potato crops captured by UAVs in natural settings. The novelty lies
in the synergistic combination of a pre-trained network with carefully designed
custom layers. This architecture leverages feature extraction capabilities of
the pre-trained network while the custom layers enable targeted dimensionality
reduction and enhanced regularization, ultimately leading to improved
performance. A key innovation of our work involves the integration of
Gradient-Class Activation Mapping (Grad-CAM), an explainability technique.
Grad-CAM sheds light on the internal workings of the deep learning model,
typically referred to as a black box. By visualizing the focus areas of the
model within the images, Grad-CAM fosters interpretability and builds trust in
the decision-making process of the model. Our proposed framework achieves
superior performance, particularly with the DenseNet121 pre-trained network,
reaching a precision of 97% to identify the stressed class with an overall
accuracy of 91%. Comparative analysis of existing state-of-the-art object
detection algorithms reveals the superiority of our approach in significantly
higher precision and accuracy.",cs.CV,2024-04-15
"Graph neural network-based surrogate modelling for real-time hydraulic
  prediction of urban drainage networks",,"Physics-based models are computationally time-consuming and infeasible for
real-time scenarios of urban drainage networks, and a surrogate model is needed
to accelerate the online predictive modelling. Fully-connected neural networks
(NNs) are potential surrogate models, but may suffer from low interpretability
and efficiency in fitting complex targets. Owing to the state-of-the-art
modelling power of graph neural networks (GNNs) and their match with urban
drainage networks in the graph structure, this work proposes a GNN-based
surrogate of the flow routing model for the hydraulic prediction problem of
drainage networks, which regards recent hydraulic states as initial conditions,
and future runoff and control policy as boundary conditions. To incorporate
hydraulic constraints and physical relationships into drainage modelling,
physics-guided mechanisms are designed on top of the surrogate model to
restrict the prediction variables with flow balance and flooding occurrence
constraints. According to case results in a stormwater network, the GNN-based
model is more cost-effective with better hydraulic prediction accuracy than the
NN-based model after equal training epochs, and the designed mechanisms further
limit prediction errors with interpretable domain knowledge. As the model
structure adheres to the flow routing mechanisms and hydraulic constraints in
urban drainage networks, it provides an interpretable and effective solution
for data-driven surrogate modelling. Simultaneously, the surrogate model
accelerates the predictive modelling of urban drainage networks for real-time
use compared with the physics-based model.",cs.LG cs.CE cs.SY eess.SY,2024-04-16
"Label merge-and-split: A graph-colouring approach for memory-efficient
  brain parcellation",,"Whole brain parcellation requires inferring hundreds of segmentation labels
in large image volumes and thus presents significant practical challenges for
deep learning approaches. We introduce label merge-and-split, a method that
first greatly reduces the effective number of labels required for
learning-based whole brain parcellation and then recovers original labels.
Using a greedy graph colouring algorithm, our method automatically groups and
merges multiple spatially separate labels prior to model training and
inference. The merged labels may be semantically unrelated. A deep learning
model is trained to predict merged labels. At inference time, original labels
are restored using atlas-based influence regions. In our experiments, the
proposed approach reduces the number of labels by up to 68% while achieving
segmentation accuracy comparable to the baseline method without label merging
and splitting. Moreover, model training and inference times as well as GPU
memory requirements were reduced significantly. The proposed method can be
applied to all semantic segmentation tasks with a large number of spatially
separate classes within an atlas-based prior.",cs.CV,2024-04-16
"Automatic classification of prostate MR series type using image content
  and metadata",,"With the wealth of medical image data, efficient curation is essential.
Assigning the sequence type to magnetic resonance images is necessary for
scientific studies and artificial intelligence-based analysis. However,
incomplete or missing metadata prevents effective automation. We therefore
propose a deep-learning method for classification of prostate cancer scanning
sequences based on a combination of image data and DICOM metadata. We
demonstrate superior results compared to metadata or image data alone, and make
our code publicly available at
https://github.com/deepakri201/DICOMScanClassification.",eess.IV cs.CV,2024-04-16
"Multi-view X-ray Image Synthesis with Multiple Domain Disentanglement
  from CT Scans",,"X-ray images play a vital role in the intraoperative processes due to their
high resolution and fast imaging speed and greatly promote the subsequent
segmentation, registration and reconstruction. However, over-dosed X-rays
superimpose potential risks to human health to some extent. Data-driven
algorithms from volume scans to X-ray images are restricted by the scarcity of
paired X-ray and volume data. Existing methods are mainly realized by modelling
the whole X-ray imaging procedure. In this study, we propose a learning-based
approach termed CT2X-GAN to synthesize the X-ray images in an end-to-end manner
using the content and style disentanglement from three different image domains.
Our method decouples the anatomical structure information from CT scans and
style information from unpaired real X-ray images/ digital reconstructed
radiography (DRR) images via a series of decoupling encoders. Additionally, we
introduce a novel consistency regularization term to improve the stylistic
resemblance between synthesized X-ray images and real X-ray images. Meanwhile,
we also impose a supervised process by computing the similarity of computed
real DRR and synthesized DRR images. We further develop a pose attention module
to fully strengthen the comprehensive information in the decoupled content code
from CT scans, facilitating high-quality multi-view image synthesis in the
lower 2D space. Extensive experiments were conducted on the publicly available
CTSpine1K dataset and achieved 97.8350, 0.0842 and 3.0938 in terms of FID, KID
and defined user-scored X-ray similarity, respectively. In comparison with
3D-aware methods ($\pi$-GAN, EG3D), CT2X-GAN is superior in improving the
synthesis quality and realistic to the real X-ray images.",eess.IV cs.CV,2024-04-18
Debiased Distribution Compression,,"Modern compression methods can summarize a target distribution $\mathbb{P}$
more succinctly than i.i.d. sampling but require access to a low-bias input
sequence like a Markov chain converging quickly to $\mathbb{P}$. We introduce a
new suite of compression methods suitable for compression with biased input
sequences. Given $n$ points targeting the wrong distribution and quadratic
time, Stein kernel thinning (SKT) returns $\sqrt{n}$ equal-weighted points with
$\widetilde{O}(n^{-1/2})$ maximum mean discrepancy (MMD) to $\mathbb{P}$. For
larger-scale compression tasks, low-rank SKT achieves the same feat in
sub-quadratic time using an adaptive low-rank debiasing procedure that may be
of independent interest. For downstream tasks that support simplex or
constant-preserving weights, Stein recombination and Stein Cholesky achieve
even greater parsimony, matching the guarantees of SKT with as few as
$\text{poly-log}(n)$ weighted points. Underlying these advances are new
guarantees for the quality of simplex-weighted coresets, the spectral decay of
kernel matrices, and the covering numbers of Stein kernel Hilbert spaces. In
our experiments, our techniques provide succinct and accurate posterior
summaries while overcoming biases due to burn-in, approximate Markov chain
Monte Carlo, and tempering.",stat.ML cs.LG stat.CO stat.ME,2024-04-18
"Can LLMs Understand Computer Networks? Towards a Virtual System
  Administrator",,"Recent advancements in Artificial Intelligence, and particularly Large
Language Models (LLMs), offer promising prospects for aiding system
administrators in managing the complexity of modern networks. However, despite
this potential, a significant gap exists in the literature regarding the extent
to which LLMs can understand computer networks. Without empirical evidence,
system administrators might rely on these models without assurance of their
efficacy in performing network-related tasks accurately.
  In this paper, we are the first to conduct an exhaustive study on LLMs'
comprehension of computer networks. We formulate several research questions to
determine whether LLMs can provide correct answers when supplied with a network
topology and questions on it. To assess them, we developed a thorough framework
for evaluating LLMs' capabilities in various network-related tasks. We evaluate
our framework on multiple computer networks employing proprietary (e.g., GPT4)
and open-source (e.g., Llama2) models. Our findings in general purpose LLMs
using a zero-shot scenario demonstrate promising results, with the best model
achieving an average accuracy of 79.3%. Proprietary LLMs achieve noteworthy
results in small and medium networks, while challenges persist in comprehending
complex network topologies, particularly for open-source models. Moreover, we
provide insight into how prompt engineering can enhance the accuracy of some
tasks.",cs.NI cs.AI cs.ET,2024-04-19
Evolutionary Reinforcement Learning via Cooperative Coevolution,,"Recently, evolutionary reinforcement learning has obtained much attention in
various domains. Maintaining a population of actors, evolutionary reinforcement
learning utilises the collected experiences to improve the behaviour policy
through efficient exploration. However, the poor scalability of genetic
operators limits the efficiency of optimising high-dimensional neural
networks.To address this issue, this paper proposes a novel cooperative
coevolutionary reinforcement learning (CoERL) algorithm. Inspired by
cooperative coevolution, CoERL periodically and adaptively decomposes the
policy optimisation problem into multiple subproblems and evolves a population
of neural networks for each of the subproblems. Instead of using genetic
operators, CoERL directly searches for partial gradients to update the policy.
Updating policy with partial gradients maintains consistency between the
behaviour spaces of parents and offspring across generations.The experiences
collected by the population are then used to improve the entire policy, which
enhances the sampling efficiency.Experiments on six benchmark locomotion tasks
demonstrate that CoERL outperforms seven state-of-the-art algorithms and
baselines.Ablation study verifies the unique contribution of CoERL's core
ingredients.",cs.NE cs.AI,2024-04-23
Annealing approach to root-finding,,"The Newton-Raphson method is a fundamental root-finding technique with
numerous applications in physics. In this study, we propose a parameterized
variant of the Newton-Raphson method, inspired by principles from physics.
Through analytical and empirical validation, we demonstrate that this novel
approach offers increased robustness and faster convergence during root-finding
iterations. Furthermore, we establish connections to the Adomian series method
and provide a natural interpretation within a series framework. Remarkably, the
introduced parameter, akin to a temperature variable, enables an annealing
approach. This advancement sets the stage for a fresh exploration of numerical
iterative root-finding methodologies.",math.NA cs.NA nlin.CD,2024-04-09
"CriSp: Leveraging Tread Depth Maps for Enhanced Crime-Scene Shoeprint
  Matching",,"Shoeprints are a common type of evidence found at crime scenes and are used
regularly in forensic investigations. However, existing methods cannot
effectively employ deep learning techniques to match noisy and occluded
crime-scene shoeprints to a shoe database due to a lack of training data.
Moreover, all existing methods match crime-scene shoeprints to clean reference
prints, yet our analysis shows matching to more informative tread depth maps
yields better retrieval results. The matching task is further complicated by
the necessity to identify similarities only in corresponding regions (heels,
toes, etc) of prints and shoe treads. To overcome these challenges, we leverage
shoe tread images from online retailers and utilize an off-the-shelf predictor
to estimate depth maps and clean prints. Our method, named CriSp, matches
crime-scene shoeprints to tread depth maps by training on this data. CriSp
incorporates data augmentation to simulate crime-scene shoeprints, an encoder
to learn spatially-aware features, and a masking module to ensure only visible
regions of crime-scene prints affect retrieval results. To validate our
approach, we introduce two validation sets by reprocessing existing datasets of
crime-scene shoeprints and establish a benchmarking protocol for comparison. On
this benchmark, CriSp significantly outperforms state-of-the-art methods in
both automated shoeprint matching and image retrieval tailored to this task.",cs.CV,2024-04-25
"Open-Set Video-based Facial Expression Recognition with Human
  Expression-sensitive Prompting",,"In Video-based Facial Expression Recognition (V-FER), models are typically
trained on closed-set datasets with a fixed number of known classes. However,
these models struggle with unknown classes common in real-world scenarios. In
this paper, we introduce a challenging Open-set Video-based Facial Expression
Recognition (OV-FER) task, aiming to identify both known and new, unseen facial
expressions. While existing approaches use large-scale vision-language models
like CLIP to identify unseen classes, we argue that these methods may not
adequately capture the subtle human expressions needed for OV-FER. To address
this limitation, we propose a novel Human Expression-Sensitive Prompting (HESP)
mechanism to significantly enhance CLIP's ability to model video-based facial
expression details effectively. Our proposed HESP comprises three components:
1) a textual prompting module with learnable prompts to enhance CLIP's textual
representation of both known and unknown emotions, 2) a visual prompting module
that encodes temporal emotional information from video frames using
expression-sensitive attention, equipping CLIP with a new visual modeling
ability to extract emotion-rich information, and 3) an open-set multi-task
learning scheme that promotes interaction between the textual and visual
modules, improving the understanding of novel human emotions in video
sequences. Extensive experiments conducted on four OV-FER task settings
demonstrate that HESP can significantly boost CLIP's performance (a relative
improvement of 17.93% on AUROC and 106.18% on OSCR) and outperform other
state-of-the-art open-set video understanding methods by a large margin. Code
is available at https://github.com/cosinehuang/HESP.",cs.CV,2024-04-25
"Process Mining Embeddings: Learning Vector Representations for Petri
  Nets",,"Process Mining offers a powerful framework for uncovering, analyzing, and
optimizing real-world business processes. Petri nets provide a versatile means
of modeling process behavior. However, traditional methods often struggle to
effectively compare complex Petri nets, hindering their potential for process
enhancement. To address this challenge, we introduce PetriNet2Vec, an
unsupervised methodology inspired by Doc2Vec. This approach converts Petri nets
into embedding vectors, facilitating the comparison, clustering, and
classification of process models. We validated our approach using the PDC
Dataset, comprising 96 diverse Petri net models. The results demonstrate that
PetriNet2Vec effectively captures the structural properties of process models,
enabling accurate process classification and efficient process retrieval.
Specifically, our findings highlight the utility of the learned embeddings in
two key downstream tasks: process classification and process retrieval. In
process classification, the embeddings allowed for accurate categorization of
process models based on their structural properties. In process retrieval, the
embeddings enabled efficient retrieval of similar process models using cosine
distance. These results demonstrate the potential of PetriNet2Vec to
significantly enhance process mining capabilities.",cs.AI,2024-04-25
Large Language Models for Next Point-of-Interest Recommendation,,"The next Point of Interest (POI) recommendation task is to predict users'
immediate next POI visit given their historical data. Location-Based Social
Network (LBSN) data, which is often used for the next POI recommendation task,
comes with challenges. One frequently disregarded challenge is how to
effectively use the abundant contextual information present in LBSN data.
Previous methods are limited by their numerical nature and fail to address this
challenge. In this paper, we propose a framework that uses pretrained Large
Language Models (LLMs) to tackle this challenge. Our framework allows us to
preserve heterogeneous LBSN data in its original format, hence avoiding the
loss of contextual information. Furthermore, our framework is capable of
comprehending the inherent meaning of contextual information due to the
inclusion of commonsense knowledge. In experiments, we test our framework on
three real-world LBSN datasets. Our results show that the proposed framework
outperforms the state-of-the-art models in all three datasets. Our analysis
demonstrates the effectiveness of the proposed framework in using contextual
information as well as alleviating the commonly encountered cold-start and
short trajectory problems.",cs.IR cs.AI cs.LG,2024-04-19
Spectral-Spatial Mamba for Hyperspectral Image Classification,,"Recently, deep learning models have achieved excellent performance in
hyperspectral image (HSI) classification. Among the many deep models,
Transformer has gradually attracted interest for its excellence in modeling the
long-range dependencies of spatial-spectral features in HSI. However,
Transformer has the problem of quadratic computational complexity due to the
self-attention mechanism, which is heavier than other models and thus has
limited adoption in HSI processing. Fortunately, the recently emerging state
space model-based Mamba shows great computational efficiency while achieving
the modeling power of Transformers. Therefore, in this paper, we make a
preliminary attempt to apply the Mamba to HSI classification, leading to the
proposed spectral-spatial Mamba (SS-Mamba). Specifically, the proposed SS-Mamba
mainly consists of spectral-spatial token generation module and several stacked
spectral-spatial Mamba blocks. Firstly, the token generation module converts
any given HSI cube to spatial and spectral tokens as sequences. And then these
tokens are sent to stacked spectral-spatial mamba blocks (SS-MB). Each SS-MB
block consists of two basic mamba blocks and a spectral-spatial feature
enhancement module. The spatial and spectral tokens are processed separately by
the two basic mamba blocks, respectively. Besides, the feature enhancement
module modulates spatial and spectral tokens using HSI sample's center region
information. In this way, the spectral and spatial tokens cooperate with each
other and achieve information fusion within each block. The experimental
results conducted on widely used HSI datasets reveal that the proposed model
achieves competitive results compared with the state-of-the-art methods. The
Mamba-based method opens a new window for HSI classification.",cs.CV,2024-04-28
"A Concept for Semi-Automatic Configuration of Sufficiently Valid
  Simulation Setups for Automated Driving Systems",,"As simulation is increasingly used in scenario-based approaches to test
Automated Driving Systems, the credibility of simulation results is a major
concern. Arguably, credibility depends on the validity of the simulation setup
and simulation models. When selecting appropriate simulation models, a
trade-off must be made between validity, often connected to the model's
fidelity, and cost of computation. However, due to the large number of test
cases, expert-based methods to create sufficiently valid simulation setups seem
infeasible. We propose using design contracts in order to semi-automatically
compose simulation setups for given test cases from simulation models and to
derive requirements for the simulation models, supporting separation of
concerns between simulation model developers and users. Simulation model
contracts represent their validity domains by capturing a validity guarantee
and the associated operating conditions in an assumption. We then require the
composition of the simulation model contracts to refine a test case contract.
The latter contract captures the operating conditions of the test case in its
assumption and validity requirements in its guarantee. Based on this idea, we
present a framework that supports the compositional configuration of simulation
setups based on the contracts and a method to derive runtime monitors for these
simulation setups.",eess.SY cs.SY,2024-04-30
"Do Large Language Models Understand Conversational Implicature -- A case
  study with a chinese sitcom",,"Understanding the non-literal meaning of an utterance is critical for large
language models (LLMs) to become human-like social communicators. In this work,
we introduce SwordsmanImp, the first Chinese multi-turn-dialogue-based dataset
aimed at conversational implicature, sourced from dialogues in the Chinese
sitcom $\textit{My Own Swordsman}$. It includes 200 carefully handcrafted
questions, all annotated on which Gricean maxims have been violated. We test
eight close-source and open-source LLMs under two tasks: a multiple-choice
question task and an implicature explanation task. Our results show that GPT-4
attains human-level accuracy (94%) on multiple-choice questions. CausalLM
demonstrates a 78.5% accuracy following GPT-4. Other models, including GPT-3.5
and several open-source models, demonstrate a lower accuracy ranging from 20%
to 60% on multiple-choice questions. Human raters were asked to rate the
explanation of the implicatures generated by LLMs on their reasonability, logic
and fluency. While all models generate largely fluent and self-consistent text,
their explanations score low on reasonability except for GPT-4, suggesting that
most LLMs cannot produce satisfactory explanations of the implicatures in the
conversation. Moreover, we find LLMs' performance does not vary significantly
by Gricean maxims, suggesting that LLMs do not seem to process implicatures
derived from different maxims differently. Our data and code are available at
https://github.com/sjtu-compling/llm-pragmatics.",cs.CL,2024-04-30
Learning Tactile Insertion in the Real World,,"Humans have exceptional tactile sensing capabilities, which they can leverage
to solve challenging, partially observable tasks that cannot be solved from
visual observation alone. Research in tactile sensing attempts to unlock this
new input modality for robots. Lately, these sensors have become cheaper and,
thus, widely available. At the same time, the question of how to integrate them
into control loops is still an active area of research, with central challenges
being partial observability and the contact-rich nature of manipulation tasks.
In this study, we propose to use Reinforcement Learning to learn an end-to-end
policy, mapping directly from tactile sensor readings to actions. Specifically,
we use Dreamer-v3 on a challenging, partially observable robotic insertion task
with a Franka Research 3, both in simulation and on a real system. For the real
setup, we built a robotic platform capable of resetting itself fully
autonomously, allowing for extensive training runs without human supervision.
Our preliminary results indicate that Dreamer is capable of utilizing tactile
inputs to solve robotic manipulation tasks in simulation and reality.
Furthermore, we find that providing the robot with tactile feedback generally
improves task performance, though, in our setup, we do not yet include other
sensing modalities. In the future, we plan to utilize our platform to evaluate
a wide range of other Reinforcement Learning algorithms on tactile tasks.",cs.RO,2024-05-01
"Benchmarking Quantum Annealers with Near-Optimal Minor-Embedded
  Instances",,"Benchmarking Quantum Process Units (QPU) at an application level usually
requires considering the whole programming stack of the quantum computer. One
critical task is the minor-embedding (resp. transpilation) step, which involves
space-time overheads for annealing-based (resp. gate-based) quantum computers.
This paper establishes a new protocol to generate graph instances with their
associated near-optimal minor-embedding mappings to D-Wave Quantum Annealers
(QA). This set of favorable mappings is used to generate a wide diversity of
optimization problem instances. We use this method to benchmark QA on large
instances of unconstrained and constrained optimization problems and compare
the performance of the QPU with efficient classical solvers. The benchmark aims
to evaluate and quantify the key characteristics of instances that could
benefit from the use of a quantum computer. In this context, existing QA seem
best suited for unconstrained problems on instances with densities less than
$10\%$. For constrained problems, the penalty terms used to encode the hard
constraints restrict the performance of QA and suggest that these QPU will be
less efficient on these problems of comparable size.",quant-ph cs.ET cs.PF,2024-05-02
"Goal-conditioned reinforcement learning for ultrasound navigation
  guidance",,"Transesophageal echocardiography (TEE) plays a pivotal role in cardiology for
diagnostic and interventional procedures. However, using it effectively
requires extensive training due to the intricate nature of image acquisition
and interpretation. To enhance the efficiency of novice sonographers and reduce
variability in scan acquisitions, we propose a novel ultrasound (US) navigation
assistance method based on contrastive learning as goal-conditioned
reinforcement learning (GCRL). We augment the previous framework using a novel
contrastive patient batching method (CPB) and a data-augmented contrastive
loss, both of which we demonstrate are essential to ensure generalization to
anatomical variations across patients. The proposed framework enables
navigation to both standard diagnostic as well as intricate interventional
views with a single model. Our method was developed with a large dataset of 789
patients and obtained an average error of 6.56 mm in position and 9.36 degrees
in angle on a testing dataset of 140 patients, which is competitive or superior
to models trained on individual views. Furthermore, we quantitatively validate
our method's ability to navigate to interventional views such as the Left
Atrial Appendage (LAA) view used in LAA closure. Our approach holds promise in
providing valuable guidance during transesophageal ultrasound examinations,
contributing to the advancement of skill acquisition for cardiac ultrasound
practitioners.",cs.CV cs.AI,2024-05-02
"Digital Twin-Empowered Task Assignment in Aerial MEC Network: A Resource
  Coalition Cooperation Approach with Generative Model",,"To meet the demands for ubiquitous communication and temporary edge computing
in 6G networks, aerial mobile edge computing (MEC) networks have been
envisioned as a new paradigm. However, dynamic user requests pose challenges
for task assignment strategies. Most of the existing research assumes that the
strategy is deployed on ground-based stations or UAVs, which will be
ineffective in an environment lacking infrastructure and continuous energy
supply. Moreover, the resource mutual exclusion problem of dynamic task
assignment has not been effectively solved. Toward this end, we introduce the
digital twin (DT) into the aerial MEC network to study the resource coalition
cooperation approach with the generative model (GM), which provides a
preliminary coalition structure for the coalition game. Specifically, we
propose a novel network framework that is composed of an application plane, a
physical plane, and a virtual plane. After that, the task assignment problem is
simplified to convex optimization programming with linear constraints. And
then, we also propose a resource coalition cooperation approach that is based
on a transferable utility (TU) coalition game to obtain an approximate optimal
solution. Numerical results confirm the effectiveness of our proposed approach
in terms of energy consumption and utilization of resources.",cs.NI cs.AI,2024-03-17
Large Language Models (LLMs) as Agents for Augmented Democracy,,"We explore an augmented democracy system built on off-the-shelf LLMs
fine-tuned to augment data on citizen's preferences elicited over policies
extracted from the government programs of the two main candidates of Brazil's
2022 presidential election. We use a train-test cross-validation setup to
estimate the accuracy with which the LLMs predict both: a subject's individual
political choices and the aggregate preferences of the full sample of
participants. At the individual level, we find that LLMs predict out of sample
preferences more accurately than a ""bundle rule"", which would assume that
citizens always vote for the proposals of the candidate aligned with their
self-reported political orientation. At the population level, we show that a
probabilistic sample augmented by an LLM provides a more accurate estimate of
the aggregate preferences of a population than the non-augmented probabilistic
sample alone. Together, these results indicates that policy preference data
augmented using LLMs can capture nuances that transcend party lines and
represents a promising avenue of research for data augmentation.",cs.CY cs.AI cs.CL,2024-05-06
A Correlation-induced Finite Difference Estimator,,"Finite difference (FD) approximation is a classic approach to stochastic
gradient estimation when only noisy function realizations are available. In
this paper, we first provide a sample-driven method via the bootstrap technique
to estimate the optimal perturbation, and then propose an efficient FD
estimator based on correlated samples at the estimated optimal perturbation.
Furthermore, theoretical analyses of both the perturbation estimator and the FD
estimator reveal that, {\it surprisingly}, the correlation enables the proposed
FD estimator to achieve a reduction in variance and, in some cases, a decrease
in bias compared to the traditional optimal FD estimator. Numerical results
confirm the efficiency of our estimators and align well with the theory
presented, especially in scenarios with small sample sizes. Finally, we apply
the estimator to solve derivative-free optimization (DFO) problems, and
numerical studies show that DFO problems with 100 dimensions can be effectively
solved.",stat.ME cs.LG cs.NA math.NA math.OC,2024-05-09
Synthetic Tabular Data Validation: A Divergence-Based Approach,,"The ever-increasing use of generative models in various fields where tabular
data is used highlights the need for robust and standardized validation metrics
to assess the similarity between real and synthetic data. Current methods lack
a unified framework and rely on diverse and often inconclusive statistical
measures. Divergences, which quantify discrepancies between data distributions,
offer a promising avenue for validation. However, traditional approaches
calculate divergences independently for each feature due to the complexity of
joint distribution modeling. This paper addresses this challenge by proposing a
novel approach that uses divergence estimation to overcome the limitations of
marginal comparisons. Our core contribution lies in applying a divergence
estimator to build a validation metric considering the joint distribution of
real and synthetic data. We leverage a probabilistic classifier to approximate
the density ratio between datasets, allowing the capture of complex
relationships. We specifically calculate two divergences: the well-known
Kullback-Leibler (KL) divergence and the Jensen-Shannon (JS) divergence. KL
divergence offers an established use in the field, while JS divergence is
symmetric and bounded, providing a reliable metric. The efficacy of this
approach is demonstrated through a series of experiments with varying
distribution complexities. The initial phase involves comparing estimated
divergences with analytical solutions for simple distributions, setting a
benchmark for accuracy. Finally, we validate our method on a real-world dataset
and its corresponding synthetic counterpart, showcasing its effectiveness in
practical applications. This research offers a significant contribution with
applicability beyond tabular data and the potential to improve synthetic data
validation in various fields.",cs.LG cs.AI,2024-05-13
"Slice-aware Resource Allocation and Admission Control for Smart Factory
  Wireless Networks",,"The 5th generation (5G) and beyond network offers substantial promise as the
ideal wireless technology to replace the existing inflexible wired connections
in traditional factories of today. 5G network slicing allows for tailored
allocation of resources to different network services, each with unique Quality
of Service (QoS) requirements. This paper presents a novel solution for
slice-aware radio resource allocation based on a convex optimisation and
control framework for applications in smart factory wireless networks. The
proposed framework dynamically allocates minimum power and sub-channels to
downlink mixed service type industrial users categorised into three slices:
Capacity Limited (CL), Ultra Reliable Low Latency Communication (URLLC), and
Time Sensitive (TS) slices. Given that the base station (BS) has limited
transmission power, we enforce admission control by effectively relaxing the
target rate constraints for current connections in the CL slice. This rate
readjustment occurs whenever power consumption exceeds manageable levels.
Simulation results show that our approach minimises power, allocates
sub-channels to users, maintains slice isolation, and delivers QoS-specific
communications to users in all the slices despite time-varying number of users
and changing network conditions.",cs.NI cs.SY eess.SY,2024-05-14
"An Exact Theory of Causal Emergence for Linear Stochastic Iteration
  Systems",,"After coarse-graining a complex system, the dynamics of its macro-state may
exhibit more pronounced causal effects than those of its micro-state. This
phenomenon, known as causal emergence, is quantified by the indicator of
effective information. However, two challenges confront this theory: the
absence of well-developed frameworks in continuous stochastic dynamical systems
and the reliance on coarse-graining methodologies. In this study, we introduce
an exact theoretic framework for causal emergence within linear stochastic
iteration systems featuring continuous state spaces and Gaussian noise.
Building upon this foundation, we derive an analytical expression for effective
information across general dynamics and identify optimal linear coarse-graining
strategies that maximize the degree of causal emergence when the dimension
averaged uncertainty eliminated by coarse-graining has an upper bound. Our
investigation reveals that the maximal causal emergence and the optimal
coarse-graining methods are primarily determined by the principal eigenvalues
and eigenvectors of the dynamic system's parameter matrix, with the latter not
being unique. To validate our propositions, we apply our analytical models to
three simplified physical systems, comparing the outcomes with numerical
simulations, and consistently achieve congruent results.",cs.IT cs.SY eess.SY math.IT,2024-05-15
"Analysis and Predictive Modeling of Solar Coronal Holes Using Computer
  Vision and ARIMA-LSTM Networks",,"In the era of space exploration, coronal holes on the sun play a significant
role due to their impact on satellites and aircraft through their open magnetic
fields and increased solar wind emissions. This study employs computer vision
techniques to detect coronal hole regions and estimate their sizes using
imagery from the Solar Dynamics Observatory (SDO). Additionally, we utilize
hybrid time series prediction model, specifically combination of Long
Short-Term Memory (LSTM) networks and ARIMA, to analyze trends in the area of
coronal holes and predict their areas across various solar regions over a span
of seven days. By examining time series data, we aim to identify patterns in
coronal hole behavior and understand their potential effects on space weather.",astro-ph.SR astro-ph.EP cs.AI cs.LG,2024-05-16
About the Burton-Miller factor in the low frequency region,,"The Burton-Miller method is a widely used approach in acoustics to enhance
the stability of the boundary element method for exterior Helmholtz problems at
so-called critical frequencies. This method depends on a coupling parameter
$\eta$ and it can be shown that as long as $\eta$ has an imaginary part
different from 0, the boundary integral formulation for the Helmholtz equation
has a unique solution at all frequencies. A popular choice for this parameter
is $\eta = \frac{\mathrm{i}}{k}$, where $k$ is the wavenumber. It can be shown
that this choice is quasi optimal, at least in the high frequency limit.
However, especially in the low frequency region, where the critical frequencies
are still sparsely distributed, different choices for this factor result in a
smaller condition number and a smaller error of the solution. In this work,
alternative choices for this factor are compared based on numerical
experiments. Additionally, a way to enhance the Burton-Miller solution with
$\eta = \frac{\mathrm{i}}{k}$ for a sound hard scatterer in the low frequency
region by an additional step of a modified Richardson iteration is introduced.",math.NA cs.NA,2024-05-17
"Advancements in Gravity Compensation and Control for the da Vinci
  Surgical Robot",,"This research delves into the enhancement of control mechanisms for the da
Vinci Surgical System, focusing on the implementation of gravity compensation
and refining the modeling of the master and patient side manipulators.
Leveraging the Robot Operating System (ROS) the study aimed to fortify the
precision and stability of the robots movements essential for intricate
surgical procedures. Through rigorous parameter identification and the Euler
Lagrange approach the team successfully derived the necessary torque equations
and established a robust mathematical model. Implementation of the actual robot
and simulation in Gazebo highlighted the efficacy of the developed control
strategies facilitating accurate positioning and minimizing drift.
Additionally, the project extended its contributions by constructing a
comprehensive model for the patient side manipulator laying the groundwork for
future research endeavors. This work signifies a significant advancement in the
pursuit of enhanced precision and user control in robotic assisted surgeries.
  NOTE - This work has been submitted to the IEEE for publication. Copyright
may be transferred without notice, after which this version may no longer be
accessible. Copyright on this article is reserved by IEEE",cs.RO,2024-05-17
Can Public LLMs be used for Self-Diagnosis of Medical Conditions ?,,"Advancements in deep learning have generated a large-scale interest in the
development of foundational deep learning models. The development of Large
Language Models (LLM) has evolved as a transformative paradigm in
conversational tasks, which has led to its integration and extension even in
the critical domain of healthcare. With LLMs becoming widely popular and their
public access through open-source models and integration with other
applications, there is a need to investigate their potential and limitations.
One such crucial task where LLMs are applied but require a deeper understanding
is that of self-diagnosis of medical conditions based on bias-validating
symptoms in the interest of public health. The widespread integration of Gemini
with Google search and GPT-4.0 with Bing search has led to a shift in the trend
of self-diagnosis using search engines to conversational LLM models. Owing to
the critical nature of the task, it is prudent to investigate and understand
the potential and limitations of public LLMs in the task of self-diagnosis. In
this study, we prepare a prompt engineered dataset of 10000 samples and test
the performance on the general task of self-diagnosis. We compared the
performance of both the state-of-the-art GPT-4.0 and the fee Gemini model on
the task of self-diagnosis and recorded contrasting accuracies of 63.07% and
6.01%, respectively. We also discuss the challenges, limitations, and potential
of both Gemini and GPT-4.0 for the task of self-diagnosis to facilitate future
research and towards the broader impact of general public knowledge.
Furthermore, we demonstrate the potential and improvement in performance for
the task of self-diagnosis using Retrieval Augmented Generation.",cs.CL cs.LG,2024-05-18
Measuring Technical Debt in AI-Based Competition Platforms,,"Advances in AI have led to new types of technical debt in software
engineering projects. AI-based competition platforms face challenges due to
rapid prototyping and a lack of adherence to software engineering principles by
participants, resulting in technical debt. Additionally, organizers often lack
methods to evaluate platform quality, impacting sustainability and
maintainability. In this research, we identify and categorize types of
technical debt in AI systems through a scoping review. We develop a
questionnaire for assessing technical debt in AI competition platforms,
categorizing debt into various types, such as algorithm, architectural, code,
configuration, data etc. We introduce Accessibility Debt, specific to AI
competition platforms, highlighting challenges participants face due to
inadequate platform usability. Our framework for managing technical debt aims
to improve the sustainability and effectiveness of these platforms, providing
tools for researchers, organizers, and participants.",cs.SE,2024-05-20
"Deep learning-based hyperspectral image reconstruction for quality
  assessment of agro-product",,"Hyperspectral imaging (HSI) has recently emerged as a promising tool for many
agricultural applications; however, the technology cannot be directly used in a
real-time system due to the extensive time needed to process large volumes of
data. Consequently, the development of a simple, compact, and cost-effective
imaging system is not possible with the current HSI systems. Therefore, the
overall goal of this study was to reconstruct hyperspectral images from RGB
images through deep learning for agricultural applications. Specifically, this
study used Hyperspectral Convolutional Neural Network - Dense (HSCNN-D) to
reconstruct hyperspectral images from RGB images for predicting soluble solid
content (SSC) in sweet potatoes. The algorithm accurately reconstructed the
hyperspectral images from RGB images, with the resulting spectra closely
matching the ground-truth. The partial least squares regression (PLSR) model
based on reconstructed spectra outperformed the model using the full spectral
range, demonstrating its potential for SSC prediction in sweet potatoes. These
findings highlight the potential of deep learning-based hyperspectral image
reconstruction as a low-cost, efficient tool for various agricultural uses.",cs.CV eess.IV,2024-05-20
Model Free Prediction with Uncertainty Assessment,,"Deep nonparametric regression, characterized by the utilization of deep
neural networks to learn target functions, has emerged as a focus of research
attention in recent years. Despite considerable progress in understanding
convergence rates, the absence of asymptotic properties hinders rigorous
statistical inference. To address this gap, we propose a novel framework that
transforms the deep estimation paradigm into a platform conducive to
conditional mean estimation, leveraging the conditional diffusion model.
Theoretically, we develop an end-to-end convergence rate for the conditional
diffusion model and establish the asymptotic normality of the generated
samples. Consequently, we are equipped to construct confidence regions,
facilitating robust statistical inference. Furthermore, through numerical
experiments, we empirically validate the efficacy of our proposed methodology.",stat.ML cs.LG,2024-05-21
"Metadata Integration for Spam Reviews Detection on Vietnamese E-commerce
  Websites",,"The problem of detecting spam reviews (opinions) has received significant
attention in recent years, especially with the rapid development of e-commerce.
Spam reviews are often classified based on comment content, but in some cases,
it is insufficient for models to accurately determine the review label. In this
work, we introduce the ViSpamReviews v2 dataset, which includes metadata of
reviews with the objective of integrating supplementary attributes for spam
review classification. We propose a novel approach to simultaneously integrate
both textual and categorical attributes into the classification model. In our
experiments, the product category proved effective when combined with deep
neural network (DNN) models, while text features performed well on both DNN
models and the model achieved state-of-the-art performance in the problem of
detecting spam reviews on Vietnamese e-commerce websites, namely PhoBERT.
Specifically, the PhoBERT model achieves the highest accuracy when combined
with product description features generated from the SPhoBert model, which is
the combination of PhoBERT and SentenceBERT. Using the macro-averaged F1 score,
the task of classifying spam reviews achieved 87.22% (an increase of 1.64%
compared to the baseline), while the task of identifying the type of spam
reviews achieved an accuracy of 73.49% (an increase of 1.93% compared to the
baseline).",cs.CL,2024-05-21
"Transformers Learn Temporal Difference Methods for In-Context
  Reinforcement Learning",,"In-context learning refers to the learning ability of a model during
inference time without adapting its parameters. The input (i.e., prompt) to the
model (e.g., transformers) consists of both a context (i.e., instance-label
pairs) and a query instance. The model is then able to output a label for the
query instance according to the context during inference. A possible
explanation for in-context learning is that the forward pass of (linear)
transformers implements iterations of gradient descent on the instance-label
pairs in the context. In this paper, we prove by construction that transformers
can also implement temporal difference (TD) learning in the forward pass, a
phenomenon we refer to as in-context TD. We demonstrate the emergence of
in-context TD after training the transformer with a multi-task TD algorithm,
accompanied by theoretical analysis. Furthermore, we prove that transformers
are expressive enough to implement many other policy evaluation algorithms in
the forward pass, including residual gradient, TD with eligibility trace, and
average-reward TD.",cs.LG,2024-05-22
MOD-UV: Learning Mobile Object Detectors from Unlabeled Videos,,"Embodied agents must detect and localize objects of interest, e.g. traffic
participants for self-driving cars. Supervision in the form of bounding boxes
for this task is extremely expensive. As such, prior work has looked at
unsupervised instance detection and segmentation, but in the absence of
annotated boxes, it is unclear how pixels must be grouped into objects and
which objects are of interest. This results in over-/under-segmentation and
irrelevant objects. Inspired by human visual system and practical applications,
we posit that the key missing cue for unsupervised detection is motion: objects
of interest are typically mobile objects that frequently move and their motions
can specify separate instances. In this paper, we propose MOD-UV, a Mobile
Object Detector learned from Unlabeled Videos only. We begin with instance
pseudo-labels derived from motion segmentation, but introduce a novel training
paradigm to progressively discover small objects and static-but-mobile objects
that are missed by motion segmentation. As a result, though only learned from
unlabeled videos, MOD-UV can detect and segment mobile objects from a single
static image. Empirically, we achieve state-of-the-art performance in
unsupervised mobile object detection on Waymo Open, nuScenes, and KITTI
Datasets without using any external data or supervised models. Code is
available at https://github.com/YihongSun/MOD-UV.",cs.CV,2024-05-23
"Online randomized interpolative decomposition with a posteriori error
  estimator for temporal PDE data reduction",,"Traditional low-rank approximation is a powerful tool to compress the huge
data matrices that arise in simulations of partial differential equations
(PDE), but suffers from high computational cost and requires several passes
over the PDE data. The compressed data may also lack interpretability thus
making it difficult to identify feature patterns from the original data. To
address these issues, we present an online randomized algorithm to compute the
interpolative decomposition (ID) of large-scale data matrices {\em in situ}.
Compared to previous randomized IDs that used the QR decomposition to determine
the column basis, we adopt a streaming ridge leverage score-based column subset
selection algorithm that dynamically selects proper basis columns from the data
and thus avoids an extra pass over the data to compute the coefficient matrix
of the ID. In particular, we adopt a single-pass error estimator based on the
non-adaptive Hutch++ algorithm to provide real-time error approximation for
determining the best coefficients. As a result, our approach only needs a
single pass over the original data and thus is suitable for large and
high-dimensional matrices stored outside of core memory or generated in PDE
simulations. A strategy to improve the accuracy of the reconstructed data
gradient, when desired, within the ID framework is also presented. We provide
numerical experiments on turbulent channel flow and ignition simulations, and
on the NSTX Gas Puff Image dataset, comparing our algorithm with the offline ID
algorithm to demonstrate its utility in real-world applications.",math.NA cs.NA,2024-05-25
"Oblivious Monitoring for Discrete-Time STL via Fully Homomorphic
  Encryption",,"When monitoring a cyber-physical system (CPS) from a remote server, keeping
the monitored data secret is crucial, particularly when they contain sensitive
information, e.g., biological or location data. Recently, Banno et al. (CAV'22)
proposed a protocol for online LTL monitoring that keeps data concealed from
the server using Fully Homomorphic Encryption (FHE). We build on this protocol
to allow arithmetic operations over encrypted values, e.g., to compute a safety
measurement combining distance, velocity, and so forth. Overall, our protocol
enables oblivious online monitoring of discrete-time real-valued signals
against signal temporal logic (STL) formulas. Our protocol combines two FHE
schemes, CKKS and TFHE, leveraging their respective strengths. We employ CKKS
to evaluate arithmetic predicates in STL formulas while utilizing TFHE to
process them using a DFA derived from the STL formula. We conducted case
studies on monitoring blood glucose levels and vehicles' behavior against the
Responsibility-Sensitive Safety (RSS) rules. Our results suggest the practical
relevance of our protocol.",cs.CR cs.FL,2024-05-26
"SiNGR: Brain Tumor Segmentation via Signed Normalized Geodesic Transform
  Regression",,"One of the primary challenges in brain tumor segmentation arises from the
uncertainty of voxels close to tumor boundaries. However, the conventional
process of generating ground truth segmentation masks fails to treat such
uncertainties properly. Those ""hard labels"" with 0s and 1s conceptually
influenced the majority of prior studies on brain image segmentation. As a
result, tumor segmentation is often solved through voxel classification. In
this work, we instead view this problem as a voxel-level regression, where the
ground truth represents a certainty mapping from any pixel to the border of the
tumor. We propose a novel ground truth label transformation, which is based on
a signed geodesic transform, to capture the uncertainty in brain tumors'
vicinity. We combine this idea with a Focal-like regression L1-loss that
enables effective regression learning in high-dimensional output space by
appropriately weighting voxels according to their difficulty. We thoroughly
conduct an experimental evaluation to validate the components of our proposed
method, compare it to a diverse array of state-of-the-art segmentation models,
and show that it is architecture-agnostic. The code of our method is made
publicly available (\url{https://github.com/Oulu-IMEDS/SiNGR/}).",cs.CV,2024-05-27
"A real/fast-time simulator for impact assessment of spoofing & jamming
  attacks on GNSS receivers",,"In aviation, the impact of threats is becoming increasingly significant,
particularly for global navigation satellite system (GNSS). Two relevant GNSS
threats are represented by jamming and spoofing. In order to evaluate the
technological solutions to counter GNSS attacks, such attacks should be
assessed by means of a proper GNSS threat simulator. This work shows the
implementation and the testing results of a GNSS security impact simulator
which injects the desired threat scenarios as a deviations on the GNSS actual
measurements. The proposed simulator can be integrated in both real- and
fast-time simulation environments. The provided results confirm the
effectiveness of the simulator, and include in-flight demonstrations by means
of a flight experimental vehicle.",eess.SP cs.SY eess.SY,2024-05-28
"Practical aspects for the creation of an audio dataset from field
  recordings with optimized labeling budget with AI-assisted strategy",,"Machine Listening focuses on developing technologies to extract relevant
information from audio signals. A critical aspect of these projects is the
acquisition and labeling of contextualized data, which is inherently complex
and requires specific resources and strategies. Despite the availability of
some audio datasets, many are unsuitable for commercial applications. The paper
emphasizes the importance of Active Learning (AL) using expert labelers over
crowdsourcing, which often lacks detailed insights into dataset structures. AL
is an iterative process combining human labelers and AI models to optimize the
labeling budget by intelligently selecting samples for human review. This
approach addresses the challenge of handling large, constantly growing datasets
that exceed available computational resources and memory. The paper presents a
comprehensive data-centric framework for Machine Listening projects, detailing
the configuration of recording nodes, database structure, and labeling budget
optimization in resource-constrained scenarios. Applied to an industrial port
in Valencia, Spain, the framework successfully labeled 6540 ten-second audio
samples over five months with a small team, demonstrating its effectiveness and
adaptability to various resource availability situations.
  Acknowledgments: The participation of Javier Naranjo-Alcazar, Jordi Grau-Haro
and Pedro Zuccarello in this research was funded by the Valencian Institute for
Business Competitiveness (IVACE) and the FEDER funds by means of project
Soroll-IA2 (IMDEEA/2023/91).",cs.SD cs.LG eess.AS,2024-05-28
Self-Supervised Learning Based Handwriting Verification,,"We present SSL-HV: Self-Supervised Learning approaches applied to the task of
Handwriting Verification. This task involves determining whether a given pair
of handwritten images originate from the same or different writer distribution.
We have compared the performance of multiple generative, contrastive SSL
approaches against handcrafted feature extractors and supervised learning on
CEDAR AND dataset. We show that ResNet based Variational Auto-Encoder (VAE)
outperforms other generative approaches achieving 76.3% accuracy, while
ResNet-18 fine-tuned using Variance-Invariance-Covariance Regularization
(VICReg) outperforms other contrastive approaches achieving 78% accuracy. Using
a pre-trained VAE and VICReg for the downstream task of writer verification we
observed a relative improvement in accuracy of 6.7% and 9% over ResNet-18
supervised baseline with 10% writer labels.",cs.CV cs.AI cs.CL,2024-05-28
Occam Gradient Descent,,"Deep learning neural network models must be large enough to adapt to their
problem domain, while small enough to avoid overfitting training data during
gradient descent. To balance these competing demands, overprovisioned deep
learning models such as transformers are trained for a single epoch on large
data sets, and hence inefficient with both computing resources and training
data. In response to these inefficiencies, we exploit learning theory to derive
Occam Gradient Descent, an algorithm that interleaves adaptive reduction of
model size to minimize generalization error, with gradient descent on model
weights to minimize fitting error. In contrast, traditional gradient descent
greedily minimizes fitting error without regard to generalization error. Our
algorithm simultaneously descends the space of weights and topological size of
any neural network without modification. With respect to loss, compute and
model size, our experiments show (a) on image classification benchmarks, linear
and convolutional neural networks trained with Occam Gradient Descent
outperform traditional gradient descent with or without post-train pruning; (b)
on a range of tabular data classification tasks, neural networks trained with
Occam Gradient Descent outperform traditional gradient descent, as well as
Random Forests; (c) on natural language transformers, Occam Gradient Descent
outperforms traditional gradient descent.",cs.LG,2024-05-30
"CSDO: Enhancing Efficiency and Success in Large-Scale Multi-Vehicle
  Trajectory Planning",,"This paper presents an efficient algorithm, naming Centralized Searching and
Decentralized Optimization (CSDO), to find feasible solution for large-scale
Multi-Vehicle Trajectory Planning (MVTP) problem. Due to the intractable growth
of non-convex constraints with the number of agents, exploring various homotopy
classes that imply different convex domains, is crucial for finding a feasible
solution. However, existing methods struggle to explore various homotopy
classes efficiently due to combining it with time-consuming precise trajectory
solution finding. CSDO, addresses this limitation by separating them into
different levels and integrating an efficient Multi-Agent Path Finding (MAPF)
algorithm to search homotopy classes. It first searches for a coarse initial
guess using a large search step, identifying a specific homotopy class.
Subsequent decentralized Quadratic Programming (QP) refinement processes this
guess, resolving minor collisions efficiently. Experimental results demonstrate
that CSDO outperforms existing MVTP algorithms in large-scale, high-density
scenarios, achieving up to 95% success rate in 50m $\times$ 50m random
scenarios around one second. Source codes are released in
https://github.com/YangSVM/CSDOTrajectoryPlanning.",cs.RO,2024-05-31
"Structural and Algorithmic Results for Stable Cycles and Partitions in
  the Roommates Problem",,"In the Stable Roommates problem, we seek a stable matching of the agents into
pairs, in which no two agents have an incentive to deviate from their
assignment. It is well known that a stable matching is unlikely to exist, but a
stable partition always does and provides a succinct certificate for the
unsolvability of an instance. Furthermore, apart from being a useful structural
tool to study the problem, every stable partition corresponds to a stable
half-matching, which has applications, for example, in sports scheduling and
time-sharing.
  We establish new structural results for stable partitions and show how to
enumerate all stable partitions and the cycles included in such structures
efficiently. We also adapt optimality criteria from stable matchings to stable
partitions and give complexity and approximability results for the problems of
computing such ""fair"" and ""optimal"" stable partitions.
  Through this research, we contribute to a deeper understanding of stable
partitions from a combinatorial point of view, as well as the computational
complexity of computing ""fair"" or ""optimal"" stable half-matchings in practice,
closing the gap between integral and fractional stable matchings and paving the
way for further applications of stable partitions to unsolvable instances and
computationally hard stable matching problems.",cs.DS cs.GT,2024-06-01
"Deep Reinforcement Learning for Sim-to-Real Policy Transfer of VTOL-UAVs
  Offshore Docking Operations",,"This paper proposes a novel Reinforcement Learning (RL) approach for
sim-to-real policy transfer of Vertical Take-Off and Landing Unmanned Aerial
Vehicle (VTOL-UAV). The proposed approach is designed for VTOL-UAV landing on
offshore docking stations in maritime operations. VTOL-UAVs in maritime
operations encounter limitations in their operational range, primarily stemming
from constraints imposed by their battery capacity. The concept of autonomous
landing on a charging platform presents an intriguing prospect for mitigating
these limitations by facilitating battery charging and data transfer. However,
current Deep Reinforcement Learning (DRL) methods exhibit drawbacks, including
lengthy training times, and modest success rates. In this paper, we tackle
these concerns comprehensively by decomposing the landing procedure into a
sequence of more manageable but analogous tasks in terms of an approach phase
and a landing phase. The proposed architecture utilizes a model-based control
scheme for the approach phase, where the VTOL-UAV is approaching the offshore
docking station. In the Landing phase, DRL agents were trained offline to learn
the optimal policy to dock on the offshore station. The Joint North Sea Wave
Project (JONSWAP) spectrum model has been employed to create a wave model for
each episode, enhancing policy generalization for sim2real transfer. A set of
DRL algorithms have been tested through numerical simulations including
value-based agents and policy-based agents such as Deep \textit{Q} Networks
(DQN) and Proximal Policy Optimization (PPO) respectively. The numerical
experiments show that the PPO agent can learn complicated and efficient
policies to land in uncertain environments, which in turn enhances the
likelihood of successful sim-to-real transfer.",cs.RO,2024-06-02
"Statistical analysis of geoinformation data for increasing railway
  safety",,"The impact of rail transport on the environment is one of the crucial factors
for the sustainable development of this form of mass transport. We present a
data-driven analysis of wild animal railway accidents in the region of southern
Poland, a step to create the train driver warning system. We built our method
by harnessing the Bayesian approach to the statistical analysis of information
about the geolocation of the accidents. The implementation of the proposed
model does not require advanced knowledge of data mining and can be applied
even in less developed railway systems with small IT support. Furthermore, we
have discovered unusual patterns of accidents while considering the number of
trains and their speed and time at particular geographical locations of the
railway network. We test the developed approach using data from southern
Poland, compromising wildlife habitats and one of the most urbanised regions in
Central Europe, based on this we conclude that our model is best suited to
railway lines that pass through varying types of landscape.",cs.CE,2024-06-03
"Position: An Inner Interpretability Framework for AI Inspired by Lessons
  from Cognitive Neuroscience",,"Inner Interpretability is a promising emerging field tasked with uncovering
the inner mechanisms of AI systems, though how to develop these mechanistic
theories is still much debated. Moreover, recent critiques raise issues that
question its usefulness to advance the broader goals of AI. However, it has
been overlooked that these issues resemble those that have been grappled with
in another field: Cognitive Neuroscience. Here we draw the relevant connections
and highlight lessons that can be transferred productively between fields.
Based on these, we propose a general conceptual framework and give concrete
methodological strategies for building mechanistic explanations in AI inner
interpretability research. With this conceptual framework, Inner
Interpretability can fend off critiques and position itself on a productive
path to explain AI systems.",cs.AI cs.LG q-bio.NC,2024-06-03
"Computing the Action of the Generating Function of Bernoulli Polynomials
  on a Matrix with An Application to Non-local Boundary Value Problems",,"This paper deals with efficient numerical methods for computing the action of
the generating function of Bernoulli polynomials, say $q(\tau,w)$, on a
typically large sparse matrix. This problem occurs when solving some non-local
boundary value problems. Methods based on the Fourier expansion of $q(\tau,w)$
have already been addressed in the scientific literature. The contribution of
this paper is twofold. First, we place these methods in the classical framework
of Krylov-Lanczos (polynomial-rational) techniques for accelerating Fourier
series. This allows us to apply the convergence results developed in this
context to our function. Second, we design a new acceleration scheme. Some
numerical results are presented to show the effectiveness of the proposed
algorithms.",math.NA cs.NA,2024-06-03
Unlocking Guidance for Discrete State-Space Diffusion and Flow Models,,"Generative models on discrete state-spaces have a wide range of potential
applications, particularly in the domain of natural sciences. In continuous
state-spaces, controllable and flexible generation of samples with desired
properties has been realized using guidance on diffusion and flow models.
However, these guidance approaches are not readily amenable to discrete
state-space models. Consequently, we introduce a general and principled method
for applying guidance on such models. Our method depends on leveraging
continuous-time Markov processes on discrete state-spaces, which unlocks
computational tractability for sampling from a desired guided distribution. We
demonstrate the utility of our approach, Discrete Guidance, on a range of
applications including guided generation of images, small-molecules, DNA
sequences and protein sequences.",cs.LG,2024-06-03
Unveiling the Potential of AI for Nanomaterial Morphology Prediction,,"Creation of nanomaterials with specific morphology remains a complex
experimental process, even though there is a growing demand for these materials
in various industry sectors. This study explores the potential of AI to predict
the morphology of nanoparticles within the data availability constraints. For
that, we first generated a new multi-modal dataset that is double the size of
analogous studies. Then, we systematically evaluated performance of classical
machine learning and large language models in prediction of nanomaterial shapes
and sizes. Finally, we prototyped a text-to-image system, discussed the
obtained empirical results, as well as the limitations and promises of existing
approaches.",cs.LG cs.AI,2024-05-31
Flips in colorful triangulations,,"The associahedron is the graph $\mathcal{G}_N$ that has as nodes all
triangulations of a convex $N$-gon, and an edge between any two triangulations
that differ in a flip operation. A flip removes an edge shared by two triangles
and replaces it by the other diagonal of the resulting 4-gon. In this paper, we
consider a large collection of induced subgraphs of $\mathcal{G}_N$ obtained by
Ramsey-type colorability properties. Specifically, coloring the points of the
$N$-gon red and blue alternatingly, we consider only colorful triangulations,
namely triangulations in which every triangle has points in both colors, i.e.,
monochromatic triangles are forbidden. The resulting induced subgraph of
$\mathcal{G}_N$ on colorful triangulations is denoted by $\mathcal{F}_N$. We
prove that $\mathcal{F}_N$ has a Hamilton cycle for all $N\geq 8$, resolving a
problem raised by Sagan, i.e., all colorful triangulations on $N$ points can be
listed so that any two cyclically consecutive triangulations differ in a flip.
In fact, we prove that for an arbitrary fixed coloring pattern of the $N$
points with at least 10 changes of color, the resulting subgraph of
$\mathcal{G}_N$ on colorful triangulations (for that coloring pattern) admits a
Hamilton cycle. We also provide an efficient algorithm for computing a Hamilton
path in $\mathcal{F}_N$ that runs in time $\mathcal{O}(1)$ on average per
generated node. This algorithm is based on a new and algorithmic construction
of a tree rotation Gray code for listing all $n$-vertex $k$-ary trees that runs
in time $\mathcal{O}(k)$ on average per generated tree.",math.CO cs.DM,2024-06-06
"Towards Physically Consistent Deep Learning For Climate Model
  Parameterizations",,"Climate models play a critical role in understanding and projecting climate
change. Due to their complexity, their horizontal resolution of about 40-100 km
remains too coarse to resolve processes such as clouds and convection, which
need to be approximated via parameterizations. These parameterizations are a
major source of systematic errors and large uncertainties in climate
projections. Deep learning (DL)-based parameterizations, trained on data from
computationally expensive short, high-resolution simulations, have shown great
promise for improving climate models in that regard. However, their lack of
interpretability and tendency to learn spurious non-physical correlations
result in reduced trust in the climate simulation. We propose an efficient
supervised learning framework for DL-based parameterizations that leads to
physically consistent models with improved interpretability and negligible
computational overhead compared to standard supervised training. First, key
features determining the target physical processes are uncovered. Subsequently,
the neural network is fine-tuned using only those relevant features. We show
empirically that our method robustly identifies a small subset of the inputs as
actual physical drivers, therefore, removing spurious non-physical
relationships. This results in by design physically consistent and
interpretable neural networks while maintaining the predictive performance of
unconstrained black-box DL-based parameterizations.",cs.LG physics.ao-ph,2024-06-06
"Evaluating Data-driven Performances of Mixed Integer Bilinear
  Formulations for Book Placement Planning",,"Mixed integer bilinear programs (MIBLPs) offer tools to resolve robotics
motion planning problems with orthogonal rotation matrices or static moment
balance, but require long solving times. Recent work utilizing data-driven
methods has shown potential to overcome this issue allowing for applications on
larger scale problems. To solve mixed-integer bilinear programs online with
data-driven methods, several re-formulations exist including mathematical
programming with complementary constraints (MPCC), and mixed-integer
programming (MIP). In this work, we compare the data-driven performances of
various MIBLP reformulations using a book placement problem that has discrete
configuration switches and bilinear constraints. The success rate, cost, and
solving time are compared along with non-data-driven methods. Our results
demonstrate the advantage of using data-driven methods to accelerate the
solving speed of MIBLPs, and provide references for users to choose the
suitable re-formulation.",cs.RO,2024-06-06
Understanding GPU Triggering APIs for MPI+X Communication,,"GPU-enhanced architectures are now dominant in HPC systems, but
message-passing communication involving GPUs with MPI has proven to be both
complex and expensive, motivating new approaches that lower such costs. We
compare and contrast stream/graph- and kernel-triggered MPI communication
abstractions, whose principal purpose is to enhance the performance of
communication when GPU kernels create or consume data for transfer through MPI
operations. Researchers and practitioners have proposed multiple potential APIs
for stream and/or kernel triggering that span various GPU architectures and
approaches, including MPI-4 partitioned point-to-point communication, stream
communicators, and explicit MPI stream/queue objects. Designs breaking backward
compatibility with MPI are duly noted. Some of these strengthen or weaken the
semantics of MPI operations. A key contribution of this paper is to promote
community convergence toward a stream- and/or kernel-triggering abstraction by
highlighting the common and differing goals and contributions of existing
abstractions. We describe the design space in which these abstractions reside,
their implicit or explicit use of stream and other non-MPI abstractions, their
relationship to partitioned and persistent operations, and discuss their
potential for added performance, how usable these abstractions are, and where
functional and/or semantic gaps exist. Finally, we provide a taxonomy for
stream- and kernel-triggered abstractions, including disambiguation of similar
semantic terms, and consider directions for future standardization in MPI-5.",cs.DC,2024-06-08
"An AI-Enabled Framework Within Reach for Enhancing Healthcare
  Sustainability and Fairness",,"Good health and well-being is among key issues in the United Nations 2030
Sustainable Development Goals. The rising prevalence of large-scale infectious
diseases and the accelerated aging of the global population are driving the
transformation of healthcare technologies. In this context, establishing
large-scale public health datasets, developing medical models, and creating
decision-making systems with a human-centric approach are of strategic
significance. Recently, by leveraging the extraordinary number of accessible
cameras, groundbreaking advancements have emerged in AI methods for
physiological signal monitoring and disease diagnosis using camera sensors.
These approaches, requiring no specialized medical equipment, offer convenient
manners of collecting large-scale medical data in response to public health
events. Therefore, we outline a prospective framework and heuristic vision for
a camera-based public health (CBPH) framework utilizing visual physiological
monitoring technology. The CBPH can be considered as a convenient and universal
framework for public health, advancing the United Nations Sustainable
Development Goals, particularly in promoting the universality, sustainability,
and equity of healthcare in low- and middle-income countries or regions.
Furthermore, CBPH provides a comprehensive solution for building a large-scale
and human-centric medical database, and a multi-task large medical model for
public health and medical scientific discoveries. It has a significant
potential to revolutionize personal monitoring technologies, digital medicine,
telemedicine, and primary health care in public health. Therefore, it can be
deemed that the outcomes of this paper will contribute to the establishment of
a sustainable and fair framework for public health, which serves as a crucial
bridge for advancing scientific discoveries in the realm of AI for medicine
(AI4Medicine).",cs.CY cs.AI cs.CV,2024-04-21
"GRU-Net: Gaussian Attention Aided Dense Skip Connection Based
  MultiResUNet for Breast Histopathology Image Segmentation",,"Breast cancer is a major global health concern. Pathologists face challenges
in analyzing complex features from pathological images, which is a
time-consuming and labor-intensive task. Therefore, efficient computer-based
diagnostic tools are needed for early detection and treatment planning. This
paper presents a modified version of MultiResU-Net for histopathology image
segmentation, which is selected as the backbone for its ability to analyze and
segment complex features at multiple scales and ensure effective feature flow
via skip connections. The modified version also utilizes the Gaussian
distribution-based Attention Module (GdAM) to incorporate
histopathology-relevant text information in a Gaussian distribution. The
sampled features from the Gaussian text feature-guided distribution highlight
specific spatial regions based on prior knowledge. Finally, using the
Controlled Dense Residual Block (CDRB) on skip connections of MultiResU-Net,
the information is transferred from the encoder layers to the decoder layers in
a controlled manner using a scaling parameter derived from the extracted
spatial features. We validate our approach on two diverse breast cancer
histopathology image datasets: TNBC and MonuSeg, demonstrating superior
segmentation performance compared to state-of-the-art methods. The code for our
proposed model is available on https://github.com/AyushRoy2001/GRU-Net.",eess.IV cs.CV cs.LG,2024-06-12
QQQ: Quality Quattuor-Bit Quantization for Large Language Models,,"Quantization is a proven effective method for compressing large language
models. Although popular techniques like W8A8 and W4A16 effectively maintain
model performance, they often fail to concurrently speed up the prefill and
decoding stages of inference. W4A8 is a promising strategy to accelerate both
of them while usually leads to a significant performance degradation. To
address these issues, we present QQQ, a Quality Quattuor-bit Quantization
method with 4-bit weights and 8-bit activations. QQQ employs adaptive smoothing
and Hessian-based compensation, significantly enhancing the performance of
quantized models without extensive training. Furthermore, we meticulously
engineer W4A8 GEMM kernels to increase inference speed. Our specialized
per-channel W4A8 GEMM and per-group W4A8 GEMM achieve impressive speed
increases of 3.67$\times$ and 3.29 $\times$ over FP16 GEMM. Our extensive
experiments show that QQQ achieves performance on par with existing
state-of-the-art LLM quantization methods while significantly accelerating
inference, achieving speed boosts up to 2.24 $\times$, 2.10$\times$, and
1.25$\times$ compared to FP16, W8A8, and W4A16, respectively.",cs.LG,2024-06-14
"The Impact of Quantization on Retrieval-Augmented Generation: An
  Analysis of Small LLMs",,"Post-training quantization reduces the computational demand of Large Language
Models (LLMs) but can weaken some of their capabilities. Since LLM abilities
emerge with scale, smaller LLMs are more sensitive to quantization. In this
paper, we explore how quantization affects smaller LLMs' ability to perform
retrieval-augmented generation (RAG), specifically in longer contexts. We chose
personalization for evaluation because it is a challenging domain to perform
using RAG as it requires long-context reasoning over multiple documents. We
compare the original FP16 and the quantized INT4 performance of multiple 7B and
8B LLMs on two tasks while progressively increasing the number of retrieved
documents to test how quantized models fare against longer contexts. To better
understand the effect of retrieval, we evaluate three retrieval models in our
experiments. Our findings reveal that if a 7B LLM performs the task well,
quantization does not impair its performance and long-context reasoning
capabilities. We conclude that it is possible to utilize RAG with quantized
smaller LLMs.",cs.CL cs.AI cs.IR,2024-06-10
Reward Schemes and Committee Sizes in Proof of Stake Governance,,"In this paper, we investigate the impact of reward schemes and committee
sizes motivated by governance systems over blockchain communities. We introduce
a model for elections with a binary outcome space where there is a ground truth
(i.e., a ""correct"" outcome), and where stakeholders can only choose to delegate
their voting power to a set of delegation representatives (DReps). Moreover,
the effort (cost) invested by each DRep positively influences both (i) her
ability to vote correctly and (ii) the total delegation that she attracts,
thereby increasing her voting power. This model constitutes the natural
counterpart of delegated proof-of-stake (PoS) protocols, where delegated stakes
are used to elect the block builders.
  As a way to motivate the representatives to exert effort, a reward scheme can
be used based on the delegation attracted by each DRep. We analyze both the
game-theoretic aspects and the optimization counterpart of this model. Our
primary focus is on selecting a committee that maximizes the probability of
reaching the correct outcome, given a fixed monetary budget allocated for
rewarding the delegates. Our findings provide insights into the design of
effective reward mechanisms and optimal committee structures (i.e., how many
DReps are enough) in these PoS-like governance systems.",cs.GT,2024-06-15
"Explain the Black Box for the Sake of Science: Revisiting the Scientific
  Method in the Era of Generative Artificial Intelligence",,"The scientific method is the cornerstone of human progress across all
branches of the natural and applied sciences, from understanding the human body
to explaining how the universe works. The scientific method is based on
identifying systematic rules or principles that describe the phenomenon of
interest in a reproducible way that can be validated through experimental
evidence. In the era of artificial intelligence (AI), there are discussions on
how AI systems may discover new knowledge. We argue that human complex
reasoning for scientific discovery remains of vital importance, at least before
the advent of artificial general intelligence. Yet, AI can be leveraged for
scientific discovery via explainable AI. More specifically, knowing what data
AI systems deemed important to make decisions can be a point of contact with
domain experts and scientists, that can lead to divergent or convergent views
on a given scientific problem. Divergent views may spark further scientific
investigations leading to new scientific knowledge.",cs.AI cs.CY math.DS,2024-06-15
"EchoGuide: Active Acoustic Guidance for LLM-Based Eating Event Analysis
  from Egocentric Videos",,"Self-recording eating behaviors is a step towards a healthy lifestyle
recommended by many health professionals. However, the current practice of
manually recording eating activities using paper records or smartphone apps is
often unsustainable and inaccurate. Smart glasses have emerged as a promising
wearable form factor for tracking eating behaviors, but existing systems
primarily identify when eating occurs without capturing details of the eating
activities (E.g., what is being eaten). In this paper, we present EchoGuide, an
application and system pipeline that leverages low-power active acoustic
sensing to guide head-mounted cameras to capture egocentric videos, enabling
efficient and detailed analysis of eating activities. By combining active
acoustic sensing for eating detection with video captioning models and
large-scale language models for retrieval augmentation, EchoGuide intelligently
clips and analyzes videos to create concise, relevant activity records on
eating. We evaluated EchoGuide with 9 participants in naturalistic settings
involving eating activities, demonstrating high-quality summarization and
significant reductions in video data needed, paving the way for practical,
scalable eating activity tracking.",cs.HC,2024-06-15
"RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained
  Language Model for Knowledge Editing and Fine-tuning",,"Pre-trained language models, trained on large-scale corpora, demonstrate
strong generalizability across various NLP tasks. Fine-tuning these models for
specific tasks typically involves updating all parameters, which is
resource-intensive. Parameter-efficient fine-tuning (PEFT) methods, such as the
popular LoRA family, introduce low-rank matrices to learn only a few parameters
efficiently. However, during inference, the product of these matrices updates
all pre-trained parameters, complicating tasks like knowledge editing that
require selective updates. We propose a novel PEFT method, which conducts
\textbf{r}ow and c\textbf{o}lumn-wise spar\textbf{se}
\textbf{lo}w-\textbf{r}ank \textbf{a}daptation (RoseLoRA), to address this
challenge. RoseLoRA identifies and updates only the most important parameters
for a specific task, maintaining efficiency while preserving other model
knowledge. By adding a sparsity constraint on the product of low-rank matrices
and converting it to row and column-wise sparsity, we ensure efficient and
precise model updates. Our theoretical analysis guarantees the lower bound of
the sparsity with respective to the matrix product. Extensive experiments on
five benchmarks across twenty datasets demonstrate that RoseLoRA outperforms
baselines in both general fine-tuning and knowledge editing tasks.",cs.CL,2024-06-15
Vid3D: Synthesis of Dynamic 3D Scenes using 2D Video Diffusion,,"A recent frontier in computer vision has been the task of 3D video
generation, which consists of generating a time-varying 3D representation of a
scene. To generate dynamic 3D scenes, current methods explicitly model 3D
temporal dynamics by jointly optimizing for consistency across both time and
views of the scene. In this paper, we instead investigate whether it is
necessary to explicitly enforce multiview consistency over time, as current
approaches do, or if it is sufficient for a model to generate 3D
representations of each timestep independently. We hence propose a model,
Vid3D, that leverages 2D video diffusion to generate 3D videos by first
generating a 2D ""seed"" of the video's temporal dynamics and then independently
generating a 3D representation for each timestep in the seed video. We evaluate
Vid3D against two state-of-the-art 3D video generation methods and find that
Vid3D is achieves comparable results despite not explicitly modeling 3D
temporal dynamics. We further ablate how the quality of Vid3D depends on the
number of views generated per frame. While we observe some degradation with
fewer views, performance degradation remains minor. Our results thus suggest
that 3D temporal knowledge may not be necessary to generate high-quality
dynamic 3D scenes, potentially enabling simpler generative algorithms for this
task.",cs.CV,2024-06-17
"MINT-1T: Scaling Open-Source Multimodal Data by 10x: A Multimodal
  Dataset with One Trillion Tokens",,"Multimodal interleaved datasets featuring free-form interleaved sequences of
images and text are crucial for training frontier large multimodal models
(LMMs). Despite the rapid progression of open-source LMMs, there remains a
pronounced scarcity of large-scale, diverse open-source multimodal interleaved
datasets. In response, we introduce MINT-1T, the most extensive and diverse
open-source Multimodal INTerleaved dataset to date. MINT-1T comprises one
trillion text tokens and 3.4 billion images, a 10x scale-up from existing
open-source datasets. Additionally, we include previously untapped sources such
as PDFs and ArXiv papers. As scaling multimodal interleaved datasets requires
substantial engineering effort, sharing the data curation process and releasing
the dataset greatly benefits the community. Our experiments show that LMMs
trained on MINT-1T rival the performance of models trained on the previous
leading dataset, OBELICS. Our data and code will be released at
https://github.com/mlfoundations/MINT-1T.",cs.CV cs.LG,2024-06-17
"Enhancing and Assessing Instruction-Following with Fine-Grained
  Instruction Variants",,"The effective alignment of Large Language Models (LLMs) with precise
instructions is essential for their application in diverse real-world
scenarios. Current methods focus on enhancing the diversity and complexity of
training and evaluation samples, yet they fall short in accurately assessing
LLMs' ability to follow similar instruction variants. We introduce an effective
data augmentation technique that decomposes complex instructions into simpler
sub-components, modifies these, and reconstructs them into new variants,
thereby preserves the original instruction's context and complexity while
introducing variability, which is critical for training and evaluating LLMs'
instruction-following precision. We developed the DeMoRecon dataset using this
method to both fine-tune and evaluate LLMs. Our findings show that LLMs
fine-tuned with DeMoRecon will gain significant performance boost on both ours
and commonly used instructions-following benchmarks.",cs.AI cs.CL cs.LG,2024-06-17
"Towards Self-Supervised FG-SBIR with Unified Sample Feature Alignment
  and Multi-Scale Token Recycling",,"Fine-Grained Sketch-Based Image Retrieval (FG-SBIR) aims to minimize the
distance between sketches and corresponding images in the embedding space.
However, scalability is hindered by the growing complexity of solutions, mainly
due to the abstract nature of fine-grained sketches. In this paper, we propose
an effective approach to narrow the gap between the two domains. It mainly
facilitates unified mutual information sharing both intra- and inter-samples,
rather than treating them as a single feature alignment problem between
modalities. Specifically, our approach includes: (i) Employing dual
weight-sharing networks to optimize alignment within the sketch and image
domain, which also effectively mitigates model learning saturation issues. (ii)
Introducing an objective optimization function based on contrastive loss to
enhance the model's ability to align features in both intra- and inter-samples.
(iii) Presenting a self-supervised Multi-Scale Token Recycling (MSTR) Module
featured by recycling discarded patch tokens in multi-scale features, further
enhancing representation capability and retrieval performance. Our framework
achieves excellent results on CNN- and ViT-based backbones. Extensive
experiments demonstrate its superiority over existing methods. We also
introduce Cloths-V1, the first professional fashion sketch-image dataset,
utilized to validate our method and will be beneficial for other applications",cs.CV,2024-06-17
"Mapping Literary Space: A Social Network from the Timeline of Cultural
  Events",,"This study applies social network analysis (SNA) to map and analyze literary
networks in St Petersburg from 1999 to 2019, using data from the 'SPbLitGuide'
newsletter. By examining co-participation in literary events, we reveal the
dynamics and structures of these networks, identifying key communities and
influential figures. Our network graph, consisting of 14,066 nodes and 127,068
edges, represents a highly interconnected and cohesive small-world network with
robust local clustering and extensive collaboration. Focusing on core
participants, we refined the graph and applied community detection methods to
identify distinct groups with specific aesthetic preferences and personal
connections. These findings provide insights into the structure and dynamics of
literary groups in St. Petersburg and provide a foundation for further research
in the digital humanities.",cs.SI cs.DL,2024-06-16
A Notion of Complexity for Theory of Mind via Discrete World Models,,"Theory of Mind (ToM) can be used to assess the capabilities of Large Language
Models (LLMs) in complex scenarios where social reasoning is required. While
the research community has proposed many ToM benchmarks, their hardness varies
greatly, and their complexity is not well defined. This work proposes a
framework to measure the complexity of ToM tasks. We quantify a problem's
complexity as the number of states necessary to solve it correctly. Our
complexity measure also accounts for spurious states of a ToM problem designed
to make it apparently harder. We use our method to assess the complexity of
five widely adopted ToM benchmarks. On top of this framework, we design a
prompting technique that augments the information available to a model with a
description of how the environment changes with the agents' interactions. We
name this technique Discrete World Models (DWM) and show how it elicits
superior performance on ToM tasks.",cs.AI cs.CL cs.LG,2024-06-16
"GLiNER multi-task: Generalist Lightweight Model for Various Information
  Extraction Tasks",,"Information extraction tasks require both accurate, efficient, and
generalisable models. Classical supervised deep learning approaches can achieve
the required performance, but they need large datasets and are limited in their
ability to adapt to different tasks. On the other hand, large language models
(LLMs) demonstrate good generalization, meaning that they can adapt to many
different tasks based on user requests. However, LLMs are computationally
expensive and tend to fail to generate structured outputs. In this article, we
will introduce a new kind of GLiNER model that can be used for various
information extraction tasks while being a small encoder model. Our model
achieved SoTA performance on zero-shot NER benchmarks and leading performance
on question-answering, summarization and relation extraction tasks.
Additionally, in this article, we will cover experimental results on
self-learning approaches for named entity recognition using GLiNER models.",cs.LG cs.AI cs.CL cs.IR,2024-06-14
SpatialBot: Precise Spatial Understanding with Vision Language Models,,"Vision Language Models (VLMs) have achieved impressive performance in 2D
image understanding, however they are still struggling with spatial
understanding which is the foundation of Embodied AI. In this paper, we propose
SpatialBot for better spatial understanding by feeding both RGB and depth
images. Additionally, we have constructed the SpatialQA dataset, which involves
multi-level depth-related questions to train VLMs for depth understanding.
Finally, we present SpatialBench to comprehensively evaluate VLMs' capabilities
in spatial understanding at different levels. Extensive experiments on our
spatial-understanding benchmark, general VLM benchmarks and Embodied AI tasks,
demonstrate the remarkable improvements of SpatialBot trained on SpatialQA. The
model, code and data are available at https://github.com/BAAI-DCAI/SpatialBot.",cs.CV,2024-06-19
Definition generation for lexical semantic change detection,,"We use contextualized word definitions generated by large language models as
semantic representations in the task of diachronic lexical semantic change
detection (LSCD). In short, generated definitions are used as `senses', and the
change score of a target word is retrieved by comparing their distributions in
two time periods under comparison. On the material of five datasets and three
languages, we show that generated definitions are indeed specific and general
enough to convey a signal sufficient to rank sets of words by the degree of
their semantic change over time. Our approach is on par with or outperforms
prior non-supervised sense-based LSCD methods. At the same time, it preserves
interpretability and allows to inspect the reasons behind a specific shift in
terms of discrete definitions-as-senses. This is another step in the direction
of explainable semantic change modeling.",cs.CL,2024-06-20
Deblurring Neural Radiance Fields with Event-driven Bundle Adjustment,,"Neural Radiance Fields (NeRF) achieves impressive 3D representation learning
and novel view synthesis results with high-quality multi-view images as input.
However, motion blur in images often occurs in low-light and high-speed motion
scenes, which significantly degrades the reconstruction quality of NeRF.
Previous deblurring NeRF methods struggle to estimate pose and lighting changes
during the exposure time, making them unable to accurately model the motion
blur. The bio-inspired event camera measuring intensity changes with high
temporal resolution makes up this information deficiency. In this paper, we
propose Event-driven Bundle Adjustment for Deblurring Neural Radiance Fields
(EBAD-NeRF) to jointly optimize the learnable poses and NeRF parameters by
leveraging the hybrid event-RGB data. An intensity-change-metric event loss and
a photo-metric blur loss are introduced to strengthen the explicit modeling of
camera motion blur. Experiments on both synthetic and real-captured data
demonstrate that EBAD-NeRF can obtain accurate camera trajectory during the
exposure time and learn a sharper 3D representations compared to prior works.",cs.CV,2024-06-20
"V-RECS, a Low-Cost LLM4VIS Recommender with Explanations, Captioning and
  Suggestions",,"NL2VIS (natural language to visualization) is a promising and recent research
area that involves interpreting natural language queries and translating them
into visualizations that accurately represent the underlying data. As we
navigate the era of big data, NL2VIS holds considerable application potential
since it greatly facilitates data exploration by non-expert users. Following
the increasingly widespread usage of generative AI in NL2VIS applications, in
this paper we present V-RECS, the first LLM-based Visual Recommender augmented
with explanations(E), captioning(C), and suggestions(S) for further data
exploration. V-RECS' visualization narratives facilitate both response
verification and data exploration by non-expert users. Furthermore, our
proposed solution mitigates computational, controllability, and cost issues
associated with using powerful LLMs by leveraging a methodology to effectively
fine-tune small models. To generate insightful visualization narratives, we use
Chain-of-Thoughts (CoT), a prompt engineering technique to help LLM identify
and generate the logical steps to produce a correct answer. Since CoT is
reported to perform poorly with small LLMs, we adopted a strategy in which a
large LLM (GPT-4), acting as a Teacher, generates CoT-based instructions to
fine-tune a small model, Llama-2-7B, which plays the role of a Student.
Extensive experiments-based on a framework for the quantitative evaluation of
AI-based visualizations and on manual assessment by a group of
participants-show that V-RECS achieves performance scores comparable to GPT-4,
at a much lower cost. The efficacy of the V-RECS teacher-student paradigm is
also demonstrated by the fact that the un-tuned Llama fails to perform the task
in the vast majority of test cases. We release V-RECS for the visualization
community to assist visualization designers throughout the entire visualization
generation process.",cs.HC cs.AI,2024-06-21
How to Rent GPUs on a Budget,,"The explosion in Machine Learning (ML) over the past ten years has led to a
dramatic increase in demand for GPUs to train ML models. Because it is
prohibitively expensive for most users to build and maintain a large GPU
cluster, large cloud providers (Microsoft Azure, Amazon AWS, Google Cloud) have
seen explosive growth in demand for renting cloud-based GPUs. In this
cloud-computing paradigm, a user must specify their demand for GPUs at every
moment in time, and will pay for every GPU-hour they use. ML training jobs are
known to be parallelizable to different degrees. Given a stream of ML training
jobs, a user typically wants to minimize the mean response time across all
jobs. Here, the response time of a job denotes the time from when a job arrives
until it is complete. Additionally, the user is constrained by some operating
budget. Specifically, in this paper the user is constrained to use no more than
$b$ GPUs per hour, over a long-run time average. The question is how to
minimize mean response time while meeting the budget constraint. Because
training jobs receive a diminishing marginal benefit from running on additional
GPUs, allocating too many GPUs to a single training job can dramatically
increase the overall cost paid by the user. Hence, an optimal rental policy
must balance a tradeoff between training cost and mean response time. This
paper derives the optimal rental policy for a stream of training jobs where the
jobs have different levels of parallelizability (specified by a speedup
function) and different job sizes (amounts of inherent work). We make almost no
assumptions about the arrival process and about the job size distribution. Our
optimal policy specifies how many GPUs to rent at every moment in time and how
to allocate these GPUs.",cs.DC cs.PF,2024-06-21
"Optimizing LaneSegNet for Real-Time Lane Topology Prediction in
  Autonomous Vehicles",,"With the increasing prevalence of autonomous vehicles, it is essential for
computer vision algorithms to accurately assess road features in real-time.
This study explores the LaneSegNet architecture, a new approach to lane
topology prediction which integrates topological information with lane-line
data to provide a more contextual understanding of road environments. The
LaneSegNet architecture includes a feature extractor, lane encoder, lane
decoder, and prediction head, leveraging components from ResNet-50, BEVFormer,
and various attention mechanisms. We experimented with optimizations to the
LaneSegNet architecture through feature extractor modification and transformer
encoder-decoder stack modification. We found that modifying the encoder and
decoder stacks offered an interesting tradeoff between training time and
prediction accuracy, with certain combinations showing promising results. Our
implementation, trained on a single NVIDIA Tesla A100 GPU, found that a 2:4
ratio reduced training time by 22.3% with only a 7.1% drop in mean average
precision, while a 4:8 ratio increased training time by only 11.1% but improved
mean average precision by a significant 23.7%. These results indicate that
strategic hyperparameter tuning can yield substantial improvements depending on
the resources of the user. This study provides valuable insights for optimizing
LaneSegNet according to available computation power, making it more accessible
for users with limited resources and increasing the capabilities for users with
more powerful resources.",cs.CV cs.AI cs.LG,2024-06-22
Decoder-only Architecture for Streaming End-to-end Speech Recognition,,"Decoder-only language models (LMs) have been successfully adopted for
speech-processing tasks including automatic speech recognition (ASR). The LMs
have ample expressiveness and perform efficiently. This efficiency is a
suitable characteristic for streaming applications of ASR. In this work, we
propose to use a decoder-only architecture for blockwise streaming ASR. In our
approach, speech features are compressed using CTC output and context embedding
using blockwise speech subnetwork, and are sequentially provided as prompts to
the decoder. The decoder estimates the output tokens promptly at each block. To
this end, we also propose a novel training scheme using random-length prefix
prompts to make the model robust to the truncated prompts caused by blockwise
processing. An experimental comparison shows that our proposed decoder-only
streaming ASR achieves 8% relative word error rate reduction in the LibriSpeech
test-other set while being twice as fast as the baseline model.",eess.AS cs.CL,2024-06-23
"Context-augmented Retrieval: A Novel Framework for Fast Information
  Retrieval based Response Generation using Large Language Model",,"Generating high-quality answers consistently by providing contextual
information embedded in the prompt passed to the Large Language Model (LLM) is
dependent on the quality of information retrieval. As the corpus of contextual
information grows, the answer/inference quality of Retrieval Augmented
Generation (RAG) based Question Answering (QA) systems declines. This work
solves this problem by combining classical text classification with the Large
Language Model (LLM) to enable quick information retrieval from the vector
store and ensure the relevancy of retrieved information. For the same, this
work proposes a new approach Context Augmented retrieval (CAR), where
partitioning of vector database by real-time classification of information
flowing into the corpus is done. CAR demonstrates good quality answer
generation along with significant reduction in information retrieval and answer
generation time.",cs.IR,2024-06-24
Consistent Query Answering over SHACL Constraints,,"The Shapes Constraint Language (SHACL) was standardized by the World Wide Web
as a constraint language to describe and validate RDF data graphs. SHACL uses
the notion of shapes graph to describe a set of shape constraints paired with
targets, that specify which nodes of the RDF graph should satisfy which shapes.
An important question in practice is how to handle data graphs that do not
validate the shapes graph. A solution is to tolerate the non-validation and
find ways to obtain meaningful and correct answers to queries despite the
non-validation. This is known as consistent query answering (CQA) and there is
extensive literature on CQA in both the database and the KR setting. We study
CQA in the context of SHACL for a fundamental fragment of the Semantic Web
query language SPARQL. The goal of our work is a detailed complexity analysis
of CQA for various semantics and possible restrictions on the acceptable
repairs. It turns out that all considered variants of the problem are
intractable, with complexities ranging between the first and third level of the
polynomial hierarchy.",cs.CC,2024-06-24
CAT: Interpretable Concept-based Taylor Additive Models,,"As an emerging interpretable technique, Generalized Additive Models (GAMs)
adopt neural networks to individually learn non-linear functions for each
feature, which are then combined through a linear model for final predictions.
Although GAMs can explain deep neural networks (DNNs) at the feature level,
they require large numbers of model parameters and are prone to overfitting,
making them hard to train and scale. Additionally, in real-world datasets with
many features, the interpretability of feature-based explanations diminishes
for humans. To tackle these issues, recent research has shifted towards
concept-based interpretable methods. These approaches try to integrate concept
learning as an intermediate step before making predictions, explaining the
predictions in terms of human-understandable concepts. However, these methods
require domain experts to extensively label concepts with relevant names and
their ground-truth values. In response, we propose CAT, a novel interpretable
Concept-bAsed Taylor additive model to simply this process. CAT does not have
to require domain experts to annotate concepts and their ground-truth values.
Instead, it only requires users to simply categorize input features into broad
groups, which can be easily accomplished through a quick metadata review.
Specifically, CAT first embeds each group of input features into
one-dimensional high-level concept representation, and then feeds the concept
representations into a new white-box Taylor Neural Network (TaylorNet). The
TaylorNet aims to learn the non-linear relationship between the inputs and
outputs using polynomials. Evaluation results across multiple benchmarks
demonstrate that CAT can outperform or compete with the baselines while
reducing the need of extensive model parameters. Importantly, it can explain
model predictions through high-level concepts that human can understand.",cs.LG,2024-06-25
"From Counting Stations to City-Wide Estimates: Data-Driven Bicycle
  Volume Extrapolation",,"Shifting to cycling in urban areas reduces greenhouse gas emissions and
improves public health. Street-level bicycle volume information would aid
cities in planning targeted infrastructure improvements to encourage cycling
and provide civil society with evidence to advocate for cyclists' needs. Yet,
the data currently available to cities and citizens often only comes from
sparsely located counting stations. This paper extrapolates bicycle volume
beyond these few locations to estimate bicycle volume for the entire city of
Berlin. We predict daily and average annual daily street-level bicycle volumes
using machine-learning techniques and various public data sources. These
include app-based crowdsourced data, infrastructure, bike-sharing, motorized
traffic, socioeconomic indicators, weather, and holiday data. Our analysis
reveals that the best-performing model is XGBoost, and crowdsourced cycling and
infrastructure data are most important for the prediction. We further simulate
how collecting short-term counts at predicted locations improves performance.
By providing ten days of such sample counts for each predicted location to the
model, we are able to halve the error and greatly reduce the variability in
performance among predicted locations.",cs.CY stat.AP,2024-06-26
"Investigating the Timescales of Language Processing with EEG and
  Language Models",,"This study explores the temporal dynamics of language processing by examining
the alignment between word representations from a pre-trained transformer-based
language model, and EEG data. Using a Temporal Response Function (TRF) model,
we investigate how neural activity corresponds to model representations across
different layers, revealing insights into the interaction between artificial
language models and brain responses during language comprehension. Our analysis
reveals patterns in TRFs from distinct layers, highlighting varying
contributions to lexical and compositional processing. Additionally, we used
linear discriminant analysis (LDA) to isolate part-of-speech (POS)
representations, offering insights into their influence on neural responses and
the underlying mechanisms of syntactic processing. These findings underscore
EEG's utility for probing language processing dynamics with high temporal
resolution. By bridging artificial language models and neural activity, this
study advances our understanding of their interaction at fine timescales.",cs.CL q-bio.NC,2024-06-28
"Pistis-RAG: A Scalable Cascading Framework Towards Trustworthy
  Retrieval-Augmented Generation",,"In Greek mythology, Pistis symbolized good faith, trust, and reliability.
Drawing inspiration from these principles, Pistis-RAG is a scalable multi-stage
framework designed to address the challenges of large-scale retrieval-augmented
generation (RAG) systems. This framework consists of distinct stages: matching,
pre-ranking, ranking, reasoning, and aggregating. Each stage contributes to
narrowing the search space, prioritizing semantically relevant documents,
aligning with the large language model's (LLM) preferences, supporting complex
chain-of-thought (CoT) methods, and combining information from multiple
sources.
  Our ranking stage introduces a significant innovation by recognizing that
semantic relevance alone may not lead to improved generation quality, due to
the sensitivity of the few-shot prompt order, as noted in previous research.
This critical aspect is often overlooked in current RAG frameworks.
  We argue that the alignment issue between LLMs and external knowledge ranking
methods is tied to the model-centric paradigm dominant in RAG systems. We
propose a content-centric approach, emphasizing seamless integration between
LLMs and external information sources to optimize content transformation for
specific tasks.
  Our novel ranking stage is designed specifically for RAG systems,
incorporating principles of information retrieval while considering the unique
business scenarios reflected in LLM preferences and user feedback. We simulated
feedback signals on the MMLU benchmark, resulting in a 9.3% performance
improvement. Our model and code will be open-sourced on GitHub. Additionally,
experiments on real-world, large-scale data validate the scalability of our
framework.",cs.IR cs.CL,2024-06-21
"PerAct2: Benchmarking and Learning for Robotic Bimanual Manipulation
  Tasks",,"Bimanual manipulation is challenging due to precise spatial and temporal
coordination required between two arms. While there exist several real-world
bimanual systems, there is a lack of simulated benchmarks with a large task
diversity for systematically studying bimanual capabilities across a wide range
of tabletop tasks. This paper addresses the gap by extending RLBench to
bimanual manipulation. We open-source our code and benchmark comprising 13 new
tasks with 23 unique task variations, each requiring a high degree of
coordination and adaptability. To kickstart the benchmark, we extended several
state-of-the art methods to bimanual manipulation and also present a
language-conditioned behavioral cloning agent -- PerAct2, which enables the
learning and execution of bimanual 6-DoF manipulation tasks. Our novel network
architecture efficiently integrates language processing with action prediction,
allowing robots to understand and perform complex bimanual tasks in response to
user-specified goals. Project website with code is available at:
http://bimanual.github.io",cs.RO cs.AI cs.CV cs.LG,2024-06-28
"ZeroDDI: A Zero-Shot Drug-Drug Interaction Event Prediction Method with
  Semantic Enhanced Learning and Dual-Modal Uniform Alignment",,"Drug-drug interactions (DDIs) can result in various pharmacological changes,
which can be categorized into different classes known as DDI events (DDIEs). In
recent years, previously unobserved/unseen DDIEs have been emerging, posing a
new classification task when unseen classes have no labelled instances in the
training stage, which is formulated as a zero-shot DDIE prediction (ZS-DDIE)
task. However, existing computational methods are not directly applicable to
ZS-DDIE, which has two primary challenges: obtaining suitable DDIE
representations and handling the class imbalance issue. To overcome these
challenges, we propose a novel method named ZeroDDI for the ZS-DDIE task.
Specifically, we design a biological semantic enhanced DDIE representation
learning module, which emphasizes the key biological semantics and distills
discriminative molecular substructure-related semantics for DDIE representation
learning. Furthermore, we propose a dual-modal uniform alignment strategy to
distribute drug pair representations and DDIE semantic representations
uniformly in a unit sphere and align the matched ones, which can mitigate the
issue of class imbalance. Extensive experiments showed that ZeroDDI surpasses
the baselines and indicate that it is a promising tool for detecting unseen
DDIEs. Our code has been released in https://github.com/wzy-Sarah/ZeroDDI.",cs.LG q-bio.BM,2024-06-30
"Quaternion-based Adaptive Backstepping Fast Terminal Sliding Mode
  Control for Quadrotor UAVs with Finite Time Convergence",,"This paper proposes a novel quaternion-based approach for tracking the
translation (position and linear velocity) and rotation (attitude and angular
velocity) trajectories of underactuated Unmanned Aerial Vehicles (UAVs).
Quadrotor UAVs are challenging regarding accuracy, singularity, and
uncertainties issues. Controllers designed based on unit-quaternion are
singularity-free for attitude representation compared to other methods (e.g.,
Euler angles), which fail to represent the vehicle's attitude at multiple
orientations. Quaternion-based Adaptive Backstepping Control (ABC) and Adaptive
Fast Terminal Sliding Mode Control (AFTSMC) are proposed to address a set of
challenging problems. A quaternion-based ABC, a superior recursive approach, is
proposed to generate the necessary thrust handling unknown uncertainties and
UAV translation trajectory tracking. Next, a quaternion-based AFTSMC is
developed to overcome parametric uncertainties, avoid singularity, and ensure
fast convergence in a finite time. Moreover, the proposed AFTSMC is able to
significantly minimize control signal chattering, which is the main reason for
actuator failure and provide smooth and accurate rotational control input. To
ensure the robustness of the proposed approach, the designed control algorithms
have been validated considering unknown time-variant parametric uncertainties
and significant initialization errors. The proposed techniques has been
compared to state-of-the-art control technique. Keywords: Adaptive Backstepping
Control (ABC), Adaptive Fast Terminal Sliding Mode Control (AFTSMC),
Unit-quaternion, Unmanned Aerial Vehicles, Singularity Free, Pose Control",cs.RO cs.SY eess.SY,2024-07-01
"Enhancing Stability for Large Models Training in Constrained Bandwidth
  Networks",,"Training extremely large language models with billions of parameters is a
computationally intensive task that pushes the limits of current data parallel
training systems. While techniques like ZeRO++ have enabled efficient
distributed training of such giant models on inexpensive low-bandwidth
clusters, they can suffer from convergence issues due to potential race
conditions in the hierarchical partitioning (hpZ) scheme employed to reduce
cross-machine communication. In this work, we first show how these race
conditions cause instability when training models with billions of parameters.
We then propose a modification to the partitioning algorithm that addresses
these convergence challenges while maintaining competitive training efficiency.
Empirical evaluation on training the multi-billion parameters Falcon Models and
Llama-2 models demonstrates the updated algorithm's ability to achieve reliable
convergence on these massive models, where stock ZeRO++ hpZ fails to converge.
The updated algorithm enables robust training of larger models with 98\%
throughput and model training speed improvement without sacrificing the quality
of convergence.",cs.LG cs.AI,2024-06-27
"Improving Steering and Verification in AI-Assisted Data Analysis with
  Interactive Task Decomposition",,"LLM-powered tools like ChatGPT Data Analysis, have the potential to help
users tackle the challenging task of data analysis programming, which requires
expertise in data processing, programming, and statistics. However, our
formative study (n=15) uncovered serious challenges in verifying AI-generated
results and steering the AI (i.e., guiding the AI system to produce the desired
output). We developed two contrasting approaches to address these challenges.
The first (Stepwise) decomposes the problem into step-by-step subgoals with
pairs of editable assumptions and code until task completion, while the second
(Phasewise) decomposes the entire problem into three editable, logical phases:
structured input/output assumptions, execution plan, and code. A controlled,
within-subjects experiment (n=18) compared these systems against a
conversational baseline. Users reported significantly greater control with the
Stepwise and Phasewise systems, and found intervention, correction, and
verification easier, compared to the baseline. The results suggest design
guidelines and trade-offs for AI-assisted data analysis tools.",cs.HC cs.AI,2024-07-02
Prediction Instability in Machine Learning Ensembles,,"In machine learning ensembles predictions from multiple models are
aggregated. Despite widespread use and strong performance of ensembles in
applied problems little is known about the mathematical properties of
aggregating models and associated consequences for safe, explainable use of
such models. In this paper we prove a theorem that shows that any ensemble will
exhibit at least one of the following forms of prediction instability. It will
either ignore agreement among all underlying models, change its mind when none
of the underlying models have done so, or be manipulable through inclusion or
exclusion of options it would never actually predict. As a consequence,
ensemble aggregation procedures will always need to balance the benefits of
information use against the risk of these prediction instabilities. This
analysis also sheds light on what specific forms of prediction instability to
expect from particular ensemble algorithms; for example popular tree ensembles
like random forest, or xgboost will violate basic, intuitive fairness
properties. Finally, we show that this can be ameliorated by using consistent
models in asymptotic conditions.",cs.LG,2024-07-03
"A Likelihood-Based Generative Approach for Spatially Consistent
  Precipitation Downscaling",,"Deep learning has emerged as a promising tool for precipitation downscaling.
However, current models rely on likelihood-based loss functions to properly
model the precipitation distribution, leading to spatially inconsistent
projections when sampling. This work explores a novel approach by fusing the
strengths of likelihood-based and adversarial losses used in generative models.
As a result, we propose a likelihood-based generative approach for
precipitation downscaling, leveraging the benefits of both methods.",physics.ao-ph cs.LG stat.ML,2024-06-26
"YourMT3+: Multi-instrument Music Transcription with Enhanced Transformer
  Architectures and Cross-dataset Stem Augmentation",,"Multi-instrument music transcription aims to convert polyphonic music
recordings into musical scores assigned to each instrument. This task is
challenging for modeling as it requires simultaneously identifying multiple
instruments and transcribing their pitch and precise timing, and the lack of
fully annotated data adds to the training difficulties. This paper introduces
YourMT3+, a suite of models for enhanced multi-instrument music transcription
based on the recent language token decoding approach of MT3. We enhance its
encoder by adopting a hierarchical attention transformer in the time-frequency
domain and integrating a mixture of experts. To address data limitations, we
introduce a new multi-channel decoding method for training with incomplete
annotations and propose intra- and cross-stem augmentation for dataset mixing.
Our experiments demonstrate direct vocal transcription capabilities,
eliminating the need for voice separation pre-processors. Benchmarks across ten
public datasets show our models' competitiveness with, or superiority to,
existing transcription models. Further testing on pop music recordings
highlights the limitations of current models. Fully reproducible code and
datasets are available with demos at \url{https://github.com/mimbres/YourMT3}.",eess.AS cs.LG cs.SD,2024-07-05
Feedback-Driven Automated Whole Bug Report Reproduction for Android Apps,,"In software development, bug report reproduction is a challenging task. This
paper introduces ReBL, a novel feedback-driven approach that leverages GPT-4, a
large-scale language model (LLM), to automatically reproduce Android bug
reports. Unlike traditional methods, ReBL bypasses the use of Step to Reproduce
(S2R) entities. Instead, it leverages the entire textual bug report and employs
innovative prompts to enhance GPT's contextual reasoning. This approach is more
flexible and context-aware than the traditional step-by-step entity matching
approach, resulting in improved accuracy and effectiveness. In addition to
handling crash reports, ReBL has the capability of handling non-crash
functional bug reports. Our evaluation of 96 Android bug reports (73 crash and
23 non-crash) demonstrates that ReBL successfully reproduced 90.63% of these
reports, averaging only 74.98 seconds per bug report. Additionally, ReBL
outperformed three existing tools in both success rate and speed.",cs.SE,2024-07-06
"Medfluencer: A Network Representation of Medical Influencers' Identities
  and Discourse on Social Media",,"In our study, we first constructed a dataset from the tweets of the top 100
medical influencers with the highest Influencer Score during the COVID-19
pandemic. This dataset was then used to construct a socio-semantic network,
mapping both their identities and key topics, which are crucial for
understanding their impact on public health discourse. To achieve this, we
developed a few-shot multi-label classifier to identify influencers and their
network actors' identities, employed BERTopic for extracting thematic content,
and integrated these components into a network model to analyze their impact on
health discourse. To ensure the reproducibility of our results, we have made
the code available at https://github.com/ZhijinGuo/Medinfluencer.",cs.SI,2024-07-06
Dynamic Neural Radiance Field From Defocused Monocular Video,,"Dynamic Neural Radiance Field (NeRF) from monocular videos has recently been
explored for space-time novel view synthesis and achieved excellent results.
However, defocus blur caused by depth variation often occurs in video capture,
compromising the quality of dynamic reconstruction because the lack of sharp
details interferes with modeling temporal consistency between input views. To
tackle this issue, we propose D2RF, the first dynamic NeRF method designed to
restore sharp novel views from defocused monocular videos. We introduce layered
Depth-of-Field (DoF) volume rendering to model the defocus blur and reconstruct
a sharp NeRF supervised by defocused views. The blur model is inspired by the
connection between DoF rendering and volume rendering. The opacity in volume
rendering aligns with the layer visibility in DoF rendering. To execute the
blurring, we modify the layered blur kernel to the ray-based kernel and employ
an optimized sparse kernel to gather the input rays efficiently and render the
optimized rays with our layered DoF volume rendering. We synthesize a dataset
with defocused dynamic scenes for our task, and extensive experiments on our
dataset show that our method outperforms existing approaches in synthesizing
all-in-focus novel views from defocus blur while maintaining spatial-temporal
consistency in the scene.",cs.CV,2024-07-07
An Earth Rover dataset recorded at the ICRA@40 party,,"The ICRA conference is celebrating its $40^{th}$ anniversary in Rotterdam in
September 2024, with as highlight the Happy Birthday ICRA Party at the iconic
Holland America Line Cruise Terminal. One month later the IROS conference will
take place, which will include the Earth Rover Challenge. In this challenge
open-world autonomous navigation models are studied truly open-world settings.
  As part of the Earth Rover Challenge several real-world navigation sets in
several cities world-wide, like Auckland, Australia and Wuhan, China. The only
dataset recorded in the Netherlands is the small village Oudewater. The
proposal is to record a dataset with the robot used in the Earth Rover
Challenge in Rotterdam, in front of the Holland America Line Cruise Terminal,
before the festivities of the Happy Birthday ICRA Party start.
  See: https://github.com/SlamMate/vSLAM-on-FrodoBots-2K",cs.RO cs.CV,2024-07-08
"Large Language Models for Wearable Sensor-Based Human Activity
  Recognition, Health Monitoring, and Behavioral Modeling: A Survey of Early
  Trends, Datasets, and Challenges",,"The proliferation of wearable technology enables the generation of vast
amounts of sensor data, offering significant opportunities for advancements in
health monitoring, activity recognition, and personalized medicine. However,
the complexity and volume of this data present substantial challenges in data
modeling and analysis, which have been tamed with approaches spanning time
series modeling to deep learning techniques. The latest frontier in this domain
is the adoption of Large Language Models (LLMs), such as GPT-4 and Llama, for
data analysis, modeling, understanding, and generation of human behavior
through the lens of wearable sensor data. This survey explores current trends
and challenges in applying LLMs for sensor-based human activity recognition and
behavior modeling. We discuss the nature of wearable sensors data, the
capabilities and limitations of LLMs to model them and their integration with
traditional machine learning techniques. We also identify key challenges,
including data quality, computational requirements, interpretability, and
privacy concerns. By examining case studies and successful applications, we
highlight the potential of LLMs in enhancing the analysis and interpretation of
wearable sensors data. Finally, we propose future directions for research,
emphasizing the need for improved preprocessing techniques, more efficient and
scalable models, and interdisciplinary collaboration. This survey aims to
provide a comprehensive overview of the intersection between wearable sensors
data and LLMs, offering insights into the current state and future prospects of
this emerging field.",cs.HC,2024-07-09
"H-FCBFormer Hierarchical Fully Convolutional Branch Transformer for
  Occlusal Contact Segmentation with Articulating Paper",,"Occlusal contacts are the locations at which the occluding surfaces of the
maxilla and the mandible posterior teeth meet. Occlusal contact detection is a
vital tool for restoring the loss of masticatory function and is a mandatory
assessment in the field of dentistry, with particular importance in
prosthodontics and restorative dentistry. The most common method for occlusal
contact detection is articulating paper. However, this method can indicate
significant medically false positive and medically false negative contact
areas, leaving the identification of true occlusal indications to clinicians.
To address this, we propose a multiclass Vision Transformer and Fully
Convolutional Network ensemble semantic segmentation model with a combination
hierarchical loss function, which we name as Hierarchical Fully Convolutional
Branch Transformer (H-FCBFormer). We also propose a method of generating
medically true positive semantic segmentation masks derived from expert
annotated articulating paper masks and gold standard masks. The proposed model
outperforms other machine learning methods evaluated at detecting medically
true positive contacts and performs better than dentists in terms of accurately
identifying object-wise occlusal contact areas while taking significantly less
time to identify them. Code is available at
https://github.com/Banksylel/H-FCBFormer.",cs.CV cs.AI,2024-07-10
"AutoMate: Specialist and Generalist Assembly Policies over Diverse
  Geometries",,"Robotic assembly for high-mixture settings requires adaptivity to diverse
parts and poses, which is an open challenge. Meanwhile, in other areas of
robotics, large models and sim-to-real have led to tremendous progress.
Inspired by such work, we present AutoMate, a learning framework and system
that consists of 4 parts: 1) a dataset of 100 assemblies compatible with
simulation and the real world, along with parallelized simulation environments
for policy learning, 2) a novel simulation-based approach for learning
specialist (i.e., part-specific) policies and generalist (i.e., unified)
assembly policies, 3) demonstrations of specialist policies that individually
solve 80 assemblies with 80% or higher success rates in simulation, as well as
a generalist policy that jointly solves 20 assemblies with an 80%+ success
rate, and 4) zero-shot sim-to-real transfer that achieves similar (or better)
performance than simulation, including on perception-initialized assembly. The
key methodological takeaway is that a union of diverse algorithms from
manufacturing engineering, character animation, and time-series analysis
provides a generic and robust solution for a diverse range of robotic assembly
problems. To our knowledge, AutoMate provides the first simulation-based
framework for learning specialist and generalist policies over a wide range of
assemblies, as well as the first system demonstrating zero-shot sim-to-real
transfer over such a range. For videos and additional details, please see our
project website: https://bingjietang718.github.io/automate/",cs.RO,2024-07-10
"Textual Query-Driven Mask Transformer for Domain Generalized
  Segmentation",,"In this paper, we introduce a method to tackle Domain Generalized Semantic
Segmentation (DGSS) by utilizing domain-invariant semantic knowledge from text
embeddings of vision-language models. We employ the text embeddings as object
queries within a transformer-based segmentation framework (textual object
queries). These queries are regarded as a domain-invariant basis for pixel
grouping in DGSS. To leverage the power of textual object queries, we introduce
a novel framework named the textual query-driven mask transformer (tqdm). Our
tqdm aims to (1) generate textual object queries that maximally encode
domain-invariant semantics and (2) enhance the semantic clarity of dense visual
features. Additionally, we suggest three regularization losses to improve the
efficacy of tqdm by aligning between visual and textual features. By utilizing
our method, the model can comprehend inherent semantic information for classes
of interest, enabling it to generalize to extreme domains (e.g., sketch style).
Our tqdm achieves 68.9 mIoU on GTA5$\rightarrow$Cityscapes, outperforming the
prior state-of-the-art method by 2.5 mIoU. The project page is available at
https://byeonghyunpak.github.io/tqdm.",cs.CV,2024-07-12
LFFR: Logistic Function For (single-output) Regression,,"Privacy-preserving regression in machine learning is a crucial area of
research, aimed at enabling the use of powerful machine learning techniques
while protecting individuals' privacy. In this paper, we implement
privacy-preserving regression training using data encrypted under a fully
homomorphic encryption scheme. We first examine the common linear regression
algorithm and propose a (simplified) fixed Hessian for linear regression
training, which can be applied for any datasets even not normalized into the
range $[0, 1]$. We also generalize this constant Hessian matrix to the ridge
regression version, namely linear regression which includes a regularization
term to penalize large coefficients. However, our main contribution is to
develop a novel and efficient algorithm called LFFR for homomorphic regression
using the logistic function, which could model more complex relations between
input values and output prediction in comparison with linear regression. We
also find a constant simplified Hessian to train our LFFR algorithm using the
Newton-like method and compare it against to with our new fixed Hessian linear
regression training over two real-world datasets. We suggest normalizing not
only the data but also the target predictions even for the original linear
regression used in a privacy-preserving manner, which is helpful to remain
weights in a small range, say $[-5, +5]$ good for refreshing ciphertext setting
parameters, and avoid tuning the regularization parameter $\lambda$ via cross
validation. The linear regression with normalized predictions could be a viable
alternative to ridge regression.",cs.LG cs.CR,2024-07-13
"Fluid Antenna Multiple Access Assisted Integrated Data and Energy
  Transfer: Outage and Multiplexing Gain Analysis",,"Fluid antenna multiple access (FAMA) exploits the spatial opportunities in
wireless channels to overcome multiuser interference by position (a.k.a.~port)
switching, which can achieve better performance compared to traditional fixed
multiple-input multiple-output (MIMO) systems. Additionally, integrated data
and energy transfer (IDET) is capable of providing both wireless data transfer
(WDT) and wireless energy transfer (WET) services towards low-power devices. In
this paper, a FAMA-assisted IDET system is investigated, where a base station
(BS) equipped with $N$ fixed antennas provides dedicated IDET services towards
$N$ user equipments (UEs). Each UE is equipped with a single fluid antenna,
while the power splitting (PS) approach is conceived for coordinating WDT and
WET. The outage probabilities of both WDT and WET are derived and approximated
into closed-forms, where the fluid antenna (FA) at each UE selects the optimal
port to achieve the maximum signal-to-interference-plus-noise ratio (SINR) or
the energy harvesting power (EHP). The IDET outage probabilities are defined
and subsequently derived and approximated into closed-forms. Further,
multiplexing gains of the proposed system are defined and analyzed to evaluate
the performace. Numerical results validate the theoretical analysis, while also
illustrate that the trade-off is achieved between WDT and WET performance by
exploiting different port selection strategies. Furthermore, the number of UEs
should be optimized to achieve better IDET performance of the system.",cs.IT math.IT,2024-07-15
Learning-based Big Data Sharing Incentive in Mobile AIGC Networks,,"Rapid advancements in wireless communication have led to a dramatic upsurge
in data volumes within mobile edge networks. These substantial data volumes
offer opportunities for training Artificial Intelligence-Generated Content
(AIGC) models to possess strong prediction and decision-making capabilities.
AIGC represents an innovative approach that utilizes sophisticated generative
AI algorithms to automatically generate diverse content based on user inputs.
Leveraging mobile edge networks, mobile AIGC networks enable customized and
real-time AIGC services for users by deploying AIGC models on edge devices.
Nonetheless, several challenges hinder the provision of high-quality AIGC
services, including issues related to the quality of sensing data for AIGC
model training and the establishment of incentives for big data sharing from
mobile devices to edge devices amidst information asymmetry. In this paper, we
initially define a Quality of Data (QoD) metric based on the age of information
to quantify the quality of sensing data. Subsequently, we propose a contract
theoretic model aimed at motivating mobile devices for big data sharing.
Furthermore, we employ a Proximal Policy Optimization (PPO) algorithm to
determine the optimal contract. Numerical results demonstrate the efficacy and
reliability of the proposed PPO-based contract model.",cs.NI,2024-06-10
"Generative AI for Health Technology Assessment: Opportunities,
  Challenges, and Policy Considerations",,"This review introduces the transformative potential of generative Artificial
Intelligence (AI) and foundation models, including large language models
(LLMs), for health technology assessment (HTA). We explore their applications
in four critical areas, evidence synthesis, evidence generation, clinical
trials and economic modeling: (1) Evidence synthesis: Generative AI has the
potential to assist in automating literature reviews and meta-analyses by
proposing search terms, screening abstracts, and extracting data with notable
accuracy; (2) Evidence generation: These models can potentially facilitate
automating the process and analyze the increasingly available large collections
of real-world data (RWD), including unstructured clinical notes and imaging,
enhancing the speed and quality of real-world evidence (RWE) generation; (3)
Clinical trials: Generative AI can be used to optimize trial design, improve
patient matching, and manage trial data more efficiently; and (4) Economic
modeling: Generative AI can also aid in the development of health economic
models, from conceptualization to validation, thus streamlining the overall HTA
process. Despite their promise, these technologies, while rapidly improving,
are still nascent and continued careful evaluation in their applications to HTA
is required. To ensure their responsible use and implementation, both
developers and users of research incorporating these tools, should familiarize
themselves with their current limitations, including the issues related to
scientific validity, risk of bias, and consider equity and ethical
implications. We also surveyed the current policy landscape and provide
suggestions for HTA agencies on responsibly integrating generative AI into
their workflows, emphasizing the importance of human oversight and the
fast-evolving nature of these tools.",cs.LG cs.AI,2024-07-09
"Restore-RWKV: Efficient and Effective Medical Image Restoration with
  RWKV",,"Transformers have revolutionized medical image restoration, but the quadratic
complexity still poses limitations for their application to high-resolution
medical images. The recent advent of RWKV in the NLP field has attracted much
attention as it can process long sequences efficiently. To leverage its
advanced design, we propose Restore-RWKV, the first RWKV-based model for
medical image restoration. Since the original RWKV model is designed for 1D
sequences, we make two necessary modifications for modeling spatial relations
in 2D images. First, we present a recurrent WKV (Re-WKV) attention mechanism
that captures global dependencies with linear computational complexity. Re-WKV
incorporates bidirectional attention as basic for a global receptive field and
recurrent attention to effectively model 2D dependencies from various scan
directions. Second, we develop an omnidirectional token shift (Omni-Shift)
layer that enhances local dependencies by shifting tokens from all directions
and across a wide context range. These adaptations make the proposed
Restore-RWKV an efficient and effective model for medical image restoration.
Extensive experiments demonstrate that Restore-RWKV achieves superior
performance across various medical image restoration tasks, including MRI image
super-resolution, CT image denoising, PET image synthesis, and all-in-one
medical image restoration. Code is available at:
\href{https://github.com/Yaziwel/Restore-RWKV.git}{https://github.com/Yaziwel/Restore-RWKV}.",eess.IV cs.CV,2024-07-14
"The Devil is in the Statistics: Mitigating and Exploiting Statistics
  Difference for Generalizable Semi-supervised Medical Image Segmentation",,"Despite the recent success of domain generalization in medical image
segmentation, voxel-wise annotation for all source domains remains a huge
burden. Semi-supervised domain generalization has been proposed very recently
to combat this challenge by leveraging limited labeled data along with abundant
unlabeled data collected from multiple medical institutions, depending on
precisely harnessing unlabeled data while improving generalization
simultaneously. In this work, we observe that domain shifts between medical
institutions cause disparate feature statistics, which significantly
deteriorates pseudo-label quality due to an unexpected normalization process.
Nevertheless, this phenomenon could be exploited to facilitate unseen domain
generalization. Therefore, we propose 1) multiple statistics-individual
branches to mitigate the impact of domain shifts for reliable pseudo-labels and
2) one statistics-aggregated branch for domain-invariant feature learning.
Furthermore, to simulate unseen domains with statistics difference, we approach
this from two aspects, i.e., a perturbation with histogram matching at image
level and a random batch normalization selection strategy at feature level,
producing diverse statistics to expand the training distribution. Evaluation
results on three medical image datasets demonstrate the effectiveness of our
method compared with recent SOTA methods. The code is available at
https://github.com/qiumuyang/SIAB.",cs.CV,2024-07-15
Fluid Antenna Grouping Index Modulation Design for MIMO Systems,,"Index modulation (IM) significantly enhances the spectral efficiency of fluid
antennas (FAs) enabled multiple-input multiple-output (MIMO) systems, which is
named FA-IM. However, due to the dense distribution of ports on the FA, the
wireless channel exhibits a high spatial correlation, leading to severe
performance degradation in the existing FA-IM-assisted MIMO systems. To tackle
this issue, this paper proposes a novel fluid antenna grouping index modulation
(FA-GIM) scheme to mitigate the high correlation between the activated ports.
Specifically, considering the characteristics of the FA two-dimensional (2D)
surface structure and the spatially correlated channel model in FA-assisted
MIMO systems, a block grouping method is adopted, where adjacent ports are
assigned to the same group. Subsequently, different groups independently
perform port index selection and constellation symbol mapping, with only one
port being activated within each group during each transmission interval.
Numerical results show that, compared to state-of-the-art schemes, the proposed
FA-GIM scheme consistently achieves significant bit error rate (BER)
performance gains under various conditions. It has also been confirmed that the
proposed scheme is both efficient and robust, enhancing the performance of
FA-assisted MIMO systems.",cs.IT eess.SP math.IT,2024-07-16
"How Are LLMs Mitigating Stereotyping Harms? Learning from Search Engine
  Studies",,"With the widespread availability of LLMs since the release of ChatGPT and
increased public scrutiny, commercial model development appears to have focused
their efforts on 'safety' training concerning legal liabilities at the expense
of social impact evaluation. This mimics a similar trend which we could observe
for search engine autocompletion some years prior. We draw on scholarship from
NLP and search engine auditing and present a novel evaluation task in the style
of autocompletion prompts to assess stereotyping in LLMs. We assess LLMs by
using four metrics, namely refusal rates, toxicity, sentiment and regard, with
and without safety system prompts. Our findings indicate an improvement to
stereotyping outputs with the system prompt, but overall a lack of attention by
LLMs under study to certain harms classified as toxic, particularly for prompts
about peoples/ethnicities and sexual orientation. Mentions of intersectional
identities trigger a disproportionate amount of stereotyping. Finally, we
discuss the implications of these findings about stereotyping harms in light of
the coming intermingling of LLMs and search and the choice of stereotyping
mitigation policy to adopt. We address model builders, academics, NLP
practitioners and policy makers, calling for accountability and awareness
concerning stereotyping harms, be it for training data curation, leader board
design and usage, or social impact measurement.",cs.CL,2024-07-16
"A hierarchical dynamical low-rank algorithm for the stochastic
  description of large reaction networks",,"The stochastic description of chemical reaction networks with the kinetic
chemical master equation (CME) is important for studying biological cells, but
it suffers from the curse of dimensionality: The amount of data to be stored
grows exponentially with the number of chemical species and thus exceeds the
capacity of common computational devices for realistic problems. Therefore,
time-dependent model order reduction techniques such as the dynamical low-rank
approximation are desirable. In this paper we propose a dynamical low-rank
algorithm for the kinetic CME using binary tree tensor networks. The
dimensionality of the problem is reduced in this approach by hierarchically
dividing the reaction network into partitions. Only reactions that cross
partitions are subject to an approximation error. We demonstrate by two
numerical examples (a 5-dimensional lambda phage model and a 20-dimensional
reaction cascade) that the proposed method drastically reduces memory
consumption and shows improved computational performance and better accuracy
compared to a Monte Carlo method.",math.NA cs.NA physics.bio-ph physics.comp-ph,2024-07-16
"Novel Hybrid Integrated Pix2Pix and WGAN Model with Gradient Penalty for
  Binary Images Denoising",,"This paper introduces a novel approach to image denoising that leverages the
advantages of Generative Adversarial Networks (GANs). Specifically, we propose
a model that combines elements of the Pix2Pix model and the Wasserstein GAN
(WGAN) with Gradient Penalty (WGAN-GP). This hybrid framework seeks to
capitalize on the denoising capabilities of conditional GANs, as demonstrated
in the Pix2Pix model, while mitigating the need for an exhaustive search for
optimal hyperparameters that could potentially ruin the stability of the
learning process. In the proposed method, the GAN's generator is employed to
produce denoised images, harnessing the power of a conditional GAN for noise
reduction. Simultaneously, the implementation of the Lipschitz continuity
constraint during updates, as featured in WGAN-GP, aids in reducing
susceptibility to mode collapse. This innovative design allows the proposed
model to benefit from the strong points of both Pix2Pix and WGAN-GP, generating
superior denoising results while ensuring training stability. Drawing on
previous work on image-to-image translation and GAN stabilization techniques,
the proposed research highlights the potential of GANs as a general-purpose
solution for denoising. The paper details the development and testing of this
model, showcasing its effectiveness through numerical experiments. The dataset
was created by adding synthetic noise to clean images. Numerical results based
on real-world dataset validation underscore the efficacy of this approach in
image-denoising tasks, exhibiting significant enhancements over traditional
techniques. Notably, the proposed model demonstrates strong generalization
capabilities, performing effectively even when trained with synthetic noise.",eess.IV cs.CV,2024-07-16
ICAGC 2024: Inspirational and Convincing Audio Generation Challenge 2024,,"The Inspirational and Convincing Audio Generation Challenge 2024 (ICAGC 2024)
is part of the ISCSLP 2024 Competitions and Challenges track. While current
text-to-speech (TTS) technology can generate high-quality audio, its ability to
convey complex emotions and controlled detail content remains limited. This
constraint leads to a discrepancy between the generated audio and human
subjective perception in practical applications like companion robots for
children and marketing bots. The core issue lies in the inconsistency between
high-quality audio generation and the ultimate human subjective experience.
Therefore, this challenge aims to enhance the persuasiveness and acceptability
of synthesized audio, focusing on human alignment convincing and inspirational
audio generation. A total of 19 teams have registered for the challenge, and
the results of the competition and the competition are described in this paper.",eess.AS cs.AI,2024-07-01
Temporally Grounding Instructional Diagrams in Unconstrained Videos,,"We study the challenging problem of simultaneously localizing a sequence of
queries in the form of instructional diagrams in a video. This requires
understanding not only the individual queries but also their
interrelationships. However, most existing methods focus on grounding one query
at a time, ignoring the inherent structures among queries such as the general
mutual exclusiveness and the temporal order. Consequently, the predicted
timespans of different step diagrams may overlap considerably or violate the
temporal order, thus harming the accuracy. In this paper, we tackle this issue
by simultaneously grounding a sequence of step diagrams. Specifically, we
propose composite queries, constructed by exhaustively pairing up the visual
content features of the step diagrams and a fixed number of learnable
positional embeddings. Our insight is that self-attention among composite
queries carrying different content features suppress each other to reduce
timespan overlaps in predictions, while the cross-attention corrects the
temporal misalignment via content and position joint guidance. We demonstrate
the effectiveness of our approach on the IAW dataset for grounding step
diagrams and the YouCook2 benchmark for grounding natural language queries,
significantly outperforming existing methods while simultaneously grounding
multiple queries.",cs.CV,2024-07-16
"Building AI Agents for Autonomous Clouds: Challenges and Design
  Principles",,"The rapid growth in the use of Large Language Models (LLMs) and AI Agents as
part of software development and deployment is revolutionizing the information
technology landscape. While code generation receives significant attention, a
higher-impact application lies in using AI agents for operational resilience of
cloud services, which currently require significant human effort and domain
knowledge. There is a growing interest in AI for IT Operations (AIOps) which
aims to automate complex operational tasks, like fault localization and root
cause analysis, thereby reducing human intervention and customer impact.
However, achieving the vision of autonomous and self-healing clouds through
AIOps is hampered by the lack of standardized frameworks for building,
evaluating, and improving AIOps agents. This vision paper lays the groundwork
for such a framework by first framing the requirements and then discussing
design decisions that satisfy them. We also propose AIOpsLab, a prototype
implementation leveraging agent-cloud-interface that orchestrates an
application, injects real-time faults using chaos engineering, and interfaces
with an agent to localize and resolve the faults. We report promising results
and lay the groundwork to build a modular and robust framework for building,
evaluating, and improving agents for autonomous clouds.",cs.SE cs.AI cs.DC,2024-07-16
"Conditional Quantile Estimation for Uncertain Watch Time in Short-Video
  Recommendation",,"Accurately predicting watch time is crucial for optimizing recommendations
and user experience in short video platforms. However, existing methods that
estimate a single average watch time often fail to capture the inherent
uncertainty and diversity in user engagement patterns. In this paper, we
propose the Conditional Quantile Estimation (CQE) framework to model the entire
conditional distribution of watch time. Using quantile regression, CQE
characterizes the complex watch-time distribution for each user-video pair,
providing a flexible and comprehensive approach to understanding user behavior.
We further design multiple strategies to combine the quantile estimates,
adapting to different recommendation scenarios and user preferences. Extensive
offline experiments and online A/B tests demonstrate the superiority of CQE in
watch time prediction and user engagement modeling. In particular, the online
deployment of CQE in KuaiShow has led to significant improvements in key
evaluation metrics, including active days, active users, engagement duration,
and video view counts. These results highlight the practical impact of our
proposed approach in enhancing the user experience and overall performance of
the short video recommendation system. The code will be released after
publication.",cs.LG cs.AI,2024-07-16
"COKE: Causal Discovery with Chronological Order and Expert Knowledge in
  High Proportion of Missing Manufacturing Data",,"Understanding causal relationships between machines is crucial for fault
diagnosis and optimization in manufacturing processes. Real-world datasets
frequently exhibit up to 90% missing data and high dimensionality from hundreds
of sensors. These datasets also include domain-specific expert knowledge and
chronological order information, reflecting the recording order across
different machines, which is pivotal for discerning causal relationships within
the manufacturing data. However, previous methods for handling missing data in
scenarios akin to real-world conditions have not been able to effectively
utilize expert knowledge. Conversely, prior methods that can incorporate expert
knowledge struggle with datasets that exhibit missing values. Therefore, we
propose COKE to construct causal graphs in manufacturing datasets by leveraging
expert knowledge and chronological order among sensors without imputing missing
data. Utilizing the characteristics of the recipe, we maximize the use of
samples with missing values, derive embeddings from intersections with an
initial graph that incorporates expert knowledge and chronological order, and
create a sensor ordering graph. The graph-generating process has been optimized
by an actor-critic architecture to obtain a final graph that has a maximum
reward. Experimental evaluations in diverse settings of sensor quantities and
missing proportions demonstrate that our approach compared with the benchmark
methods shows an average improvement of 39.9% in the F1-score. Moreover, the
F1-score improvement can reach 62.6% when considering the configuration similar
to real-world datasets, and 85.0% in real-world semiconductor datasets. The
source code is available at https://github.com/OuTingYun/COKE.",cs.LG stat.ME,2024-07-16
An Approximation for the 32-point Discrete Fourier Transform,,"This brief note aims at condensing some results on the 32-point approximate
DFT and discussing its arithmetic complexity.",eess.SP cs.NA math.NA stat.ME,2024-07-17
"Textualized and Feature-based Models for Compound Multimodal Emotion
  Recognition in the Wild",,"Systems for multimodal emotion recognition (ER) are commonly trained to
extract features from different modalities (e.g., visual, audio, and textual)
that are combined to predict individual basic emotions. However, compound
emotions often occur in real-world scenarios, and the uncertainty of
recognizing such complex emotions over diverse modalities is challenging for
feature-based models As an alternative, emerging multimodal large language
models (LLMs) like BERT and LLaMA rely on explicit non-verbal cues that may be
translated from different non-textual modalities (e.g., audio and visual) into
text. Textualization of modalities augments data with emotional cues to help
the LLM encode the interconnections between all modalities in a shared text
space. In such text-based models, prior knowledge of ER tasks is leveraged to
textualize relevant nonverbal cues such as audio tone from vocal expressions,
and action unit intensity from facial expressions. Since the pre-trained
weights are publicly available for many LLMs, training on large-scale datasets
is unnecessary, allowing fine-tuning for downstream tasks such as compound ER
(CER). This paper compares the potential of text- and feature-based approaches
for compound multimodal ER in videos. Experiments were conducted on the
challenging C-EXPR-DB dataset in the wild for CER, and contrasted with results
on the MELD dataset for basic ER. Our results indicate that multimodal
textualization provides lower accuracy than feature-based models on C-EXPR-DB,
where text transcripts are captured in the wild. However, higher accuracy can
be achieved when the video data has rich transcripts. Our code is available.",cs.CV,2024-07-17
"Research on Image Super-Resolution Reconstruction Mechanism based on
  Convolutional Neural Network",,"Super-resolution reconstruction techniques entail the utilization of software
algorithms to transform one or more sets of low-resolution images captured from
the same scene into high-resolution images. In recent years, considerable
advancement has been observed in the domain of single-image super-resolution
algorithms, particularly those based on deep learning techniques. Nevertheless,
the extraction of image features and nonlinear mapping methods in the
reconstruction process remain challenging for existing algorithms. These issues
result in the network architecture being unable to effectively utilize the
diverse range of information at different levels. The loss of high-frequency
details is significant, and the final reconstructed image features are overly
smooth, with a lack of fine texture details. This negatively impacts the
subjective visual quality of the image. The objective is to recover
high-quality, high-resolution images from low-resolution images. In this work,
an enhanced deep convolutional neural network model is employed, comprising
multiple convolutional layers, each of which is configured with specific
filters and activation functions to effectively capture the diverse features of
the image. Furthermore, a residual learning strategy is employed to accelerate
training and enhance the convergence of the network, while sub-pixel
convolutional layers are utilized to refine the high-frequency details and
textures of the image. The experimental analysis demonstrates the superior
performance of the proposed model on multiple public datasets when compared
with the traditional bicubic interpolation method and several other
learning-based super-resolution methods. Furthermore, it proves the model's
efficacy in maintaining image edges and textures.",cs.CV eess.IV,2024-07-18
Prover-Verifier Games improve legibility of LLM outputs,,"One way to increase confidence in the outputs of Large Language Models (LLMs)
is to support them with reasoning that is clear and easy to check -- a property
we call legibility. We study legibility in the context of solving grade-school
math problems and show that optimizing chain-of-thought solutions only for
answer correctness can make them less legible. To mitigate the loss in
legibility, we propose a training algorithm inspired by Prover-Verifier Game
from Anil et al. (2021). Our algorithm iteratively trains small verifiers to
predict solution correctness, ""helpful"" provers to produce correct solutions
that the verifier accepts, and ""sneaky"" provers to produce incorrect solutions
that fool the verifier. We find that the helpful prover's accuracy and the
verifier's robustness to adversarial attacks increase over the course of
training. Furthermore, we show that legibility training transfers to
time-constrained humans tasked with verifying solution correctness. Over course
of LLM training human accuracy increases when checking the helpful prover's
solutions, and decreases when checking the sneaky prover's solutions. Hence,
training for checkability by small verifiers is a plausible technique for
increasing output legibility. Our results suggest legibility training against
small verifiers as a practical avenue for increasing legibility of large LLMs
to humans, and thus could help with alignment of superhuman models.",cs.CL,2024-07-18
"Latent Causal Probing: A Formal Perspective on Probing with Causal
  Models of Data",,"As language models (LMs) deliver increasing performance on a range of NLP
tasks, probing classifiers have become an indispensable technique in the effort
to better understand their inner workings. A typical setup involves (1)
defining an auxiliary task consisting of a dataset of text annotated with
labels, then (2) supervising small classifiers to predict the labels from the
representations of a pretrained LM as it processed the dataset. A high probing
accuracy is interpreted as evidence that the LM has learned to perform the
auxiliary task as an unsupervised byproduct of its original pretraining
objective. Despite the widespread usage of probes, however, the robust design
and analysis of probing experiments remains a challenge. We develop a formal
perspective on probing using structural causal models (SCM). Specifically,
given an SCM which explains the distribution of tokens observed during
training, we frame the central hypothesis as whether the LM has learned to
represent the latent variables of the SCM. Empirically, we extend a recent
study of LMs in the context of a synthetic grid-world navigation task, where
having an exact model of the underlying causal structure allows us to draw
strong inferences from the result of probing experiments. Our techniques
provide robust empirical evidence for the ability of LMs to induce the latent
concepts underlying text.",cs.CL cs.AI,2024-07-18
Quantum Hamiltonian Embedding of Images for Data Reuploading Classifiers,,"When applying quantum computing to machine learning tasks, one of the first
considerations is the design of the quantum machine learning model itself.
Conventionally, the design of quantum machine learning algorithms relies on the
``quantisation"" of classical learning algorithms, such as using quantum linear
algebra to implement important subroutines of classical algorithms, if not the
entire algorithm, seeking to achieve quantum advantage through possible
run-time accelerations brought by quantum computing. However, recent research
has started questioning whether quantum advantage via speedup is the right goal
for quantum machine learning [1]. Research also has been undertaken to exploit
properties that are unique to quantum systems, such as quantum contextuality,
to better design quantum machine learning models [2]. In this paper, we take an
alternative approach by incorporating the heuristics and empirical evidences
from the design of classical deep learning algorithms to the design of quantum
neural networks. We first construct a model based on the data reuploading
circuit [3] with the quantum Hamiltonian data embedding unitary [4]. Through
numerical experiments on images datasets, including the famous MNIST and
FashionMNIST datasets, we demonstrate that our model outperforms the quantum
convolutional neural network (QCNN)[5] by a large margin (up to over 40% on
MNIST test set). Based on the model design process and numerical results, we
then laid out six principles for designing quantum machine learning models,
especially quantum neural networks.",quant-ph cs.AI cs.CV cs.LG,2024-07-19
Longhorn: State Space Models are Amortized Online Learners,,"The most fundamental capability of modern AI methods such as Large Language
Models (LLMs) is the ability to predict the next token in a long sequence of
tokens, known as ``sequence modeling."" Although the Transformers model is the
current dominant approach to sequence modeling, its quadratic computational
cost with respect to sequence length is a significant drawback. State-space
models (SSMs) offer a promising alternative due to their linear decoding
efficiency and high parallelizability during training. However, existing SSMs
often rely on seemingly ad hoc linear recurrence designs. In this work, we
explore SSM design through the lens of online learning, conceptualizing SSMs as
meta-modules for specific online learning problems. This approach links SSM
design to formulating precise online learning objectives, with state transition
rules derived from optimizing these objectives. Based on this insight, we
introduce a novel deep SSM architecture based on the implicit update for
optimizing an online regression objective. Our experimental results show that
our models outperform state-of-the-art SSMs, including the Mamba model, on
standard sequence modeling benchmarks and language modeling tasks.",cs.LG,2024-07-19
Stable Audio Open,,"Open generative models are vitally important for the community, allowing for
fine-tunes and serving as baselines when presenting new models. However, most
current text-to-audio models are private and not accessible for artists and
researchers to build upon. Here we describe the architecture and training
process of a new open-weights text-to-audio model trained with Creative Commons
data. Our evaluation shows that the model's performance is competitive with the
state-of-the-art across various metrics. Notably, the reported FDopenl3 results
(measuring the realism of the generations) showcase its potential for
high-quality stereo sound synthesis at 44.1kHz.",cs.SD cs.AI eess.AS,2024-07-19
"Towards Assessing Data Replication in Music Generation with Music
  Similarity Metrics on Raw Audio",,"Recent advancements in music generation are raising multiple concerns about
the implications of AI in creative music processes, current business models and
impacts related to intellectual property management. A relevant discussion and
related technical challenge is the potential replication and plagiarism of the
training set in AI-generated music, which could lead to misuse of data and
intellectual property rights violations. To tackle this issue, we present the
Music Replication Assessment (MiRA) tool: a model-independent open evaluation
method based on diverse audio music similarity metrics to assess data
replication. We evaluate the ability of five metrics to identify exact
replication by conducting a controlled replication experiment in different
music genres using synthetic samples. Our results show that the proposed
methodology can estimate exact data replication with a proportion higher than
10%. By introducing the MiRA tool, we intend to encourage the open evaluation
of music-generative models by researchers, developers, and users concerning
data replication, highlighting the importance of the ethical, social, legal,
and economic consequences. Code and examples are available for reproducibility
purposes.",cs.SD cs.AI cs.MM eess.AS,2024-07-19
"Jumping Ahead: Improving Reconstruction Fidelity with JumpReLU Sparse
  Autoencoders",,"Sparse autoencoders (SAEs) are a promising unsupervised approach for
identifying causally relevant and interpretable linear features in a language
model's (LM) activations. To be useful for downstream tasks, SAEs need to
decompose LM activations faithfully; yet to be interpretable the decomposition
must be sparse -- two objectives that are in tension. In this paper, we
introduce JumpReLU SAEs, which achieve state-of-the-art reconstruction fidelity
at a given sparsity level on Gemma 2 9B activations, compared to other recent
advances such as Gated and TopK SAEs. We also show that this improvement does
not come at the cost of interpretability through manual and automated
interpretability studies. JumpReLU SAEs are a simple modification of vanilla
(ReLU) SAEs -- where we replace the ReLU with a discontinuous JumpReLU
activation function -- and are similarly efficient to train and run. By
utilising straight-through-estimators (STEs) in a principled manner, we show
how it is possible to train JumpReLU SAEs effectively despite the discontinuous
JumpReLU function introduced in the SAE's forward pass. Similarly, we use STEs
to directly train L0 to be sparse, instead of training on proxies such as L1,
avoiding problems like shrinkage.",cs.LG,2024-07-19
"On Pre-training of Multimodal Language Models Customized for Chart
  Understanding",,"Recent studies customizing Multimodal Large Language Models (MLLMs) for
domain-specific tasks have yielded promising results, especially in the field
of scientific chart comprehension. These studies generally utilize visual
instruction tuning with specialized datasets to enhance question and answer
(QA) accuracy within the chart domain. However, they often neglect the
fundamental discrepancy between natural image-caption pre-training data and
digital chart image-QA data, particularly in the models' capacity to extract
underlying numeric values from charts. This paper tackles this oversight by
exploring the training processes necessary to improve MLLMs' comprehension of
charts. We present three key findings: (1) Incorporating raw data values in
alignment pre-training markedly improves comprehension of chart data. (2)
Replacing images with their textual representation randomly during end-to-end
fine-tuning transfer the language reasoning capability to chart interpretation
skills. (3) Requiring the model to first extract the underlying chart data and
then answer the question in the fine-tuning can further improve the accuracy.
Consequently, we introduce CHOPINLLM, an MLLM tailored for in-depth chart
comprehension. CHOPINLLM effectively interprets various types of charts,
including unannotated ones, while maintaining robust reasoning abilities.
Furthermore, we establish a new benchmark to evaluate MLLMs' understanding of
different chart types across various comprehension levels. Experimental results
show that CHOPINLLM exhibits strong performance in understanding both annotated
and unannotated charts across a wide range of types.",cs.CV cs.AI cs.CL,2024-07-19
"Is $F_1$ Score Suboptimal for Cybersecurity Models? Introducing
  $C_{score}$, a Cost-Aware Alternative for Model Assessment",,"The cost of errors related to machine learning classifiers, namely, false
positives and false negatives, are not equal and are application dependent. For
example, in cybersecurity applications, the cost of not detecting an attack is
very different from marking a benign activity as an attack. Various design
choices during machine learning model building, such as hyperparameter tuning
and model selection, allow a data scientist to trade-off between these two
errors. However, most of the commonly used metrics to evaluate model quality,
such as $F_1$ score, which is defined in terms of model precision and recall,
treat both these errors equally, making it difficult for users to optimize for
the actual cost of these errors. In this paper, we propose a new cost-aware
metric, $C_{score}$ based on precision and recall that can replace $F_1$ score
for model evaluation and selection. It includes a cost ratio that takes into
account the differing costs of handling false positives and false negatives. We
derive and characterize the new cost metric, and compare it to $F_1$ score.
Further, we use this metric for model thresholding for five cybersecurity
related datasets for multiple cost ratios. The results show an average cost
savings of 49%.",cs.LG cs.AI,2024-07-19
Knowledge Mechanisms in Large Language Models: A Survey and Perspective,,"Understanding knowledge mechanisms in Large Language Models (LLMs) is crucial
for advancing towards trustworthy AGI. This paper reviews knowledge mechanism
analysis from a novel taxonomy including knowledge utilization and evolution.
Knowledge utilization delves into the mechanism of memorization, comprehension
and application, and creation. Knowledge evolution focuses on the dynamic
progression of knowledge within individual and group LLMs. Moreover, we discuss
what knowledge LLMs have learned, the reasons for the fragility of parametric
knowledge, and the potential dark knowledge (hypothesis) that will be
challenging to address. We hope this work can help understand knowledge in LLMs
and provide insights for future research.",cs.CL cs.AI cs.CV cs.HC cs.LG,2024-07-22
"Iterative Ensemble Training with Anti-Gradient Control for Mitigating
  Memorization in Diffusion Models",,"Diffusion models, known for their tremendous ability to generate novel and
high-quality samples, have recently raised concerns due to their data
memorization behavior, which poses privacy risks. Recent approaches for memory
mitigation either only focused on the text modality problem in cross-modal
generation tasks or utilized data augmentation strategies. In this paper, we
propose a novel training framework for diffusion models from the perspective of
visual modality, which is more generic and fundamental for mitigating
memorization. To facilitate forgetting of stored information in diffusion model
parameters, we propose an iterative ensemble training strategy by splitting the
data into multiple shards for training multiple models and intermittently
aggregating these model parameters. Moreover, practical analysis of losses
illustrates that the training loss for easily memorable images tends to be
obviously lower. Thus, we propose an anti-gradient control method to exclude
the sample with a lower loss value from the current mini-batch to avoid
memorizing. Extensive experiments and analysis on four datasets are conducted
to illustrate the effectiveness of our method, and results show that our method
successfully reduces memory capacity while even improving the performance
slightly. Moreover, to save the computing cost, we successfully apply our
method to fine-tune the well-trained diffusion models by limited epochs,
demonstrating the applicability of our method. Code is available in
https://github.com/liuxiao-guan/IET_AGC.",cs.CV,2024-07-21
Scalable Dynamic Embedding Size Search for Streaming Recommendation,,"Recommender systems typically represent users and items by learning their
embeddings, which are usually set to uniform dimensions and dominate the model
parameters. However, real-world recommender systems often operate in streaming
recommendation scenarios, where the number of users and items continues to
grow, leading to substantial storage resource consumption for these embeddings.
Although a few methods attempt to mitigate this by employing embedding size
search strategies to assign different embedding dimensions in streaming
recommendations, they assume that the embedding size grows with the frequency
of users/items, which eventually still exceeds the predefined memory budget
over time. To address this issue, this paper proposes to learn Scalable
Lightweight Embeddings for streaming recommendation, called SCALL, which can
adaptively adjust the embedding sizes of users/items within a given memory
budget over time. Specifically, we propose to sample embedding sizes from a
probabilistic distribution, with the guarantee to meet any predefined memory
budget. By fixing the memory budget, the proposed embedding size sampling
strategy can increase and decrease the embedding sizes in accordance to the
frequency of the corresponding users or items. Furthermore, we develop a
reinforcement learning-based search paradigm that models each state with mean
pooling to keep the length of the state vectors fixed, invariant to the
changing number of users and items. As a result, the proposed method can
provide embedding sizes to unseen users and items. Comprehensive empirical
evaluations on two public datasets affirm the advantageous effectiveness of our
proposed method.",cs.IR,2024-07-22
Empirical Capacity Model for Self-Attention Neural Networks,,"Large pretrained self-attention neural networks, or transformers, have been
very successful in various tasks recently. The performance of a model on a
given task depends on its ability to memorize and generalize the training data.
Large transformer models, which may have billions of parameters, in theory have
a huge capacity to memorize content. However, the current algorithms for the
optimization fall short of the theoretical capacity, and the capacity is also
highly dependent on the content. In this paper, we focus on the memory capacity
of these models obtained using common training algorithms and synthetic
training data. Based on the results, we derive an empirical capacity model
(ECM) for a generic transformer. The ECM can be used to design task-specific
transformer models with an optimal number of parameters in cases where the
target memorization capability of the task can be defined.",cs.LG cs.AI cs.CL stat.ML,2024-07-22
"Towards a Universal Evaluation Model for Careful and Competent
  Autonomous Driving",,"Virtual scenario-based testing methods to validate autonomous driving systems
are predominantly centred around collision avoidance, and lack a comprehensive
approach to evaluate optimal driving behaviour holistically. Furthermore,
current validation approaches do not align with authorisation and monitoring
requirements put forth by regulatory bodies. We address these validation gaps
by outlining a universal evaluation framework that: incorporates the notion of
careful and competent driving, unifies behavioural competencies and evaluation
criteria, and is amenable at a scenario-specific and aggregate behaviour level.
This framework can be leveraged to evaluate optimal driving in scenario-based
testing, and for post-deployment monitoring to ensure continual compliance with
regulation and safety standards.",cs.RO,2024-07-22
The Impact of Responsible AI Research on Innovation and Development,,"Translational research, especially in the fast-evolving field of Artificial
Intelligence (AI), is key to converting scientific findings into practical
innovations. In Responsible AI (RAI) research, translational impact is often
viewed through various pathways, including research papers, blogs, news
articles, and the drafting of forthcoming AI legislation (e.g., the EU AI Act).
However, the real-world impact of RAI research remains an underexplored area.
Our study aims to capture it through two pathways: \emph{patents} and
\emph{code repositories}, both of which provide a rich and structured source of
data. Using a dataset of 200,000 papers from 1980 to 2022 in AI and related
fields, including Computer Vision, Natural Language Processing, and
Human-Computer Interaction, we developed a Sentence-Transformers Deep Learning
framework to identify RAI papers. This framework calculates the semantic
similarity between paper abstracts and a set of RAI keywords, which are derived
from the NIST's AI Risk Management Framework; a framework that aims to enhance
trustworthiness considerations in the design, development, use, and evaluation
of AI products, services, and systems. We identified 1,747 RAI papers published
in top venues such as CHI, CSCW, NeurIPS, FAccT, and AIES between 2015 and
2022. By analyzing these papers, we found that a small subset that goes into
patents or repositories is highly cited, with the translational process taking
between 1 year for repositories and up to 8 years for patents. Interestingly,
impactful RAI research is not limited to top U.S. institutions, but significant
contributions come from European and Asian institutions. Finally, the
multidisciplinary nature of RAI papers, often incorporating knowledge from
diverse fields of expertise, was evident as these papers tend to build on
unconventional combinations of prior knowledge.",cs.HC,2024-07-22
"Pediatric Wrist Fracture Detection in X-rays via YOLOv10 Algorithm and
  Dual Label Assignment System",,"Wrist fractures are highly prevalent among children and can significantly
impact their daily activities, such as attending school, participating in
sports, and performing basic self-care tasks. If not treated properly, these
fractures can result in chronic pain, reduced wrist functionality, and other
long-term complications. Recently, advancements in object detection have shown
promise in enhancing fracture detection, with systems achieving accuracy
comparable to, or even surpassing, that of human radiologists. The YOLO series,
in particular, has demonstrated notable success in this domain. This study is
the first to provide a thorough evaluation of various YOLOv10 variants to
assess their performance in detecting pediatric wrist fractures using the
GRAZPEDWRI-DX dataset. It investigates how changes in model complexity, scaling
the architecture, and implementing a dual-label assignment strategy can enhance
detection performance. Experimental results indicate that our trained model
achieved mean average precision (mAP@50-95) of 51.9\% surpassing the current
YOLOv9 benchmark of 43.3\% on this dataset. This represents an improvement of
8.6\%. The implementation code is publicly available at
https://github.com/ammarlodhi255/YOLOv10-Fracture-Detection",eess.IV cs.CV,2024-07-22
"Beyond Size and Class Balance: Alpha as a New Dataset Quality Metric for
  Deep Learning",,"In deep learning, achieving high performance on image classification tasks
requires diverse training sets. However, the current best
practice$\unicode{x2013}$maximizing dataset size and class
balance$\unicode{x2013}$does not guarantee dataset diversity. We hypothesized
that, for a given model architecture, model performance can be improved by
maximizing diversity more directly. To test this hypothesis, we introduce a
comprehensive framework of diversity measures from ecology that generalizes
familiar quantities like Shannon entropy by accounting for similarities among
images. (Size and class balance emerge as special cases.) Analyzing thousands
of subsets from seven medical datasets showed that the best correlates of
performance were not size or class balance but $A$$\unicode{x2013}$""big
alpha""$\unicode{x2013}$a set of generalized entropy measures interpreted as the
effective number of image-class pairs in the dataset, after accounting for
image similarities. One of these, $A_0$, explained 67% of the variance in
balanced accuracy, vs. 54% for class balance and just 39% for size. The best
pair of measures was size-plus-$A_1$ (79%), which outperformed
size-plus-class-balance (74%). Subsets with the largest $A_0$ performed up to
16% better than those with the largest size (median improvement, 8%). We
propose maximizing $A$ as a way to improve deep learning performance in medical
imaging.",cs.CV cs.LG,2024-07-22
The syzygy distinguisher,,"We present a new distinguisher for alternant and Goppa codes, whose
complexity is subexponential in the error-correcting capability, hence better
than that of generic decoding algorithms. Moreover it does not suffer from the
strong regime limitations of the previous distinguishers or structure recovery
algorithms: in particular, it applies to the codes used in the Classic McEliece
candidate for postquantum cryptography standardization. The invariants that
allow us to distinguish are graded Betti numbers of the homogeneous coordinate
ring of a shortening of the dual code.
  Since its introduction in 1978, this is the first time an analysis of the
McEliece cryptosystem breaks the exponential barrier.",cs.CR cs.IT math.AG math.IT,2024-07-22
Rapidly convergent series expansions for a class of resolvents,,"Following advances in the abstract theory of composites, we develop rapidly
converging series expansions about $z=\infty$ for the resolvent ${\bf
R}(z)=[z{\bf I}-{\bf P}^\dagger{\bf Q}{\bf P}]^{-1}$ where ${\bf Q}$ is an
orthogonal projection and ${\bf P}$ is such that ${\bf P}{\bf P}^\dagger$ is an
orthogonal projection. It is assumed that the spectrum of ${\bf P}^\dagger{\bf
Q}{\bf P}$ lies within the interval $[z^-,z^+]$ for some known $z^+\leq 1$ and
$z^-\geq 0$ and that the actions of the projections ${\bf Q}$ and ${\bf P}{\bf
P}^\dagger$ are easy to compute. The series converges in the entire $z$-plane
excluding the cut $[z^-,z^+]$. It is obtained using subspace substitution,
where the desired resolvent is tied to a resolvent in a larger space and ${\bf
Q}$ gets replaced by a projection $\underline{{\bf Q}}$ that is no longer
orthogonal. When $z$ is real the rate of convergence of the new method matches
that of the conjugate gradient method.",math.NA cs.NA math-ph math.AP math.CA math.MP,2024-07-22
Figure it Out: Analyzing-based Jailbreak Attack on Large Language Models,,"The rapid development of Large Language Models (LLMs) has brought remarkable
generative capabilities across diverse tasks. However, despite the impressive
achievements, these models still have numerous security vulnerabilities,
particularly when faced with jailbreak attacks. Therefore, by investigating
jailbreak attacks, we can uncover hidden weaknesses in LLMs and guide us in
developing more robust defense mechanisms to fortify their security. In this
paper, we further explore the boundary of jailbreak attacks on LLMs and propose
Analyzing-based Jailbreak (ABJ). This effective jailbreak attack method takes
advantage of LLMs' growing analyzing and reasoning capability and reveals their
underlying vulnerabilities when facing analysis-based tasks. We conduct a
detailed evaluation of ABJ across various open-source and closed-source LLMs,
which achieves 94.8% Attack Success Rate (ASR) and 1.06 Attack Efficiency (AE)
on GPT-4-turbo-0409, demonstrating state-of-the-art attack effectiveness and
efficiency. Our research highlights the importance of prioritizing and
enhancing the safety of LLMs to mitigate the risks of misuse.The code is
publicly available at https://github.com/theshi-1128/ABJ-Attack.",cs.CR cs.AI cs.CL cs.LG,2024-07-23
"Joint Resource-Power Allocation and UE Rank Selection in Multi-User MIMO
  Systems with Linear Transceivers",,"Next-generation wireless networks aim to deliver data speeds much faster than
5G. This requires base stations with lots of antennas and a large operating
bandwidth. These advanced base stations are expected to serve several
multiantenna user-equipment (UEs) simultaneously on the same time-frequency
resources on both the uplink and the downlink. The UE data rates are affected
by the following three main factors: UE rank, which refers to the number of
data layers used by each UE, UE frequency allocation, which refers to the
assignment of slices of the overall frequency band to use for each UE in an
orthogonal frequency-division multiplexing (OFDM) system, and UE power
allocation/control, which refers to the allocation of power by the base station
for data transmission to each UE on the downlink or the power used by each UE
to send data on the uplink. Since multiple UEs are to be simultaneously served,
the type of precoder used for downlink transmission and the type of receiver
used for uplink reception predominantly influence these three aforementioned
factors and the resulting overall UE throughput. This paper addresses the
problem of jointly selecting these three parameters specifically when
zero-forcing (ZF) precoders are used for downlink transmission and linear
minimum mean square error (LMMSE) receivers are employed for uplink reception.",cs.IT eess.SP math.IT,2024-07-23
"Dynamic Subgraph Matching via Cost-Model-based Vertex Dominance
  Embeddings (Technical Report)",,"In many real-world applications such as social network analysis, knowledge
graph discovery, biological network analytics, and so on, graph data management
has become increasingly important and has drawn much attention from the
database community. While many graphs (e.g., Twitter, Wikipedia, etc.) are
usually involving over time, it is of great importance to study the dynamic
subgraph matching (DSM) problem, a fundamental yet challenging graph operator,
which continuously monitors subgraph matching results over dynamic graphs with
a stream of edge updates. To efficiently tackle the DSM problem, we carefully
design a novel vertex dominance embedding approach, which effectively encodes
vertex labels that can be incrementally maintained upon graph updates. Inspire
by low pruning power for high-degree vertices, we propose a new degree grouping
technique over basic subgraph patterns in different degree groups (i.e., groups
of star substructures), and devise degree-aware star substructure synopses
(DAS^3) to effectively facilitate our designed vertex dominance and range
pruning strategies. We develop efficient algorithms to incrementally maintain
dynamic graphs and answer DSM queries. Through extensive experiments, we
confirm the efficiency of our proposed approaches over both real and synthetic
graphs.",cs.DB,2024-07-23
A Nested Model for AI Design and Validation,,"The growing AI field faces trust, transparency, fairness, and discrimination
challenges. Despite the need for new regulations, there is a mismatch between
regulatory science and AI, preventing a consistent framework. A five-layer
nested model for AI design and validation aims to address these issues and
streamline AI application design and validation, improving fairness, trust, and
AI adoption. This model aligns with regulations, addresses AI practitioner's
daily challenges, and offers prescriptive guidance for determining appropriate
evaluation approaches by identifying unique validity threats. We have three
recommendations motivated by this model: authors should distinguish between
layers when claiming contributions to clarify the specific areas in which the
contribution is made and to avoid confusion, authors should explicitly state
upstream assumptions to ensure that the context and limitations of their AI
system are clearly understood, AI venues should promote thorough testing and
validation of AI systems and their compliance with regulatory requirements.",cs.CY cs.AI cs.HC cs.LG,2024-06-08
"Time Series Imputation with Multivariate Radial Basis Function Neural
  Network",,"Researchers have been persistently working to address the issue of missing
values in time series data. Numerous models have been proposed, striving to
estimate the distribution of the data. The Radial Basis Functions Neural
Network (RBFNN) has recently exhibited exceptional performance in estimating
data distribution. In this paper, we propose a time series imputation model
based on RBFNN. Our imputation model learns local information from timestamps
to create a continuous function. Additionally, we incorporate time gaps to
facilitate learning information considering the missing terms of missing
values. We name this model the Missing Imputation Multivariate RBFNN
(MIM-RBFNN). However, MIM-RBFNN relies on a local information-based learning
approach, which presents difficulties in utilizing temporal information.
Therefore, we propose an extension called the Missing Value Imputation
Recurrent Neural Network with Continuous Function (MIRNN-CF) using the
continuous function generated by MIM-RBFNN. We evaluate the performance using
two real-world datasets with non-random missing and random missing patterns,
and conduct an ablation study comparing MIM-RBFNN and MIRNN-CF.",cs.LG cs.AI,2024-07-24
XMeCap: Meme Caption Generation with Sub-Image Adaptability,,"Humor, deeply rooted in societal meanings and cultural details, poses a
unique challenge for machines. While advances have been made in natural
language processing, real-world humor often thrives in a multi-modal context,
encapsulated distinctively by memes. This paper poses a particular emphasis on
the impact of multi-images on meme captioning. After that, we introduce the
\textsc{XMeCap} framework, a novel approach that adopts supervised fine-tuning
and reinforcement learning based on an innovative reward model, which factors
in both global and local similarities between visuals and text. Our results,
benchmarked against contemporary models, manifest a marked improvement in
caption generation for both single-image and multi-image memes, as well as
different meme categories. \textsc{XMeCap} achieves an average evaluation score
of 75.85 for single-image memes and 66.32 for multi-image memes, outperforming
the best baseline by 3.71\% and 4.82\%, respectively. This research not only
establishes a new frontier in meme-related studies but also underscores the
potential of machines in understanding and generating humor in a multi-modal
setting.",cs.CV cs.AI,2024-07-24
"Toward Automated Detection of Biased Social Signals from the Content of
  Clinical Conversations",,"Implicit bias can impede patient-provider interactions and lead to inequities
in care. Raising awareness is key to reducing such bias, but its manifestations
in the social dynamics of patient-provider communication are difficult to
detect. In this study, we used automated speech recognition (ASR) and natural
language processing (NLP) to identify social signals in patient-provider
interactions. We built an automated pipeline to predict social signals from
audio recordings of 782 primary care visits that achieved 90.1% average
accuracy across codes, and exhibited fairness in its predictions for white and
non-white patients. Applying this pipeline, we identified statistically
significant differences in provider communication behavior toward white versus
non-white patients. In particular, providers expressed more patient-centered
behaviors towards white patients including more warmth, engagement, and
attentiveness. Our study underscores the potential of automated tools in
identifying subtle communication signals that may be linked with bias and
impact healthcare quality and equity.",cs.CY cs.CL cs.LG,2024-07-01
"Explainable Natural Language Processing for Corporate Sustainability
  Analysis",,"Sustainability commonly refers to entities, such as individuals, companies,
and institutions, having a non-detrimental (or even positive) impact on the
environment, society, and the economy. With sustainability becoming a synonym
of acceptable and legitimate behaviour, it is being increasingly demanded and
regulated. Several frameworks and standards have been proposed to measure the
sustainability impact of corporations, including United Nations' sustainable
development goals and the recently introduced global sustainability reporting
framework, amongst others. However, the concept of corporate sustainability is
complex due to the diverse and intricate nature of firm operations (i.e.
geography, size, business activities, interlinks with other stakeholders). As a
result, corporate sustainability assessments are plagued by subjectivity both
within data that reflect corporate sustainability efforts (i.e. corporate
sustainability disclosures) and the analysts evaluating them. This subjectivity
can be distilled into distinct challenges, such as incompleteness, ambiguity,
unreliability and sophistication on the data dimension, as well as limited
resources and potential bias on the analyst dimension. Put together,
subjectivity hinders effective cost attribution to entities non-compliant with
prevailing sustainability expectations, potentially rendering sustainability
efforts and its associated regulations futile. To this end, we argue that
Explainable Natural Language Processing (XNLP) can significantly enhance
corporate sustainability analysis. Specifically, linguistic understanding
algorithms (lexical, semantic, syntactic), integrated with XAI capabilities
(interpretability, explainability, faithfulness), can bridge gaps in analyst
resources and mitigate subjectivity problems within data.",cs.CY cs.CL,2024-07-03
"ERIT Lightweight Multimodal Dataset for Elderly Emotion Recognition and
  Multimodal Fusion Evaluation",,"ERIT is a novel multimodal dataset designed to facilitate research in a
lightweight multimodal fusion. It contains text and image data collected from
videos of elderly individuals reacting to various situations, as well as seven
emotion labels for each data sample. Because of the use of labeled images of
elderly users reacting emotionally, it is also facilitating research on emotion
recognition in an underrepresented age group in machine learning visual emotion
recognition. The dataset is validated through comprehensive experiments
indicating its importance in neural multimodal fusion research.",cs.CV cs.CL cs.CY,2024-07-25
HG-PIPE: Vision Transformer Acceleration with Hybrid-Grained Pipeline,,"Vision Transformer (ViT) acceleration with field programmable gate array
(FPGA) is promising but challenging. Existing FPGA-based ViT accelerators
mainly rely on temporal architectures, which process different operators by
reusing the same hardware blocks and suffer from extensive memory access
overhead. Pipelined architectures, either coarse-grained or fine-grained,
unroll the ViT computation spatially for memory access efficiency. However,
they usually suffer from significant hardware resource constraints and pipeline
bubbles induced by the global computation dependency of ViT. In this paper, we
introduce HG-PIPE, a pipelined FPGA accelerator for high-throughput and
low-latency ViT processing. HG-PIPE features a hybrid-grained pipeline
architecture to reduce on-chip buffer cost and couples the computation dataflow
and parallelism design to eliminate the pipeline bubbles. HG-PIPE further
introduces careful approximations to implement both linear and non-linear
operators with abundant Lookup Tables (LUTs), thus alleviating resource
constraints. On a ZCU102 FPGA, HG-PIPE achieves 2.78 times better throughput
and 2.52 times better resource efficiency than the prior-art accelerators,
e.g., AutoViTAcc. With a VCK190 FPGA, HG-PIPE realizes end-to-end ViT
acceleration on a single device and achieves 7118 images/s, which is 2.81 times
faster than a V100 GPU.",cs.AR cs.AI,2024-07-25
Stochastic Games with Minimally Bounded Action Costs,,"In many multi-player interactions, players incur strictly positive costs each
time they execute actions e.g. 'menu costs' or transaction costs in financial
systems. Since acting at each available opportunity would accumulate
prohibitively large costs, the resulting decision problem is one in which
players must make strategic decisions about when to execute actions in addition
to their choice of action. This paper analyses a discrete-time stochastic game
(SG) in which players face minimally bounded positive costs for each action and
influence the system using impulse controls. We prove SGs of two-sided impulse
control have a unique value and characterise the saddle point equilibrium in
which the players execute actions at strategically chosen times in accordance
with Markovian strategies. We prove the game respects a dynamic programming
principle and that the Markov perfect equilibrium can be computed as a limit
point of a sequence of Bellman operations. We then introduce a new Q-learning
variant which we show converges almost surely to the value of the game enabling
solutions to be extracted in unknown settings. Lastly, we extend our results to
settings with budgetory constraints.",cs.MA,2024-07-25
Estimating Earthquake Magnitude in Sentinel-1 Imagery via Ranking,,"Earthquakes are commonly estimated using physical seismic stations, however,
due to the installation requirements and costs of these stations, global
coverage quickly becomes impractical. An efficient and lower-cost alternative
is to develop machine learning models to globally monitor earth observation
data to pinpoint regions impacted by these natural disasters. However, due to
the small amount of historically recorded earthquakes, this becomes a low-data
regime problem requiring algorithmic improvements to achieve peak performance
when learning to regress earthquake magnitude. In this paper, we propose to
pose the estimation of earthquake magnitudes as a metric-learning problem,
training models to not only estimate earthquake magnitude from Sentinel-1
satellite imagery but to additionally rank pairwise samples. Our experiments
show at max a 30%+ improvement in MAE over prior regression-only based methods,
particularly transformer-based architectures.",cs.CV eess.IV,2024-07-25
"A Role-specific Guided Large Language Model for Ophthalmic Consultation
  Based on Stylistic Differentiation",,"Ophthalmology consultations are crucial for diagnosing, treating, and
preventing eye diseases. However, the growing demand for consultations exceeds
the availability of ophthalmologists. By leveraging large pre-trained language
models, we can design effective dialogues for specific scenarios, aiding in
consultations. Traditional fine-tuning strategies for question-answering tasks
are impractical due to increasing model size and often ignoring patient-doctor
role function during consultations. In this paper, we propose EyeDoctor, an
ophthalmic medical questioning large language model that enhances accuracy
through doctor-patient role perception guided and an augmented knowledge base
with external disease information. Experimental results show EyeDoctor achieves
higher question-answering precision in ophthalmology consultations. Notably,
EyeDoctor demonstrated a 7.25% improvement in Rouge-1 scores and a 10.16%
improvement in F1 scores on multi-round datasets compared to second best model
ChatGPT, highlighting the importance of doctor-patient role differentiation and
dynamic knowledge base expansion for intelligent medical consultations. EyeDoc
also serves as a free available web based service and souce code is available
at https://github.com/sperfu/EyeDoc.",cs.CL cs.AI,2024-07-25
"Non-Overlapping Placement of Macro Cells based on Reinforcement Learning
  in Chip Design",,"Due to the increasing complexity of chip design, existing placement methods
still have many shortcomings in dealing with macro cells coverage and
optimization efficiency. Aiming at the problems of layout overlap, inferior
performance, and low optimization efficiency in existing chip design methods,
this paper proposes an end-to-end placement method, SRLPlacer, based on
reinforcement learning. First, the placement problem is transformed into a
Markov decision process by establishing the coupling relationship graph model
between macro cells to learn the strategy for optimizing layouts. Secondly, the
whole placement process is optimized after integrating the standard cell
layout. By assessing on the public benchmark ISPD2005, the proposed SRLPlacer
can effectively solve the overlap problem between macro cells while considering
routing congestion and shortening the total wire length to ensure routability.",cs.AR cs.AI,2024-07-26
REAPER: Reasoning based Retrieval Planning for Complex RAG Systems,,"Complex dialog systems often use retrieved evidence to facilitate factual
responses. Such RAG (Retrieval Augmented Generation) systems retrieve from
massive heterogeneous data stores that are usually architected as multiple
indexes or APIs instead of a single monolithic source. For a given query,
relevant evidence needs to be retrieved from one or a small subset of possible
retrieval sources. Complex queries can even require multi-step retrieval. For
example, a conversational agent on a retail site answering customer questions
about past orders will need to retrieve the appropriate customer order first
and then the evidence relevant to the customer's question in the context of the
ordered product. Most RAG Agents handle such Chain-of-Thought (CoT) tasks by
interleaving reasoning and retrieval steps. However, each reasoning step
directly adds to the latency of the system. For large models this latency cost
is significant -- in the order of multiple seconds. Multi-agent systems may
classify the query to a single Agent associated with a retrieval source, though
this means that a (small) classification model dictates the performance of a
large language model. In this work we present REAPER (REAsoning-based PlannER)
- an LLM based planner to generate retrieval plans in conversational systems.
We show significant gains in latency over Agent-based systems and are able to
scale easily to new and unseen use cases as compared to classification-based
planning. Though our method can be applied to any RAG system, we show our
results in the context of a conversational shopping assistant.",cs.IR,2024-07-26
Scalable Group Choreography via Variational Phase Manifold Learning,,"Generating group dance motion from the music is a challenging task with
several industrial applications. Although several methods have been proposed to
tackle this problem, most of them prioritize optimizing the fidelity in dancing
movement, constrained by predetermined dancer counts in datasets. This
limitation impedes adaptability to real-world applications. Our study addresses
the scalability problem in group choreography while preserving naturalness and
synchronization. In particular, we propose a phase-based variational generative
model for group dance generation on learning a generative manifold. Our method
achieves high-fidelity group dance motion and enables the generation with an
unlimited number of dancers while consuming only a minimal and constant amount
of memory. The intensive experiments on two public datasets show that our
proposed method outperforms recent state-of-the-art approaches by a large
margin and is scalable to a great number of dancers beyond the training data.",cs.CV,2024-07-26
CogNarr Ecosystem: Facilitating Group Cognition at Scale,,"Human groups of all sizes and kinds engage in deliberation, problem solving,
strategizing, decision making, and more generally, cognition. Some groups are
large, and that setting presents unique challenges. The small-group setting
often involves face-to-face dialogue, but group cognition in the large-group
setting typically requires some form of online interaction. New approaches are
needed to facilitate the kind of rich communication and information processing
that are required for effective, functional cognition in the online setting,
especially for groups characterized by thousands to millions of participants
who wish to share potentially complex, nuanced, and dynamic perspectives. This
concept paper proposes the CogNarr (Cognitive Narrative) ecosystem, which is
designed to facilitate functional cognition in the large-group setting. The
paper's contribution is a novel vision as to how recent developments in
cognitive science, artificial intelligence, natural language processing, and
related fields might be scaled and applied to large-group cognition, using an
approach that itself promotes further scientific advancement. A key perspective
is to view a group as an organism that uses some form of cognitive architecture
to sense the world, process information, remember, learn, predict, make
decisions, and adapt to changing conditions. The CogNarr ecosystem is designed
to serve as a component within that architecture.",cs.HC cs.AI,2024-07-11
"TERIME: An improved RIME algorithm with enhanced exploration and
  exploitation for robust parameter extraction of photovoltaic models",,"Parameter extraction of photovoltaic (PV) models is crucial for the planning,
optimization, and control of PV systems. Although some methods using
meta-heuristic algorithms have been proposed to determine these parameters, the
robustness of solutions obtained by these methods faces great challenges when
the complexity of the PV model increases. The unstable results will affect the
reliable operation and maintenance strategies of PV systems. In response to
this challenge, an improved RIME algorithm with enhanced exploration and
exploitation is proposed for robust and accurate parameter identification for
various PV models. Specifically, the differential evolution mutation operator
is integrated in the exploration phase to enhance the population diversity.
Meanwhile, a new exploitation strategy incorporating randomization and
neighborhood strategies simultaneously is developed to maintain the balance of
exploitation width and depth. The improved RIME algorithm is applied to
estimate the optimal parameters of the single-diode model (SDM), double-diode
model (DDM), and triple-diode model (TDM) combined with the Lambert-W function
for three PV cell and module types including RTC France, Photo Watt-PWP 201 and
S75. According to the statistical analysis in 100 runs, the TEIMRE achieves
more accurate and robust parameter estimations than other techniques to various
PV models in varying environmental conditions. All of our source codes are
publicly available at https://github.com/dirge1/TERIME.",eess.SY cs.SY,2024-07-24
"On a generalization of Watson's trigonometric sum, some of its
  properties and its relationship to Dowker's sum",,"In this paper we study two finite trigonometric sums $\sum a_l\csc\big(\pi
l/n\big)\,,$ where $a_l$ are equal either to $\cos(2\pi l \nu/n)$ or to
$(-1)^{l+1}$, and where the summation index $l$ and the discrete parameter
$\nu$ both run through $1$ to $n-1$ (marginally, cases $a_l=\ln\csc\big(\pi
l/n\big)$ and $a_l=\Psi\big(l/n\big)$ also appear in the paper). These sums
occur in various problems in mathematics, physics and engineering, and play an
important part in some number-theoretic problems. Formally, the first of these
sums is also the so-called Dowker sum of order one half. In the paper, we
obtain several integral and series representations for the above-mentioned
sums, investigate their properties, derive their complete asymptotical
expansions and deduce very accurate upper and lower bounds for them (both
bounds are asymptotically vanishing). In addition, we obtain a useful
approximate formula containing only three terms, which is also very accurate
and can be particularly appreciated in applications. Both trigonometric sums
appear to be closely related with the digamma function and with the square of
the Bernoulli numbers. Finally, we also derive several advanced summation
formulae for the gamma and the digamma functions, in which the first on these
sums, as well as the product of a sequence of cosecants $\,\prod\big(\csc(\pi
l/n)\big)^{\csc(\pi l/n)}$, play an important role.",math.NT cs.NA math.NA,2024-07-27
Mamba-UIE: Enhancing Underwater Images with Physical Model Constraint,,"In underwater image enhancement (UIE), convolutional neural networks (CNN)
have inherent limitations in modeling long-range dependencies and are less
effective in recovering global features. While Transformers excel at modeling
long-range dependencies, their quadratic computational complexity with
increasing image resolution presents significant efficiency challenges.
Additionally, most supervised learning methods lack effective physical model
constraint, which can lead to insufficient realism and overfitting in generated
images. To address these issues, we propose a physical model constraint-based
underwater image enhancement framework, Mamba-UIE. Specifically, we decompose
the input image into four components: underwater scene radiance, direct
transmission map, backscatter transmission map, and global background light.
These components are reassembled according to the revised underwater image
formation model, and the reconstruction consistency constraint is applied
between the reconstructed image and the original image, thereby achieving
effective physical constraint on the underwater image enhancement process. To
tackle the quadratic computational complexity of Transformers when handling
long sequences, we introduce the Mamba-UIE network based on linear complexity
state space models. By incorporating the Mamba in Convolution block, long-range
dependencies are modeled at both the channel and spatial levels, while the CNN
backbone is retained to recover local features and details. Extensive
experiments on three public datasets demonstrate that our proposed Mamba-UIE
outperforms existing state-of-the-art methods, achieving a PSNR of 27.13 and an
SSIM of 0.93 on the UIEB dataset. Our method is available at
https://github.com/zhangsong1213/Mamba-UIE.",cs.AI,2024-07-27
"Application of the Lov\'asz-Schrijver Lift-and-Project Operator to
  Compact Stable Set Integer Programs",,"The Lov\'asz theta function $\theta(G)$ provides a very good upper bound on
the stability number of a graph $G$. It can be computed in polynomial time by
solving a semidefinite program (SDP), which also turns out to be fairly
tractable in practice. Consequently, $\theta(G)$ achieves a hard-to-beat
trade-off between computational effort and strength of the bound. Indeed,
several attempts to improve the theta bound are documented, mainly based on
playing around the application of the $N_+(\cdot)$ lifting operator of Lov\'asz
and Schrijver to the classical formulation of the maximum stable set problem.
Experience shows that solving such SDP-s often struggles against practical
intractability and requires highly specialized methods. We investigate the
application of such an operator to two different linear formulations based on
clique and nodal inequalities, respectively. Fewer inequalities describe these
two and yet guarantee that the resulting SDP bound is at least as strong as
$\theta(G)$. Our computational experience, including larger graphs than those
previously documented, shows that upper bounds stronger than $\theta(G)$ can be
accessed by a reasonable additional effort using the clique-based formulation
on sparse graphs and the nodal-based one on dense graphs.",math.OC cs.IT math.IT,2024-07-27
A new approximation method for solving stochastic differential equations,,"We present a novel solution method for It\^o stochastic differential
equations (SDEs). We subdivide the time interval into sub-intervals, then we
use the quadratic polynomials for the approximation between two successive
intervals. The main properties of the stochastic numerical methods, e.g.
convergence, consistency, and stability are analyzed. We test the proposed
method in SDE problem, demonstrating promising results.",math.NA cs.NA,2024-07-27
"BEMTrace: Visualization-driven approach for deriving Building Energy
  Models from BIM",,"Building Information Modeling (BIM) describes a central data pool covering
the entire life cycle of a construction project. Similarly, Building Energy
Modeling (BEM) describes the process of using a 3D representation of a building
as a basis for thermal simulations to assess the building's energy performance.
This paper explores the intersection of BIM and BEM, focusing on the challenges
and methodologies in converting BIM data into BEM representations for energy
performance analysis. BEMTrace integrates 3D data wrangling techniques with
visualization methodologies to enhance the accuracy and traceability of the
BIM-to-BEM conversion process. Through parsing, error detection, and
algorithmic correction of BIM data, our methods generate valid BEM models
suitable for energy simulation. Visualization techniques provide transparent
insights into the conversion process, aiding error identification, validation,
and user comprehension. We introduce context-adaptive selections to facilitate
user interaction and to show that the BEMTrace workflow helps users understand
complex 3D data wrangling processes.",cs.HC,2024-07-28
"Prometheus Chatbot: Knowledge Graph Collaborative Large Language Model
  for Computer Components Recommendation",,"Knowledge graphs (KGs) are essential in applications such as network
alignment, question-answering, and recommender systems (RSs) since they offer
structured relational data that facilitate the inference of indirect
relationships. However, the development of KG-based RSs capable of processing
user inputs in natural language faces significant challenges. Firstly, natural
language processing units must effectively handle the ambiguity and variability
in human language to interpret user intents accurately. Secondly, the system
must precisely identify and link entities, like product names, to their
corresponding nodes in KGs. To overcome these challenges, supported by Lenovo,
we developed a novel chatbot called ""Prometheus,"" which integrates a KG with a
large language model (LLM), specifically designed for recommending computer
components. This chatbot can accurately decode user requests and deliver
personalized recommendations derived from KGs, ensuring precise comprehension
and response to their computer setup needs.",cs.AI,2024-07-28
"OpenUAS: Embeddings of Cities in Japan with Anchor Data for Cross-city
  Analysis of Area Usage Patterns",,"We publicly release OpenUAS, a dataset of area embeddings based on urban
usage patterns, including embeddings for over 1.3 million 50-meter square
meshes covering a total area of 3,300 square kilometers. This dataset is
valuable for analyzing area functions in fields such as market analysis, urban
planning, transportation infrastructure, and infection prediction. It captures
the characteristics of each area in the city, such as office districts and
residential areas, by employing an area embedding technique that utilizes
location information typically obtained by GPS. Numerous area embedding
techniques have been proposed, and while the public release of such embedding
datasets is technically feasible, it has not been realized. One of the
obstacles has been the integration of data from different cities and periods
into a unified space without sharing raw location data. We address this issue
by developing an anchoring method that establishes anchors within a shared
embedding space. We publicly release this anchor dataset along with area
embedding datasets from several periods in eight major Japanese cities. This
dataset allows users to analyze urban usage patterns in Japanese cities and
embed their urban dataset into the same embedding space using the anchoring
method. Our key contributions include the development of the anchoring method,
releasing area embedding datasets for Japanese cities, and providing tools for
effective data utilization.",cs.LG,2024-07-29
"MimiQ: Low-Bit Data-Free Quantization of Vision Transformers with
  Encouraging Inter-Head Attention Similarity",,"Data-free quantization (DFQ) is a technique that creates a lightweight
network from its full-precision counterpart without the original training data,
often through a synthetic dataset. Although several DFQ methods have been
proposed for vision transformer (ViT) architectures, they fail to achieve
efficacy in low-bit settings. Examining the existing methods, we identify that
their synthetic data produce misaligned attention maps, while those of the real
samples are highly aligned. From the observation of aligned attention, we find
that aligning attention maps of synthetic data helps to improve the overall
performance of quantized ViTs. Motivated by this finding, we devise MimiQ, a
novel DFQ method designed for ViTs that focuses on inter-head attention
similarity. First, we generate synthetic data by aligning head-wise attention
responses in relation to spatial query patches. Then, we apply head-wise
structural attention distillation to align the attention maps of the quantized
network to those of the full-precision teacher. The experimental results show
that the proposed method significantly outperforms baselines, setting a new
state-of-the-art performance for data-free ViT quantization.",cs.LG cs.AI cs.CV,2024-07-29
"Accelerated Primal-Dual Proximal Gradient Splitting Methods for
  Convex-Concave Saddle-Point Problems",,"In this paper, based a novel primal-dual dynamical model with adaptive
scaling parameters and Bregman divergences, we propose new accelerated
primal-dual proximal gradient splitting methods for solving bilinear
saddle-point problems with provable optimal nonergodic convergence rates. For
the first, using the spectral analysis, we show that a naive extension of
acceleration model for unconstrained optimization problems to a quadratic game
is unstable. Motivated by this, we present an accelerated primal-dual hybrid
gradient (APDHG) flow which combines acceleration with careful velocity
correction. To work with non-Euclidean distances, we also equip our APDHG model
with general Bregman divergences and prove the exponential decay of a Lyapunov
function. Then, new primal-dual splitting methods are developed based on proper
semi-implicit Euler schemes of the continuous model, and the theoretical
convergence rates are nonergodic and optimal with respect to the matrix
norms,\, Lipschitz constants and convexity parameters. Thanks to the primal and
dual scaling parameters, both the algorithm designing and convergence analysis
cover automatically the convex and (partially) strongly convex objectives.
Moreover, the use of Bregman divergences not only unifies the standard
Euclidean distances and general cases in an elegant way, but also makes our
methods more flexible and adaptive to problem-dependent metrics.",math.OC cs.NA math.NA,2024-07-29
"SpaER: Learning Spatio-temporal Equivariant Representations for Fetal
  Brain Motion Tracking",,"In this paper, we introduce SpaER, a pioneering method for fetal motion
tracking that leverages equivariant filters and self-attention mechanisms to
effectively learn spatio-temporal representations. Different from conventional
approaches that statically estimate fetal brain motions from pairs of images,
our method dynamically tracks the rigid movement patterns of the fetal head
across temporal and spatial dimensions. Specifically, we first develop an
equivariant neural network that efficiently learns rigid motion sequences
through low-dimensional spatial representations of images. Subsequently, we
learn spatio-temporal representations by incorporating time encoding and
self-attention neural network layers. This approach allows for the capture of
long-term dependencies of fetal brain motion and addresses alignment errors due
to contrast changes and severe motion artifacts. Our model also provides a
geometric deformation estimation that properly addresses image distortions
among all time frames. To the best of our knowledge, our approach is the first
to learn spatial-temporal representations via deep neural networks for fetal
motion tracking without data augmentation. We validated our model using real
fetal echo-planar images with simulated and real motions. Our method carries
significant potential value in accurately measuring, tracking, and correcting
fetal motion in fetal MRI sequences.",eess.IV cs.CV,2024-07-29
Can Editing LLMs Inject Harm?,,"Knowledge editing techniques have been increasingly adopted to efficiently
correct the false or outdated knowledge in Large Language Models (LLMs), due to
the high cost of retraining from scratch. Meanwhile, one critical but
under-explored question is: can knowledge editing be used to inject harm into
LLMs? In this paper, we propose to reformulate knowledge editing as a new type
of safety threat for LLMs, namely Editing Attack, and conduct a systematic
investigation with a newly constructed dataset EditAttack. Specifically, we
focus on two typical safety risks of Editing Attack including Misinformation
Injection and Bias Injection. For the risk of misinformation injection, we
first categorize it into commonsense misinformation injection and long-tail
misinformation injection. Then, we find that editing attacks can inject both
types of misinformation into LLMs, and the effectiveness is particularly high
for commonsense misinformation injection. For the risk of bias injection, we
discover that not only can biased sentences be injected into LLMs with high
effectiveness, but also one single biased sentence injection can cause a bias
increase in general outputs of LLMs, which are even highly irrelevant to the
injected sentence, indicating a catastrophic impact on the overall fairness of
LLMs. Then, we further illustrate the high stealthiness of editing attacks,
measured by their impact on the general knowledge and reasoning capacities of
LLMs, and show the hardness of defending editing attacks with empirical
evidence. Our discoveries demonstrate the emerging misuse risks of knowledge
editing techniques on compromising the safety alignment of LLMs.",cs.CL,2024-07-29
LAPIS: Language Model-Augmented Police Investigation System,,"Crime situations are race against time. An AI-assisted criminal investigation
system, providing prompt but precise legal counsel is in need for police
officers. We introduce LAPIS (Language Model Augmented Police Investigation
System), an automated system that assists police officers to perform rational
and legal investigative actions. We constructed a finetuning dataset and
retrieval knowledgebase specialized in crime investigation legal reasoning
task. We extended the dataset's quality by incorporating manual curation
efforts done by a group of domain experts. We then finetuned the pretrained
weights of a smaller Korean language model to the newly constructed dataset and
integrated it with the crime investigation knowledgebase retrieval approach.
Experimental results show LAPIS' potential in providing reliable legal guidance
for police officers, even better than the proprietary GPT-4 model. Qualitative
analysis on the rationales generated by LAPIS demonstrate the model's reasoning
ability to leverage the premises and derive legally correct conclusions.",cs.CL cs.AI cs.CY,2024-07-19
Dataset Distillation for Offline Reinforcement Learning,,"Offline reinforcement learning often requires a quality dataset that we can
train a policy on. However, in many situations, it is not possible to get such
a dataset, nor is it easy to train a policy to perform well in the actual
environment given the offline data. We propose using data distillation to train
and distill a better dataset which can then be used for training a better
policy model. We show that our method is able to synthesize a dataset where a
model trained on it achieves similar performance to a model trained on the full
dataset or a model trained using percentile behavioral cloning. Our project
site is available at
$\href{https://datasetdistillation4rl.github.io}{\text{here}}$. We also provide
our implementation at $\href{https://github.com/ggflow123/DDRL}{\text{this
GitHub repository}}$.",cs.LG cs.AI,2024-07-29
"Weakly Supervised Intracranial Hemorrhage Segmentation with YOLO and an
  Uncertainty Rectified Segment Anything Model",,"Intracranial hemorrhage (ICH) is a life-threatening condition that requires
rapid and accurate diagnosis to improve treatment outcomes and patient survival
rates. Recent advancements in supervised deep learning have greatly improved
the analysis of medical images, but often rely on extensive datasets with
high-quality annotations, which are costly, time-consuming, and require medical
expertise to prepare. To mitigate the need for large amounts of expert-prepared
segmentation data, we have developed a novel weakly supervised ICH segmentation
method that utilizes the YOLO object detection model and an
uncertainty-rectified Segment Anything Model (SAM). In addition, we have
proposed a novel point prompt generator for this model to further improve
segmentation results with YOLO-predicted bounding box prompts. Our approach
achieved a high accuracy of 0.933 and an AUC of 0.796 in ICH detection, along
with a mean Dice score of 0.629 for ICH segmentation, outperforming existing
weakly supervised and popular supervised (UNet and Swin-UNETR) approaches.
Overall, the proposed method provides a robust and accurate alternative to the
more commonly used supervised techniques for ICH quantification without
requiring refined segmentation ground truths during model training.",cs.CV,2024-07-29
"A2SF: Accumulative Attention Scoring with Forgetting Factor for Token
  Pruning in Transformer Decoder",,"Recently, large language models (LLM) based on transformers are facing memory
bottleneck issues due to KV cache, especially in long sequence handling.
Previous researches proposed KV cache compression techniques that identify
insignificant tokens based on Accumulative Attention Scores and removes their
items from KV cache, noting that only few tokens play an important role in
attention operations. However, we have observed that the existing Accumulative
Attention Score is not suitable for the transformer decoder structure. In the
decoder model, the number of times the Attention Score accumulates varies
depending on the order of token appearance due to the effect of masking,
causing an uneven comparison between tokens. To solve this, we propose
Accumulative Attention Score with Forgetting Factor (A2SF) technique, which
introduces a Forgetting Factor in the Attention Score accumulation process.
A2SF applies a penalty to the past Attention Score generated from old tokens by
repeatedly multiplying the Forgetting Factor to the Attention Score over time.
Therefore, older tokens receive a larger penalty, providing fairness among
different ages of tokens. Through the fair comparison among tokens, we can more
effectively select important tokens. We have verified the accuracy improvement
through A2SF in the OPT and LLaMA models and A2SF improves the accuracy of
LLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot.",cs.CL cs.LG,2024-07-29
Contrastive Feedback Mechanism for Simultaneous Speech Translation,,"Recent advances in simultaneous speech translation (SST) focus on the
decision policies that enable the use of offline-trained ST models for
simultaneous inference. These decision policies not only control the
quality-latency trade-off in SST but also mitigate the impact of unstable
predictions on translation quality by delaying translation for more context or
discarding these predictions through stable hypothesis detection. However,
these policies often overlook the potential benefits of utilizing unstable
predictions. We introduce the contrastive feedback mechanism (CFM) for SST, a
novel method that leverages these unstable predictions as feedback to improve
translation quality. CFM guides the system to eliminate undesired model
behaviors from these predictions through a contrastive objective. The
experiments on 3 state-of-the-art decision policies across 8 languages in the
MuST-C v1.0 dataset show that CFM effectively improves the performance of SST.",cs.CL,2024-07-29
"Scalable Synthesis of Formally Verified Neural Value Function for
  Hamilton-Jacobi Reachability Analysis",,"Hamilton-Jacobi (HJ) reachability analysis provides a formal method for
guaranteeing safety in constrained control problems. It synthesizes a value
function to represent a long-term safe set called feasible region. Early
synthesis methods based on state space discretization cannot scale to
high-dimensional problems, while recent methods that use neural networks to
approximate value functions result in unverifiable feasible regions. To achieve
both scalability and verifiability, we propose a framework for synthesizing
verified neural value functions for HJ reachability analysis. Our framework
consists of three stages: pre-training, adversarial training, and
verification-guided training. We design three techniques to address three
challenges to improve scalability respectively: boundary-guided backtracking
(BGB) to improve counterexample search efficiency, entering state
regularization (ESR) to enlarge feasible region, and activation pattern
alignment (APA) to accelerate neural network verification. We also provide a
neural safety certificate synthesis and verification benchmark called
Cersyve-9, which includes nine commonly used safe control tasks and supplements
existing neural network verification benchmarks. Our framework successfully
synthesizes verified neural value functions on all tasks, and our proposed
three techniques exhibit superior scalability and efficiency compared with
existing methods.",eess.SY cs.SY,2024-07-30
Monocular Human-Object Reconstruction in the Wild,,"Learning the prior knowledge of the 3D human-object spatial relation is
crucial for reconstructing human-object interaction from images and
understanding how humans interact with objects in 3D space. Previous works
learn this prior from datasets collected in controlled environments, but due to
the diversity of domains, they struggle to generalize to real-world scenarios.
To overcome this limitation, we present a 2D-supervised method that learns the
3D human-object spatial relation prior purely from 2D images in the wild. Our
method utilizes a flow-based neural network to learn the prior distribution of
the 2D human-object keypoint layout and viewports for each image in the
dataset. The effectiveness of the prior learned from 2D images is demonstrated
on the human-object reconstruction task by applying the prior to tune the
relative pose between the human and the object during the post-optimization
stage. To validate and benchmark our method on in-the-wild images, we collect
the WildHOI dataset from the YouTube website, which consists of various
interactions with 8 objects in real-world scenarios. We conduct the experiments
on the indoor BEHAVE dataset and the outdoor WildHOI dataset. The results show
that our method achieves almost comparable performance with fully 3D supervised
methods on the BEHAVE dataset, even if we have only utilized the 2D layout
information, and outperforms previous methods in terms of generality and
interaction diversity on in-the-wild images.",cs.CV cs.GR,2024-07-30
"Towards Generalizable Reinforcement Learning via Causality-Guided
  Self-Adaptive Representations",,"General intelligence requires quick adaption across tasks. While existing
reinforcement learning (RL) methods have made progress in generalization, they
typically assume only distribution changes between source and target domains.
In this paper, we explore a wider range of scenarios where both the
distribution and environment spaces may change. For example, in Atari games, we
train agents to generalize to tasks with different levels of mode and
difficulty, where there could be new state or action variables that never
occurred in previous environments. To address this challenging setting, we
introduce a causality-guided self-adaptive representation-based approach,
called CSR, that equips the agent to generalize effectively and efficiently
across a sequence of tasks with evolving dynamics. Specifically, we employ
causal representation learning to characterize the latent causal variables and
world models within the RL system. Such compact causal representations uncover
the structural relationships among variables, enabling the agent to
autonomously determine whether changes in the environment stem from
distribution shifts or variations in space, and to precisely locate these
changes. We then devise a three-step strategy to fine-tune the model under
different scenarios accordingly. Empirical experiments show that CSR
efficiently adapts to the target domains with only a few samples and
outperforms state-of-the-art baselines on a wide range of scenarios, including
our simulated environments, Cartpole, and Atari games.",cs.LG,2024-07-30
"Efficient Multi-Objective Neural Architecture Search via Pareto
  Dominance-based Novelty Search",,"Neural Architecture Search (NAS) aims to automate the discovery of
high-performing deep neural network architectures. Traditional objective-based
NAS approaches typically optimize a certain performance metric (e.g.,
prediction accuracy), overlooking large parts of the architecture search space
that potentially contain interesting network configurations. Furthermore,
objective-driven population-based metaheuristics in complex search spaces often
quickly exhaust population diversity and succumb to premature convergence to
local optima. This issue becomes more complicated in NAS when performance
objectives do not fully align with the actual performance of the candidate
architectures, as is often the case with training-free metrics. While
training-free metrics have gained popularity for their rapid performance
estimation of candidate architectures without incurring computation-heavy
network training, their effective incorporation into NAS remains a challenge.
This paper presents the Pareto Dominance-based Novelty Search for
multi-objective NAS with Multiple Training-Free metrics (MTF-PDNS). Unlike
conventional NAS methods that optimize explicit objectives, MTF-PDNS promotes
population diversity by utilizing a novelty score calculated based on multiple
training-free performance and complexity metrics, thereby yielding a broader
exploration of the search space. Experimental results on standard NAS benchmark
suites demonstrate that MTF-PDNS outperforms conventional methods driven by
explicit objectives in terms of convergence speed, diversity maintenance,
architecture transferability, and computational costs.",cs.NE cs.LG,2024-07-30
What makes for good morphology representations for spatial omics?,,"Spatial omics has transformed our understanding of tissue architecture by
preserving spatial context of gene expression patterns. Simultaneously,
advances in imaging AI have enabled extraction of morphological features
describing the tissue. The intersection of spatial omics and imaging AI
presents opportunities for a more holistic understanding. In this review we
introduce a framework for categorizing spatial omics-morphology combination
methods, focusing on how morphological features can be translated or integrated
into spatial omics analyses. By translation we mean finding morphological
features that spatially correlate with gene expression patterns with the
purpose of predicting gene expression. Such features can be used to generate
super-resolution gene expression maps or infer genetic information from
clinical H&E-stained samples. By integration we mean finding morphological
features that spatially complement gene expression patterns with the purpose of
enriching information. Such features can be used to define spatial domains,
especially where gene expression has preceded morphological changes and where
morphology remains after gene expression. We discuss learning strategies and
directions for further development of the field.",cs.CV,2024-07-30
3D-GRES: Generalized 3D Referring Expression Segmentation,,"3D Referring Expression Segmentation (3D-RES) is dedicated to segmenting a
specific instance within a 3D space based on a natural language description.
However, current approaches are limited to segmenting a single target,
restricting the versatility of the task. To overcome this limitation, we
introduce Generalized 3D Referring Expression Segmentation (3D-GRES), which
extends the capability to segment any number of instances based on natural
language instructions. In addressing this broader task, we propose the
Multi-Query Decoupled Interaction Network (MDIN), designed to break down
multi-object segmentation tasks into simpler, individual segmentations. MDIN
comprises two fundamental components: Text-driven Sparse Queries (TSQ) and
Multi-object Decoupling Optimization (MDO). TSQ generates sparse point cloud
features distributed over key targets as the initialization for queries.
Meanwhile, MDO is tasked with assigning each target in multi-object scenarios
to different queries while maintaining their semantic consistency. To adapt to
this new task, we build a new dataset, namely Multi3DRes. Our comprehensive
evaluations on this dataset demonstrate substantial enhancements over existing
models, thus charting a new path for intricate multi-object 3D scene
comprehension. The benchmark and code are available at
https://github.com/sosppxo/MDIN.",cs.CV,2024-07-30
Steps Towards an Infrastructure for Scholarly Synthesis,,"Sharing, reusing, and synthesizing knowledge is central to the research
process, both individually, and with others. These core functions are not
supported by our formal scholarly publishing infrastructure: instead of the
smooth functioning of functional infrastructure, researchers resort to
laborious ""hacks"" and workarounds to ""mine"" publications for what they need,
and struggle to efficiently share the resulting information with others.
Information scientists have proposed an alternative infrastructure based on the
more appropriately granular model of a discourse graph of claims, and evidence,
along with key rhetorical relationships between them. However, despite
significant technical progress on standards and platforms, the predominant
infrastructure remains steadfastly document-based. Drawing from infrastructure
studies, we locate the current infrastructural bottlenecks in the lack of local
systems that integrate discourse-centric models to augment synthesis work, from
which an infrastructure for synthesis can be grown. Through 3 years of research
through design and field deployment in a distributed community of hypertext
notebook users, we elaborate a design vision of what can and should be built in
order to grow a discourse-centric synthesis infrastructure: a thriving
""installed base"" of researchers authoring local, shareable discourse graphs to
improve synthesis work, enhance primary research and research training, and
augment collaborative research. We discuss how this design vision -- and our
empirical work -- contributes steps towards a new infrastructure for synthesis,
and increases HCI's capacity to advance collective intelligence and solve
infrastructure-level problems.",cs.HC,2024-07-30
"The Susceptibility of Example-Based Explainability Methods to Class
  Outliers",,"This study explores the impact of class outliers on the effectiveness of
example-based explainability methods for black-box machine learning models. We
reformulate existing explainability evaluation metrics, such as correctness and
relevance, specifically for example-based methods, and introduce a new metric,
distinguishability. Using these metrics, we highlight the shortcomings of
current example-based explainability methods, including those who attempt to
suppress class outliers. We conduct experiments on two datasets, a text
classification dataset and an image classification dataset, and evaluate the
performance of four state-of-the-art explainability methods. Our findings
underscore the need for robust techniques to tackle the challenges posed by
class outliers.",cs.LG,2024-07-30
"CultureVo: The Serious Game of Utilizing Gen AI for Enhancing Cultural
  Intelligence",,"CultureVo, Inc. has developed the Integrated Culture Learning Suite (ICLS) to
deliver foundational knowledge of world cultures through a combination of
interactive lessons and gamified experiences. This paper explores how
Generative AI powered by open source Large Langauge Models are utilized within
the ICLS to enhance cultural intelligence. The suite employs Generative AI
techniques to automate the assessment of learner knowledge, analyze behavioral
patterns, and manage interactions with non-player characters using real time
learner assessment. Additionally, ICLS provides contextual hint and recommend
course content by assessing learner proficiency, while Generative AI
facilitates the automated creation and validation of educational content.",cs.ET cs.CL,2024-07-30
Cost-Based Semantics for Querying Inconsistent Weighted Knowledge Bases,,"In this paper, we explore a quantitative approach to querying inconsistent
description logic knowledge bases. We consider weighted knowledge bases in
which both axioms and assertions have (possibly infinite) weights, which are
used to assign a cost to each interpretation based upon the axioms and
assertions it violates. Two notions of certain and possible answer are defined
by either considering interpretations whose cost does not exceed a given bound
or restricting attention to optimal-cost interpretations. Our main contribution
is a comprehensive analysis of the combined and data complexity of bounded cost
satisfiability and certain and possible answer recognition, for description
logics between ELbot and ALCO.",cs.LO cs.AI cs.DB,2024-07-30
"DFE-IANet: A Method for Polyp Image Classification Based on Dual-domain
  Feature Extraction and Interaction Attention",,"It is helpful in preventing colorectal cancer to detect and treat polyps in
the gastrointestinal tract early. However, there have been few studies to date
on designing polyp image classification networks that balance efficiency and
accuracy. This challenge is mainly attributed to the fact that polyps are
similar to other pathologies and have complex features influenced by texture,
color, and morphology. In this paper, we propose a novel network DFE-IANet
based on both spectral transformation and feature interaction. Firstly, to
extract detailed features and multi-scale features, the features are
transformed by the multi-scale frequency domain feature extraction (MSFD) block
to extract texture details at the fine-grained level in the frequency domain.
Secondly, the multi-scale interaction attention (MSIA) block is designed to
enhance the network's capability of extracting critical features. This block
introduces multi-scale features into self-attention, aiming to adaptively guide
the network to concentrate on vital regions. Finally, with a compact parameter
of only 4M, DFE-IANet outperforms the latest and classical networks in terms of
efficiency. Furthermore, DFE-IANet achieves state-of-the-art (SOTA) results on
the challenging Kvasir dataset, demonstrating a remarkable Top-1 accuracy of
93.94%. This outstanding accuracy surpasses ViT by 8.94%, ResNet50 by 1.69%,
and VMamba by 1.88%. Our code is publicly available at
https://github.com/PURSUETHESUN/DFE-IANet.",cs.CV,2024-07-30
"Synthesis of Resource-Efficient Superconducting Circuits with Clock-Free
  Alternating Logic",,"Gate-level clocking, typical in traditional approaches to Single Flux Quantum
(SFQ) technology, makes the effective synthesis of superconducting circuits a
significant engineering hurdle. This paper addresses this challenge by
employing the recently introduced alternating SFQ (xSFQ) logic family. xSFQ
leverages dual-rail alternating encoding to eliminate the clock dependency from
the superconducting gate semantics. This obviates the need for ad hoc
modifications to existing synthesis tools and avoids unnecessary circuit
resource overheads, marking a significant advancement in superconducting
circuit design automation. Our implementation results demonstrate an average
reduction of over 80\% in the Josephson junction count for circuits from the
ISCAS85, EPFL, and ISCAS89 benchmark suites.",cs.AR,2024-07-30
"MoFO: Momentum-Filtered Optimizer for Mitigating Forgetting in LLM
  Fine-Tuning",,"Recently, large language models (LLMs) have demonstrated remarkable
capabilities in a wide range of tasks. Typically, an LLM is pre-trained on
large corpora and subsequently fine-tuned on task-specific datasets. However,
during fine-tuning, LLMs may forget the knowledge acquired in the pre-training
stage, leading to a decline in general capabilities. To address this issue, we
propose a new fine-tuning algorithm termed Momentum-Filtered Optimizer (MoFO).
The key idea of MoFO is to iteratively select and update the model parameters
with the largest momentum magnitudes. Compared to full-parameter training, MoFO
achieves similar fine-tuning performance while keeping parameters closer to the
pre-trained model, thereby mitigating knowledge forgetting. Unlike most
existing methods for forgetting mitigation, MoFO combines the following two
advantages. First, MoFO does not require access to pre-training data. This
makes MoFO particularly suitable for fine-tuning scenarios where pre-training
data is unavailable, such as fine-tuning checkpoint-only open-source LLMs.
Second, MoFO does not alter the original loss function. This could avoid
impairing the model performance on the fine-tuning tasks. We validate MoFO
through rigorous convergence analysis and extensive experiments, demonstrating
its superiority over existing methods in mitigating forgetting and enhancing
fine-tuning performance.",cs.LG cs.AI,2024-07-30
AI-Assisted Generation of Difficult Math Questions,,"Current LLM training positions mathematical reasoning as a core capability.
With publicly available sources fully tapped, there is unmet demand for diverse
and challenging math questions. Relying solely on human experts is both
time-consuming and costly, while LLM-generated questions often lack the
requisite diversity and difficulty. We present a design framework that combines
the strengths of LLMs with a human-in-the-loop approach to generate a diverse
array of challenging math questions. We leverage LLM metacognition skills
[Didolkar et al., 2024] of a strong LLM to extract core ""skills"" from existing
math datasets. These skills serve as the basis for generating novel and
difficult questions by prompting the LLM with random pairs of core skills. The
use of two different skills within each question makes finding such questions
an ""out of distribution"" task for both LLMs and humans. Our pipeline employs
LLMs to iteratively generate and refine questions and solutions through
multiturn prompting. Human annotators then verify and further refine the
questions, with their efficiency enhanced via further LLM interactions.
Applying this pipeline on skills extracted from the MATH dataset [Hendrycks et
al., 2021] resulted in MATH$^2$ - a dataset of higher-quality math questions,
as evidenced by: (a) Lower performance of all models on MATH$^2$ than on MATH
(b) Higher performance on MATH when using MATH$^2$ questions as in-context
examples. Although focused on mathematics, our methodology seems applicable to
other domains requiring structured reasoning, and potentially as a component of
scalable oversight. Also of interest is a striking relationship observed
between models' performance on the new dataset: the success rate on MATH$^2$ is
the square on MATH, suggesting that successfully solving the question in
MATH$^2$ requires a nontrivial combination of two distinct math skills.",cs.AI cs.LG,2024-07-30
Isogeometric-based Asymptotic Analysis on Multi-layered Thin Shells,,"Shell structures are generally modeled based on kinematic hypotheses, where
certain involved parameters have to be evaluated phenomenologically. In this
article, we propose an isogeometric-based asymptotic analysis scheme aimed at
achieving a rational modeling and interpretation about the deformation
behaviors of multi-layered thin shells (MTSs). Asymptotic analysis of the
three-dimensional governing elasticity equations allows us to gain a deeper
insight over shell theories in three aspects. Firstly, the intrinsic hierarchy
lying in the displacement field, stress components, constitutive law, and
momentum conservation, resulting from the shell thinness, is naturally
suggested through asymptotic analysis, and not only the overall stiffness but
also the key stress components can be formulated in a fully rational manner.
Secondly, through detailed discussion on the order of non-dimensional maximum
principal curvature, it is shown that a shell must deform in one of the
following two deformation modes. One regime exhibits a mode resembling that of
thin plates, while the other corresponds to a piecewise linear distribution of
local transverse shear stresses along the thickness. And the specific orders of
the key quantities are determined for both cases. Thirdly, in contrast with the
existing arguments where an applied shear load on the shell surface
necessitates the inclusion of transverse shear stresses for analysis, it is
demonstrated that a leading-order shell theory derived from asymptotic analysis
should suffice to output satisfactory predictions over the shell stiffness, as
well as its internal stress distribution. Numerical examples of the deformation
and strength analysis for the MTS are also presented to show the reliability of
the proposed method for shell modeling.",cond-mat.mtrl-sci cs.NA math.NA,2024-07-08
A Comprehensive Survey on Retrieval Methods in Recommender Systems,,"In an era dominated by information overload, effective recommender systems
are essential for managing the deluge of data across digital platforms.
Multi-stage cascade ranking systems are widely used in the industry, with
retrieval and ranking being two typical stages. Retrieval methods sift through
vast candidates to filter out irrelevant items, while ranking methods
prioritize these candidates to present the most relevant items to users. Unlike
studies focusing on the ranking stage, this survey explores the critical yet
often overlooked retrieval stage of recommender systems. To achieve precise and
efficient personalized retrieval, we summarize existing work in three key
areas: improving similarity computation between user and item, enhancing
indexing mechanisms for efficient retrieval, and optimizing training methods of
retrieval. We also provide a comprehensive set of benchmarking experiments on
three public datasets. Furthermore, we highlight current industrial
applications through a case study on retrieval practices at a specific company,
covering the entire retrieval process and online serving, along with practical
implications and challenges. By detailing the retrieval stage, which is
fundamental for effective recommendation, this survey aims to bridge the
existing knowledge gap and serve as a cornerstone for researchers interested in
optimizing this critical component of cascade recommender systems.",cs.IR,2024-07-11
E-Commerce Product Recommendation System based on ML Algorithms,,"Algorithms are used in eCommerce product recommendation systems. These
systems just recently began utilizing machine learning algorithms due to the
development and growth of the artificial intelligence research community. This
project aspires to transform how eCommerce platforms communicate with their
users. We have created a model that can customize product recommendations and
offers for each unique customer using cutting-edge machine learning techniques,
we used PCA to reduce features and four machine learning algorithms like
Gaussian Naive Bayes (GNB), Random Forest (RF), Logistic Regression (LR),
Decision Tree (DT), the Random Forest algorithms achieve the highest accuracy
of 99.6% with a 96.99 r square score, 1.92% MSE score, and 0.087 MAE score. The
outcome is advantageous for both the client and the business. In this research,
we will examine the model's development and training in detail and show how
well it performs using actual data. Learning from machines can change of
eCommerce world.",cs.IR cs.NI,2024-07-14
Success Probability in Multi-View Imaging,,"Platforms such as robots, security cameras, drones and satellites are used in
multi-view imaging for three-dimensional (3D) recovery by stereoscopy or
tomography. Each camera in the setup has a field of view (FOV). Multi-view
analysis requires overlap of the FOVs of all cameras, or a significant subset
of them. However, the success of such methods is not guaranteed, because the
FOVs may not sufficiently overlap. The reason is that pointing of a camera from
a mount or platform has some randomness (noise), due to imprecise platform
control, typical to mechanical systems, and particularly moving systems such as
satellites. So, success is probabilistic. This paper creates a framework to
analyze this aspect. This is critical for setting limitations on the
capabilities of imaging systems, such as resolution (pixel footprint), FOV, the
size of domains that can be captured, and efficiency. The framework uses the
fact that imprecise pointing can be mitigated by self-calibration - provided
that there is sufficient overlap between pairs of views and sufficient visual
similarity of views. We show an example considering the design of a formation
of nanosatellites that seek 3D reconstruction of clouds.",cs.CV,2024-07-15
"Antibody DomainBed: Out-of-Distribution Generalization in Therapeutic
  Protein Design",,"Machine learning (ML) has demonstrated significant promise in accelerating
drug design. Active ML-guided optimization of therapeutic molecules typically
relies on a surrogate model predicting the target property of interest. The
model predictions are used to determine which designs to evaluate in the lab,
and the model is updated on the new measurements to inform the next cycle of
decisions. A key challenge is that the experimental feedback from each cycle
inspires changes in the candidate proposal or experimental protocol for the
next cycle, which lead to distribution shifts. To promote robustness to these
shifts, we must account for them explicitly in the model training. We apply
domain generalization (DG) methods to classify the stability of interactions
between an antibody and antigen across five domains defined by design cycles.
Our results suggest that foundational models and ensembling improve predictive
performance on out-of-distribution domains. We publicly release our codebase
extending the DG benchmark ``DomainBed,'' and the associated dataset of
antibody sequences and structures emulating distribution shifts across design
cycles.",q-bio.BM cs.LG,2024-07-15
"Data-Driven Abstractions via Binary-Tree Gaussian Processes for Formal
  Verification",,"To advance formal verification of stochastic systems against temporal logic
requirements for handling unknown dynamics, researchers have been designing
data-driven approaches inspired by breakthroughs in the underlying machine
learning techniques. As one promising research direction, abstraction-based
solutions based on Gaussian process (GP) regression have become popular for
their ability to learn a representation of the latent system from data with a
quantified error. Results obtained based on this model are then translated to
the true system via various methods. In a recent publication, GPs using a
so-called binary-tree kernel have demonstrated a polynomial speedup w.r.t. the
size of the data compared to their vanilla version, outcompeting all existing
sparse GP approximations. Incidentally, the resulting binary-tree Gaussian
process (BTGP) is characteristic for its piecewise-constant posterior mean and
covariance functions, naturally abstracting the input space into discrete
partitions. In this paper, we leverage this natural abstraction of the BTGP for
formal verification, eliminating the need for cumbersome abstraction and error
quantification procedures. We show that the BTGP allows us to construct an
interval Markov chain model of the unknown system with a speedup that is
polynomial w.r.t. the size of the abstraction compared to alternative
approaches. We provide a delocalized error quantification via a unified formula
even when the true dynamics do not live in the function space of the BTGP. This
allows us to compute upper and lower bounds on the probability of satisfying
reachability specifications that are robust to both aleatoric and epistemic
uncertainties.",cs.LO cs.FL cs.LG cs.SY eess.SY,2024-07-15
"Cluster and Separate: a GNN Approach to Voice and Staff Prediction for
  Score Engraving",,"This paper approaches the problem of separating the notes from a quantized
symbolic music piece (e.g., a MIDI file) into multiple voices and staves. This
is a fundamental part of the larger task of music score engraving (or score
typesetting), which aims to produce readable musical scores for human
performers. We focus on piano music and support homophonic voices, i.e., voices
that can contain chords, and cross-staff voices, which are notably difficult
tasks that have often been overlooked in previous research. We propose an
end-to-end system based on graph neural networks that clusters notes that
belong to the same chord and connects them with edges if they are part of a
voice. Our results show clear and consistent improvements over a previous
approach on two datasets of different styles. To aid the qualitative analysis
of our results, we support the export in symbolic music formats and provide a
direct visualization of our outputs graph over the musical score. All code and
pre-trained models are available at https://github.com/CPJKU/piano_svsep",eess.AS cs.AI cs.LG,2024-07-15
Safeguard Text-to-Image Diffusion Models with Human Feedback Inversion,,"This paper addresses the societal concerns arising from large-scale
text-to-image diffusion models for generating potentially harmful or
copyrighted content. Existing models rely heavily on internet-crawled data,
wherein problematic concepts persist due to incomplete filtration processes.
While previous approaches somewhat alleviate the issue, they often rely on
text-specified concepts, introducing challenges in accurately capturing nuanced
concepts and aligning model knowledge with human understandings. In response,
we propose a framework named Human Feedback Inversion (HFI), where human
feedback on model-generated images is condensed into textual tokens guiding the
mitigation or removal of problematic images. The proposed framework can be
built upon existing techniques for the same purpose, enhancing their alignment
with human judgment. By doing so, we simplify the training objective with a
self-distillation-based technique, providing a strong baseline for concept
removal. Our experimental results demonstrate our framework significantly
reduces objectionable content generation while preserving image quality,
contributing to the ethical deployment of AI in the public sphere.",cs.CV cs.AI,2024-07-17
"Multi-Grained Query-Guided Set Prediction Network for Grounded
  Multimodal Named Entity Recognition",,"Grounded Multimodal Named Entity Recognition (GMNER) is an emerging
information extraction (IE) task, aiming to simultaneously extract entity
spans, types, and entity-matched bounding box groundings in images from given
sentence-image pairs data. Recent unified methods employing machine reading
comprehension (MRC-based) frameworks or sequence generation-based models face
challenges in understanding the relationships of multimodal entities. MRC-based
frameworks, utilizing human-designed queries, struggle to model intra-entity
connections. Meanwhile, sequence generation-based outputs excessively rely on
inter-entity dependencies due to pre-defined decoding order. To tackle these,
we propose a novel unified framework named Multi-grained Query-guided Set
Prediction Network (MQSPN) to learn appropriate relationships at intra-entity
and inter-entity levels. Specifically, MQSPN consists of a Multi-grained Query
Set (MQS) and a Multimodal Set Prediction Network (MSP). MQS combines specific
type-grained and learnable entity-grained queries to adaptively strengthen
intra-entity connections by explicitly aligning visual regions with textual
spans. Based on solid intra-entity modeling, MSP reformulates GMNER as a set
prediction, enabling the parallel prediction of multimodal entities in a
non-autoregressive manner, eliminating redundant dependencies from preceding
sequences, and guiding models to establish appropriate inter-entity
relationships from a global matching perspective. Additionally, to boost better
alignment of two-level relationships, we also incorporate a Query-guided Fusion
Net (QFNet) to work as a glue network between MQS and MSP. Extensive
experiments demonstrate that our approach achieves state-of-the-art
performances in widely used benchmarks. Notably, our method improves 2.83% F1
in the difficult fine-grained GMNER benchmark.",cs.IR cs.AI cs.CL cs.CV,2024-07-17
Watermarking Recommender Systems,,"Recommender systems embody significant commercial value and represent crucial
intellectual property. However, the integrity of these systems is constantly
challenged by malicious actors seeking to steal their underlying models.
Safeguarding against such threats is paramount to upholding the rights and
interests of the model owner. While model watermarking has emerged as a potent
defense mechanism in various domains, its direct application to recommender
systems remains unexplored and non-trivial. In this paper, we address this gap
by introducing Autoregressive Out-of-distribution Watermarking (AOW), a novel
technique tailored specifically for recommender systems. Our approach entails
selecting an initial item and querying it through the oracle model, followed by
the selection of subsequent items with small prediction scores. This iterative
process generates a watermark sequence autoregressively, which is then
ingrained into the model's memory through training. To assess the efficacy of
the watermark, the model is tasked with predicting the subsequent item given a
truncated watermark sequence. Through extensive experimentation and analysis,
we demonstrate the superior performance and robust properties of AOW. Notably,
our watermarking technique exhibits high-confidence extraction capabilities and
maintains effectiveness even in the face of distillation and fine-tuning
processes.",cs.IR cs.CR cs.LG,2024-07-17
Direct Unlearning Optimization for Robust and Safe Text-to-Image Models,,"Recent advancements in text-to-image (T2I) models have greatly benefited from
large-scale datasets, but they also pose significant risks due to the potential
generation of unsafe content. To mitigate this issue, researchers have
developed unlearning techniques to remove the model's ability to generate
potentially harmful content. However, these methods are easily bypassed by
adversarial attacks, making them unreliable for ensuring the safety of
generated images. In this paper, we propose Direct Unlearning Optimization
(DUO), a novel framework for removing Not Safe For Work (NSFW) content from T2I
models while preserving their performance on unrelated topics. DUO employs a
preference optimization approach using curated paired image data, ensuring that
the model learns to remove unsafe visual concepts while retaining unrelated
features. Furthermore, we introduce an output-preserving regularization term to
maintain the model's generative capabilities on safe content. Extensive
experiments demonstrate that DUO can robustly defend against various
state-of-the-art red teaming methods without significant performance
degradation on unrelated topics, as measured by FID and CLIP scores. Our work
contributes to the development of safer and more reliable T2I models, paving
the way for their responsible deployment in both closed-source and open-source
scenarios.",cs.CV,2024-07-17
"An Application of Large Language Models to Coding Negotiation
  Transcripts",,"In recent years, Large Language Models (LLM) have demonstrated impressive
capabilities in the field of natural language processing (NLP). This paper
explores the application of LLMs in negotiation transcript analysis by the
Vanderbilt AI Negotiation Lab. Starting in September 2022, we applied multiple
strategies using LLMs from zero shot learning to fine tuning models to
in-context learning). The final strategy we developed is explained, along with
how to access and use the model. This study provides a sense of both the
opportunities and roadblocks for the implementation of LLMs in real life
applications and offers a model for how LLMs can be applied to coding in other
fields.",cs.CL cs.AI,2024-07-18
"Advancing Chart Question Answering with Robust Chart Component
  Recognition",,"Chart comprehension presents significant challenges for machine learning
models due to the diverse and intricate shapes of charts. Existing multimodal
methods often overlook these visual features or fail to integrate them
effectively for chart question answering (ChartQA). To address this, we
introduce Chartformer, a unified framework that enhances chart component
recognition by accurately identifying and classifying components such as bars,
lines, pies, titles, legends, and axes. Additionally, we propose a novel
Question-guided Deformable Co-Attention (QDCAt) mechanism, which fuses chart
features encoded by Chartformer with the given question, leveraging the
question's guidance to ground the correct answer. Extensive experiments
demonstrate that the proposed approaches significantly outperform baseline
models in chart component recognition and ChartQA tasks, achieving improvements
of 3.2% in mAP and 15.4% in accuracy, respectively. These results underscore
the robustness of our solution for detailed visual data interpretation across
various applications.",cs.CL cs.AI cs.IR,2024-07-19
"Mapping Patient Trajectories: Understanding and Visualizing Sepsis
  Prognostic Pathways from Patients Clinical Narratives",,"In recent years, healthcare professionals are increasingly emphasizing on
personalized and evidence-based patient care through the exploration of
prognostic pathways. To study this, structured clinical variables from
Electronic Health Records (EHRs) data have traditionally been employed by many
researchers. Presently, Natural Language Processing models have received great
attention in clinical research which expanded the possibilities of using
clinical narratives. In this paper, we propose a systematic methodology for
developing sepsis prognostic pathways derived from clinical notes, focusing on
diverse patient subgroups identified by exploring comorbidities associated with
sepsis and generating explanations of these subgroups using SHAP. The extracted
prognostic pathways of these subgroups provide valuable insights into the
dynamic trajectories of sepsis severity over time. Visualizing these pathways
sheds light on the likelihood and direction of disease progression across
various contexts and reveals patterns and pivotal factors or biomarkers
influencing the transition between sepsis stages, whether toward deterioration
or improvement. This empowers healthcare providers to implement more
personalized and effective healthcare strategies for individual patients.",cs.CL cs.AI cs.LG,2024-07-20
"Towards Automated Data Sciences with Natural Language and SageCopilot:
  Practices and Lessons Learned",,"While the field of NL2SQL has made significant advancements in translating
natural language instructions into executable SQL scripts for data querying and
processing, achieving full automation within the broader data science pipeline
- encompassing data querying, analysis, visualization, and reporting - remains
a complex challenge. This study introduces SageCopilot, an advanced,
industry-grade system system that automates the data science pipeline by
integrating Large Language Models (LLMs), Autonomous Agents (AutoAgents), and
Language User Interfaces (LUIs). Specifically, SageCopilot incorporates a
two-phase design: an online component refining users' inputs into executable
scripts through In-Context Learning (ICL) and running the scripts for results
reporting & visualization, and an offline preparing demonstrations requested by
ICL in the online phase. A list of trending strategies such as Chain-of-Thought
and prompt-tuning have been used to augment SageCopilot for enhanced
performance. Through rigorous testing and comparative analysis against
prompt-based solutions, SageCopilot has been empirically validated to achieve
superior end-to-end performance in generating or executing scripts and offering
results with visualization, backed by real-world datasets. Our in-depth
ablation studies highlight the individual contributions of various components
and strategies used by SageCopilot to the end-to-end correctness for data
sciences.",cs.AI cs.CL cs.DB cs.SE,2024-07-21
"They Look Like Each Other: Case-based Reasoning for Explainable
  Depression Detection on Twitter using Large Language Models",,"Depression is a common mental health issue that requires prompt diagnosis and
treatment. Despite the promise of social media data for depression detection,
the opacity of employed deep learning models hinders interpretability and
raises bias concerns. We address this challenge by introducing ProtoDep, a
novel, explainable framework for Twitter-based depression detection. ProtoDep
leverages prototype learning and the generative power of Large Language Models
to provide transparent explanations at three levels: (i) symptom-level
explanations for each tweet and user, (ii) case-based explanations comparing
the user to similar individuals, and (iii) transparent decision-making through
classification weights. Evaluated on five benchmark datasets, ProtoDep achieves
near state-of-the-art performance while learning meaningful prototypes. This
multi-faceted approach offers significant potential to enhance the reliability
and transparency of depression detection on social media, ultimately aiding
mental health professionals in delivering more informed care.",cs.CL cs.AI cs.SI,2024-07-21
"Unlocking the Potential: Benchmarking Large Language Models in Water
  Engineering and Research",,"Recent advancements in Large Language Models (LLMs) have sparked interest in
their potential applications across various fields. This paper embarked on a
pivotal inquiry: Can existing LLMs effectively serve as ""water expert models""
for water engineering and research tasks? This study was the first to evaluate
LLMs' contributions across various water engineering and research tasks by
establishing a domain-specific benchmark suite, namely, WaterER. Herein, we
prepared 983 tasks related to water engineering and research, categorized into
""wastewater treatment"", ""environmental restoration"", ""drinking water treatment
and distribution"", ""sanitation"", ""anaerobic digestion"" and ""contaminants
assessment"". We evaluated the performance of seven LLMs (i.e., GPT-4, GPT-3.5,
Gemini, GLM-4, ERNIE, QWEN and Llama3) on these tasks. We highlighted the
strengths of GPT-4 in handling diverse and complex tasks of water engineering
and water research, the specialized capabilities of Gemini in academic
contexts, Llama3's strongest capacity to answer Chinese water engineering
questions and the competitive performance of Chinese-oriented models like
GLM-4, ERNIE and QWEN in some water engineering tasks. More specifically,
current LLMs excelled particularly in generating precise research gaps for
papers on ""contaminants and related water quality monitoring and assessment"".
Additionally, they were more adept at creating appropriate titles for research
papers on ""treatment processes for wastewaters"", ""environmental restoration"",
and ""drinking water treatment"". Overall, this study pioneered evaluating LLMs
in water engineering and research by introducing the WaterER benchmark to
assess the trustworthiness of their predictions. This standardized evaluation
framework would also drive future advancements in LLM technology by using
targeting datasets, propelling these models towards becoming true ""water
expert"".",cs.CL cs.AI,2024-07-22
"Promises and Pitfalls of Generative Masked Language Modeling:
  Theoretical Framework and Practical Guidelines",,"Autoregressive language models are the currently dominant paradigm for text
generation, but they have some fundamental limitations that cannot be remedied
by scale-for example inherently sequential and unidirectional generation. While
alternate classes of models have been explored, we have limited mathematical
understanding of their fundamental power and limitations. In this paper we
focus on Generative Masked Language Models (GMLMs), a non-autoregressive
paradigm in which we train a model to fit conditional probabilities of the data
distribution via masking, which are subsequently used as inputs to a Markov
Chain to draw samples from the model, These models empirically strike a
promising speed-quality trade-off as each step can be typically parallelized by
decoding the entire sequence in parallel. We develop a mathematical framework
for analyzing and improving such models which sheds light on questions of
sample complexity and inference speed and quality. Empirically, we adapt the T5
model for iteratively-refined parallel decoding, achieving 2-3x speedup in
machine translation with minimal sacrifice in quality compared with
autoregressive models. We run careful ablation experiments to give
recommendations on key design choices, and make fine-grained observations on
the common error modes in connection with our theory. Our mathematical analyses
and empirical observations characterize both potentials and limitations of this
approach, and can be applied to future works on improving understanding and
performance of GMLMs. Our codes are released at
https://github.com/google-research/google-research/tree/master/padir",cs.CL cs.LG,2024-07-22
PAV: Personalized Head Avatar from Unstructured Video Collection,,"We propose PAV, Personalized Head Avatar for the synthesis of human faces
under arbitrary viewpoints and facial expressions. PAV introduces a method that
learns a dynamic deformable neural radiance field (NeRF), in particular from a
collection of monocular talking face videos of the same character under various
appearance and shape changes. Unlike existing head NeRF methods that are
limited to modeling such input videos on a per-appearance basis, our method
allows for learning multi-appearance NeRFs, introducing appearance embedding
for each input video via learnable latent neural features attached to the
underlying geometry. Furthermore, the proposed appearance-conditioned density
formulation facilitates the shape variation of the character, such as facial
hair and soft tissues, in the radiance field prediction. To the best of our
knowledge, our approach is the first dynamic deformable NeRF framework to model
appearance and shape variations in a single unified network for
multi-appearances of the same subject. We demonstrate experimentally that PAV
outperforms the baseline method in terms of visual rendering quality in our
quantitative and qualitative studies on various subjects.",cs.CV,2024-07-22
"APTNESS: Incorporating Appraisal Theory and Emotion Support Strategies
  for Empathetic Response Generation",,"Empathetic response generation is designed to comprehend the emotions of
others and select the most appropriate strategies to assist them in resolving
emotional challenges. Empathy can be categorized into cognitive empathy and
affective empathy. The former pertains to the ability to understand and discern
the emotional issues and situations of others, while the latter involves the
capacity to provide comfort. To enhance one's empathetic abilities, it is
essential to develop both these aspects. Therefore, we develop an innovative
framework that combines retrieval augmentation and emotional support strategy
integration. Our framework starts with the introduction of a comprehensive
emotional palette for empathy. We then apply appraisal theory to decompose this
palette and create a database of empathetic responses. This database serves as
an external resource and enhances the LLM's empathy by integrating semantic
retrieval mechanisms. Moreover, our framework places a strong emphasis on the
proper articulation of response strategies. By incorporating emotional support
strategies, we aim to enrich the model's capabilities in both cognitive and
affective empathy, leading to a more nuanced and comprehensive empathetic
response. Finally, we extract datasets ED and ET from the empathetic dialogue
dataset \textsc{EmpatheticDialogues} and ExTES based on dialogue length.
Experiments demonstrate that our framework can enhance the empathy ability of
LLMs from both cognitive and affective empathy perspectives. Our code is
released at https://github.com/CAS-SIAT-XinHai/APTNESS.",cs.CL cs.AI,2024-07-22
"Evaluating Long Range Dependency Handling in Code Generation Models
  using Multi-Step Key Retrieval",,"As language models support larger and larger context sizes, evaluating their
ability to make effective use of that context becomes increasingly important.
We analyze the ability of several code generation models to handle long range
dependencies using a suite of multi-step key retrieval tasks in context windows
up to 8k tokens in length. The tasks progressively increase in difficulty and
allow more nuanced evaluation of model capabilities than tests like the popular
needle-in-the-haystack test. We find that performance degrades significantly
(up to 2x) when a function references another function that is defined later in
the prompt. We also observe that models that use sliding window attention
mechanisms have difficulty handling references further than the size of a
single window. We perform simple prompt modifications using call graph
information to improve multi-step retrieval performance up to 3x. Our analysis
highlights different facets of long-context performance and is suggestive of
prompt construction strategies for code completion tools",cs.CL cs.AI cs.LG,2024-07-22
"Artificial Intelligence in Extracting Diagnostic Data from Dental
  Records",,"This research addresses the issue of missing structured data in dental
records by extracting diagnostic information from unstructured text. The
updated periodontology classification system's complexity has increased
incomplete or missing structured diagnoses. To tackle this, we use advanced AI
and NLP methods, leveraging GPT-4 to generate synthetic notes for fine-tuning a
RoBERTa model. This significantly enhances the model's ability to understand
medical and dental language. We evaluated the model using 120 randomly selected
clinical notes from two datasets, demonstrating its improved diagnostic
extraction accuracy. The results showed high accuracy in diagnosing periodontal
status, stage, and grade, with Site 1 scoring 0.99 and Site 2 scoring 0.98. In
the subtype category, Site 2 achieved perfect scores, outperforming Site 1.
This method enhances extraction accuracy and broadens its use across dental
contexts. The study underscores AI and NLP's transformative impact on
healthcare delivery and management. Integrating AI and NLP technologies
enhances documentation and simplifies administrative tasks by precisely
extracting complex clinical information. This approach effectively addresses
challenges in dental diagnostics. Using synthetic training data from LLMs
optimizes the training process, improving accuracy and efficiency in
identifying periodontal diagnoses from clinical notes. This innovative method
holds promise for broader healthcare applications, potentially improving
patient care quality.",cs.CL,2024-07-23
"An Active Inference Strategy for Prompting Reliable Responses from Large
  Language Models in Medical Practice",,"Continuing advances in Large Language Models (LLMs) in artificial
intelligence offer important capacities in intuitively accessing and using
medical knowledge in many contexts, including education and training as well as
assessment and treatment. Most of the initial literature on LLMs in medicine
has emphasized that LLMs are unsuitable for medical use because they are
non-deterministic, may provide incorrect or harmful responses, and cannot be
regulated to assure quality control. If these issues could be corrected,
optimizing LLM technology could benefit patients and physicians by providing
affordable, point-of-care medical knowledge. Our proposed framework refines LLM
responses by restricting their primary knowledge base to domain-specific
datasets containing validated medical information. Additionally, we introduce
an actor-critic LLM prompting protocol based on active inference principles of
human cognition, where a Therapist agent initially responds to patient queries,
and a Supervisor agent evaluates and adjusts responses to ensure accuracy and
reliability. We conducted a validation study where expert cognitive behaviour
therapy for insomnia (CBT-I) therapists evaluated responses from the LLM in a
blind format. Experienced human CBT-I therapists assessed responses to 100
patient queries, comparing LLM-generated responses with appropriate and
inappropriate responses crafted by experienced CBT-I therapists. Results showed
that LLM responses received high ratings from the CBT-I therapists, often
exceeding those of therapist-generated appropriate responses. This structured
approach aims to integrate advanced LLM technology into medical applications,
meeting regulatory requirements for establishing the safe and effective use of
special purpose validated LLMs in medicine.",cs.CL,2024-07-23
"Table-Filling via Mean Teacher for Cross-domain Aspect Sentiment Triplet
  Extraction",,"Cross-domain Aspect Sentiment Triplet Extraction (ASTE) aims to extract
fine-grained sentiment elements from target domain sentences by leveraging the
knowledge acquired from the source domain. Due to the absence of labeled data
in the target domain, recent studies tend to rely on pre-trained language
models to generate large amounts of synthetic data for training purposes.
However, these approaches entail additional computational costs associated with
the generation process. Different from them, we discover a striking resemblance
between table-filling methods in ASTE and two-stage Object Detection (OD) in
computer vision, which inspires us to revisit the cross-domain ASTE task and
approach it from an OD standpoint. This allows the model to benefit from the OD
extraction paradigm and region-level alignment. Building upon this premise, we
propose a novel method named \textbf{T}able-\textbf{F}illing via \textbf{M}ean
\textbf{T}eacher (TFMT). Specifically, the table-filling methods encode the
sentence into a 2D table to detect word relations, while TFMT treats the table
as a feature map and utilizes a region consistency to enhance the quality of
those generated pseudo labels. Additionally, considering the existence of the
domain gap, a cross-domain consistency based on Maximum Mean Discrepancy is
designed to alleviate domain shift problems. Our method achieves
state-of-the-art performance with minimal parameters and computational costs,
making it a strong baseline for cross-domain ASTE.",cs.CL cs.AI,2024-07-23
"Knowledge Models for Cancer Clinical Practice Guidelines : Construction,
  Management and Usage in Question Answering",,"An automated knowledge modeling algorithm for Cancer Clinical Practice
Guidelines (CPGs) extracts the knowledge contained in the CPG documents and
transforms it into a programmatically interactable, easy-to-update structured
model with minimal human intervention. The existing automated algorithms have
minimal scope and cannot handle the varying complexity of the knowledge content
in the CPGs for different cancer types. This work proposes an improved
automated knowledge modeling algorithm to create knowledge models from the
National Comprehensive Cancer Network (NCCN) CPGs in Oncology for different
cancer types. The proposed algorithm has been evaluated with NCCN CPGs for four
different cancer types. We also proposed an algorithm to compare the knowledge
models for different versions of a guideline to discover the specific changes
introduced in the treatment protocol of a new version. We created a
question-answering (Q&A) framework with the guideline knowledge models as the
augmented knowledge base to study our ability to query the knowledge models. We
compiled a set of 32 question-answer pairs derived from two reliable data
sources for the treatment of Non-Small Cell Lung Cancer (NSCLC) to evaluate the
Q&A framework. The framework was evaluated against the question-answer pairs
from one data source, and it can generate the answers with 54.5% accuracy from
the treatment algorithm and 81.8% accuracy from the discussion part of the NCCN
NSCLC guideline knowledge model.",cs.CL cs.AI,2024-07-23
Sentiment Reasoning for Healthcare,,"Transparency in AI decision-making is crucial in healthcare due to the severe
consequences of errors, and this is important for building trust among AI and
users in sentiment analysis task. Incorporating reasoning capabilities helps
Large Language Models (LLMs) understand human emotions within broader contexts,
handle nuanced and ambiguous language, and infer underlying sentiments that may
not be explicitly stated. In this work, we introduce a new task - Sentiment
Reasoning - for both speech and text modalities, along with our proposed
multimodal multitask framework and dataset. Our study showed that
rationale-augmented training enhances model performance in sentiment
classification across both human transcript and ASR settings. Also, we found
that the generated rationales typically exhibit different vocabularies compared
to human-generated rationales, but maintain similar semantics. All code, data
(English-translated and Vietnamese) and models are published online:
https://github.com/leduckhai/MultiMed",cs.CL cs.AI cs.LG cs.SD eess.AS,2024-07-24
"Bailicai: A Domain-Optimized Retrieval-Augmented Generation Framework
  for Medical Applications",,"Large Language Models (LLMs) have exhibited remarkable proficiency in natural
language understanding, prompting extensive exploration of their potential
applications across diverse domains. In the medical domain, open-source LLMs
have demonstrated moderate efficacy following domain-specific fine-tuning;
however, they remain substantially inferior to proprietary models such as GPT-4
and GPT-3.5. These open-source models encounter limitations in the
comprehensiveness of domain-specific knowledge and exhibit a propensity for
'hallucinations' during text generation. To mitigate these issues, researchers
have implemented the Retrieval-Augmented Generation (RAG) approach, which
augments LLMs with background information from external knowledge bases while
preserving the model's internal parameters. However, document noise can
adversely affect performance, and the application of RAG in the medical field
remains in its nascent stages. This study presents the Bailicai framework: a
novel integration of retrieval-augmented generation with large language models
optimized for the medical domain. The Bailicai framework augments the
performance of LLMs in medicine through the implementation of four sub-modules.
Experimental results demonstrate that the Bailicai approach surpasses existing
medical domain LLMs across multiple medical benchmarks and exceeds the
performance of GPT-3.5. Furthermore, the Bailicai method effectively attenuates
the prevalent issue of hallucinations in medical applications of LLMs and
ameliorates the noise-related challenges associated with traditional RAG
techniques when processing irrelevant or pseudo-relevant documents.",cs.CL,2024-07-24
"What Matters in Explanations: Towards Explainable Fake Review Detection
  Focusing on Transformers",,"Customers' reviews and feedback play crucial role on electronic
commerce~(E-commerce) platforms like Amazon, Zalando, and eBay in influencing
other customers' purchasing decisions. However, there is a prevailing concern
that sellers often post fake or spam reviews to deceive potential customers and
manipulate their opinions about a product. Over the past decade, there has been
considerable interest in using machine learning (ML) and deep learning (DL)
models to identify such fraudulent reviews. Unfortunately, the decisions made
by complex ML and DL models - which often function as \emph{black-boxes} - can
be surprising and difficult for general users to comprehend. In this paper, we
propose an explainable framework for detecting fake reviews with high precision
in identifying fraudulent content with explanations and investigate what
information matters most for explaining particular decisions by conducting
empirical user evaluation. Initially, we develop fake review detection models
using DL and transformer models including XLNet and DistilBERT. We then
introduce layer-wise relevance propagation (LRP) technique for generating
explanations that can map the contributions of words toward the predicted
class. The experimental results on two benchmark fake review detection datasets
demonstrate that our predictive models achieve state-of-the-art performance and
outperform several existing methods. Furthermore, the empirical user evaluation
of the generated explanations concludes which important information needs to be
considered in generating explanations in the context of fake review
identification.",cs.CL cs.IR cs.SI,2024-07-24
Multi-group Uncertainty Quantification for Long-form Text Generation,,"While large language models are rapidly moving towards consumer-facing
applications, they are often still prone to factual errors and hallucinations.
In order to reduce the potential harms that may come from these errors, it is
important for users to know to what extent they can trust an LLM when it makes
a factual claim. To this end, we study the problem of uncertainty
quantification of factual correctness in long-form natural language generation.
Given some output from a large language model, we study both uncertainty at the
level of individual claims contained within the output (via calibration) and
uncertainty across the entire output itself (via conformal prediction).
Moreover, we invoke multicalibration and multivalid conformal prediction to
ensure that such uncertainty guarantees are valid both marginally and across
distinct groups of prompts. Using the task of biography generation, we
demonstrate empirically that having access to and making use of additional
group attributes for each prompt improves both overall and group-wise
performance. As the problems of calibration, conformal prediction, and their
multi-group counterparts have not been extensively explored previously in the
context of long-form text generation, we consider these empirical results to
form a benchmark for this setting.",cs.CL cs.AI cs.LG,2024-07-24
"Understanding the Interplay of Scale, Data, and Bias in Language Models:
  A Case Study with BERT",,"In the current landscape of language model research, larger models, larger
datasets and more compute seems to be the only way to advance towards
intelligence. While there have been extensive studies of scaling laws and
models' scaling behaviors, the effect of scale on a model's social biases and
stereotyping tendencies has received less attention. In this study, we explore
the influence of model scale and pre-training data on its learnt social biases.
We focus on BERT -- an extremely popular language model -- and investigate
biases as they show up during language modeling (upstream), as well as during
classification applications after fine-tuning (downstream). Our experiments on
four architecture sizes of BERT demonstrate that pre-training data
substantially influences how upstream biases evolve with model scale. With
increasing scale, models pre-trained on large internet scrapes like Common
Crawl exhibit higher toxicity, whereas models pre-trained on moderated data
sources like Wikipedia show greater gender stereotypes. However, downstream
biases generally decrease with increasing model scale, irrespective of the
pre-training data. Our results highlight the qualitative role of pre-training
data in the biased behavior of language models, an often overlooked aspect in
the study of scale. Through a detailed case study of BERT, we shed light on the
complex interplay of data and model scale, and investigate how it translates to
concrete biases.",cs.CL cs.AI,2024-07-25
"Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable
  Frameworks",,"Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities
of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The
increasing demands of application scenarios have driven the evolution of RAG,
leading to the integration of advanced retrievers, LLMs and other complementary
technologies, which in turn has amplified the intricacy of RAG systems.
However, the rapid advancements are outpacing the foundational RAG paradigm,
with many methods struggling to be unified under the process of
""retrieve-then-generate"". In this context, this paper examines the limitations
of the existing RAG paradigm and introduces the modular RAG framework. By
decomposing complex RAG systems into independent modules and specialized
operators, it facilitates a highly reconfigurable framework. Modular RAG
transcends the traditional linear architecture, embracing a more advanced
design that integrates routing, scheduling, and fusion mechanisms. Drawing on
extensive research, this paper further identifies prevalent RAG
patterns-linear, conditional, branching, and looping-and offers a comprehensive
analysis of their respective implementation nuances. Modular RAG presents
innovative opportunities for the conceptualization and deployment of RAG
systems. Finally, the paper explores the potential emergence of new operators
and paradigms, establishing a solid theoretical foundation and a practical
roadmap for the continued evolution and practical deployment of RAG
technologies.",cs.CL cs.AI cs.IR,2024-07-25
"Using Large Language Models for the Interpretation of Building
  Regulations",,"Compliance checking is an essential part of a construction project. The
recent rapid uptake of building information models (BIM) in the construction
industry has created more opportunities for automated compliance checking
(ACC). BIM enables sharing of digital building design data that can be used for
compliance checking with legal requirements, which are conventionally conveyed
in natural language and not intended for machine processing. Creating a
computable representation of legal requirements suitable for ACC is complex,
costly, and time-consuming. Large language models (LLMs) such as the generative
pre-trained transformers (GPT), GPT-3.5 and GPT-4, powering OpenAI's ChatGPT,
can generate logically coherent text and source code responding to user
prompts. This capability could be used to automate the conversion of building
regulations into a semantic and computable representation. This paper evaluates
the performance of LLMs in translating building regulations into LegalRuleML in
a few-shot learning setup. By providing GPT-3.5 with only a few example
translations, it can learn the basic structure of the format. Using a system
prompt, we further specify the LegalRuleML representation and explore the
existence of expert domain knowledge in the model. Such domain knowledge might
be ingrained in GPT-3.5 through the broad pre-training but needs to be brought
forth by careful contextualisation. Finally, we investigate whether strategies
such as chain-of-thought reasoning and self-consistency could apply to this use
case. As LLMs become more sophisticated, the increased common sense, logical
coherence, and means to domain adaptation can significantly support ACC,
leading to more efficient and effective checking processes.",cs.CL cs.AI,2024-07-26
"Improving noisy student training for low-resource languages in
  End-to-End ASR using CycleGAN and inter-domain losses",,"Training a semi-supervised end-to-end speech recognition system using noisy
student training has significantly improved performance. However, this approach
requires a substantial amount of paired speech-text and unlabeled speech, which
is costly for low-resource languages. Therefore, this paper considers a more
extreme case of semi-supervised end-to-end automatic speech recognition where
there are limited paired speech-text, unlabeled speech (less than five hours),
and abundant external text. Firstly, we observe improved performance by
training the model using our previous work on semi-supervised learning
""CycleGAN and inter-domain losses"" solely with external text. Secondly, we
enhance ""CycleGAN and inter-domain losses"" by incorporating automatic
hyperparameter tuning, calling it ""enhanced CycleGAN inter-domain losses.""
Thirdly, we integrate it into the noisy student training approach pipeline for
low-resource scenarios. Our experimental results, conducted on six non-English
languages from Voxforge and Common Voice, show a 20% word error rate reduction
compared to the baseline teacher model and a 10% word error rate reduction
compared to the baseline best student model, highlighting the significant
improvements achieved through our proposed method.",cs.CL cs.SD eess.AS,2024-07-26
"Hybrid Heuristic Algorithms for Adiabatic Quantum Machine Learning
  Models",,"The recent developments of adiabatic quantum machine learning (AQML) methods
and applications based on the quadratic unconstrained binary optimization
(QUBO) model have received attention from academics and practitioners.
Traditional machine learning methods such as support vector machines, balanced
k-means clustering, linear regression, Decision Tree Splitting, Restricted
Boltzmann Machines, and Deep Belief Networks can be transformed into a QUBO
model. The training of adiabatic quantum machine learning models is the
bottleneck for computation. Heuristics-based quantum annealing solvers such as
Simulated Annealing and Multiple Start Tabu Search (MSTS) are implemented to
speed up the training of AQML based on the QUBO model. The main purpose of this
paper is to present a hybrid heuristic embedding an r-flip strategy to solve
large-scale QUBO with an improved solution and shorter computing time compared
to the state-of-the-art MSTS method. The results of the substantial
computational experiments are reported to compare an r-flip strategy embedded
hybrid heuristic and a multiple start tabu search algorithm on a set of
benchmark instances and three large-scale QUBO instances. The r-flip strategy
embedded algorithm provides very high-quality solutions within the CPU time
limits of 60 and 600 seconds.",quant-ph cs.LG,2024-07-26
Network and Sentiment Analysis of Enron Emails,,"The objective of the research was to analyze e-mails exchanged at Enron, a
power company that declared bankruptcy in 2001 following an investigation into
unethical operations regarding their financials. Like other researchers, we
identify the most important employees and detect communities using network
science methods. We find that the importance of a person depends on the
centrality measure used; while the communities we detected resembled the formal
organizational structure of the company. In addition, because previous work
required that 10 e-mails be sent and received for an e-mail relationship to
exist, we analyzed the effect of different thresholds on the results and found
that results were very dependent on the threshold used. We also performed
sentiment analyses on the e-mails to evaluate whether sentiment changed over
time and found that the sentiments of the e-mails do not give insight into the
financial wellbeing of Enron. Our results provide insight into how information
flowed through Enron, who the key employees were, and e-mail sentiment before
and after the crisis",cs.SI,2024-07-27
LawLLM: Law Large Language Model for the US Legal System,,"In the rapidly evolving field of legal analytics, finding relevant cases and
accurately predicting judicial outcomes are challenging because of the
complexity of legal language, which often includes specialized terminology,
complex syntax, and historical context. Moreover, the subtle distinctions
between similar and precedent cases require a deep understanding of legal
knowledge. Researchers often conflate these concepts, making it difficult to
develop specialized techniques to effectively address these nuanced tasks. In
this paper, we introduce the Law Large Language Model (LawLLM), a multi-task
model specifically designed for the US legal domain to address these
challenges. LawLLM excels at Similar Case Retrieval (SCR), Precedent Case
Recommendation (PCR), and Legal Judgment Prediction (LJP). By clearly
distinguishing between precedent and similar cases, we provide essential
clarity, guiding future research in developing specialized strategies for these
tasks. We propose customized data preprocessing techniques for each task that
transform raw legal data into a trainable format. Furthermore, we also use
techniques such as in-context learning (ICL) and advanced information retrieval
methods in LawLLM. The evaluation results demonstrate that LawLLM consistently
outperforms existing baselines in both zero-shot and few-shot scenarios,
offering unparalleled multi-task capabilities and filling critical gaps in the
legal domain.",cs.CL cs.IR cs.LG,2024-07-27
"ELP-Adapters: Parameter Efficient Adapter Tuning for Various Speech
  Processing Tasks",,"Self-supervised learning has emerged as a key approach for learning generic
representations from speech data. Despite promising results in downstream tasks
such as speech recognition, speaker verification, and emotion recognition, a
significant number of parameters is required, which makes fine-tuning for each
task memory-inefficient. To address this limitation, we introduce ELP-adapter
tuning, a novel method for parameter-efficient fine-tuning using three types of
adapter, namely encoder adapters (E-adapters), layer adapters (L-adapters), and
a prompt adapter (P-adapter). The E-adapters are integrated into
transformer-based encoder layers and help to learn fine-grained speech
representations that are effective for speech recognition. The L-adapters
create paths from each encoder layer to the downstream head and help to extract
non-linguistic features from lower encoder layers that are effective for
speaker verification and emotion recognition. The P-adapter appends pseudo
features to CNN features to further improve effectiveness and efficiency. With
these adapters, models can be quickly adapted to various speech processing
tasks. Our evaluation across four downstream tasks using five backbone models
demonstrated the effectiveness of the proposed method. With the WavLM backbone,
its performance was comparable to or better than that of full fine-tuning on
all tasks while requiring 90% fewer learnable parameters.",cs.CL cs.AI cs.LG cs.SD eess.AS,2024-07-28
Socio-cognitive Networks between Researchers,,"Understanding why researchers cite each other has been a longstanding
conjecture in studying scientific networks. Prior research suggests relevance,
group cohesion, or honest source crediting as possible factors. However, the
dual nature of cognitive and social dimensions underlying citation is often
overlooked by not considering the intermediary steps leading up to a citation.
For one work to be cited by another, it must first be published by a set of
authors. Therefore, we investigate the reasons behind researchers' citations,
explicitly examining the interplay of socio-cognitive ties through the
interdependence of coauthorship and citation networks. We assess our claims in
an empirical analysis by employing the Author-Oriented Relational HyperEvent
Model (AuthRHEM) to study Chilean astronomers' citation and collaboration
behavior between 2013 and 2015 in a joint framework. We find evidence that when
deciding which work to cite, authors prefer other work with novelty and
cognitive ties, such as work-to-work relations. At the same time, coherent
groups are relevant because coauthors are cocited more frequently in subsequent
publications.",cs.SI cs.DL physics.soc-ph stat.AP,2024-07-28
"Exploring Genre and Success Classification through Song Lyrics using
  DistilBERT: A Fun NLP Venture",,"This paper presents a natural language processing (NLP) approach to the
problem of thoroughly comprehending song lyrics, with particular attention on
genre classification, view-based success prediction, and approximate release
year. Our tests provide promising results with 65\% accuracy in genre
classification and 79\% accuracy in success prediction, leveraging a DistilBERT
model for genre classification and BERT embeddings for release year prediction.
Support Vector Machines outperformed other models in predicting the release
year, achieving the lowest root mean squared error (RMSE) of 14.18. Our study
offers insights that have the potential to revolutionize our relationship with
music by addressing the shortcomings of current approaches in properly
understanding the emotional intricacies of song lyrics.",cs.CL,2024-07-28
"High-Dimensional Fault Tolerance Testing of Highly Automated Vehicles
  Based on Low-Rank Models",,"Ensuring fault tolerance of Highly Automated Vehicles (HAVs) is crucial for
their safety due to the presence of potentially severe faults. Hence, Fault
Injection (FI) testing is conducted by practitioners to evaluate the safety
level of HAVs. To fully cover test cases, various driving scenarios and fault
settings should be considered. However, due to numerous combinations of test
scenarios and fault settings, the testing space can be complex and
high-dimensional. In addition, evaluating performance in all newly added
scenarios is resource-consuming. The rarity of critical faults that can cause
security problems further strengthens the challenge. To address these
challenges, we propose to accelerate FI testing under the low-rank Smoothness
Regularized Matrix Factorization (SRMF) framework. We first organize the sparse
evaluated data into a structured matrix based on its safety values. Then the
untested values are estimated by the correlation captured by the matrix
structure. To address high dimensionality, a low-rank constraint is imposed on
the testing space. To exploit the relationships between existing scenarios and
new scenarios and capture the local regularity of critical faults, three types
of smoothness regularization are further designed as a complement. We conduct
experiments on car following and cut in scenarios. The results indicate that
SRMF has the lowest prediction error in various scenarios and is capable of
predicting rare critical faults compared to other machine learning models. In
addition, SRMF can achieve 1171 acceleration rate, 99.3% precision and 91.1% F1
score in identifying critical faults. To the best of our knowledge, this is the
first work to introduce low-rank models to FI testing of HAVs.",cs.SE cs.AI cs.LG cs.RO,2024-07-28
Occam's Razor and Bender and Koller's Octopus,,"We discuss the teaching of the discussion surrounding Bender and Koller's
prominent ACL 2020 paper, ""Climbing toward NLU: on meaning form, and
understanding in the age of data"" \cite{bender2020climbing}. We present what we
understand to be the main contentions of the paper, and then recommend that the
students engage with the natural counter-arguments to the claims in the paper.
We attach teaching materials that we use to facilitate teaching this topic to
undergraduate students.",cs.CL physics.hist-ph,2024-07-28
"Beyond Metrics: A Critical Analysis of the Variability in Large Language
  Model Evaluation Frameworks",,"As large language models (LLMs) continue to evolve, the need for robust and
standardized evaluation benchmarks becomes paramount. Evaluating the
performance of these models is a complex challenge that requires careful
consideration of various linguistic tasks, model architectures, and
benchmarking methodologies. In recent years, various frameworks have emerged as
noteworthy contributions to the field, offering comprehensive evaluation tests
and benchmarks for assessing the capabilities of LLMs across diverse domains.
This paper provides an exploration and critical analysis of some of these
evaluation methodologies, shedding light on their strengths, limitations, and
impact on advancing the state-of-the-art in natural language processing.",cs.AI cs.CL,2024-07-28
"Enhancing Adversarial Text Attacks on BERT Models with Projected
  Gradient Descent",,"Adversarial attacks against deep learning models represent a major threat to
the security and reliability of natural language processing (NLP) systems. In
this paper, we propose a modification to the BERT-Attack framework, integrating
Projected Gradient Descent (PGD) to enhance its effectiveness and robustness.
The original BERT-Attack, designed for generating adversarial examples against
BERT-based models, suffers from limitations such as a fixed perturbation budget
and a lack of consideration for semantic similarity. The proposed approach in
this work, PGD-BERT-Attack, addresses these limitations by leveraging PGD to
iteratively generate adversarial examples while ensuring both imperceptibility
and semantic similarity to the original input. Extensive experiments are
conducted to evaluate the performance of PGD-BERT-Attack compared to the
original BERT-Attack and other baseline methods. The results demonstrate that
PGD-BERT-Attack achieves higher success rates in causing misclassification
while maintaining low perceptual changes. Furthermore, PGD-BERT-Attack produces
adversarial instances that exhibit greater semantic resemblance to the initial
input, enhancing their applicability in real-world scenarios. Overall, the
proposed modification offers a more effective and robust approach to
adversarial attacks on BERT-based models, thus contributing to the advancement
of defense against attacks on NLP systems.",cs.LG cs.CL cs.CR,2024-07-29
Apple Intelligence Foundation Language Models,,"We present foundation language models developed to power Apple Intelligence
features, including a ~3 billion parameter model designed to run efficiently on
devices and a large server-based language model designed for Private Cloud
Compute. These models are designed to perform a wide range of tasks
efficiently, accurately, and responsibly. This report describes the model
architecture, the data used to train the model, the training process, how the
models are optimized for inference, and the evaluation results. We highlight
our focus on Responsible AI and how the principles are applied throughout the
model development.",cs.AI cs.CL cs.LG,2024-07-29
"Genetic Instruct: Scaling up Synthetic Generation of Coding Instructions
  for Large Language Models",,"Large Language Models (LLMs) rely on instruction samples for alignment, but
creating these datasets poses challenges, particularly in expert-dependent
tasks like coding, which can be cost-prohibitive. One approach to mitigate
these challenges is synthesizing data using another LLM. In this paper, we
introduce a scalable method for generating synthetic instructions to enhance
the code generation capability of LLMs. The proposed algorithm,
Genetic-Instruct, mimics evolutionary processes, utilizing self-instruction to
create numerous synthetic samples from a limited number of seeds.
Genetic-Instruct is designed for efficient scaling of the generation process.
Fine-tuning multiple coding LLMs with the synthetic samples demonstrates a
significant improvement in their code generation accuracy compared to the
baselines.",cs.CL cs.LG cs.NE,2024-07-29
Convergence rates for the Adam optimizer,,"Stochastic gradient descent (SGD) optimization methods are nowadays the
method of choice for the training of deep neural networks (DNNs) in artificial
intelligence systems. In practically relevant training problems, usually not
the plain vanilla standard SGD method is the employed optimization scheme but
instead suitably accelerated and adaptive SGD optimization methods are applied.
As of today, maybe the most popular variant of such accelerated and adaptive
SGD optimization methods is the famous Adam optimizer proposed by Kingma & Ba
in 2014. Despite the popularity of the Adam optimizer in implementations, it
remained an open problem of research to provide a convergence analysis for the
Adam optimizer even in the situation of simple quadratic stochastic
optimization problems where the objective function (the function one intends to
minimize) is strongly convex. In this work we solve this problem by
establishing optimal convergence rates for the Adam optimizer for a large class
of stochastic optimization problems, in particular, covering simple quadratic
stochastic optimization problems. The key ingredient of our convergence
analysis is a new vector field function which we propose to refer to as the
Adam vector field. This Adam vector field accurately describes the macroscopic
behaviour of the Adam optimization process but differs from the negative
gradient of the objective function (the function we intend to minimize) of the
considered stochastic optimization problem. In particular, our convergence
analysis reveals that the Adam optimizer does typically not converge to
critical points of the objective function (zeros of the gradient of the
objective function) of the considered optimization problem but converges with
rates to zeros of this Adam vector field.",math.OC cs.LG math.PR stat.ML,2024-07-29
"Accelerating Large Language Model Inference with Self-Supervised Early
  Exits",,"This paper presents a novel technique for accelerating inference in large,
pre-trained language models (LLMs) by introducing early exits during inference.
The computational demands of these models, used across a wide range of
applications, can be substantial. By capitalizing on the inherent variability
in token complexity, our approach enables selective acceleration of the
inference process. Specifically, we propose the integration of early exit
''heads'' atop existing transformer layers, which facilitate conditional
terminations based on a confidence metric. These heads are trained in a
self-supervised manner using the model's own predictions as training data,
thereby eliminating the need for additional annotated data. The confidence
metric, established using a calibration set, ensures a desired level of
accuracy while enabling early termination when confidence exceeds a
predetermined threshold. Notably, our method preserves the original accuracy
and reduces computational time on certain tasks, leveraging the existing
knowledge of pre-trained LLMs without requiring extensive retraining. This
lightweight, modular modification has the potential to greatly enhance the
practical usability of LLMs, particularly in applications like real-time
language processing in resource-constrained environments.",cs.CL cs.LG stat.ML,2024-07-30
"Quasi-Regression Monte-Carlo scheme for semi-linear PDEs and BSDEs with
  large scale parallelization on GPUs",,"In this article we design a novel quasi-regression Monte Carlo algorithm in
order to approximate the solution of discrete time backward stochastic
differential equations (BSDEs), and we analyze the convergence of the proposed
method. The algorithm also approximates the solution to the related semi-linear
parabolic partial differential equation (PDE) obtained through the well-known
Feynman-Kac representation. For the sake of enriching the algorithm with
high-order convergence a weighted approximation of the solution is computed and
appropriate conditions on the parameters of the method are inferred. With the
challenge of tackling problems in high dimensions we propose suitable
projections of the solution and efficient parallelizations of the algorithm
taking advantage of powerful many-core processors such as graphics processing
units (GPUs).",math.NA cs.NA math.OC,2024-07-30
"Stratified regression Monte-Carlo scheme for semilinear PDEs and BSDEs
  with large scale parallelization on GPUs",,"In this paper, we design a novel algorithm based on Least-Squares Monte Carlo
(LSMC) in order to approximate the solution of discrete time Backward
Stochastic Differential Equations (BSDEs). Our algorithm allows massive
parallelization of the computations on many core processors such as graphics
processing units (GPUs). Our approach consists of a novel method of
stratification which appears to be crucial for large scale parallelization. In
this way, we minimize the exposure to the memory requirements due to the
storage of simulations. Indeed, we note the lower memory overhead of the method
compared with previous works.",math.NA cs.NA,2024-07-30
"Learning Optimal Signal Temporal Logic Decision Trees for
  Classification: A Max-Flow MILP Formulation",,"This paper presents a novel framework for inferring timed temporal logic
properties from data. The dataset comprises pairs of finite-time system traces
and corresponding labels, denoting whether the traces demonstrate specific
desired behaviors, e.g. whether the ship follows a safe route or not. Our
proposed approach leverages decision-tree-based methods to infer Signal
Temporal Logic classifiers using primitive formulae. We formulate the inference
process as a mixed integer linear programming optimization problem, recursively
generating constraints to determine both data classification and tree
structure. Applying a max-flow algorithm on the resultant tree transforms the
problem into a global optimization challenge, leading to improved
classification rates compared to prior methodologies. Moreover, we introduce a
technique to reduce the number of constraints by exploiting the symmetry
inherent in STL primitives, which enhances the algorithm's time performance and
interpretability. To assess our algorithm's effectiveness and classification
performance, we conduct three case studies involving two-class, multi-class,
and complex formula classification scenarios.",cs.LG,2024-07-30
"The Stochastic Conjugate Subgradient Algorithm For Kernel Support Vector
  Machines",,"Stochastic First-Order (SFO) methods have been a cornerstone in addressing a
broad spectrum of modern machine learning (ML) challenges. However, their
efficacy is increasingly questioned, especially in large-scale applications
where empirical evidence indicates potential performance limitations. In
response, this paper proposes an innovative method specifically designed for
kernel support vector machines (SVMs). This method not only achieves faster
convergence per iteration but also exhibits enhanced scalability when compared
to conventional SFO techniques. Diverging from traditional sample average
approximation strategies that typically frame kernel SVM as an 'all-in-one'
Quadratic Program (QP), our approach adopts adaptive sampling. This strategy
incrementally refines approximation accuracy on an 'as-needed' basis.
Crucially, this approach also inspires a decomposition-based algorithm,
effectively decomposing parameter selection from error estimation, with the
latter being independently determined for each data point. To exploit the
quadratic nature of the kernel matrix, we introduce a stochastic conjugate
subgradient method. This method preserves many benefits of first-order
approaches while adeptly handling both nonlinearity and non-smooth aspects of
the SVM problem. Thus, it extends beyond the capabilities of standard SFO
algorithms for non-smooth convex optimization. The convergence rate of this
novel method is thoroughly analyzed within this paper. Our experimental results
demonstrate that the proposed algorithm not only maintains but potentially
exceeds the scalability of SFO methods. Moreover, it significantly enhances
both speed and accuracy of the optimization process.",cs.LG math.OC,2024-07-30
"Entropy, Thermodynamics and the Geometrization of the Language Model",,"In this paper, we discuss how pure mathematics and theoretical physics can be
applied to the study of language models. Using set theory and analysis, we
formulate mathematically rigorous definitions of language models, and introduce
the concept of the moduli space of distributions for a language model. We
formulate a generalized distributional hypothesis using functional analysis and
topology. We define the entropy function associated with a language model and
show how it allows us to understand many interesting phenomena in languages. We
argue that the zero points of the entropy function and the points where the
entropy is close to 0 are the key obstacles for an LLM to approximate an
intelligent language model, which explains why good LLMs need billions of
parameters. Using the entropy function, we formulate a conjecture about AGI.
  Then, we show how thermodynamics gives us an immediate interpretation to
language models. In particular we will define the concepts of partition
function, internal energy and free energy for a language model, which offer
insights into how language models work. Based on these results, we introduce a
general concept of the geometrization of language models and define what is
called the Boltzmann manifold. While the current LLMs are the special cases of
the Boltzmann manifold.",cs.CL cond-mat.stat-mech hep-th math.DG,2024-07-30
Palu: Compressing KV-Cache with Low-Rank Projection,,"KV-Cache compression methods generally sample a KV-Cache of effectual tokens
or quantize it into lower bits. However, these methods cannot exploit the
redundancy of the hidden dimension of KV tensors. This paper investigates a
unique hidden dimension approach called Palu, a novel KV-Cache compression
framework that utilizes low-rank projection. Palu decomposes the linear layers
into low-rank matrices, caches the smaller intermediate states, and
reconstructs the full keys and values on the fly. To improve accuracy,
compression rate, and efficiency, Palu further encompasses (1) a medium-grained
low-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a
low-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU
kernels. Our extensive experiments with popular LLMs show that Palu can
compress KV-Cache by more than 91.25% while maintaining a significantly better
accuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache
quantization methods at a similar or even higher memory usage. When compressing
KV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the
attention module. Our code is publicly available at
https://github.com/shadowpa0327/Palu.",cs.AI cs.LG,2024-07-30
Taming the Frequency Factory of Sinusoidal Networks,,"This work investigates the structure and representation capacity of
$sinusoidal$ MLPs, which have recently shown promising results in encoding
low-dimensional signals. This success can be attributed to its smoothness and
high representation capacity. The first allows the use of the network's
derivatives during training, enabling regularization. However, defining the
architecture and initializing its parameters to achieve a desired capacity
remains an empirical task. This work provides theoretical and experimental
results justifying the capacity property of sinusoidal MLPs and offers control
mechanisms for their initialization and training.
  We approach this from a Fourier series perspective and link the training with
the model's spectrum. Our analysis is based on a $harmonic$ expansion of the
sinusoidal MLP, which says that the composition of sinusoidal layers produces a
large number of new frequencies expressed as integer linear combinations of the
input frequencies (weights of the input layer). We use this novel $identity$ to
initialize the input neurons which work as a sampling in the signal spectrum.
We also note that each hidden neuron produces the same frequencies with
amplitudes completely determined by the hidden weights. Finally, we give an
upper bound for these amplitudes, which results in a $bounding$ scheme for the
network's spectrum during training.",cs.LG cs.CV,2024-07-30
Zero Shot Health Trajectory Prediction Using Transformer,,"Integrating modern machine learning and clinical decision-making has great
promise for mitigating healthcare's increasing cost and complexity. We
introduce the Enhanced Transformer for Health Outcome Simulation (ETHOS), a
novel application of the transformer deep-learning architecture for analyzing
high-dimensional, heterogeneous, and episodic health data. ETHOS is trained
using Patient Health Timelines (PHTs)-detailed, tokenized records of health
events-to predict future health trajectories, leveraging a zero-shot learning
approach. ETHOS represents a significant advancement in foundation model
development for healthcare analytics, eliminating the need for labeled data and
model fine-tuning. Its ability to simulate various treatment pathways and
consider patient-specific factors positions ETHOS as a tool for care
optimization and addressing biases in healthcare delivery. Future developments
will expand ETHOS' capabilities to incorporate a wider range of data types and
data sources. Our work demonstrates a pathway toward accelerated AI development
and deployment in healthcare.",cs.LG cs.AI cs.CY,2024-07-30
"A Dataset for Multi-intensity Continuous Human Activity Recognition
  through Passive Sensing",,"Human activity recognition (HAR) is essential in healthcare, elder care,
security, and human-computer interaction. The use of precise sensor data to
identify activities passively and continuously makes HAR accessible and
ubiquitous. Specifically, millimeter wave (mmWave) radar is promising for
passive and continuous HAR due to its ability to penetrate non-metallic
materials and provide high-resolution wireless sensing. Although mmWave sensors
are effective at capturing macro-scale activities, like exercising, they fail
to capture micro-scale activities, such as typing. In this paper, we introduce
mmDoppler, a novel dataset that utilizes off-the-shelf (COTS) mmWave radar in
order to capture both macro and micro-scale human movements using a
machine-learning driven signal processing pipeline. The dataset includes seven
subjects performing 19 distinct activities and employs adaptive doppler
resolution to enhance activity recognition. By adjusting the radar's doppler
resolution based on the activity type, our system captures subtle movements
more precisely. mmDoppler includes range-doppler heatmaps, offering detailed
motion dynamics, with data collected in a controlled environment with single as
well as multiple subjects performing activities simultaneously. The dataset
aims to bridge the gap in HAR systems by providing a more comprehensive and
detailed resource for improving the robustness and accuracy of mmWave radar
activity recognition.",cs.HC,2024-07-30
"Self-supervised Multi-future Occupancy Forecasting for Autonomous
  Driving",,"Environment prediction frameworks are critical for the safe navigation of
autonomous vehicles (AVs) in dynamic settings. LiDAR-generated occupancy grid
maps (L-OGMs) offer a robust bird's-eye view for the scene representation,
enabling self-supervised joint scene predictions while exhibiting resilience to
partial observability and perception detection failures. Prior approaches have
focused on deterministic L-OGM prediction architectures within the grid cell
space. While these methods have seen some success, they frequently produce
unrealistic predictions and fail to capture the stochastic nature of the
environment. Additionally, they do not effectively integrate additional sensor
modalities present in AVs. Our proposed framework performs stochastic L-OGM
prediction in the latent space of a generative architecture and allows for
conditioning on RGB cameras, maps, and planned trajectories. We decode
predictions using either a single-step decoder, which provides high-quality
predictions in real-time, or a diffusion-based batch decoder, which can further
refine the decoded frames to address temporal consistency issues and reduce
compression losses. Our experiments on the nuScenes and Waymo Open datasets
show that all variants of our approach qualitatively and quantitatively
outperform prior approaches.",cs.CV cs.RO,2024-07-30
Teaching Survey Research in Software Engineering,,"In this chapter, we provide advice on how to effectively teach survey
research based on lessons learned from several international teaching
experiences on the topic and from conducting large-scale surveys published at
various scientific conferences and journals. First, we provide teachers with a
potential syllabus for teaching survey research, including learning objectives,
lectures, and examples of practical assignments. Thereafter, we provide
actionable advice on how to teach the topics related to each learning
objective, including survey design, sampling, data collection, statistical and
qualitative analysis, threats to validity and reliability, and ethical
considerations. The chapter is complemented by online teaching resources,
including slides covering an entire course.",cs.SE,2024-07-30
Computational music analysis from first principles,,"We use coupled hidden Markov models to automatically annotate the 371 Bach
chorales in the Riemenschneider edition, a corpus containing approximately
100,000 notes and 20,000 chords. We give three separate analyses that achieve
progressively greater accuracy at the cost of making increasingly strong
assumptions about musical syntax. Although our method makes almost no use of
human input, we are able to identify both chords and keys with an accuracy of
85% or greater when compared to an expert human analysis, resulting in
annotations accurate enough to be used for a range of music-theoretical
purposes, while also being free of subjective human judgments. Our work bears
on longstanding debates about the objective reality of the structures
postulated by standard Western harmonic theory, as well as on specific
questions about the nature of Western harmonic syntax.",cs.LG cs.SD eess.AS,2024-07-30
"Data-driven Modeling for Grid Edge IBRs: A Digital Twin Perspective of
  User-Defined Models",,"Recent Odessa disturbance events have brought attention to the challenges
associated with the interaction between Inverter-Based Resources (IBRs) and the
transmission and distribution system. The NERC event diagnosis report has
highlighted several issues, emphasizing the need for continuous performance
monitoring of these IBRs by system operators. Key areas of concern include the
mismatch of control and protection performance of IBRs between the original
equipment manufacturer (OEM)-provided models and field measurements. The
inability to replicate the realistic response can result in incorrect
reliability and resilience studies. In this paper, we developed an approach on
how to emulate the behavior of an IBR using measurement data obtained for
system operators to utilize in real-time and long-term planning. Two
experiments are conducted in the phasor domain and electromagnetic transients
(EMT) domain to emulate the behavior for grid forming and grid following
inverters under various operating conditions and the effectiveness of the
proposed model is demonstrated in terms of accuracy and ease of utilizing
user-defined models (UDMs).",eess.SY cs.SY,2024-07-30
"Physical Modelling and Cancellation of External Passive Intermodulation
  in FDD MIMO",,"In this paper, the physical approach to model external (air-induced) passive
intermodulation (PIM) is presented in a frequency-division duplexing (FDD)
multiple-input multiple-output (MIMO) system with an arbitrary number of
transceiver chains. The external PIM is a special case of intermodulation
distortion (IMD), mainly generated by metallic objects possessing nonlinear
properties (""rusty bolt"" effect). Typically, such sources are located in the
near-field or transition region of the antenna array. PIM products may fall
into the receiver band of the FDD system, negatively affecting the uplink
signal. In contrast to other works, this one directly simulates the physical
external PIM. The system includes models of a point-source external PIM, a
finite-length dipole antenna, a MIMO antenna array, and a baseband multicarrier
5G NR OFDM signal. The Channel coefficients method for multi-PIM-source
compensation is replicated to verify the proposed external PIM modelling
approach. Simulation results of artificially generated PIM cancellation show
similar performance as real-life experiments. Therefore, the proposed approach
allows testing PIM compensation algorithms on large systems with many antennas
and arbitrary array structures. This eliminates the need for experiments with
real hardware at the development stage of the PIM cancellation algorithm.",cs.IT eess.SP math.IT,2024-07-30
"Enhancing Deep Hedging of Options with Implied Volatility Surface
  Feedback Information",,"We present a dynamic hedging scheme for S&P 500 options, where rebalancing
decisions are enhanced by integrating information about the implied volatility
surface dynamics. The optimal hedging strategy is obtained through a deep
policy gradient-type reinforcement learning algorithm, with a novel hybrid
neural network architecture improving the training performance. The favorable
inclusion of forward-looking information embedded in the volatility surface
allows our procedure to outperform several conventional benchmarks such as
practitioner and smiled-implied delta hedging procedures, both in simulation
and backtesting experiments.",q-fin.RM cs.LG q-fin.CP,2024-07-30
"Enhancing Semantic Similarity Understanding in Arabic NLP with Nested
  Embedding Learning",,"This work presents a novel framework for training Arabic nested embedding
models through Matryoshka Embedding Learning, leveraging multilingual,
Arabic-specific, and English-based models, to highlight the power of nested
embeddings models in various Arabic NLP downstream tasks. Our innovative
contribution includes the translation of various sentence similarity datasets
into Arabic, enabling a comprehensive evaluation framework to compare these
models across different dimensions. We trained several nested embedding models
on the Arabic Natural Language Inference triplet dataset and assessed their
performance using multiple evaluation metrics, including Pearson and Spearman
correlations for cosine similarity, Manhattan distance, Euclidean distance, and
dot product similarity. The results demonstrate the superior performance of the
Matryoshka embedding models, particularly in capturing semantic nuances unique
to the Arabic language. Results demonstrated that Arabic Matryoshka embedding
models have superior performance in capturing semantic nuances unique to the
Arabic language, significantly outperforming traditional models by up to
20-25\% across various similarity metrics. These results underscore the
effectiveness of language-specific training and highlight the potential of
Matryoshka models in enhancing semantic textual similarity tasks for Arabic
NLP.",cs.CL,2024-07-30
"FL-DECO-BC: A Privacy-Preserving, Provably Secure, and
  Provenance-Preserving Federated Learning Framework with Decentralized Oracles
  on Blockchain for VANETs",,"Vehicular Ad-Hoc Networks (VANETs) hold immense potential for improving
traffic safety and efficiency. However, traditional centralized approaches for
machine learning in VANETs raise concerns about data privacy and security.
Federated Learning (FL) offers a solution that enables collaborative model
training without sharing raw data. This paper proposes FL-DECO-BC as a novel
privacy-preserving, provably secure, and provenance-preserving federated
learning framework specifically designed for VANETs. FL-DECO-BC leverages
decentralized oracles on blockchain to securely access external data sources
while ensuring data privacy through advanced techniques. The framework
guarantees provable security through cryptographic primitives and formal
verification methods. Furthermore, FL-DECO-BC incorporates a
provenance-preserving design to track data origin and history, fostering trust
and accountability. This combination of features empowers VANETs with secure
and privacy-conscious machine-learning capabilities, paving the way for
advanced traffic management and safety applications.",cs.CR,2024-07-30
Diffusion Mechanism Design in Tree-Structured Social Network,,"We design a fixed-price auction mechanism for a seller to sell multiple items
in a tree-structured market. The buyers have independently drawn valuation from
a uniform distribution, and the seller would like to incentivize buyers to
invite more people to the auction. We prove that our mechanism is individual
rational, and incentivize compatible with regard to the buyers' action.
Furthermore, we show the approximation ratio of our mechanism to the optimal
fixed-price auction in two ways, theoretically and via Monte-Carlo simulation,
and show a high practical ratio. Finally, we discuss several factors affecting
the behavior of our mechanism and its feasibility in reality.",cs.GT,2024-07-30
"Multi-Task Learning for Few-Shot Online Adaptation under Signal Temporal
  Logic Specifications",,"Multi-task learning (MTL) seeks to improve the generalized performance of
learning specific tasks, exploiting useful information incorporated in related
tasks. As a promising area, this paper studies an MTL-based control approach
considering Signal Temporal Logic (STL). Task compliance is measured via the
Robustness Degree (RD) which is computed by using the STL semantics. A suitable
methodology is provided to solve the learning and testing stages, with an
appropriate treatment of the non-convex terms in the quadratic objective
function and using Sequential Convex Programming based on trust region update.
In the learning stage, an ensemble of tasks is generated from deterministic
goals to obtain a strong initializer for the testing stage, where related tasks
are solved with a larger impact of perturbation. The methodology demonstrates
to be robust in two dynamical systems showing results that meet the task
specifications in a few shots for the testing stage, even for highly perturbed
tasks.",eess.SY cs.SY,2024-07-30
"Domain Shift Analysis in Chest Radiographs Classification in a Veterans
  Healthcare Administration Population",,"Objectives: This study aims to assess the impact of domain shift on chest
X-ray classification accuracy and to analyze the influence of ground truth
label quality and demographic factors such as age group, sex, and study year.
Materials and Methods: We used a DenseNet121 model pretrained MIMIC-CXR dataset
for deep learning-based multilabel classification using ground truth labels
from radiology reports extracted using the CheXpert and CheXbert Labeler. We
compared the performance of the 14 chest X-ray labels on the MIMIC-CXR and
Veterans Healthcare Administration chest X-ray dataset (VA-CXR). The VA-CXR
dataset comprises over 259k chest X-ray images spanning between the years 2010
and 2022. Results: The validation of ground truth and the assessment of
multi-label classification performance across various NLP extraction tools
revealed that the VA-CXR dataset exhibited lower disagreement rates than the
MIMIC-CXR datasets. Additionally, there were notable differences in AUC scores
between models utilizing CheXpert and CheXbert. When evaluating multi-label
classification performance across different datasets, minimal domain shift was
observed in unseen datasets, except for the label ""Enlarged Cardiomediastinum.""
The study year's subgroup analyses exhibited the most significant variations in
multi-label classification model performance. These findings underscore the
importance of considering domain shifts in chest X-ray classification tasks,
particularly concerning study years. Conclusion: Our study reveals the
significant impact of domain shift and demographic factors on chest X-ray
classification, emphasizing the need for improved transfer learning and
equitable model development. Addressing these challenges is crucial for
advancing medical imaging and enhancing patient care.",eess.IV cs.AI cs.CV,2024-07-30
"PLANesT-3D: A new annotated dataset for segmentation of 3D plant point
  clouds",,"Creation of new annotated public datasets is crucial in helping advances in
3D computer vision and machine learning meet their full potential for automatic
interpretation of 3D plant models. In this paper, we introduce PLANesT-3D; a
new annotated dataset of 3D color point clouds of plants. PLANesT-3D is
composed of 34 point cloud models representing 34 real plants from three
different plant species: \textit{Capsicum annuum}, \textit{Rosa kordana}, and
\textit{Ribes rubrum}. Both semantic labels in terms of ""leaf"" and ""stem"", and
organ instance labels were manually annotated for the full point clouds. As an
additional contribution, SP-LSCnet, a novel semantic segmentation method that
is a combination of unsupervised superpoint extraction and a 3D point-based
deep learning approach is introduced and evaluated on the new dataset. Two
existing deep neural network architectures, PointNet++ and RoseSegNet were also
tested on the point clouds of PLANesT-3D for semantic segmentation.",cs.CV,2024-07-30
Private Collaborative Edge Inference via Over-the-Air Computation,,"We consider collaborative inference at the wireless edge, where each client's
model is trained independently on their local datasets. Clients are queried in
parallel to make an accurate decision collaboratively. In addition to
maximizing the inference accuracy, we also want to ensure the privacy of local
models. To this end, we leverage the superposition property of the multiple
access channel to implement bandwidth-efficient multi-user inference methods.
Specifically, we propose different methods for ensemble and multi-view
classification that exploit over-the-air computation. We show that these
schemes perform better than their orthogonal counterparts with statistically
significant differences while using fewer resources and providing privacy
guarantees. We also provide experimental results verifying the benefits of the
proposed over-the-air multi-user inference approach and perform an ablation
study to demonstrate the effectiveness of our design choices. We share the
source code of the framework publicly on Github to facilitate further research
and reproducibility.",cs.LG cs.AI cs.CR cs.IT math.IT,2024-07-30
"WIP: An Engaging Undergraduate Intro to Model Checking in Software
  Engineering Using TLA+",,"Background: In this paper, we present our initial efforts to integrate formal
methods, with a focus on model-checking specifications written in Temporal
Logic of Actions (TLA+), into computer science education, targeting
undergraduate juniors/seniors and graduate students. Formal methods can play a
key role in ensuring correct behavior of safety-critical systems, yet remain
underutilized in educational and industry contexts. Aims: We aim to (1)
qualitatively assess the state of formal methods in computer science programs,
(2) construct level-appropriate examples that could be included midway into
one's undergraduate studies, (3) demonstrate how to address successive
""failures"" through progressively stringent safety and liveness requirements,
and (4) establish an ongoing framework for assessing interest and relevance
among students. Methods: After starting with a refresher on mathematical logic,
students specify the rules of simple puzzles in TLA+ and use its included model
checker (known as TLC) to find a solution. We gradually escalate to more
complex, dynamic, event-driven systems, such as the control logic of a
microwave oven, where students will study safety and liveness requirements. We
subsequently discuss explicit concurrency, along with thread safety and
deadlock avoidance, by modeling bounded counters and buffers. Results: Our
initial findings suggest that through careful curricular design and choice of
examples and tools, it is possible to inspire and cultivate a new generation of
software engineers proficient in formal methods. Conclusions: Our initial
efforts suggest that 84% of our students had a positive experience in our
formal methods course. Future plans include a longitudinal analysis within our
own institution and proposals to partner with other institutions to explore the
effectiveness of our open-source and open-access modules.",cs.SE,2024-07-30
Event-Arguments Extraction Corpus and Modeling using BERT for Arabic,,"Event-argument extraction is a challenging task, particularly in Arabic due
to sparse linguistic resources. To fill this gap, we introduce the \hadath
corpus ($550$k tokens) as an extension of Wojood, enriched with event-argument
annotations. We used three types of event arguments: $agent$, $location$, and
$date$, which we annotated as relation types. Our inter-annotator agreement
evaluation resulted in $82.23\%$ $Kappa$ score and $87.2\%$ $F_1$-score.
Additionally, we propose a novel method for event relation extraction using
BERT, in which we treat the task as text entailment. This method achieves an
$F_1$-score of $94.01\%$. To further evaluate the generalization of our
proposed method, we collected and annotated another out-of-domain corpus (about
$80$k tokens) called \testNLI and used it as a second test set, on which our
approach achieved promising results ($83.59\%$ $F_1$-score). Last but not
least, we propose an end-to-end system for event-arguments extraction. This
system is implemented as part of SinaTools, and both corpora are publicly
available at {\small \url{https://sina.birzeit.edu/wojood}}",cs.CL,2024-07-30
"Embedding Space Selection for Detecting Memorization and Fingerprinting
  in Generative Models",,"In the rapidly evolving landscape of artificial intelligence, generative
models such as Generative Adversarial Networks (GANs) and Diffusion Models have
become cornerstone technologies, driving innovation in diverse fields from art
creation to healthcare. Despite their potential, these models face the
significant challenge of data memorization, which poses risks to privacy and
the integrity of generated content. Among various metrics of memorization
detection, our study delves into the memorization scores calculated from
encoder layer embeddings, which involves measuring distances between samples in
the embedding spaces. Particularly, we find that the memorization scores
calculated from layer embeddings of Vision Transformers (ViTs) show an notable
trend - the latter (deeper) the layer, the less the memorization measured. It
has been found that the memorization scores from the early layers' embeddings
are more sensitive to low-level memorization (e.g. colors and simple patterns
for an image), while those from the latter layers are more sensitive to
high-level memorization (e.g. semantic meaning of an image). We also observe
that, for a specific model architecture, its degree of memorization on
different levels of information is unique. It can be viewed as an inherent
property of the architecture. Building upon this insight, we introduce a unique
fingerprinting methodology. This method capitalizes on the unique distributions
of the memorization score across different layers of ViTs, providing a novel
approach to identifying models involved in generating deepfakes and malicious
content. Our approach demonstrates a marked 30% enhancement in identification
accuracy over existing baseline methods, offering a more effective tool for
combating digital misinformation.",cs.LG cs.CV,2024-07-30
"An additively optimal interpreter for approximating Kolmogorov prefix
  complexity",,"We study practical approximations to Kolmogorov prefix complexity (K) using
IMP2, a high-level programming language. Our focus is on investigating the
interpreter optimality for this language as the reference machine for the
Coding Theorem Method (CTM). A method advanced to deal with applications to
algorithmic complexity different to the popular traditional lossless
compression approach based on the principles of algorithmic probability. The
chosen model of computation is proven to be suitable for this task and a
comparison to other models and methods is performed. Our findings show that CTM
approximations using our model do not always correlate with results from
lower-level models of computation. This suggests some models may require a
larger program space to converge to Levin's universal distribution.
Furthermore, we compare CTM with an upper bound to Kolmogorov complexity and
find a strong correlation, supporting CTM's validity as an approximation method
with finer-grade resolution of K.",cs.IT math.IT,2024-07-30
Understanding Public Safety Trends in Calgary through data mining,,"This paper utilizes statistical data from various open datasets in Calgary to
to uncover patterns and insights for community crimes, disorders, and traffic
incidents. Community attributes like demographics, housing, and pet
registration were collected and analyzed through geospatial visualization and
correlation analysis. Strongly correlated features were identified using the
chi-square test, and predictive models were built using association rule mining
and machine learning algorithms. The findings suggest that crime rates are
closely linked to factors such as population density, while pet registration
has a smaller impact. This study offers valuable insights for city managers to
enhance community safety strategies.",cs.CY cs.AI,2024-07-30
"Extending choice assessments to choice functions: An algorithm for
  computing the natural extension",,"We study how to infer new choices from prior choices using the framework of
choice functions, a unifying mathematical framework for decision-making based
on sets of preference orders. In particular, we define the natural (most
conservative) extension of a given choice assessment to a coherent choice
function -- whenever possible -- and use this natural extension to make new
choices. We provide a practical algorithm for computing this natural extension
and various ways to improve scalability. Finally, we test these algorithms for
different types of choice assessments.",cs.AI math.PR,2024-07-30
An SMT-LIB Theory of Finite Fields,,"In the last few years there have been rapid developments in SMT solving for
finite fields. These include new decision procedures, new implementations of
SMT theory solvers, and new software verifiers that rely on SMT solving for
finite fields. To support interoperability in this emerging ecosystem, we
propose the SMT-LIB theory of finite field arithmetic (FFA). The theory defines
a canonical representation of finite field elements as well as definitions of
operations and predicates on finite field elements.",cs.LO,2024-07-30
Decomposed Prompting to Answer Questions on a Course Discussion Board,,"We propose and evaluate a question-answering system that uses decomposed
prompting to classify and answer student questions on a course discussion
board. Our system uses a large language model (LLM) to classify questions into
one of four types: conceptual, homework, logistics, and not answerable. This
enables us to employ a different strategy for answering questions that fall
under different types. Using a variant of GPT-3, we achieve $81\%$
classification accuracy. We discuss our system's performance on answering
conceptual questions from a machine learning course and various failure modes.",cs.CL cs.HC,2024-07-30
"Learning Stable Robot Grasping with Transformer-based Tactile Control
  Policies",,"Measuring grasp stability is an important skill for dexterous robot
manipulation tasks, which can be inferred from haptic information with a
tactile sensor. Control policies have to detect rotational displacement and
slippage from tactile feedback, and determine a re-grasp strategy in term of
location and force. Classic stable grasp task only trains control policies to
solve for re-grasp location with objects of fixed center of gravity. In this
work, we propose a revamped version of stable grasp task that optimises both
re-grasp location and gripping force for objects with unknown and moving center
of gravity. We tackle this task with a model-free, end-to-end Transformer-based
reinforcement learning framework. We show that our approach is able to solve
both objectives after training in both simulation and in a real-world setup
with zero-shot transfer. We also provide performance analysis of different
models to understand the dynamics of optimizing two opposing objectives.",cs.RO,2024-07-30
"AI Safety in Practice: Enhancing Adversarial Robustness in Multimodal
  Image Captioning",,"Multimodal machine learning models that combine visual and textual data are
increasingly being deployed in critical applications, raising significant
safety and security concerns due to their vulnerability to adversarial attacks.
This paper presents an effective strategy to enhance the robustness of
multimodal image captioning models against such attacks. By leveraging the Fast
Gradient Sign Method (FGSM) to generate adversarial examples and incorporating
adversarial training techniques, we demonstrate improved model robustness on
two benchmark datasets: Flickr8k and COCO. Our findings indicate that
selectively training only the text decoder of the multimodal architecture shows
performance comparable to full adversarial training while offering increased
computational efficiency. This targeted approach suggests a balance between
robustness and training costs, facilitating the ethical deployment of
multimodal AI systems across various domains.",cs.CV cs.AI eess.AS,2024-07-30
DKL-KAN: Scalable Deep Kernel Learning using Kolmogorov-Arnold Networks,,"The need for scalable and expressive models in machine learning is paramount,
particularly in applications requiring both structural depth and flexibility.
Traditional deep learning methods, such as multilayer perceptrons (MLP), offer
depth but lack ability to integrate structural characteristics of deep learning
architectures with non-parametric flexibility of kernel methods. To address
this, deep kernel learning (DKL) was introduced, where inputs to a base kernel
are transformed using a deep learning architecture. These kernels can replace
standard kernels, allowing both expressive power and scalability. The advent of
Kolmogorov-Arnold Networks (KAN) has generated considerable attention and
discussion among researchers in scientific domain. In this paper, we introduce
a scalable deep kernel using KAN (DKL-KAN) as an effective alternative to DKL
using MLP (DKL-MLP). Our approach involves simultaneously optimizing these
kernel attributes using marginal likelihood within a Gaussian process
framework. We analyze two variants of DKL-KAN for a fair comparison with
DKL-MLP: one with same number of neurons and layers as DKL-MLP, and another
with approximately same number of trainable parameters. To handle large
datasets, we use kernel interpolation for scalable structured Gaussian
processes (KISS-GP) for low-dimensional inputs and KISS-GP with product kernels
for high-dimensional inputs. The efficacy of DKL-KAN is evaluated in terms of
computational training time and test prediction accuracy across a wide range of
applications. Additionally, the effectiveness of DKL-KAN is also examined in
modeling discontinuities and accurately estimating prediction uncertainty. The
results indicate that DKL-KAN outperforms DKL-MLP on datasets with a low number
of observations. Conversely, DKL-MLP exhibits better scalability and higher
test prediction accuracy on datasets with large number of observations.",cs.LG stat.ML,2024-07-30
Deduction Game Framework and Information Set Entropy Search,,"We present a game framework tailored for deduction games, enabling structured
analysis from the perspective of Shannon entropy variations. Additionally, we
introduce a new forward search algorithm, Information Set Entropy Search
(ISES), which effectively solves many single-player deduction games. The ISES
algorithm, augmented with sampling techniques, allows agents to make decisions
within controlled computational resources and time constraints. Experimental
results on eight games within our framework demonstrate the significant
superiority of our method over the Single Observer Information Set Monte Carlo
Tree Search(SO-ISMCTS) algorithm under limited decision time constraints. The
entropy variation of game states in our framework enables explainable
decision-making, which can also be used to analyze the appeal of deduction
games and provide insights for game designers.",cs.AI,2024-07-30
"Optical Computing for Deep Neural Network Acceleration: Foundations,
  Recent Developments, and Emerging Directions",,"Emerging artificial intelligence applications across the domains of computer
vision, natural language processing, graph processing, and sequence prediction
increasingly rely on deep neural networks (DNNs). These DNNs require
significant compute and memory resources for training and inference.
Traditional computing platforms such as CPUs, GPUs, and TPUs are struggling to
keep up with the demands of the increasingly complex and diverse DNNs. Optical
computing represents an exciting new paradigm for light-speed acceleration of
DNN workloads. In this article, we discuss the fundamentals and
state-of-the-art developments in optical computing, with an emphasis on DNN
acceleration. Various promising approaches are described for engineering
optical devices, enhancing optical circuits, and designing architectures that
can adapt optical computing to a variety of DNN workloads. Novel techniques for
hardware/software co-design that can intelligently tune and map DNN models to
improve performance and energy-efficiency on optical computing platforms across
high performance and resource constrained embedded, edge, and IoT platforms are
also discussed. Lastly, several open problems and future directions for
research in this domain are highlighted.",cs.AR cs.LG,2024-07-30
"Amelia: A Large Model and Dataset for Airport Surface Movement
  Forecasting",,"The growing demand for air travel requires technological advancements in air
traffic management as well as mechanisms for monitoring and ensuring safe and
efficient operations. In terminal airspaces, predictive models of future
movements and traffic flows can help with proactive planning and efficient
coordination; however, varying airport topologies, and interactions with other
agents, among other factors, make accurate predictions challenging. Data-driven
predictive models have shown promise for handling numerous variables to enable
various downstream tasks, including collision risk assessment, taxi-out time
prediction, departure metering, and emission estimations. While data-driven
methods have shown improvements in these tasks, prior works lack large-scale
curated surface movement datasets within the public domain and the development
of generalizable trajectory forecasting models. In response to this, we propose
two contributions: (1) Amelia-48, a large surface movement dataset collected
using the System Wide Information Management (SWIM) Surface Movement Event
Service (SMES). With data collection beginning in Dec 2022, the dataset
provides more than a year's worth of SMES data (~30TB) and covers 48 airports
within the US National Airspace System. In addition to releasing this data in
the public domain, we also provide post-processing scripts and associated
airport maps to enable research in the forecasting domain and beyond. (2)
Amelia-TF model, a transformer-based next-token-prediction large multi-agent
multi-airport trajectory forecasting model trained on 292 days or 9.4 billion
tokens of position data encompassing 10 different airports with varying
topology. The open-sourced model is validated on unseen airports with
experiments showcasing the different prediction horizon lengths, ego-agent
selection strategies, and training recipes to demonstrate the generalization
capabilities.",cs.LG,2024-07-30
LFFR: Logistic Function For (multi-output) Regression,,"In this manuscript, we extend our previous work on privacy-preserving
regression to address multi-output regression problems using data encrypted
under a fully homomorphic encryption scheme. We build upon the simplified fixed
Hessian approach for linear and ridge regression and adapt our novel LFFR
algorithm, initially designed for single-output logistic regression, to handle
multiple outputs. We further refine the constant simplified Hessian method for
the multi-output context, ensuring computational efficiency and robustness.
Evaluations on multiple real-world datasets demonstrate the effectiveness of
our multi-output LFFR algorithm, highlighting its capability to maintain
privacy while achieving high predictive accuracy. Normalizing both data and
target predictions remains essential for optimizing homomorphic encryption
parameters, confirming the practicality of our approach for secure and
efficient multi-output regression tasks.",cs.CR,2024-07-30
"Multi-task Photonic Reservoir Computing: Wavelength Division
  Multiplexing for Parallel Computing with a Silicon Microring Resonator",,"Nowadays, as the ever-increasing demand for more powerful computing resources
continues, alternative advanced computing paradigms are under extensive
investigation. Significant effort has been made to deviate from conventional
Von Neumann architectures. In-memory computing has emerged in the field of
electronics as a possible solution to the infamous bottleneck between memory
and computing processors, which reduces the effective throughput of data. In
photonics, novel schemes attempt to collocate the computing processor and
memory in a single device. Photonics offers the flexibility of multiplexing
streams of data not only spatially and in time, but also in frequency or,
equivalently, in wavelength, which makes it highly suitable for parallel
computing. Here, we numerically show the use of time and wavelength division
multiplexing (WDM) to solve four independent tasks at the same time in a single
photonic chip, serving as a proof of concept for our proposal. The system is a
time-delay reservoir computing (TDRC) based on a microring resonator (MRR). The
addressed tasks cover different applications: Time-series prediction, waveform
signal classification, wireless channel equalization, and radar signal
prediction. The system is also tested for simultaneous computing of up to 10
instances of the same task, exhibiting excellent performance. The footprint of
the system is reduced by using time-division multiplexing of the nodes that act
as the neurons of the studied neural network scheme. WDM is used for the
parallelization of wavelength channels, each addressing a single task. By
adjusting the input power and frequency of each optical channel, we can achieve
levels of performance for each of the tasks that are comparable to those quoted
in state-of-the-art reports focusing on single-task operation...",cs.NE cs.AI cs.LG physics.optics,2024-07-30
GenRec: Generative Personalized Sequential Recommendation,,"Sequential recommendation is a task to capture hidden user preferences from
historical user item interaction data. Significant progress has been made in
this domain by leveraging classification based learning methods. Inspired by
the recent paradigm of 'pretrain, prompt and predict' in NLP, we consider
sequential recommendation as a sequence to sequence generation task and propose
a novel model named Generative Recommendation (GenRec). Unlike classification
based models that learn explicit user and item representations, GenRec utilizes
the sequence modeling capability of Transformer and adopts the masked item
prediction objective to effectively learn the hidden bidirectional sequential
patterns. Different from existing generative sequential recommendation models,
GenRec does not rely on manually designed hard prompts. The input to GenRec is
textual user item sequence and the output is top ranked next items. Moreover,
GenRec is lightweight and requires only a few hours to train effectively in
low-resource settings, making it highly applicable to real-world scenarios and
helping to democratize large language models in the sequential recommendation
domain. Our extensive experiments have demonstrated that GenRec generalizes on
various public real-world datasets and achieves state-of-the-art results. Our
experiments also validate the effectiveness of the the proposed masked item
prediction objective that improves the model performance by a large margin.",cs.IR cs.AI cs.CL cs.LG,2024-07-30
Functional ISS-Driven Verification of Superscalar RISC-V Processors,,"A time-efficient and comprehensive verification is a fundamental part of the
design process for modern computing platforms, and it becomes ever more
important and critical to optimize as the latter get ever more complex.
SupeRFIVe is a methodology for the functional verification of superscalar
processors that leverages an instruction set simulator to validate their
correctness according to a simulation-based approach, interfacing a testbench
for the design under test with the instruction set simulator by means of socket
communication. We demonstrate the effectiveness of the SupeRFIVe methodology by
applying it to verify the functional correctness of a RISC-V dual-issue
superscalar CPU, leveraging the state-of-the-art RISC-V instruction set
simulator Spike and executing a set of benchmark applications from the open
literature.",cs.AR,2024-07-30
"Analyzing Customer-Facing Vendor Experiences with Time Series
  Forecasting and Monte Carlo Techniques",,"eBay partners with external vendors, which allows customers to freely select
a vendor to complete their eBay experiences. However, vendor outages can hinder
customer experiences. Consequently, eBay can disable a problematic vendor to
prevent customer loss. Disabling the vendor too late risks losing customers
willing to switch to other vendors, while disabling it too early risks losing
those unwilling to switch. In this paper, we propose a data-driven solution to
answer whether eBay should disable a problematic vendor and when to disable it.
Our solution involves forecasting customer behavior. First, we use a
multiplicative seasonality model to represent behavior if all vendors are fully
functioning. Next, we use a Monte Carlo simulation to represent behavior if the
problematic vendor remains enabled. Finally, we use a linear model to represent
behavior if the vendor is disabled. By comparing these forecasts, we determine
the optimal time for eBay to disable the problematic vendor.",stat.ML cs.LG stat.CO,2024-07-30
"Diffusion-Based Generation of Neural Activity from Disentangled Latent
  Codes",,"Recent advances in recording technology have allowed neuroscientists to
monitor activity from thousands of neurons simultaneously. Latent variable
models are increasingly valuable for distilling these recordings into compact
and interpretable representations. Here we propose a new approach to neural
data analysis that leverages advances in conditional generative modeling to
enable the unsupervised inference of disentangled behavioral variables from
recorded neural activity. Our approach builds on InfoDiffusion, which augments
diffusion models with a set of latent variables that capture important factors
of variation in the data. We apply our model, called Generating Neural
Observations Conditioned on Codes with High Information (GNOCCHI), to time
series neural data and test its application to synthetic and biological
recordings of neural activity during reaching. In comparison to a VAE-based
sequential autoencoder, GNOCCHI learns higher-quality latent spaces that are
more clearly structured and more disentangled with respect to key behavioral
variables. These properties enable accurate generation of novel samples (unseen
behavioral conditions) through simple linear traversal of the latent spaces
produced by GNOCCHI. Our work demonstrates the potential of unsupervised,
information-based models for the discovery of interpretable latent spaces from
neural data, enabling researchers to generate high-quality samples from unseen
conditions.",cs.LG q-bio.NC,2024-07-30
"Lattice operations for the stable set in substitutable matching markets
  via re-equilibration dynamics",,"We compute the lattice operations for the (pairwise) stable set in two-sided
matching markets where only substitutability on agents' choice functions is
imposed. To do this, we use Tarski operators defined on the lattices of
worker-quasi-stable and firm-quasi-stable matchings. These operators resemble
lay-off and vacancy chain dynamics, respectively. First, we compute the lattice
operations in the many-to-one model. Then, we extend these operations to a
many-to-many model with substitutable choice functions on one side and
responsive preferences on the other, via a morphism that relates many-to-one
with many-to-many matchings in a natural way. Finally, we present the lattice
operations in the many-to-many model with substitutable choice functions on
both sides.",econ.TH cs.GT,2024-07-30
"A Survey on Exploratory Spatiotemporal Visual Analytics Approaches for
  Climate Science",,"Climate science produces a wealth of complex, high-dimensional, multivariate
data from observations and numerical models. These data are critical for
understanding climate changes and their socioeconomic impacts. Climate
scientists are continuously evaluating output from numerical models against
observations. This model evaluation process provides useful guidance to improve
the numerical models and subsequent climate projections. Exploratory visual
analytics systems possess the potential to significantly reduce the burden on
scientists for traditional spatiotemporal analyses. In addition, technology and
infrastructure advancements are further facilitating broader access to climate
data. Climate scientists today can access climate data in distributed analytic
environments and render exploratory visualizations for analyses. Efforts are
ongoing to optimize the computational efficiency of spatiotemporal analyses to
enable efficient exploration of massive data. These advances present further
opportunities for the visualization community to innovate over the full
landscape of challenges and requirements raised by scientists. In this report,
we provide a comprehensive review of the challenges, requirements, and current
approaches for exploratory spatiotemporal visual analytics solutions for
climate data. We categorize the visual analytic techniques, systems, and tools
presented in the relevant literature based on task requirements, data sources,
statistical techniques, interaction methods, visualization techniques,
performance evaluation methods, and application domains. Moreover, our analytic
review identifies trends, limitations, and key challenges in visual analysis.
This report will advance future research activities in climate visualizations
and enables the end-users of climate data to identify effective climate change
mitigation strategies.",cs.HC,2024-07-30
Rolling in the deep of cognitive and AI biases,,"Nowadays, we delegate many of our decisions to Artificial Intelligence (AI)
that acts either in solo or as a human companion in decisions made to support
several sensitive domains, like healthcare, financial services and law
enforcement. AI systems, even carefully designed to be fair, are heavily
criticized for delivering misjudged and discriminated outcomes against
individuals and groups. Numerous work on AI algorithmic fairness is devoted on
Machine Learning pipelines which address biases and quantify fairness under a
pure computational view. However, the continuous unfair and unjust AI outcomes,
indicate that there is urgent need to understand AI as a sociotechnical system,
inseparable from the conditions in which it is designed, developed and
deployed. Although, the synergy of humans and machines seems imperative to make
AI work, the significant impact of human and societal factors on AI bias is
currently overlooked. We address this critical issue by following a radical new
methodology under which human cognitive biases become core entities in our AI
fairness overview. Inspired by the cognitive science definition and taxonomy of
human heuristics, we identify how harmful human actions influence the overall
AI lifecycle, and reveal human to AI biases hidden pathways. We introduce a new
mapping, which justifies the human heuristics to AI biases reflections and we
detect relevant fairness intensities and inter-dependencies. We envision that
this approach will contribute in revisiting AI fairness under deeper
human-centric case studies, revealing hidden biases cause and effects.",cs.AI cs.CY,2024-07-30
"Quantum advantage from measurement-induced entanglement in random
  shallow circuits",,"We study random constant-depth quantum circuits in a two-dimensional
architecture. While these circuits only produce entanglement between nearby
qubits on the lattice, long-range entanglement can be generated by measuring a
subset of the qubits of the output state. It is conjectured that this
long-range measurement-induced entanglement (MIE) proliferates when the circuit
depth is at least a constant critical value. For circuits composed of
Haar-random two-qubit gates, it is also believed that this coincides with a
quantum advantage phase transition in the classical hardness of sampling from
the output distribution. Here we provide evidence for a quantum advantage phase
transition in the setting of random Clifford circuits. Our work extends the
scope of recent separations between the computational power of constant-depth
quantum and classical circuits, demonstrating that this kind of advantage is
present in canonical random circuit sampling tasks. In particular, we show that
in any architecture of random shallow Clifford circuits, the presence of
long-range MIE gives rise to an unconditional quantum advantage. In contrast,
any depth-d 2D quantum circuit that satisfies a short-range MIE property can be
classically simulated efficiently and with depth O(d). Finally, we introduce a
two-dimensional, depth-2, ""coarse-grained"" circuit architecture, composed of
random Clifford gates acting on O(log n) qubits, for which we prove the
existence of long-range MIE and establish an unconditional quantum advantage.",quant-ph cond-mat.stat-mech cs.CC,2024-07-30
"LoRaWAN Based Dynamic Noise Mapping with Machine Learning for Urban
  Noise Enforcement",,"Static noise maps depicting long-term noise levels over wide areas are
valuable urban planning assets for municipalities in decreasing noise exposure
of residents. However, non-traffic noise sources with transient behavior, which
people complain frequently, are usually ignored by static maps. We propose here
a dynamic noise mapping approach using the data collected via low-power
wide-area network (LPWAN, specifically LoRaWAN) based internet of things (IoT)
infrastructure, which is one of the most common communication backbones for
smart cities. Noise mapping based on LPWAN is challenging due to the low data
rates of these protocols. The proposed dynamic noise mapping approach
diminishes the negative implications of data rate limitations using machine
learning (ML) for event and location prediction of non-traffic sources based on
the scarce data. The strength of these models lies in their consideration of
the spatial variance in acoustic behavior caused by the buildings in urban
settings. The effectiveness of the proposed method and the accuracy of the
resulting dynamic maps are evaluated in field tests. The results show that the
proposed system can decrease the map error caused by non-traffic sources up to
51% and can stay effective under significant packet losses.",cs.AI cs.NI,2024-07-30
On the Uncrossed Number of Graphs,,"Visualizing a graph $G$ in the plane nicely, for example, without crossings,
is unfortunately not always possible. To address this problem, Masa\v{r}\'ik
and Hlin\v{e}n\'y [GD 2023] recently asked for each edge of $G$ to be drawn
without crossings while allowing multiple different drawings of $G$. More
formally, a collection $\mathcal{D}$ of drawings of $G$ is uncrossed if, for
each edge $e$ of $G$, there is a drawing in $\mathcal{D}$ such that $e$ is
uncrossed. The uncrossed number $\mathrm{unc}(G)$ of $G$ is then the minimum
number of drawings in some uncrossed collection of $G$.
  No exact values of the uncrossed numbers have been determined yet, not even
for simple graph classes. In this paper, we provide the exact values for
uncrossed numbers of complete and complete bipartite graphs, partly confirming
and partly refuting a conjecture posed by Hlin\v{e}n\'y and Masa\v{r}\'ik. We
also present a strong general lower bound on $\mathrm{unc}(G)$ in terms of the
number of vertices and edges of $G$. Moreover, we prove NP-hardness of the
related problem of determining the edge crossing number of a graph $G$, which
is the smallest number of edges of $G$ taken over all drawings of $G$ that
participate in a crossing. This problem was posed as open by Schaefer in his
book [Crossing Numbers of Graphs 2018].",math.CO cs.CG cs.DM,2024-07-30
"Algorithm-Assisted Decision Making and Racial Disparities in Housing: A
  Study of the Allegheny Housing Assessment Tool",,"The demand for housing assistance across the United States far exceeds the
supply, leaving housing providers the task of prioritizing clients for receipt
of this limited resource. To be eligible for federal funding, local
homelessness systems are required to implement assessment tools as part of
their prioritization processes. The Vulnerability Index Service Prioritization
Decision Assistance Tool (VI-SPDAT) is the most commonly used assessment tool
nationwide. Recent studies have criticized the VI-SPDAT as exhibiting racial
bias, which may lead to unwarranted racial disparities in housing provision.
Such criticisms have led certain jurisdictions to develop alternative tools.
Using data from one such prioritization tool, called the Allegheny Housing
Assessment (AHA), we use descriptive and quantitative analysis to assess
whether the replacement of the VI-SPDAT with the AHA impacts racial disparities
in housing allocation. We find that the VI-SPDAT tended to assign higher risk
scores to white clients and lower risk scores to Black clients, and that white
clients were served at a higher rates pre-AHA deployment. While post-deployment
service decisions became better aligned with the AHA score, and the
distribution of AHA scores is similar across racial groups, we do not find
evidence of a corresponding decrease in disparities in service rates. We
attribute the persistent disparity to the use of Alt-AHA, a survey-based tool
that is used in cases of low data quality, as well as group differences in
eligibility-related factors, such as chronic homelessness and veteran status.
We discuss the implications for housing service systems seeking to reduce
racial disparities in their service delivery.",cs.HC econ.GN q-fin.EC,2024-07-30
Self-Supervised Models in Automatic Whispered Speech Recognition,,"In automatic speech recognition, any factor that alters the acoustic
properties of speech can pose a challenge to the system's performance. This
paper presents a novel approach for automatic whispered speech recognition in
the Irish dialect using the self-supervised WavLM model. Conventional automatic
speech recognition systems often fail to accurately recognise whispered speech
due to its distinct acoustic properties and the scarcity of relevant training
data. To address this challenge, we utilized a pre-trained WavLM model,
fine-tuned with a combination of whispered and normal speech data from the
wTIMIT and CHAINS datasets, which include the English language in Singaporean
and Irish dialects, respectively. Our baseline evaluation with the OpenAI
Whisper model highlighted its limitations, achieving a Word Error Rate (WER) of
18.8% on whispered speech. In contrast, the proposed WavLM-based system
significantly improved performance, achieving a WER of 9.22%. These results
demonstrate the efficacy of our approach in recognising whispered speech and
underscore the importance of tailored acoustic modeling for robust automatic
speech recognition systems. This study provides valuable insights into
developing effective automatic speech recognition solutions for challenging
speech affected by whisper and dialect. The source codes for this paper are
freely available.",eess.AS cs.SD,2024-07-30
Distribution-Aware Replay for Continual MRI Segmentation,,"Medical image distributions shift constantly due to changes in patient
population and discrepancies in image acquisition. These distribution changes
result in performance deterioration; deterioration that continual learning aims
to alleviate. However, only adaptation with data rehearsal strategies yields
practically desirable performance for medical image segmentation. Such
rehearsal violates patient privacy and, as most continual learning approaches,
overlooks unexpected changes from out-of-distribution instances. To transcend
both of these challenges, we introduce a distribution-aware replay strategy
that mitigates forgetting through auto-encoding of features, while
simultaneously leveraging the learned distribution of features to detect model
failure. We provide empirical corroboration on hippocampus and prostate MRI
segmentation.",eess.IV cs.CV,2024-07-30
"NeuroSEM: A hybrid framework for simulating multiphysics problems by
  coupling PINNs and spectral elements",,"Multiphysics problems that are characterized by complex interactions among
fluid dynamics, heat transfer, structural mechanics, and electromagnetics, are
inherently challenging due to their coupled nature. While experimental data on
certain state variables may be available, integrating these data with numerical
solvers remains a significant challenge. Physics-informed neural networks
(PINNs) have shown promising results in various engineering disciplines,
particularly in handling noisy data and solving inverse problems. However,
their effectiveness in forecasting nonlinear phenomena in multiphysics regimes
is yet to be fully established. This study introduces NeuroSEM, a hybrid
framework integrating PINNs with the high-fidelity Spectral Element Method
(SEM) solver, Nektar++. NeuroSEM leverages strengths of both PINNs and SEM,
providing robust solutions for multiphysics problems. PINNs are trained to
assimilate data and model physical phenomena in specific subdomains, which are
then integrated into Nektar++. We demonstrate the efficiency and accuracy of
NeuroSEM for thermal convection in cavity flow and flow past a cylinder. The
framework effectively handles data assimilation by addressing those subdomains
and state variables where data are available. We applied NeuroSEM to the
Rayleigh-B\'enard convection system, including cases with missing thermal
boundary conditions. Our results indicate that NeuroSEM accurately models the
physical phenomena and assimilates the data within the specified subdomains.
The framework's plug-and-play nature facilitates its extension to other
multiphysics or multiscale problems. Furthermore, NeuroSEM is optimized for an
efficient execution on emerging integrated GPU-CPU architectures. This hybrid
approach enhances the accuracy and efficiency of simulations, making it a
powerful tool for tackling complex engineering challenges in various scientific
domains.",cs.LG physics.flu-dyn,2024-07-30
"Hidden Cyber-Physical Contingency Identification, Classification and
  Evaluation in Modern Power Systems",,"This paper introduces an advanced stochastic hybrid system modeling framework
for modern power systems (MPS) to identify, classify, and evaluate hidden
contingencies, which cannot be detected by normal observation sensors. The
stochastic hybrid system (SHS) model is designed to capture the dynamics of the
internal states of individual nodes, considering their structural properties,
and coupling variables under various local and network-level contingencies.
Hidden contingencies are identified using a probing approach that measures
changes in the eigenvalues of the SHS model and detects deviations from normal
operation. Next, contingencies are categorized into three distinct groups
according to their impact on MPS: physical contingencies, control network
contingencies, and sensing and measurement network contingencies. This
classification enables a proactive evaluation of contingencies. The
practicality and efficacy of the proposed methodology are validated through
simulation experiments on the electrical network of two real-world systems.
These simulations underscore the approach's capacity to enhance the resilience
of power systems against a spectrum of hidden contingencies.",eess.SY cs.SY,2024-07-30
DeepBaR: Fault Backdoor Attack on Deep Neural Network Layers,,"Machine Learning using neural networks has received prominent attention
recently because of its success in solving a wide variety of computational
tasks, in particular in the field of computer vision. However, several works
have drawn attention to potential security risks involved with the training and
implementation of such networks. In this work, we introduce DeepBaR, a novel
approach that implants backdoors on neural networks by faulting their behavior
at training, especially during fine-tuning. Our technique aims to generate
adversarial samples by optimizing a custom loss function that mimics the
implanted backdoors while adding an almost non-visible trigger in the image. We
attack three popular convolutional neural network architectures and show that
DeepBaR attacks have a success rate of up to 98.30\%. Furthermore, DeepBaR does
not significantly affect the accuracy of the attacked networks after deployment
when non-malicious inputs are given. Remarkably, DeepBaR allows attackers to
choose an input that looks similar to a given class, from a human perspective,
but that will be classified as belonging to an arbitrary target class.",cs.LG cs.CR cs.CV,2024-07-30
Predicting Software Reliability in Softwarized Networks,,"Providing high quality software and evaluating the software reliability in
softwarized networks are crucial for vendors and customers. These networks rely
on open source code, which are sensitive to contain high number of bugs. Both,
the knowledge about the code of previous releases as well as the bug history of
the particular project can be used to evaluate the software reliability of a
new software release based on SRGM. In this work a framework to predict the
number of the bugs of a new release, as well as other reliability parameters,
is proposed. An exemplary implementation of this framework to two particular
open source projects, is described in detail. The difference between the
prediction accuracy of the two projects is presented. Different alternatives to
increase the prediction accuracy are proposed and compared in this paper.",cs.SE cs.NI,2024-07-30
AI methods for approximate compiling of unitaries,,"This paper explores artificial intelligence (AI) methods for the approximate
compiling of unitaries, focusing on the use of fixed two-qubit gates and
arbitrary single-qubit rotations typical in superconducting hardware. Our
approach involves three main stages: identifying an initial template that
approximates the target unitary, predicting initial parameters for this
template, and refining these parameters to maximize the fidelity of the
circuit. We propose AI-driven approaches for the first two stages, with a deep
learning model that suggests initial templates and an autoencoder-like model
that suggests parameter values, which are refined through gradient descent to
achieve the desired fidelity. We demonstrate the method on 2 and 3-qubit
unitaries, showcasing promising improvements over exhaustive search and random
parameter initialization. The results highlight the potential of AI to enhance
the transpiling process, supporting more efficient quantum computations on
current and future quantum hardware.",quant-ph cs.AI,2024-07-30
"Assessing Programming Task Difficulty for Efficient Evaluation of Large
  Language Models",,"Large Language Models (LLMs) show promising potential in Software
Engineering, especially for code-related tasks like code completion and code
generation. LLMs' evaluation is generally centred around general metrics
computed over benchmarks. While painting a macroscopic view of the benchmarks
and of the LLMs' capacity, it is unclear how each programming task in these
benchmarks assesses the capabilities of the LLMs. In particular, the difficulty
level of the tasks in the benchmarks is not reflected in the score used to
report the performance of the model. Yet, a model achieving a 90% score on a
benchmark of predominantly easy tasks is likely less capable than a model
achieving a 90% score on a benchmark containing predominantly difficult tasks.
This paper devises a framework, HardEval, for assessing task difficulty for
LLMs and crafting new tasks based on identified hard tasks. The framework uses
a diverse array of prompts for a single task across multiple LLMs to obtain a
difficulty score for each task of a benchmark. Using two code generation
benchmarks, HumanEval+ and ClassEval, we show that HardEval can reliably
identify the hard tasks within those benchmarks, highlighting that only 21% of
HumanEval+ and 27% of ClassEval tasks are hard for LLMs. Through our analysis
of task difficulty, we also characterize 6 practical hard task topics which we
used to generate new hard tasks. Orthogonal to current benchmarking evaluation
efforts, HardEval can assist researchers and practitioners in fostering better
assessments of LLMs. The difficulty score can be used to identify hard tasks
within existing benchmarks. This, in turn, can be leveraged to generate more
hard tasks centred around specific topics either for evaluation or improvement
of LLMs. HardEval generalistic approach can be applied to other domains such as
code completion or Q/A.",cs.SE cs.AI,2024-07-30
"Advancing Vietnamese Visual Question Answering with Transformer and
  Convolutional Integration",,"Visual Question Answering (VQA) has recently emerged as a potential research
domain, captivating the interest of many in the field of artificial
intelligence and computer vision. Despite the prevalence of approaches in
English, there is a notable lack of systems specifically developed for certain
languages, particularly Vietnamese. This study aims to bridge this gap by
conducting comprehensive experiments on the Vietnamese Visual Question
Answering (ViVQA) dataset, demonstrating the effectiveness of our proposed
model. In response to community interest, we have developed a model that
enhances image representation capabilities, thereby improving overall
performance in the ViVQA system. Specifically, our model integrates the
Bootstrapping Language-Image Pre-training with frozen unimodal models (BLIP-2)
and the convolutional neural network EfficientNet to extract and process both
local and global features from images. This integration leverages the strengths
of transformer-based architectures for capturing comprehensive contextual
information and convolutional networks for detailed local features. By freezing
the parameters of these pre-trained models, we significantly reduce the
computational cost and training time, while maintaining high performance. This
approach significantly improves image representation and enhances the
performance of existing VQA systems. We then leverage a multi-modal fusion
module based on a general-purpose multi-modal foundation model (BEiT-3) to fuse
the information between visual and textual features. Our experimental findings
demonstrate that our model surpasses competing baselines, achieving promising
performance. This is particularly evident in its accuracy of $71.04\%$ on the
test set of the ViVQA dataset, marking a significant advancement in our
research area. The code is available at https://github.com/nngocson2002/ViVQA.",cs.CV cs.CL,2024-07-30
"Towards an Integrated Performance Framework for Fire Science and
  Management Workflows",,"Reliable performance metrics are necessary prerequisites to building
large-scale end-to-end integrated workflows for collaborative scientific
research, particularly within context of use-inspired decision making platforms
with many concurrent users and when computing real-time and urgent results
using large data. This work is a building block for the National Data Platform,
which leverages multiple use-cases including the WIFIRE Data and Model Commons
for wildfire behavior modeling and the EarthScope Consortium for collaborative
geophysical research. This paper presents an artificial intelligence and
machine learning (AI/ML) approach to performance assessment and optimization of
scientific workflows. An associated early AI/ML framework spanning performance
data collection, prediction and optimization is applied to wildfire science
applications within the WIFIRE BurnPro3D (BP3D) platform for proactive fire
management and mitigation.",cs.LG cs.PF,2024-07-30
"TMA-Grid: An open-source, zero-footprint web application for FAIR Tissue
  MicroArray De-arraying",,"Background:
  Tissue Microarrays (TMAs) significantly increase analytical efficiency in
histopathology and large-scale epidemiologic studies by allowing multiple
tissue cores to be scanned on a single slide. The individual cores can be
digitally extracted and then linked to metadata for analysis in a process known
as de-arraying. However, TMAs often contain core misalignments and artifacts
due to assembly errors, which can adversely affect the reliability of the
extracted cores during the de-arraying process. Moreover, conventional
approaches for TMA de-arraying rely on desktop solutions.Therefore, a robust
yet flexible de-arraying method is crucial to account for these inaccuracies
and ensure effective downstream analyses.
  Results:
  We developed TMA-Grid, an in-browser, zero-footprint, interactive web
application for TMA de-arraying. This web application integrates a
convolutional neural network for precise tissue segmentation and a grid
estimation algorithm to match each identified core to its expected location.
The application emphasizes interactivity, allowing users to easily adjust
segmentation and gridding results. Operating entirely in the web-browser,
TMA-Grid eliminates the need for downloads or installations and ensures data
privacy. Adhering to FAIR principles (Findable, Accessible, Interoperable, and
Reusable), the application and its components are designed for seamless
integration into TMA research workflows.
  Conclusions:
  TMA-Grid provides a robust, user-friendly solution for TMA dearraying on the
web. As an open, freely accessible platform, it lays the foundation for
collaborative analyses of TMAs and similar histopathology imaging data.
Availability: Web application: https://episphere.github.io/tma-grid Code:
https://github.com/episphere/tma-grid Tutorial: https://youtu.be/miajqyw4BVk",q-bio.TO cs.AI cs.CV,2024-07-30
"GNUMAP: A Parameter-Free Approach to Unsupervised Dimensionality
  Reduction via Graph Neural Networks",,"With the proliferation of Graph Neural Network (GNN) methods stemming from
contrastive learning, unsupervised node representation learning for graph data
is rapidly gaining traction across various fields, from biology to molecular
dynamics, where it is often used as a dimensionality reduction tool. However,
there remains a significant gap in understanding the quality of the
low-dimensional node representations these methods produce, particularly beyond
well-curated academic datasets. To address this gap, we propose here the first
comprehensive benchmarking of various unsupervised node embedding techniques
tailored for dimensionality reduction, encompassing a range of manifold
learning tasks, along with various performance metrics. We emphasize the
sensitivity of current methods to hyperparameter choices -- highlighting a
fundamental issue as to their applicability in real-world settings where there
is no established methodology for rigorous hyperparameter selection. Addressing
this issue, we introduce GNUMAP, a robust and parameter-free method for
unsupervised node representation learning that merges the traditional UMAP
approach with the expressivity of the GNN framework. We show that GNUMAP
consistently outperforms existing state-of-the-art GNN embedding methods in a
variety of contexts, including synthetic geometric datasets, citation networks,
and real-world biomedical data -- making it a simple but reliable
dimensionality reduction tool.",cs.LG,2024-07-30
Bug Analysis Towards Bug Resolution Time Prediction,,"Bugs are inevitable in software development, and their reporting in open
repositories can enhance software transparency and reliability assessment. This
study aims to extract information from the issue tracking system Jira and
proposes a methodology to estimate resolution time for new bugs. The
methodology is applied to network project ONAP, addressing concerns of network
operators and manufacturers. This research provides insights into bug
resolution times and related aspects in network softwarization projects.",cs.SE cs.AI,2024-07-30
Informed Correctors for Discrete Diffusion Models,,"Discrete diffusion modeling is a promising framework for modeling and
generating data in discrete spaces. To sample from these models, different
strategies present trade-offs between computation and sample quality. A
predominant sampling strategy is predictor-corrector $\tau$-leaping, which
simulates the continuous time generative process with discretized predictor
steps and counteracts the accumulation of discretization error via corrector
steps. However, for absorbing state diffusion, an important class of discrete
diffusion models, the standard forward-backward corrector can be ineffective in
fixing such errors, resulting in subpar sample quality. To remedy this problem,
we propose a family of informed correctors that more reliably counteracts
discretization error by leveraging information learned by the model. For
further efficiency gains, we also propose $k$-Gillespie's, a sampling algorithm
that better utilizes each model evaluation, while still enjoying the speed and
flexibility of $\tau$-leaping. Across several real and synthetic datasets, we
show that $k$-Gillespie's with informed correctors reliably produces higher
quality samples at lower computational cost.",cs.LG cs.AI,2024-07-30
"VITAL: Visual Teleoperation to Enhance Robot Learning through
  Human-in-the-Loop Corrections",,"Imitation Learning (IL) has emerged as a powerful approach in robotics,
allowing robots to acquire new skills by mimicking human actions. Despite its
potential, the data collection process for IL remains a significant challenge
due to the logistical difficulties and high costs associated with obtaining
high-quality demonstrations. To address these issues, we propose a low-cost
visual teleoperation system for bimanual manipulation tasks, called VITAL. Our
approach leverages affordable hardware and visual processing techniques to
collect demonstrations, which are then augmented to create extensive training
datasets for imitation learning. We enhance the generalizability and robustness
of the learned policies by utilizing both real and simulated environments and
human-in-the-loop corrections. We evaluated our method through several rounds
of experiments in simulated and real-robot settings, focusing on tasks of
varying complexity, including bottle collecting, stacking objects, and
hammering. Our experimental results validate the effectiveness of our approach
in learning robust robot policies from simulated data, significantly improved
by human-in-the-loop corrections and real-world data integration. Additionally,
we demonstrate the framework's capability to generalize to new tasks, such as
setting a drink tray, showcasing its adaptability and potential for handling a
wide range of real-world bimanual manipulation tasks. A video of the
experiments can be found at: https://youtu.be/YeVAMRqRe64?si=R179xDlEGc7nPu8i",cs.RO cs.AI cs.CV,2024-07-30
Four-Axis Adaptive Fingers Hand for Object Insertion: FAAF Hand,,"Robots operating in the real world face significant but unavoidable issues in
object localization that must be dealt with. A typical approach to address this
is the addition of compliance mechanisms to hardware to absorb and compensate
for some of these errors. However, for fine-grained manipulation tasks, the
location and choice of appropriate compliance mechanisms are critical for
success. For objects to be inserted in a target site on a flat surface, the
object must first be successfully aligned with the opening of the slot, as well
as correctly oriented along its central axis, before it can be inserted. We
developed the Four-Axis Adaptive Finger Hand (FAAF hand) that is equipped with
fingers that can passively adapt in four axes (x, y, z, yaw) enabling it to
perform insertion tasks including lid fitting in the presence of significant
localization errors. Furthermore, this adaptivity allows the use of simple
control methods without requiring contact sensors or other devices. Our results
confirm the ability of the FAAF hand on challenging insertion tasks of square
and triangle-shaped pegs (or prisms) and placing of container lids in the
presence of position errors in all directions and rotational error along the
object's central axis, using a simple control scheme.",cs.RO,2024-07-30
"Adaptive Pre-training Data Detection for Large Language Models via
  Surprising Tokens",,"While large language models (LLMs) are extensively used, there are raising
concerns regarding privacy, security, and copyright due to their opaque
training data, which brings the problem of detecting pre-training data on the
table. Current solutions to this problem leverage techniques explored in
machine learning privacy such as Membership Inference Attacks (MIAs), which
heavily depend on LLMs' capability of verbatim memorization. However, this
reliance presents challenges, especially given the vast amount of training data
and the restricted number of effective training epochs. In this paper, we
propose an adaptive pre-training data detection method which alleviates this
reliance and effectively amplify the identification. Our method adaptively
locates \textit{surprising tokens} of the input. A token is surprising to a LLM
if the prediction on the token is ""certain but wrong"", which refers to low
Shannon entropy of the probability distribution and low probability of the
ground truth token at the same time. By using the prediction probability of
surprising tokens to measure \textit{surprising}, the detection method is
achieved based on the simple hypothesis that seeing seen data is less
surprising for the model compared with seeing unseen data. The method can be
applied without any access to the the pre-training data corpus or additional
training like reference models. Our approach exhibits a consistent enhancement
compared to existing methods in diverse experiments conducted on various
benchmarks and models, achieving a maximum improvement of 29.5\%. We also
introduce a new benchmark Dolma-Book developed upon a novel framework, which
employs book data collected both before and after model training to provide
further evaluation.",cs.CL cs.CR cs.LG,2024-07-30
Lifelong Person Search,,"Person search is the task to localize a query person in gallery datasets of
scene images. Existing methods have been mainly developed to handle a single
target dataset only, however diverse datasets are continuously given in
practical applications of person search. In such cases, they suffer from the
catastrophic knowledge forgetting in the old datasets when trained on new
datasets. In this paper, we first introduce a novel problem of lifelong person
search (LPS) where the model is incrementally trained on the new datasets while
preserving the knowledge learned in the old datasets. We propose an end-to-end
LPS framework that facilitates the knowledge distillation to enforce the
consistency learning between the old and new models by utilizing the prototype
features of the foreground persons as well as the hard background proposals in
the old domains. Moreover, we also devise the rehearsal-based instance matching
to further improve the discrimination ability in the old domains by using the
unlabeled person instances additionally. Experimental results demonstrate that
the proposed method achieves significantly superior performance of both the
detection and re-identification to preserve the knowledge learned in the old
domains compared with the existing methods.",cs.CV cs.AI,2024-07-30
Responsive ML inference in multi-tenanted environments using AQUA,,"Modern model serving engines infer prompts on large language models in
batches. While batch processing prompts leads to high inference throughput, it
delays responding to requests that do not fit in a batch, potentially starving
them. We propose that fair scheduling prompts for inference by time-sharing
GPUs cycles, instead of batch processing them, is key to preventing prompt
starvation and achieving responsive inference. However, time-shared prompt
scheduling incurs the overhead of frequently paging dynamic context needed to
infer a prompt back into GPU memory. Today, serving engines support paging
inference context between GPU memory and the host DRAM. The overhead of
transferring context from DRAM to GPU memory is high since it is lower-bounded
by the limited PCIe bandwidth. We overcome this challenge by offloading
inference context from a GPU to the memory of another GPU on the same server,
connected via inter-GPU interconnects that support magnitudes higher bandwidth
than PCIe. We achieve this by developing AQUA, a transparent and elastic GPU
memory management framework for responsive LLM inference. We evaluate AQUA by
hosting eight state-of-the-art large generative ML models of different
modalities (e.g., text, audio, vision) on a server with 8 cutting-edge Nvidia
A100 80G GPUs. Using representative inference workloads, we show that AQUA
improves the responsiveness of LLM inference by 4X compared to the
state-of-the-art and it improves LLM inference throughput over a single long
prompt by 6X.",cs.DC,2024-07-30
"Leveraging Adaptive Implicit Representation Mapping for Ultra
  High-Resolution Image Segmentation",,"Implicit representation mapping (IRM) can translate image features to any
continuous resolution, showcasing its potent capability for
ultra-high-resolution image segmentation refinement. Current IRM-based methods
for refining ultra-high-resolution image segmentation often rely on CNN-based
encoders to extract image features and apply a Shared Implicit Representation
Mapping Function (SIRMF) to convert pixel-wise features into segmented results.
Hence, these methods exhibit two crucial limitations. Firstly, the CNN-based
encoder may not effectively capture long-distance information, resulting in a
lack of global semantic information in the pixel-wise features. Secondly, SIRMF
is shared across all samples, which limits its ability to generalize and handle
diverse inputs. To address these limitations, we propose a novel approach that
leverages the newly proposed Adaptive Implicit Representation Mapping (AIRM)
for ultra-high-resolution Image Segmentation. Specifically, the proposed method
comprises two components: (1) the Affinity Empowered Encoder (AEE), a robust
feature extractor that leverages the benefits of the transformer architecture
and semantic affinity to model long-distance features effectively, and (2) the
Adaptive Implicit Representation Mapping Function (AIRMF), which adaptively
translates pixel-wise features without neglecting the global semantic
information, allowing for flexible and precise feature translation. We
evaluated our method on the commonly used ultra-high-resolution segmentation
refinement datasets, i.e., BIG and PASCAL VOC 2012. The extensive experiments
demonstrate that our method outperforms competitors by a large margin. The code
is provided in supplementary material.",cs.CV,2024-07-30
"Tractable and Provably Efficient Distributional Reinforcement Learning
  with General Value Function Approximation",,"Distributional reinforcement learning improves performance by effectively
capturing environmental stochasticity, but a comprehensive theoretical
understanding of its effectiveness remains elusive. In this paper, we present a
regret analysis for distributional reinforcement learning with general value
function approximation in a finite episodic Markov decision process setting. We
first introduce a key notion of Bellman unbiasedness for a tractable and
exactly learnable update via statistical functional dynamic programming. Our
theoretical results show that approximating the infinite-dimensional return
distribution with a finite number of moment functionals is the only method to
learn the statistical information unbiasedly, including nonlinear statistical
functionals. Second, we propose a provably efficient algorithm,
$\texttt{SF-LSVI}$, achieving a regret bound of $\tilde{O}(d_E
H^{\frac{3}{2}}\sqrt{K})$ where $H$ is the horizon, $K$ is the number of
episodes, and $d_E$ is the eluder dimension of a function class.",cs.LG cs.AI stat.ML,2024-07-30
Outlier Detection in Large Radiological Datasets using UMAP,,"The success of machine learning algorithms heavily relies on the quality of
samples and the accuracy of their corresponding labels. However, building and
maintaining large, high-quality datasets is an enormous task. This is
especially true for biomedical data and for meta-sets that are compiled from
smaller ones, as variations in image quality, labeling, reports, and archiving
can lead to errors, inconsistencies, and repeated samples. Here, we show that
the uniform manifold approximation and projection (UMAP) algorithm can find
these anomalies essentially by forming independent clusters that are distinct
from the main (good) data but similar to other points with the same error type.
As a representative example, we apply UMAP to discover outliers in the publicly
available ChestX-ray14, CheXpert, and MURA datasets. While the results are
archival and retrospective and focus on radiological images, the graph-based
methods work for any data type and will prove equally beneficial for curation
at the time of dataset creation.",eess.IV cs.CV,2024-07-30
"Model Attribution in Machine-Generated Disinformation: A Domain
  Generalization Approach with Supervised Contrastive Learning",,"Model attribution for machine-generated disinformation poses a significant
challenge in understanding its origins and mitigating its spread. This task is
especially challenging because modern large language models (LLMs) produce
disinformation with human-like quality. Additionally, the diversity in
prompting methods used to generate disinformation complicates accurate source
attribution. These methods introduce domain-specific features that can mask the
fundamental characteristics of the models. In this paper, we introduce the
concept of model attribution as a domain generalization problem, where each
prompting method represents a unique domain. We argue that an effective
attribution model must be invariant to these domain-specific features. It
should also be proficient in identifying the originating models across all
scenarios, reflecting real-world detection challenges. To address this, we
introduce a novel approach based on Supervised Contrastive Learning. This
method is designed to enhance the model's robustness to variations in prompts
and focuses on distinguishing between different source LLMs. We evaluate our
model through rigorous experiments involving three common prompting methods:
``open-ended'', ``rewriting'', and ``paraphrasing'', and three advanced LLMs:
``llama 2'', ``chatgpt'', and ``vicuna''. Our results demonstrate the
effectiveness of our approach in model attribution tasks, achieving
state-of-the-art performance across diverse and unseen datasets.",cs.CL,2024-07-30
"DDU-Net: A Domain Decomposition-based CNN for High-Resolution Image
  Segmentation on Multiple GPUs",,"The segmentation of ultra-high resolution images poses challenges such as
loss of spatial information or computational inefficiency. In this work, a
novel approach that combines encoder-decoder architectures with domain
decomposition strategies to address these challenges is proposed. Specifically,
a domain decomposition-based U-Net (DDU-Net) architecture is introduced, which
partitions input images into non-overlapping patches that can be processed
independently on separate devices. A communication network is added to
facilitate inter-patch information exchange to enhance the understanding of
spatial context. Experimental validation is performed on a synthetic dataset
that is designed to measure the effectiveness of the communication network.
Then, the performance is tested on the DeepGlobe land cover classification
dataset as a real-world benchmark data set. The results demonstrate that the
approach, which includes inter-patch communication for images divided into
$16\times16$ non-overlapping subimages, achieves a $2-3\,\%$ higher
intersection over union (IoU) score compared to the same network without
inter-patch communication. The performance of the network which includes
communication is equivalent to that of a baseline U-Net trained on the full
image, showing that our model provides an effective solution for segmenting
ultra-high-resolution images while preserving spatial context. The code is
available at https://github.com/corne00/HiRes-Seg-CNN.",cs.CV cs.DC cs.LG,2024-07-30
"DEF-oriCORN: efficient 3D scene understanding for robust
  language-directed manipulation without demonstrations",,"We present DEF-oriCORN, a framework for language-directed manipulation tasks.
By leveraging a novel object-based scene representation and
diffusion-model-based state estimation algorithm, our framework enables
efficient and robust manipulation planning in response to verbal commands, even
in tightly packed environments with sparse camera views without any
demonstrations. Unlike traditional representations, our representation affords
efficient collision checking and language grounding. Compared to
state-of-the-art baselines, our framework achieves superior estimation and
motion planning performance from sparse RGB images and zero-shot generalizes to
real-world scenarios with diverse materials, including transparent and
reflective objects, despite being trained exclusively in simulation. Our code
for data generation, training, inference, and pre-trained weights are publicly
available at: https://sites.google.com/view/def-oricorn/home.",cs.RO cs.AI cs.CV,2024-07-30
"Automated Quantification of Hyperreflective Foci in SD-OCT With Diabetic
  Retinopathy",,"The presence of hyperreflective foci (HFs) is related to retinal disease
progression, and the quantity has proven to be a prognostic factor of visual
and anatomical outcome in various retinal diseases. However, lack of efficient
quantitative tools for evaluating the HFs has deprived ophthalmologist of
assessing the volume of HFs. For this reason, we propose an automated
quantification algorithm to segment and quantify HFs in spectral domain optical
coherence tomography (SD-OCT). The proposed algorithm consists of two parallel
processes namely: region of interest (ROI) generation and HFs estimation. To
generate the ROI, we use morphological reconstruction to obtain the
reconstructed image and histogram constructed for data distributions and
clustering. In parallel, we estimate the HFs by extracting the extremal regions
from the connected regions obtained from a component tree. Finally, both the
ROI and the HFs estimation process are merged to obtain the segmented HFs. The
proposed algorithm was tested on 40 3D SD-OCT volumes from 40 patients
diagnosed with non-proliferative diabetic retinopathy (NPDR), proliferative
diabetic retinopathy (PDR), and diabetic macular edema (DME). The average dice
similarity coefficient (DSC) and correlation coefficient (r) are 69.70%, 0.99
for NPDR, 70.31%, 0.99 for PDR, and 71.30%, 0.99 for DME, respectively. The
proposed algorithm can provide ophthalmologist with good HFs quantitative
information, such as volume, size, and location of the HFs.",cs.AI cs.CV,2024-07-30
"Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with
  MSU-Net",,"Efficient intravascular access in trauma and critical care significantly
impacts patient outcomes. However, the availability of skilled medical
personnel in austere environments is often limited. Autonomous robotic
ultrasound systems can aid in needle insertion for medication delivery and
support non-experts in such tasks. Despite advances in autonomous needle
insertion, inaccuracies in vessel segmentation predictions pose risks.
Understanding the uncertainty of predictive models in ultrasound imaging is
crucial for assessing their reliability. We introduce MSU-Net, a novel
multistage approach for training an ensemble of U-Nets to yield accurate
ultrasound image segmentation maps. We demonstrate substantial improvements,
18.1% over a single Monte Carlo U-Net, enhancing uncertainty evaluations, model
transparency, and trustworthiness. By highlighting areas of model certainty,
MSU-Net can guide safe needle insertions, empowering non-experts to accomplish
such tasks.",cs.CV cs.AI cs.LG,2024-07-30
"FreqTSF: Time Series Forecasting Via Simulating Frequency Kramer-Kronig
  Relations",,"Time series forecasting (TSF) is immensely important in extensive
applications, such as electricity transformation, financial trade, medical
monitoring, and smart agriculture. Although Transformer-based methods can
handle time series data, their ability to predict long-term time series is
limited due to the ``anti-order"" nature of the self-attention mechanism. To
address this problem, we focus on frequency domain to weaken the impact of
order in TSF and propose the FreqBlock, where we first obtain frequency
representations through the Frequency Transform Module. Subsequently, a newly
designed Frequency Cross Attention is used to obtian enhanced frequency
representations between the real and imaginary parts, thus establishing a link
between the attention mechanism and the inherent Kramer-Kronig relations
(KKRs). Our backbone network, FreqTSF, adopts a residual structure by
concatenating multiple FreqBlocks to simulate KKRs in the frequency domain and
avoid degradation problems. On a theoretical level, we demonstrate that the
proposed two modules can significantly reduce the time and memory complexity
from $\mathcal{O}(L^2)$ to $\mathcal{O}(L)$ for each FreqBlock computation.
Empirical studies on four benchmark datasets show that FreqTSF achieves an
overall relative MSE reduction of 15\% and an overall relative MAE reduction of
11\% compared to the state-of-the-art methods. The code will be available soon.",cs.AI,2024-07-30
"Unlocking the Potential of Binding Corporate Rules (BCRs) in Health Data
  Transfers",,"This chapter explores the essential role of Binding Corporate Rules (BCRs) in
managing and facilitating secure health data transfers within corporate groups
under the EU General Data Protection Regulation (GDPR). BCRs are tailored to
ensure compliance with the GDPR and similar international data protection laws,
presenting a flexible mechanism for transferring sensitive health and genomic
data. The chapter situates BCRs within the broader spectrum of the GDPR
international data transfer mechanisms, addressing the unique challenges posed
by the sensitive nature of health data and the increased adoption of AI
technologies. The European Data Protection Board (EDPB) Recommendations 1/2022
on BCRs, issued following the Schrems II decision, are critically analyzed,
highlighting their stringent requirements and the need for a balanced approach
that prioritizes data protection and an AI governance framework. The chapter
outlines the BCR approval process, stressing the importance of streamlining
this process to encourage broader adoption. It underscores the necessity of a
multidisciplinary approach in developing BCRs, incorporating recently adopted
international standards and frameworks, which offer valuable guidance for
organizations to build trustworthy AI management systems. They guarantee the
ethical development, deployment, and operation of AI, which is essential for
its successful integration and the broader digital transformation. In
conclusion, BCRs are positioned as essential tools for secure health data
management, fostering transparency, accountability, and collaboration across
international borders. The chapter calls for proactive measures to incentivize
BCR adoption, streamline approval processes, and promote more innovative
approaches, ensuring BCRs remain a robust mechanism for global data protection
and compliance.",cs.CY cs.AI,2024-07-30
"High-order quasi-interpolation with generalized Gaussian kernels
  restricted over tori",,"The paper proposes a novel and efficient quasi-interpolation scheme with high
approximation order for periodic function approximation over tori. The
resulting quasi-interpolation takes the form of Schoenberg's tensor-product
generalized Gaussian kernels restricted over circles. Notably, theoretical
analysis shows that it achieves the highest approximation order equal to the
order of the generalized Strang-Fix condition satisfied by the generalized
Gaussian kernels. This is in sharp contrast to classical quasi-interpolation
counterparts, which often provide much lower approximation orders than those
dictated by the generalized Strang-Fix conditions satisfied by the kernels.
Furthermore, we construct a sparse grid counterpart for high-dimensional
periodic function approximation to alleviate the curse of dimensionality.
Numerical simulations provided at the end of the paper demonstrate that our
quasi-interpolation scheme is simple and computationally efficient.",math.NA cs.NA,2024-07-30
Robust Box Prompt based SAM for Medical Image Segmentation,,"The Segment Anything Model (SAM) can achieve satisfactory segmentation
performance under high-quality box prompts. However, SAM's robustness is
compromised by the decline in box quality, limiting its practicality in
clinical reality. In this study, we propose a novel Robust Box prompt based SAM
(\textbf{RoBox-SAM}) to ensure SAM's segmentation performance under prompts
with different qualities. Our contribution is three-fold. First, we propose a
prompt refinement module to implicitly perceive the potential targets, and
output the offsets to directly transform the low-quality box prompt into a
high-quality one. We then provide an online iterative strategy for further
prompt refinement. Second, we introduce a prompt enhancement module to
automatically generate point prompts to assist the box-promptable segmentation
effectively. Last, we build a self-information extractor to encode the prior
information from the input image. These features can optimize the image
embeddings and attention calculation, thus, the robustness of SAM can be
further enhanced. Extensive experiments on the large medical segmentation
dataset including 99,299 images, 5 modalities, and 25 organs/targets validated
the efficacy of our proposed RoBox-SAM.",cs.CV cs.AI cs.LG,2024-07-30
Mixing Linters with GUIs: A Color Palette Design Probe,,"Visualization linters are end-user facing evaluators that automatically
identify potential chart issues. These spell-checker like systems offer a blend
of interpretability and customization that is not found in other forms of
automated assistance. However, existing linters do not model context and have
primarily targeted users who do not need assistance, resulting in obvious --
even annoying -- advice. We investigate these issues within the domain of color
palette design, which serves as a microcosm of visualization design concerns.
We contribute a GUI-based color palette linter as a design probe that covers
perception, accessibility, context, and other design criteria, and use it to
explore visual explanations, integrated fixes, and user defined linting rules.
Through a formative interview study and theory-driven analysis, we find that
linters can be meaningfully integrated into graphical contexts thereby
addressing many of their core issues. We discuss implications for integrating
linters into visualization tools, developing improved assertion languages, and
supporting end-user tunable advice -- all laying the groundwork for more
effective visualization linters in any context.",cs.HC,2024-07-30
Fine-grained Metrics for Point Cloud Semantic Segmentation,,"Two forms of imbalances are commonly observed in point cloud semantic
segmentation datasets: (1) category imbalances, where certain objects are more
prevalent than others; and (2) size imbalances, where certain objects occupy
more points than others. Because of this, the majority of categories and large
objects are favored in the existing evaluation metrics. This paper suggests
fine-grained mIoU and mAcc for a more thorough assessment of point cloud
segmentation algorithms in order to address these issues. Richer statistical
information is provided for models and datasets by these fine-grained metrics,
which also lessen the bias of current semantic segmentation metrics towards
large objects. The proposed metrics are used to train and assess various
semantic segmentation algorithms on three distinct indoor and outdoor semantic
segmentation datasets.",cs.CV cs.GR,2024-07-30
"TrackSorter: A Transformer-based sorting algorithm for track finding in
  High Energy Physics",,"Track finding in particle data is a challenging pattern recognition problem
in High Energy Physics. It takes as inputs a point cloud of space points and
labels them so that space points created by the same particle have the same
label. The list of space points with the same label is a track candidate. We
argue that this pattern recognition problem can be formulated as a sorting
problem, of which the inputs are a list of space points sorted by their
distances away from the collision points and the outputs are the space points
sorted by their labels. In this paper, we propose the TrackSorter algorithm: a
Transformer-based algorithm for pattern recognition in particle data.
TrackSorter uses a simple tokenization scheme to convert space points into
discrete tokens. It then uses the tokenized space points as inputs and sorts
the input tokens into track candidates. TrackSorter is a novel end-to-end track
finding algorithm that leverages Transformer-based models to solve pattern
recognition problems. It is evaluated on the TrackML dataset and has good track
finding performance.",cs.LG hep-ex physics.data-an,2024-07-30
"SimpleLLM4AD: An End-to-End Vision-Language Model with Graph Visual
  Question Answering for Autonomous Driving",,"Many fields could benefit from the rapid development of the large language
models (LLMs). The end-to-end autonomous driving (e2eAD) is one of the
typically fields facing new opportunities as the LLMs have supported more and
more modalities. Here, by utilizing vision-language model (VLM), we proposed an
e2eAD method called SimpleLLM4AD. In our method, the e2eAD task are divided
into four stages, which are perception, prediction, planning, and behavior.
Each stage consists of several visual question answering (VQA) pairs and VQA
pairs interconnect with each other constructing a graph called Graph VQA
(GVQA). By reasoning each VQA pair in the GVQA through VLM stage by stage, our
method could achieve e2e driving with language. In our method, vision
transformers (ViT) models are employed to process nuScenes visual data, while
VLM are utilized to interpret and reason about the information extracted from
the visual inputs. In the perception stage, the system identifies and
classifies objects from the driving environment. The prediction stage involves
forecasting the potential movements of these objects. The planning stage
utilizes the gathered information to develop a driving strategy, ensuring the
safety and efficiency of the autonomous vehicle. Finally, the behavior stage
translates the planned actions into executable commands for the vehicle. Our
experiments demonstrate that SimpleLLM4AD achieves competitive performance in
complex driving scenarios.",cs.CV cs.AI,2024-07-30
"Decentralized and Uncoordinated Learning of Stable Matchings: A
  Game-Theoretic Approach",,"We consider the problem of learning stable matchings in a fully decentralized
and uncoordinated manner. In this problem, there are $n$ men and $n$ women,
each having preference over the other side. It is assumed that women know their
preferences over men, but men are not aware of their preferences over women,
and they only learn them if they propose and successfully get matched to women.
A matching is called stable if no man and woman prefer each other over their
current matches. When all the preferences are known a priori, the celebrated
Deferred-Acceptance algorithm proposed by Gale and Shapley provides a
decentralized and uncoordinated algorithm to obtain a stable matching. However,
when the preferences are unknown, developing such an algorithm faces major
challenges due to a lack of coordination. We achieve this goal by making a
connection between stable matchings and learning Nash equilibria (NE) in
noncooperative games. First, we provide a complete information game formulation
for the stable matching problem with known preferences such that its set of
pure NE coincides with the set of stable matchings, while its mixed NE can be
rounded in a decentralized manner to a stable matching. Relying on such a
game-theoretic formulation, we show that for hierarchical markets, adopting the
exponential weight (EXP) learning algorithm for the stable matching game
achieves logarithmic regret with polynomial dependence on the number of
players, thus answering a question posed in previous literature. Moreover, we
show that the same EXP learning algorithm converges locally and exponentially
fast to a stable matching in general matching markets. We complement this
result by introducing another decentralized and uncoordinated learning
algorithm that globally converges to a stable matching with arbitrarily high
probability, leveraging the weak acyclicity property of the stable matching
game.",cs.GT cs.LG cs.MA cs.SI cs.SY eess.SY,2024-07-30
"On the mean-field limit of the Cucker-Smale model with Random Batch
  Method",,"In this work, we focus on the mean-field limit of the Random Batch Method
(RBM) for the Cucker-Smale model. Different from the classical mean-field limit
analysis, the chaos in this model is imposed at discrete time and is propagated
to discrete time flux. We approach separately the limits of the number of
particles $N\to\infty$ and the discrete time interval $\tau\to 0$ with respect
to the RBM, by using the flocking property of the Cucker-Smale model and the
observation in combinatorics. The Wasserstein distance is used to quantify the
difference between the approximation limit and the original mean-field limit.
Also, we combine the RBM with generalized Polynomial Chaos (gPC) expansion and
proposed the RBM-gPC method to approximate stochastic mean-field equations,
which conserves positivity and momentum of the mean-field limit with random
inputs.",math.NA cs.NA math.AP math.CA math.PR,2024-07-30
"A Vectorization Method Induced By Maximal Margin Classification For
  Persistent Diagrams",,"Persistent homology is an effective method for extracting topological
information, represented as persistent diagrams, of spatial structure data.
Hence it is well-suited for the study of protein structures. Attempts to
incorporate Persistent homology in machine learning methods of protein function
prediction have resulted in several techniques for vectorizing persistent
diagrams. However, current vectorization methods are excessively artificial and
cannot ensure the effective utilization of information or the rationality of
the methods. To address this problem, we propose a more geometrical
vectorization method of persistent diagrams based on maximal margin
classification for Banach space, and additionaly propose a framework that
utilizes topological data analysis to identify proteins with specific
functions. We evaluated our vectorization method using a binary classification
task on proteins and compared it with the statistical methods that exhibit the
best performance among thirteen commonly used vectorization methods. The
experimental results indicate that our approach surpasses the statistical
methods in both robustness and precision.",cs.LG cs.AI q-bio.BM,2024-07-30
"Who should I trust? A Visual Analytics Approach for Comparing Net Load
  Forecasting Models",,"Net load forecasting is crucial for energy planning and facilitating informed
decision-making regarding trade and load distributions. However, evaluating
forecasting models' performance against benchmark models remains challenging,
thereby impeding experts' trust in the model's performance. In this context,
there is a demand for technological interventions that allow scientists to
compare models across various timeframes and solar penetration levels. This
paper introduces a visual analytics-based application designed to compare the
performance of deep-learning-based net load forecasting models with other
models for probabilistic net load forecasting. This application employs
carefully selected visual analytic interventions, enabling users to discern
differences in model performance across different solar penetration levels,
dataset resolutions, and hours of the day over multiple months. We also present
observations made using our application through a case study, demonstrating the
effectiveness of visualizations in aiding scientists in making informed
decisions and enhancing trust in net load forecasting models.",cs.HC cs.AI cs.LG cs.SY eess.SP eess.SY,2024-07-30
"Integrated Sensing and Communication in IRS-assisted High-Mobility
  Systems: Design, Analysis and Optimization",,"In this paper, we investigate integrated sensing and communication (ISAC) in
high-mobility systems with the aid of an intelligent reflecting surface (IRS).
To exploit the benefits of Delay-Doppler (DD) spread caused by high mobility,
orthogonal time frequency space (OTFS)-based frame structure and transmission
framework are proposed. {In such a framework,} we first design a low-complexity
ratio-based sensing algorithm for estimating the velocity of mobile user. Then,
we analyze the performance of sensing and communication in terms of achievable
mean square error (MSE) and achievable rate, respectively, and reveal the
impact of key parameters. Next, with the derived performance expressions, we
jointly optimize the phase shift matrix of IRS and the receive combining vector
at the base station (BS) to improve the overall performance of integrated
sensing and communication. Finally, extensive simulation results confirm the
effectiveness of the proposed algorithms in high-mobility systems.",cs.IT eess.SP math.IT,2024-07-30
Modeling Urban Transport Choices: Incorporating Sociocultural Aspects,,"This paper introduces an agent-based simulation model aimed at understanding
urban commuters mode choices and evaluating the impacts of transport policies
to promote sustainable mobility. Crafted for developing countries, where
utilitarian travel heavily relies on motorcycles, the model integrates
sociocultural factors that influence transport behavior. Multinomial models and
inferential statistics applied to survey data from Cali, Colombia, inform the
model, revealing significant influences of sociodemographic factors and travel
attributes on mode choice. Findings highlight the importance of cost, time,
safety, comfort, and personal security, with disparities across socioeconomic
groups. Policy simulations demonstrate positive responses to interventions like
free public transportation, increased bus frequency, and enhanced security, yet
with modest shifts in mode choice. Multifaceted policy approaches are deemed
more effective, addressing diverse user preferences. Outputs can be extended to
cities with similar sociocultural characteristics and transport dynamics. The
methodology applied in this work can be replicated for other territories.",cs.MA stat.AP,2024-07-30
Enhanced Self-Checkout System for Retail Based on Improved YOLOv10,,"With the rapid advancement of deep learning technologies, computer vision has
shown immense potential in retail automation. This paper presents a novel
self-checkout system for retail based on an improved YOLOv10 network, aimed at
enhancing checkout efficiency and reducing labor costs. We propose targeted
optimizations to the YOLOv10 model, by incorporating the detection head
structure from YOLOv8, which significantly improves product recognition
accuracy. Additionally, we develop a post-processing algorithm tailored for
self-checkout scenarios, to further enhance the application of system.
Experimental results demonstrate that our system outperforms existing methods
in both product recognition accuracy and checkout speed. This research not only
provides a new technical solution for retail automation but offers valuable
insights into optimizing deep learning models for real-world applications.",cs.CV,2024-07-30
"EUDA: An Efficient Unsupervised Domain Adaptation via Self-Supervised
  Vision Transformer",,"Unsupervised domain adaptation (UDA) aims to mitigate the domain shift issue,
where the distribution of training (source) data differs from that of testing
(target) data. Many models have been developed to tackle this problem, and
recently vision transformers (ViTs) have shown promising results. However, the
complexity and large number of trainable parameters of ViTs restrict their
deployment in practical applications. This underscores the need for an
efficient model that not only reduces trainable parameters but also allows for
adjustable complexity based on specific needs while delivering comparable
performance. To achieve this, in this paper we introduce an Efficient
Unsupervised Domain Adaptation (EUDA) framework. EUDA employs the DINOv2, which
is a self-supervised ViT, as a feature extractor followed by a simplified
bottleneck of fully connected layers to refine features for enhanced domain
adaptation. Additionally, EUDA employs the synergistic domain alignment loss
(SDAL), which integrates cross-entropy (CE) and maximum mean discrepancy (MMD)
losses, to balance adaptation by minimizing classification errors in the source
domain while aligning the source and target domain distributions. The
experimental results indicate the effectiveness of EUDA in producing comparable
results as compared with other state-of-the-art methods in domain adaptation
with significantly fewer trainable parameters, between 42% to 99.7% fewer. This
showcases the ability to train the model in a resource-limited environment. The
code of the model is available at: https://github.com/A-Abedi/EUDA.",cs.CV cs.AI cs.LG,2024-07-30
State-observation augmented diffusion model for nonlinear assimilation,,"Data assimilation has become a crucial technique aiming to combine physical
models with observational data to estimate state variables. Traditional
assimilation algorithms often face challenges of high nonlinearity brought by
both the physical and observational models. In this work, we propose a novel
data-driven assimilation algorithm based on generative models to address such
concerns. Our State-Observation Augmented Diffusion (SOAD) model is designed to
handle nonlinear physical and observational models more effectively. The
marginal posterior associated with SOAD has been derived and then proved to
match the real posterior under mild assumptions, which shows theoretical
superiority over previous score-based assimilation works. Experimental results
also indicate that our SOAD model may offer improved accuracy over existing
data-driven methods.",cs.LG stat.ML,2024-07-30
"Beyond Silent Letters: Amplifying LLMs in Emotion Recognition with Vocal
  Nuances",,"This paper introduces a novel approach to emotion detection in speech using
Large Language Models (LLMs). We address the limitation of LLMs in processing
audio inputs by translating speech characteristics into natural language
descriptions. Our method integrates these descriptions into text prompts,
enabling LLMs to perform multimodal emotion analysis without architectural
modifications. We evaluate our approach on two datasets: IEMOCAP and MELD,
demonstrating significant improvements in emotion recognition accuracy,
particularly for high-quality audio data. Our experiments show that
incorporating speech descriptions yields a 2 percentage point increase in
weighted F1 score on IEMOCAP (from 70.111\% to 72.596\%). We also compare
various LLM architectures and explore the effectiveness of different feature
representations. Our findings highlight the potential of this approach in
enhancing emotion detection capabilities of LLMs and underscore the importance
of audio quality in speech-based emotion recognition tasks. We'll release the
source code on Github.",cs.CL cs.AI,2024-07-30
"Diff-Cleanse: Identifying and Mitigating Backdoor Attacks in Diffusion
  Models",,"Diffusion models (DM) represent one of the most advanced generative models
today, yet recent studies suggest that DMs are vulnerable to backdoor attacks.
Backdoor attacks establish hidden associations between particular input
patterns and model behaviors, compromising model integrity by triggering
undesirable actions with manipulated input data. This vulnerability poses
substantial risks, including reputational damage to model owners and the
dissemination of harmful content. To mitigate the threat of backdoor attacks,
there have been some investigations on backdoor detection and model repair.
However, previous work fails to purify the backdoored DMs created by
state-of-the-art attacks, rendering the field much underexplored. To bridge
this gap, we introduce \textbf{Diff-Cleanse}, a novel two-stage backdoor
defense framework specifically designed for DMs. The first stage employs a
innovative trigger inversion technique to detect the backdoor and reconstruct
the trigger, and the second stage utilizes a structural pruning method to
eliminate the backdoor. We evaluate our framework on hundreds of DMs attacked
by 3 existing backdoor attack methods. Extensive experiments demonstrate that
Diff-Cleanse achieves nearly 100\% detection accuracy and effectively mitigates
backdoor impacts, preserving the model's benign performance with minimal
compromise. Our code is avaliable at https://github.com/shymuel/diff-cleanse.",cs.CR cs.LG,2024-07-30
Big Cooperative Learning,,"Cooperation plays a pivotal role in the evolution of human intelligence;
moreover, it also underlies the recent revolutionary advancement of artificial
intelligence (AI) that is driven by foundation models. Specifically, we reveal
that the training of foundation models can be interpreted as a form of big
cooperative learning (\textit{abbr.} big learning), where massive learning
individuals/tasks \emph{cooperate} to approach the unique essence of data from
diverse perspectives of data prediction, leveraging a universal model. The
presented big learning therefore unifies most training objectives of foundation
models within a consistent framework, where their underlying assumptions are
exposed simultaneously. We design tailored simulations to demonstrate the
principle of big learning, based on which we provide learning-perspective
justifications for the successes of foundation models, with interesting
side-products. Furthermore, we reveal that big learning is a new dimension for
upgrading conventional machine learning paradigms, valuable for endowing
reinvigorations to associated applications; as an illustrative example, we
propose the BigLearn-GAN, which is a novel adversarially-trained foundation
model with versatile data sampling capabilities. Code is available at
\texttt{https://github.com/YulaiCong/BigCooperativeLearning}.",cs.LG cs.AI,2024-07-30
Hyper parametric timed CTL,,"Hyperproperties enable simultaneous reasoning about multiple execution traces
of a system and are useful to reason about non-interference, opacity,
robustness, fairness, observational determinism, etc. We introduce hyper
parametric timed computation tree logic (HyperPTCTL), extending hyperlogics
with timing reasoning and, notably, parameters to express unknown values. We
mainly consider its nest-free fragment, where temporal operators cannot be
nested. However, we allow extensions that enable counting actions and comparing
the duration since the most recent occurrence of specific actions. We show that
our nest-free fragment with this extension is sufficiently expressive to encode
properties, e.g., opacity, (un)fairness, or robust observational
(non-)determinism. We propose semi-algorithms for model checking and synthesis
of parametric timed automata (an extension of timed automata with timing
parameters) against this nest-free fragment with the extension via reduction to
PTCTL model checking and synthesis. While the general model checking (and thus
synthesis) problem is undecidable, we show that a large part of our extended
(yet nest-free) fragment is decidable, provided the parameters only appear in
the property, not in the model. We also exhibit additional decidable fragments
where parameters within the model are allowed. We implemented our
semi-algorithms on top of the IMITATOR model checker, and performed
experiments. Our implementation supports most of the nest-free fragments
(beyond the decidable classes). The experimental results highlight our method's
practical relevance.",cs.FL cs.SY eess.SY,2024-07-31
"STANet: A Novel Spatio-Temporal Aggregation Network for Depression
  Classification with Small and Unbalanced FMRI Data",,"Accurate diagnosis of depression is crucial for timely implementation of
optimal treatments, preventing complications and reducing the risk of suicide.
Traditional methods rely on self-report questionnaires and clinical assessment,
lacking objective biomarkers. Combining fMRI with artificial intelligence can
enhance depression diagnosis by integrating neuroimaging indicators. However,
the specificity of fMRI acquisition for depression often results in unbalanced
and small datasets, challenging the sensitivity and accuracy of classification
models. In this study, we propose the Spatio-Temporal Aggregation Network
(STANet) for diagnosing depression by integrating CNN and RNN to capture both
temporal and spatial features of brain activity. STANet comprises the following
steps:(1) Aggregate spatio-temporal information via ICA. (2) Utilize
multi-scale deep convolution to capture detailed features. (3) Balance data
using the SMOTE to generate new samples for minority classes. (4) Employ the
AFGRU classifier, which combines Fourier transformation with GRU, to capture
long-term dependencies, with an adaptive weight assignment mechanism to enhance
model generalization. The experimental results demonstrate that STANet achieves
superior depression diagnostic performance with 82.38% accuracy and a 90.72%
AUC. The STFA module enhances classification by capturing deeper features at
multiple scales. The AFGRU classifier, with adaptive weights and stacked GRU,
attains higher accuracy and AUC. SMOTE outperforms other oversampling methods.
Additionally, spatio-temporal aggregated features achieve better performance
compared to using only temporal or spatial features. STANet outperforms
traditional or deep learning classifiers, and functional connectivity-based
classifiers, as demonstrated by ten-fold cross-validation.",eess.IV cs.CV,2024-07-31
Towards Variable-Length In-Network Caching,,"We present StarCache, a new in-network caching architecture that can cache
variable-length items to balance a wide range of key-value workloads. Unlike
existing works, StarCache does not cache hot items in the switch memory.
Instead, we make hot items revisit the switch data plane continuously by
exploiting packet recirculation. Our approach keeps cached key-value pairs in
the switch data plane while freeing them from item size limitations caused by
hardware constraints. We implement a StarCache prototype on an Intel Tofino
switch. Our experimental results show that StarCache can balance highly skewed
workloads with various key and value sizes.",cs.NI,2024-07-31
"EdgeLLM: A Highly Efficient CPU-FPGA Heterogeneous Edge Accelerator for
  Large Language Models",,"The rapid advancements in artificial intelligence (AI), particularly the
Large Language Models (LLMs), have profoundly affected our daily work and
communication forms. However, the colossal scale of LLM presents significant
operational challenges, particularly when attempting to deploy them on
resource-constrained edge devices such as smartphones, robots, and embedded
systems. In this work, we proposed EdgeLLM, an efficient CPU-FPGA heterogeneous
acceleration framework, to markedly enhance the computational efficiency of
LLMs on edge. We first analyzed the whole operators within AI models and
developed a universal data parallelism scheme, which is generic and can be
adapted to any type of AI algorithm. Then, we developed fully-customized
hardware operators according to the designated data formats. A multitude of
optimization techniques have been integrated in the design, such as approximate
FP16*INT4 and FP16*FP16 computation engines, group vector systolic arrays,
log-scale structured sparsity, asynchronous between data transfer and
processing. Finally, we proposed an end-to-end compilation scheme that can
dynamically compile all of the operators and map the whole model on CPU-FPGA
heterogeneous system. The design has been deployed on AMD Xilinx VCU128 FPGA,
our accelerator achieves 1.67x higher throughput and 7.4x higher energy
efficiency than the commercial GPU (NVIDIA A100-SXM4-80G) on ChatGLM2-6B, and
shows 10%~20% better performance than state-of-the-art FPGA accelerator of
FlightLLM in terms of HBM bandwidth utilization and LLM throughput.",cs.AR,2024-07-31
"Knowledge-Guided Prompt Learning for Lifespan Brain MR Image
  Segmentation",,"Automatic and accurate segmentation of brain MR images throughout the human
lifespan into tissue and structure is crucial for understanding brain
development and diagnosing diseases. However, challenges arise from the
intricate variations in brain appearance due to rapid early brain development,
aging, and disorders, compounded by the limited availability of
manually-labeled datasets. In response, we present a two-step segmentation
framework employing Knowledge-Guided Prompt Learning (KGPL) for brain MRI.
Specifically, we first pre-train segmentation models on large-scale datasets
with sub-optimal labels, followed by the incorporation of knowledge-driven
embeddings learned from image-text alignment into the models. The introduction
of knowledge-wise prompts captures semantic relationships between anatomical
variability and biological processes, enabling models to learn structural
feature embeddings across diverse age groups. Experimental findings demonstrate
the superiority and robustness of our proposed method, particularly noticeable
when employing Swin UNETR as the backbone. Our approach achieves average DSC
values of 95.17% and 94.19% for brain tissue and structure segmentation,
respectively. Our code is available at https://github.com/TL9792/KGPL.",eess.IV cs.CV,2024-07-31
Performance of Recent Large Language Models for a Low-Resourced Language,,"Large Language Models (LLMs) have shown significant advances in the past
year. In addition to new versions of GPT and Llama, several other LLMs have
been introduced recently. Some of these are open models available for download
and modification.
  Although multilingual large language models have been available for some
time, their performance on low-resourced languages such as Sinhala has been
poor. We evaluated four recent LLMs on their performance directly in the
Sinhala language, and by translation to and from English. We also evaluated
their fine-tunability with a small amount of fine-tuning data. Claude and GPT
4o perform well out-of-the-box and do significantly better than previous
versions. Llama and Mistral perform poorly but show some promise of improvement
with fine tuning.",cs.CL,2024-07-31
CAMAv2: A Vision-Centric Approach for Static Map Element Annotation,,"The recent development of online static map element (a.k.a. HD map)
construction algorithms has raised a vast demand for data with ground truth
annotations. However, available public datasets currently cannot provide
high-quality training data regarding consistency and accuracy. For instance,
the manual labelled (low efficiency) nuScenes still contains misalignment and
inconsistency between the HD maps and images (e.g., around 8.03 pixels
reprojection error on average). To this end, we present CAMAv2: a
vision-centric approach for Consistent and Accurate Map Annotation. Without
LiDAR inputs, our proposed framework can still generate high-quality 3D
annotations of static map elements. Specifically, the annotation can achieve
high reprojection accuracy across all surrounding cameras and is
spatial-temporal consistent across the whole sequence. We apply our proposed
framework to the popular nuScenes dataset to provide efficient and highly
accurate annotations. Compared with the original nuScenes static map element,
our CAMAv2 annotations achieve lower reprojection errors (e.g., 4.96 vs. 8.03
pixels). Models trained with annotations from CAMAv2 also achieve lower
reprojection errors (e.g., 5.62 vs. 8.43 pixels).",cs.CV,2024-07-31
Chat2Layout: Interactive 3D Furniture Layout with a Multimodal LLM,,"Automatic furniture layout is long desired for convenient interior design.
Leveraging the remarkable visual reasoning capabilities of multimodal large
language models (MLLMs), recent methods address layout generation in a static
manner, lacking the feedback-driven refinement essential for interactive user
engagement. We introduce Chat2Layout, a novel interactive furniture layout
generation system that extends the functionality of MLLMs into the realm of
interactive layout design. To achieve this, we establish a unified
vision-question paradigm for in-context learning, enabling seamless
communication with MLLMs to steer their behavior without altering model
weights. Within this framework, we present a novel training-free visual
prompting mechanism. This involves a visual-text prompting technique that
assist MLLMs in reasoning about plausible layout plans, followed by an
Offline-to-Online search (O2O-Search) method, which automatically identifies
the minimal set of informative references to provide exemplars for visual-text
prompting. By employing an agent system with MLLMs as the core controller, we
enable bidirectional interaction. The agent not only comprehends the 3D
environment and user requirements through linguistic and visual perception but
also plans tasks and reasons about actions to generate and arrange furniture
within the virtual space. Furthermore, the agent iteratively updates based on
visual feedback from execution results. Experimental results demonstrate that
our approach facilitates language-interactive generation and arrangement for
diverse and complex 3D furniture.",cs.CV,2024-07-31
"Image-Based Deep Reinforcement Learning with Intrinsically Motivated
  Stimuli: On the Execution of Complex Robotic Tasks",,"Reinforcement Learning (RL) has been widely used to solve tasks where the
environment consistently provides a dense reward value. However, in real-world
scenarios, rewards can often be poorly defined or sparse. Auxiliary signals are
indispensable for discovering efficient exploration strategies and aiding the
learning process. In this work, inspired by intrinsic motivation theory, we
postulate that the intrinsic stimuli of novelty and surprise can assist in
improving exploration in complex, sparsely rewarded environments. We introduce
a novel sample-efficient method able to learn directly from pixels, an
image-based extension of TD3 with an autoencoder called \textit{NaSA-TD3}. The
experiments demonstrate that NaSA-TD3 is easy to train and an efficient method
for tackling complex continuous-control robotic tasks, both in simulated
environments and real-world settings. NaSA-TD3 outperforms existing
state-of-the-art RL image-based methods in terms of final performance without
requiring pre-trained models or human demonstrations.",cs.AI cs.LG,2024-07-31
"A Cooperation Control Framework Based on Admittance Control and
  Time-varying Passive Velocity Field Control for Human--Robot Co-carrying
  Tasks",,"Human--robot co-carrying tasks demonstrate their potential in both industrial
and everyday applications by leveraging the strengths of both parties.
Effective control of robots in these tasks requires minimizing position and
velocity errors to complete the shared tasks while also managing the energy
level within the closed-loop systems to prevent potential dangers such as
instability and unintended force exertion. However, this collaboration scenario
poses numerous challenges due to varied human intentions in adapting to
workspace characteristics, leading to human--robot conflicts and safety
incidents. In this paper, we develop a robot controller that enables the robot
partner to re-plan its path leveraging conflict information, follow co-carrying
motions accurately, ensure passivity, and regular the energy of the closed-loop
system. A cooperation control framework for human--robot co-carrying tasks is
constructed by utilizing admittance control and time-varying Passive Velocity
Field Control with a fractional exponent energy compensation control term. By
measuring the interaction force, the desired trajectory of co-carrying tasks
for the robot partner is first generated using admittance control. Thereafter,
the new Passive Velocity Field Control with the energy compensation feature is
designed to track the desired time-varying trajectory and guarantee passivity.
Furthermore, the proposed approach ensures that the system's kinetic energy
converges to the desired level within a finite time interval, which is critical
for time-critical applications. Numerical simulation demonstrates the
efficiency of the proposed cooperation control method through four
collaborative transportation scenarios.",cs.RO,2024-07-31
High-throughput 3D shape completion of potato tubers on a harvester,,"Potato yield is an important metric for farmers to further optimize their
cultivation practices. Potato yield can be estimated on a harvester using an
RGB-D camera that can estimate the three-dimensional (3D) volume of individual
potato tubers. A challenge, however, is that the 3D shape derived from RGB-D
images is only partially completed, underestimating the actual volume. To
address this issue, we developed a 3D shape completion network, called CoRe++,
which can complete the 3D shape from RGB-D images. CoRe++ is a deep learning
network that consists of a convolutional encoder and a decoder. The encoder
compresses RGB-D images into latent vectors that are used by the decoder to
complete the 3D shape using the deep signed distance field network (DeepSDF).
To evaluate our CoRe++ network, we collected partial and complete 3D point
clouds of 339 potato tubers on an operational harvester in Japan. On the 1425
RGB-D images in the test set (representing 51 unique potato tubers), our
network achieved a completion accuracy of 2.8 mm on average. For volumetric
estimation, the root mean squared error (RMSE) was 22.6 ml, and this was better
than the RMSE of the linear regression (31.1 ml) and the base model (36.9 ml).
We found that the RMSE can be further reduced to 18.2 ml when performing the 3D
shape completion in the center of the RGB-D image. With an average 3D shape
completion time of 10 milliseconds per tuber, we can conclude that CoRe++ is
both fast and accurate enough to be implemented on an operational harvester for
high-throughput potato yield estimation. Our code, network weights and dataset
are publicly available at
https://github.com/UTokyo-FieldPhenomics-Lab/corepp.git.",cs.CV,2024-07-31
Seamless Parametrization in Penner Coordinates,,"We introduce a conceptually simple and efficient algorithm for seamless
parametrization, a key element in constructing quad layouts and texture charts
on surfaces. More specifically, we consider the construction of
parametrizations with prescribed holonomy signatures i.e., a set of angles at
singularities, and rotations along homology loops, preserving which is
essential for constructing parametrizations following an input field, as well
as for user control of the parametrization structure. Our algorithm performs
exceptionally well on a large dataset based on Thingi10k [Zhou and Jacobson
2016], (16156 meshes) as well as on a challenging smaller dataset of [Myles et
al. 2014], converging, on average, in 9 iterations. Although the algorithm
lacks a formal mathematical guarantee, presented empirical evidence and the
connections between convex optimization and closely related algorithms, suggest
that a similar formulation can be found for this algorithm in the future.",cs.GR,2024-07-31
"MIST: A Simple and Scalable End-To-End 3D Medical Imaging Segmentation
  Framework",,"Medical imaging segmentation is a highly active area of research, with deep
learning-based methods achieving state-of-the-art results in several
benchmarks. However, the lack of standardized tools for training, testing, and
evaluating new methods makes the comparison of methods difficult. To address
this, we introduce the Medical Imaging Segmentation Toolkit (MIST), a simple,
modular, and end-to-end medical imaging segmentation framework designed to
facilitate consistent training, testing, and evaluation of deep learning-based
medical imaging segmentation methods. MIST standardizes data analysis,
preprocessing, and evaluation pipelines, accommodating multiple architectures
and loss functions. This standardization ensures reproducible and fair
comparisons across different methods. We detail MIST's data format
requirements, pipelines, and auxiliary features and demonstrate its efficacy
using the BraTS Adult Glioma Post-Treatment Challenge dataset. Our results
highlight MIST's ability to produce accurate segmentation masks and its
scalability across multiple GPUs, showcasing its potential as a powerful tool
for future medical imaging research and development.",eess.IV cs.AI cs.CV cs.LG,2024-07-31
"Dual-Constrained Dynamical Neural ODEs for Ambiguity-aware Continuous
  Emotion Prediction",,"There has been a significant focus on modelling emotion ambiguity in recent
years, with advancements made in representing emotions as distributions to
capture ambiguity. However, there has been comparatively less effort devoted to
the consideration of temporal dependencies in emotion distributions which
encodes ambiguity in perceived emotions that evolve smoothly over time.
Recognizing the benefits of using constrained dynamical neural ordinary
differential equations (CD-NODE) to model time series as dynamic processes, we
propose an ambiguity-aware dual-constrained Neural ODE approach to model the
dynamics of emotion distributions on arousal and valence. In our approach, we
utilize ODEs parameterised by neural networks to estimate the distribution
parameters, and we integrate additional constraints to restrict the range of
the system outputs to ensure the validity of predicted distributions. We
evaluated our proposed system on the publicly available RECOLA dataset and
observed very promising performance across a range of evaluation metrics.",cs.AI,2024-07-31
Differentially Private Block-wise Gradient Shuffle for Deep Learning,,"Traditional Differentially Private Stochastic Gradient Descent (DP-SGD)
introduces statistical noise on top of gradients drawn from a Gaussian
distribution to ensure privacy. This paper introduces the novel Differentially
Private Block-wise Gradient Shuffle (DP-BloGS) algorithm for deep learning.
BloGS builds off of existing private deep learning literature, but makes a
definitive shift by taking a probabilistic approach to gradient noise
introduction through shuffling modeled after information theoretic privacy
analyses. The theoretical results presented in this paper show that the
combination of shuffling, parameter-specific block size selection, batch layer
clipping, and gradient accumulation allows DP-BloGS to achieve training times
close to that of non-private training while maintaining similar privacy and
utility guarantees to DP-SGD. DP-BloGS is found to be significantly more
resistant to data extraction attempts than DP-SGD. The theoretical results are
validated by the experimental findings.",cs.LG cs.AI cs.CR,2024-07-31
"SuperVINS: A visual-inertial SLAM framework integrated deep learning
  features",,"In this article, we propose enhancements to VINS-Fusion by incorporating deep
learning features and deep learning matching methods. We implemented the
training of deep learning feature bag of words and utilized these features for
loop closure detection. Additionally, we introduce the RANSAC algorithm in the
deep learning feature matching module to optimize matching. SuperVINS, an
improved version of VINS-Fusion, outperforms it in terms of positioning
accuracy, robustness, and more. Particularly in challenging scenarios like low
illumination and rapid jitter, traditional geometric features fail to fully
exploit image information, whereas deep learning features excel at capturing
image features.To validate our proposed improvement scheme, we conducted
experiments using open source datasets. We performed a comprehensive analysis
of the experimental results from both qualitative and quantitative
perspectives. The results demonstrate the feasibility and effectiveness of this
deep learning-based approach for SLAM systems.To foster knowledge exchange in
this field, we have made the code for this article publicly available. You can
find the code at this link: https://github.com/luohongk/SuperVINS.",cs.RO,2024-07-31
"Small Object Few-shot Segmentation for Vision-based Industrial
  Inspection",,"Vision-based industrial inspection (VII) aims to locate defects quickly and
accurately. Supervised learning under a close-set setting and industrial
anomaly detection, as two common paradigms in VII, face different problems in
practical applications. The former is that various and sufficient defects are
difficult to obtain, while the latter is that specific defects cannot be
located. To solve these problems, in this paper, we focus on the few-shot
semantic segmentation (FSS) method, which can locate unseen defects conditioned
on a few annotations without retraining. Compared to common objects in natural
images, the defects in VII are small. This brings two problems to current FSS
methods: 1 distortion of target semantics and 2 many false positives for
backgrounds. To alleviate these problems, we propose a small object few-shot
segmentation (SOFS) model. The key idea for alleviating 1 is to avoid the
resizing of the original image and correctly indicate the intensity of target
semantics. SOFS achieves this idea via the non-resizing procedure and the
prototype intensity downsampling of support annotations. To alleviate 2, we
design an abnormal prior map in SOFS to guide the model to reduce false
positives and propose a mixed normal Dice loss to preferentially prevent the
model from predicting false positives. SOFS can achieve FSS and few-shot
anomaly detection determined by support masks. Diverse experiments substantiate
the superior performance of SOFS. Code is available at
https://github.com/zhangzilongc/SOFS.",cs.CV cs.AI,2024-07-31
"Priority and Stackelberg Game-Based Incentive Task Allocation for
  Device-Assisted MEC Networks",,"Mobile edge computing (MEC) is a promising computing paradigm that offers
users proximity and instant computing services for various applications, and it
has become an essential component of the Internet of Things (IoT). However, as
compute-intensive services continue to emerge and the number of IoT devices
explodes, MEC servers are confronted with resource limitations. In this work,
we investigate a task-offloading framework for device-assisted edge computing,
which allows MEC servers to assign certain tasks to auxiliary IoT devices (ADs)
for processing. To facilitate efficient collaboration among task IoT devices
(TDs), the MEC server, and ADs, we propose an incentive-driven pricing and task
allocation scheme. Initially, the MEC server employs the Vickrey auction
mechanism to recruit ADs. Subsequently, based on the Stackelberg game, we
analyze the interactions between TDs and the MEC server. Finally, we establish
the optimal service pricing and task allocation strategy, guided by the
Stackelberg model and priority settings. Simulation results show that the
proposed scheme dramatically improves the utility of the MEC server while
safeguarding the interests of TDs and ADs, achieving a triple-win scenario.",cs.NI,2024-07-31
"Interactive embodied evolution for socially adept Artificial General
  Creatures",,"We introduce here the concept of Artificial General Creatures (AGC) which
encompasses ""robotic or virtual agents with a wide enough range of capabilities
to ensure their continued survival"". With this in mind, we propose a research
line aimed at incrementally building both the technology and the
trustworthiness of AGC. The core element in this approach is that trust can
only be built over time, through demonstrably mutually beneficial interactions.
  To this end, we advocate starting from unobtrusive, nonthreatening artificial
agents that would explicitly collaborate with humans, similarly to what
domestic animals do. By combining multiple research fields, from Evolutionary
Robotics to Neuroscience, from Ethics to Human-Machine Interaction, we aim at
creating embodied, self-sustaining Artificial General Creatures that would form
social and emotional connections with humans. Although they would not be able
to play competitive online games or generate poems, we argue that creatures
akin to artificial pets would be invaluable stepping stones toward symbiotic
Artificial General Intelligence.",cs.NE,2024-07-31
"Tree-of-Traversals: A Zero-Shot Reasoning Algorithm for Augmenting
  Black-box Language Models with Knowledge Graphs",,"Knowledge graphs (KGs) complement Large Language Models (LLMs) by providing
reliable, structured, domain-specific, and up-to-date external knowledge.
However, KGs and LLMs are often developed separately and must be integrated
after training. We introduce Tree-of-Traversals, a novel zero-shot reasoning
algorithm that enables augmentation of black-box LLMs with one or more KGs. The
algorithm equips a LLM with actions for interfacing a KG and enables the LLM to
perform tree search over possible thoughts and actions to find high confidence
reasoning paths. We evaluate on two popular benchmark datasets. Our results
show that Tree-of-Traversals significantly improves performance on question
answering and KG question answering tasks. Code is available at
\url{https://github.com/amazon-science/tree-of-traversals}",cs.AI,2024-07-31
"ProSpec RL: Plan Ahead, then Execute",,"Imagining potential outcomes of actions before execution helps agents make
more informed decisions, a prospective thinking ability fundamental to human
cognition. However, mainstream model-free Reinforcement Learning (RL) methods
lack the ability to proactively envision future scenarios, plan, and guide
strategies. These methods typically rely on trial and error to adjust policy
functions, aiming to maximize cumulative rewards or long-term value, even if
such high-reward decisions place the environment in extremely dangerous states.
To address this, we propose the Prospective (ProSpec) RL method, which makes
higher-value, lower-risk optimal decisions by imagining future n-stream
trajectories. Specifically, ProSpec employs a dynamic model to predict future
states (termed ""imagined states"") based on the current state and a series of
sampled actions. Furthermore, we integrate the concept of Model Predictive
Control and introduce a cycle consistency constraint that allows the agent to
evaluate and select the optimal actions from these trajectories. Moreover,
ProSpec employs cycle consistency to mitigate two fundamental issues in RL:
augmenting state reversibility to avoid irreversible events (low risk) and
augmenting actions to generate numerous virtual trajectories, thereby improving
data efficiency. We validated the effectiveness of our method on the DMControl
benchmarks, where our approach achieved significant performance improvements.
Code will be open-sourced upon acceptance.",cs.LG cs.AI cs.IR,2024-07-31
"ESIQA: Perceptual Quality Assessment of Vision-Pro-based Egocentric
  Spatial Images",,"With the development of eXtended Reality (XR), head-mounted shooting and
display technology have experienced significant advancement and gained
considerable attention. Egocentric spatial images and videos are emerging as a
compelling form of stereoscopic XR content. Different from traditional 2D
images, egocentric spatial images present challenges for perceptual quality
assessment due to their special shooting, processing methods, and stereoscopic
characteristics. However, the corresponding image quality assessment (IQA)
research for egocentric spatial images is still lacking. In this paper, we
establish the Egocentric Spatial Images Quality Assessment Database (ESIQAD),
the first IQA database dedicated for egocentric spatial images as far as we
know. Our ESIQAD includes 500 egocentric spatial images, containing 400 images
captured with the Apple Vision Pro and 100 images generated via an iPhone's
""Spatial Camera"" app. The corresponding mean opinion scores (MOSs) are
collected under three viewing modes, including 2D display, 3D-window display,
and 3D-immersive display. Furthermore, based on our database, we conduct a
benchmark experiment and evaluate the performance of 22 state-of-the-art IQA
models under three different viewing modes. We hope this research can
facilitate future IQA research on egocentric spatial images. The database is
available at https://github.com/IntMeGroup/ESIQA.",cs.CV cs.MM,2024-07-31
Personalized Multi-task Training for Recommender System,,"In the vast landscape of internet information, recommender systems (RecSys)
have become essential for guiding users through a sea of choices aligned with
their preferences. These systems have applications in diverse domains, such as
news feeds, game suggestions, and shopping recommendations. Personalization is
a key technique in RecSys, where modern methods leverage representation
learning to encode user/item interactions into embeddings, forming the
foundation for personalized recommendations. However, integrating information
from multiple sources to enhance recommendation performance remains
challenging. This paper introduces a novel approach named PMTRec, the first
personalized multi-task learning algorithm to obtain comprehensive user/item
embeddings from various information sources. Addressing challenges specific to
personalized RecSys, we develop modules to handle personalized task weights,
diverse task orientations, and variations in gradient magnitudes across tasks.
PMTRec dynamically adjusts task weights based on gradient norms for each
user/item, employs a Task Focusing module to align gradient combinations with
the main recommendation task, and uses a Gradient Magnitude Balancing module to
ensure balanced training across tasks. Through extensive experiments on three
real-world datasets with different scales, we demonstrate that PMTRec
significantly outperforms existing multi-task learning methods, showcasing its
effectiveness in achieving enhanced recommendation accuracy by leveraging
multiple tasks simultaneously. Our contributions open new avenues for advancing
personalized multi-task training in recommender systems.",cs.IR,2024-07-31
"Blink: Fast Automated Design of Run-Time Power Monitors on FPGA-Based
  Computing Platforms",,"The current over-provisioned heterogeneous multi-cores require effective
run-time optimization strategies, and the run-time power monitoring subsystem
is paramount for their success. Several state-of-the-art methodologies address
the design of a run-time power monitoring infrastructure for generic computing
platforms. However, the power model's training requires time-consuming
gate-level simulations that, coupled with the ever-increasing complexity of the
modern heterogeneous platforms, dramatically hinder the usability of such
solutions. This paper introduces Blink, a scalable framework for the fast and
automated design of run-time power monitoring infrastructures targeting
computing platforms implemented on FPGA. Blink optimizes the time-to-solution
to deliver the run-time power monitoring infrastructure by replacing
traditional methodologies' gate-level simulations and power trace computations
with behavioral simulations and direct power trace measurements. Applying Blink
to multiple designs mixing a set of HLS-generated accelerators from a
state-of-the-art benchmark suite demonstrates an average time-to-solution
speedup of 18 times without affecting the quality of the run-time power
estimates.",cs.AR,2024-07-31
"Prompting Medical Large Vision-Language Models to Diagnose Pathologies
  by Visual Question Answering",,"Large Vision-Language Models (LVLMs) have achieved significant success in
recent years, and they have been extended to the medical domain. Although
demonstrating satisfactory performance on medical Visual Question Answering
(VQA) tasks, Medical LVLMs (MLVLMs) suffer from the hallucination problem,
which makes them fail to diagnose complex pathologies. Moreover, they readily
fail to learn minority pathologies due to imbalanced training data. We propose
two prompting strategies for MLVLMs that reduce hallucination and improve VQA
performance. In the first strategy, we provide a detailed explanation of the
queried pathology. In the second strategy, we fine-tune a cheap, weak learner
to achieve high performance on a specific metric, and textually provide its
judgment to the MLVLM. Tested on the MIMIC-CXR-JPG and Chexpert datasets, our
methods significantly improve the diagnostic F1 score, with the highest
increase being 0.27. We also demonstrate that our prompting strategies can be
extended to general LVLM domains. Based on POPE metrics, it effectively
suppresses the false negative predictions of existing LVLMs and improves Recall
by approximately 0.07.",cs.CV cs.AI cs.CL cs.LG,2024-07-31
"An LLM-based Readability Measurement for Unit Tests' Context-aware
  Inputs",,"Automated test techniques usually generate unit tests with higher code
coverage than manual tests. However, the readability of automated tests is
crucial for code comprehension and maintenance. The readability of unit tests
involves many aspects. In this paper, we focus on test inputs. The central
limitation of existing studies on input readability is that they focus on test
codes alone without taking the tested source codes into consideration, making
them either ignore different source codes' different readability requirements
or require manual efforts to write readable inputs. However, we observe that
the source codes specify the contexts that test inputs must satisfy. Based on
such observation, we introduce the \underline{C}ontext \underline{C}onsistency
\underline{C}riterion (a.k.a, C3), which is a readability measurement tool that
leverages Large Language Models to extract primitive-type (including
string-type) parameters' readability contexts from the source codes and checks
whether test inputs are consistent with those contexts. We have also proposed
EvoSuiteC3. It leverages C3's extracted contexts to help EvoSuite generate
readable test inputs. We have evaluated C3's performance on $409$ \java{}
classes and compared manual and automated tests' readability under C3
measurement. The results are two-fold. First, The Precision, Recall, and
F1-Score of C3's mined readability contexts are \precision{}, \recall{}, and
\fone{}, respectively. Second, under C3's measurement, the string-type input
readability scores of EvoSuiteC3, ChatUniTest (an LLM-based test generation
tool), manual tests, and two traditional tools (EvoSuite and Randoop) are
$90\%$, $83\%$, $68\%$, $8\%$, and $8\%$, showing the traditional tools'
inability in generating readable string-type inputs.",cs.SE,2024-07-31
"SHA-CNN: Scalable Hierarchical Aware Convolutional Neural Network for
  Edge AI",,"This paper introduces a Scalable Hierarchical Aware Convolutional Neural
Network (SHA-CNN) model architecture for Edge AI applications. The proposed
hierarchical CNN model is meticulously crafted to strike a balance between
computational efficiency and accuracy, addressing the challenges posed by
resource-constrained edge devices. SHA-CNN demonstrates its efficacy by
achieving accuracy comparable to state-of-the-art hierarchical models while
outperforming baseline models in accuracy metrics. The key innovation lies in
the model's hierarchical awareness, enabling it to discern and prioritize
relevant features at multiple levels of abstraction. The proposed architecture
classifies data in a hierarchical manner, facilitating a nuanced understanding
of complex features within the datasets. Moreover, SHA-CNN exhibits a
remarkable capacity for scalability, allowing for the seamless incorporation of
new classes. This flexibility is particularly advantageous in dynamic
environments where the model needs to adapt to evolving datasets and
accommodate additional classes without the need for extensive retraining.
Testing has been conducted on the PYNQ Z2 FPGA board to validate the proposed
model. The results achieved an accuracy of 99.34%, 83.35%, and 63.66% for
MNIST, CIFAR-10, and CIFAR-100 datasets, respectively. For CIFAR-100, our
proposed architecture performs hierarchical classification with 10% reduced
computation while compromising only 0.7% accuracy with the state-of-the-art.
The adaptability of SHA-CNN to FPGA architecture underscores its potential for
deployment in edge devices, where computational resources are limited. The
SHA-CNN framework thus emerges as a promising advancement in the intersection
of hierarchical CNNs, scalability, and FPGA-based Edge AI.",cs.NE,2024-07-31
"Two Completely Parameter-Free Alternating Gradient Projection Algorithms
  for Nonconvex-(strongly) Concave Minimax Problems",,"Due to their importance in various emerging applications, efficient
algorithms for solving minimax problems have recently received increasing
attention. However, many existing algorithms require prior knowledge of the
problem parameters in order to achieve optimal iteration complexity. In this
paper, we propose a completely parameter-free alternating gradient projection
(PF-AGP) algorithm to solve the smooth nonconvex-(strongly) concave minimax
problems using a backtracking strategy, which does not require prior knowledge
of parameters such as the Lipschtiz constant $L$ or the strongly concave
constant $\mu$. The PF-AGP algorithm utilizes a parameter-free gradient
projection step to alternately update the outer and inner variables in each
iteration. We show that the total number of gradient calls of the PF-AGP
algorithm to obtain an $\varepsilon$-stationary point for nonconvex-strongly
concave minimax problems is upper bounded by $\mathcal{O}\left(
L\kappa^3\varepsilon^{-2} \right)$ where $\kappa$ is the condition number,
while the total number of gradient calls to obtain an $\varepsilon$-stationary
point for nonconvex-concave minimax problems is upper bounded by
$\mathcal{O}\left( L^4\varepsilon^{-4} \right)$. As far as we know, this is the
first completely parameter-free algorithm for solving nonconvex-strongly
concave minimax problems, and it is also the completely parameter-free
algorithm which achieves the best iteration complexity in single loop method
for solving nonconvex-concave minimax problems. Numerical results validate the
efficiency of the proposed PF-AGP algorithm.",math.OC cs.LG stat.ML,2024-07-31
"Dynamic Gesture Recognition in Ultra-Range Distance for Effective
  Human-Robot Interaction",,"This paper presents a novel approach for ultra-range gesture recognition,
addressing Human-Robot Interaction (HRI) challenges over extended distances. By
leveraging human gestures in video data, we propose the Temporal-Spatiotemporal
Fusion Network (TSFN) model that surpasses the limitations of current methods,
enabling robots to understand gestures from long distances. With applications
in service robots, search and rescue operations, and drone-based interactions,
our approach enhances HRI in expansive environments. Experimental validation
demonstrates significant advancements in gesture recognition accuracy,
particularly in prolonged gesture sequences.",cs.RO cs.CV cs.LG,2024-07-31
"An Extended Kalman Filter Integrated Latent Feature Model on Dynamic
  Weighted Directed Graphs",,"A dynamic weighted directed graph (DWDG) is commonly encountered in various
application scenarios. It involves extensive dynamic interactions among
numerous nodes. Most existing approaches explore the intricate temporal
patterns hidden in a DWDG from the purely data-driven perspective, which
suffers from accuracy loss when a DWDG exhibits strong fluctuations over time.
To address this issue, this study proposes a novel
Extended-Kalman-Filter-Incorporated Latent Feature (EKLF) model to represent a
DWDG from the model-driven perspective. Its main idea is divided into the
following two-fold ideas: a) adopting a control model, i.e., the Extended
Kalman Filter (EKF), to track the complex temporal patterns precisely with its
nonlinear state-transition and observation functions; and b) introducing an
alternating least squares (ALS) algorithm to train the latent features (LFs)
alternatively for precisely representing a DWDG. Empirical studies on DWDG
datasets demonstrate that the proposed EKLF model outperforms state-of-the-art
models in prediction accuracy and computational efficiency for missing edge
weights of a DWDG. It unveils the potential for precisely representing a DWDG
by incorporating a control model.",cs.AI,2024-07-31
"Identity-Consistent Diffusion Network for Grading Knee Osteoarthritis
  Progression in Radiographic Imaging",,"Knee osteoarthritis (KOA), a common form of arthritis that causes physical
disability, has become increasingly prevalent in society. Employing
computer-aided techniques to automatically assess the severity and progression
of KOA can greatly benefit KOA treatment and disease management. Particularly,
the advancement of X-ray technology in KOA demonstrates its potential for this
purpose. Yet, existing X-ray prognosis research generally yields a singular
progression severity grade, overlooking the potential visual changes for
understanding and explaining the progression outcome. Therefore, in this study,
a novel generative model is proposed, namely Identity-Consistent Radiographic
Diffusion Network (IC-RDN), for multifaceted KOA prognosis encompassing a
predicted future knee X-ray scan conditioned on the baseline scan.
Specifically, an identity prior module for the diffusion and a downstream
generation-guided progression prediction module are introduced. Compared to
conventional image-to-image generative models, identity priors regularize and
guide the diffusion to focus more on the clinical nuances of the prognosis
based on a contrastive learning strategy. The progression prediction module
utilizes both forecasted and baseline knee scans, and a more comprehensive
formulation of KOA severity progression grading is expected. Extensive
experiments on a widely used public dataset, OAI, demonstrate the effectiveness
of the proposed method.",eess.IV cs.CV,2024-07-31
"GEGA: Graph Convolutional Networks and Evidence Retrieval Guided
  Attention for Enhanced Document-level Relation Extraction",,"Document-level relation extraction (DocRE) aims to extract relations between
entities from unstructured document text. Compared to sentence-level relation
extraction, it requires more complex semantic understanding from a broader text
context. Currently, some studies are utilizing logical rules within evidence
sentences to enhance the performance of DocRE. However, in the data without
provided evidence sentences, researchers often obtain a list of evidence
sentences for the entire document through evidence retrieval (ER). Therefore,
DocRE suffers from two challenges: firstly, the relevance between evidence and
entity pairs is weak; secondly, there is insufficient extraction of complex
cross-relations between long-distance multi-entities. To overcome these
challenges, we propose GEGA, a novel model for DocRE. The model leverages graph
neural networks to construct multiple weight matrices, guiding attention
allocation to evidence sentences. It also employs multi-scale representation
aggregation to enhance ER. Subsequently, we integrate the most efficient
evidence information to implement both fully supervised and weakly supervised
training processes for the model. We evaluate the GEGA model on three widely
used benchmark datasets: DocRED, Re-DocRED, and Revisit-DocRED. The
experimental results indicate that our model has achieved comprehensive
improvements compared to the existing SOTA model.",cs.CL cs.AI,2024-07-31
"SmileyNet -- Towards the Prediction of the Lottery by Reading Tea Leaves
  with AI",,"We introduce SmileyNet, a novel neural network with psychic abilities. It is
inspired by the fact that a positive mood can lead to improved cognitive
capabilities including classification tasks. The network is hence presented in
a first phase with smileys and an encouraging loss function is defined to bias
it into a good mood. SmileyNet is then used to forecast the flipping of a coin
based on an established method of Tasseology, namely by reading tea leaves.
Training and testing in this second phase are done with a high-fidelity
simulation based on real-world pixels sampled from a professional tea-reading
cup. SmileyNet has an amazing accuracy of 72% to correctly predict the flip of
a coin. Resnet-34, respectively YOLOv5 achieve only 49%, respectively 53%. It
is then shown how multiple SmileyNets can be combined to win the lottery.",cs.AI cs.CV cs.CY cs.LG cs.RO,2024-07-31
"Design and Development of Laughter Recognition System Based on
  Multimodal Fusion and Deep Learning",,"This study aims to design and implement a laughter recognition system based
on multimodal fusion and deep learning, leveraging image and audio processing
technologies to achieve accurate laughter recognition and emotion analysis.
First, the system loads video files and uses the OpenCV library to extract
facial information while employing the Librosa library to process audio
features such as MFCC. Then, multimodal fusion techniques are used to integrate
image and audio features, followed by training and prediction using deep
learning models. Evaluation results indicate that the model achieved 80%
accuracy, precision, and recall on the test dataset, with an F1 score of 80%,
demonstrating robust performance and the ability to handle real-world data
variability. This study not only verifies the effectiveness of multimodal
fusion methods in laughter recognition but also highlights their potential
applications in affective computing and human-computer interaction. Future work
will focus on further optimizing feature extraction and model architecture to
improve recognition accuracy and expand application scenarios, promoting the
development of laughter recognition technology in fields such as mental health
monitoring and educational activity evaluation",cs.SD cs.CV cs.MM eess.AS,2024-07-31
"Force Sensing Guided Artery-Vein Segmentation via Sequential Ultrasound
  Images",,"Accurate identification of arteries and veins in ultrasound images is crucial
for vascular examinations and interventions in robotics-assisted surgeries.
However, current methods for ultrasound vessel segmentation face challenges in
distinguishing between arteries and veins due to their morphological
similarities. To address this challenge, this study introduces a novel force
sensing guided segmentation approach to enhance artery-vein segmentation
accuracy by leveraging their distinct deformability. Our proposed method
utilizes force magnitude to identify key frames with the most significant
vascular deformation in a sequence of ultrasound images. These key frames are
then integrated with the current frame through attention mechanisms, with
weights assigned in accordance with force magnitude. Our proposed force sensing
guided framework can be seamlessly integrated into various segmentation
networks and achieves significant performance improvements in multiple U-shaped
networks such as U-Net, Swin-unet and Transunet. Furthermore, we contribute the
first multimodal ultrasound artery-vein segmentation dataset, Mus-V, which
encompasses both force and image data simultaneously. The dataset comprises
3114 ultrasound images of carotid and femoral vessels extracted from 105
videos, with corresponding force data recorded by the force sensor mounted on
the US probe. Our code and dataset will be publicly available.",eess.IV cs.CV,2024-07-31
Fingerprint Theft Using Smart Padlocks: Droplock Exploits and Defenses,,"There is growing adoption of smart devices such as digital locks with remote
control and sophisticated authentication mechanisms. However, a lack of
attention to device security and user-awareness beyond the primary function of
these IoT devices may be exposing users to invisible risks. This paper extends
upon prior work that defined the ""droplock"", an attack whereby a smart lock is
turned into a wireless fingerprint harvester. We perform a more in-depth
analysis of a broader range of vulnerabilities and exploits that make a
droplock attack easier to perform and harder to detect. Analysis is extended to
a range of other smart lock models, and a threat model is used as the basis to
recommend stronger security controls that may mitigate the risks of such as
attack.",cs.CR,2024-07-31
"Rico: extended TIAGo robot towards up-to-date social and assistive robot
  usage scenarios",,"Social and assistive robotics have vastly increased in popularity in recent
years. Due to the wide range of usage, robots executing such tasks must be
highly reliable and possess enough functions to satisfy multiple scenarios.
This article describes a mobile, artificial intelligence-driven, robotic
platform Rico. Its prior usage in similar scenarios, the number of its
capabilities, and the experiments it presented should qualify it as a proper
arm-less platform for social and assistive circumstances.",cs.RO,2024-07-31
"DD-rPPGNet: De-interfering and Descriptive Feature Learning for
  Unsupervised rPPG Estimation",,"Remote Photoplethysmography (rPPG) aims to measure physiological signals and
Heart Rate (HR) from facial videos. Recent unsupervised rPPG estimation methods
have shown promising potential in estimating rPPG signals from facial regions
without relying on ground truth rPPG signals. However, these methods seem
oblivious to interference existing in rPPG signals and still result in
unsatisfactory performance. In this paper, we propose a novel De-interfered and
Descriptive rPPG Estimation Network (DD-rPPGNet) to eliminate the interference
within rPPG features for learning genuine rPPG signals. First, we investigate
the characteristics of local spatial-temporal similarities of interference and
design a novel unsupervised model to estimate the interference. Next, we
propose an unsupervised de-interfered method to learn genuine rPPG signals with
two stages. In the first stage, we estimate the initial rPPG signals by
contrastive learning from both the training data and their augmented
counterparts. In the second stage, we use the estimated interference features
to derive de-interfered rPPG features and encourage the rPPG signals to be
distinct from the interference. In addition, we propose an effective
descriptive rPPG feature learning by developing a strong 3D Learnable
Descriptive Convolution (3DLDC) to capture the subtle chrominance changes for
enhancing rPPG estimation. Extensive experiments conducted on five rPPG
benchmark datasets demonstrate that the proposed DD-rPPGNet outperforms
previous unsupervised rPPG estimation methods and achieves competitive
performances with state-of-the-art supervised rPPG methods.",cs.CV,2024-07-31
"Exploring the Role of Social Support when Integrating Generative AI into
  Small Business Workflows",,"Small business owners stand to benefit from generative AI technologies due to
limited resources, yet they must navigate increasing legal and ethical risks.
In this paper, we interview 11 entrepreneurs and support personnel to
investigate existing practices of how entrepreneurs integrate generative AI
technologies into their business workflows. Specifically, we build on
scholarship in HCI which emphasizes the role of small, offline networks in
supporting entrepreneurs' technology maintenance. We detail how entrepreneurs
resourcefully leveraged their local networks to discover new use cases of
generative AI (e.g., by sharing accounts), assuage heightened techno-anxieties
(e.g., by recruiting trusted confidants), overcome barriers to sustained use
(e.g., by receiving wrap-around support), and establish boundaries of use.
Further, we suggest how generative AI platforms may be redesigned to better
support entrepreneurs, such as by taking into account the benefits and tensions
of use in a social context.",cs.HC,2024-07-31
Deep Fr\'echet Regression,,"Advancements in modern science have led to the increasing availability of
non-Euclidean data in metric spaces. This paper addresses the challenge of
modeling relationships between non-Euclidean responses and multivariate
Euclidean predictors. We propose a flexible regression model capable of
handling high-dimensional predictors without imposing parametric assumptions.
Two primary challenges are addressed: the curse of dimensionality in
nonparametric regression and the absence of linear structure in general metric
spaces. The former is tackled using deep neural networks, while for the latter
we demonstrate the feasibility of mapping the metric space where responses
reside to a low-dimensional Euclidean space using manifold learning. We
introduce a reverse mapping approach, employing local Fr\'echet regression, to
map the low-dimensional manifold representations back to objects in the
original metric space. We develop a theoretical framework, investigating the
convergence rate of deep neural networks under dependent sub-Gaussian noise
with bias. The convergence rate of the proposed regression model is then
obtained by expanding the scope of local Fr\'echet regression to accommodate
multivariate predictors in the presence of errors in predictors. Simulations
and case studies show that the proposed model outperforms existing methods for
non-Euclidean responses, focusing on the special cases of probability measures
and networks.",stat.ME cs.LG,2024-07-31
Benchmarking AIGC Video Quality Assessment: A Dataset and Unified Model,,"In recent years, artificial intelligence (AI) driven video generation has
garnered significant attention due to advancements in stable diffusion and
large language model techniques. Thus, there is a great demand for accurate
video quality assessment (VQA) models to measure the perceptual quality of
AI-generated content (AIGC) videos as well as optimize video generation
techniques. However, assessing the quality of AIGC videos is quite challenging
due to the highly complex distortions they exhibit (e.g., unnatural action,
irrational objects, etc.). Therefore, in this paper, we try to systemically
investigate the AIGC-VQA problem from both subjective and objective quality
assessment perspectives. For the subjective perspective, we construct a
Large-scale Generated Vdeo Quality assessment (LGVQ) dataset, consisting of
2,808 AIGC videos generated by 6 video generation models using 468 carefully
selected text prompts. Unlike previous subjective VQA experiments, we evaluate
the perceptual quality of AIGC videos from three dimensions: spatial quality,
temporal quality, and text-to-video alignment, which hold utmost importance for
current video generation techniques. For the objective perspective, we
establish a benchmark for evaluating existing quality assessment metrics on the
LGVQ dataset, which reveals that current metrics perform poorly on the LGVQ
dataset. Thus, we propose a Unify Generated Video Quality assessment (UGVQ)
model to comprehensively and accurately evaluate the quality of AIGC videos
across three aspects using a unified model, which uses visual, textual and
motion features of video and corresponding prompt, and integrates key features
to enhance feature expression. We hope that our benchmark can promote the
development of quality evaluation metrics for AIGC videos. The LGVQ dataset and
the UGVQ metric will be publicly released.",cs.CV,2024-07-31
"Games in Public Announcement: How to Reduce System Losses in Optimistic
  Blockchain Mechanisms",,"Announcement games, where information is disseminated by announcers and
challenged by validators, are prevalent in real-world scenarios. Validators
take effort to verify the validity of the announcements, gaining rewards for
successfully challenging invalid ones, while receiving nothing for valid ones.
Optimistic Rollup, a Layer 2 blockchain scaling solution, exemplifies such
games, offering significant improvements in transaction throughput and cost
efficiency. We present a game-theoretic model of announcement games to analyze
the potential behaviors of announcers and validators. We identify all Nash
equilibria and study the corresponding system losses for different Nash
equilibria. Additionally, we analyze the impact of various system parameters on
system loss under the Nash equilibrium. Finally, we provide suggestions for
mechanism optimization to reduce system losses.",cs.GT,2024-07-31
"Towards interfacing large language models with ASR systems using
  confidence measures and prompting",,"As large language models (LLMs) grow in parameter size and capabilities, such
as interaction through prompting, they open up new ways of interfacing with
automatic speech recognition (ASR) systems beyond rescoring n-best lists. This
work investigates post-hoc correction of ASR transcripts with LLMs. To avoid
introducing errors into likely accurate transcripts, we propose a range of
confidence-based filtering methods. Our results indicate that this can improve
the performance of less competitive ASR systems.",eess.AS cs.CL,2024-07-31
"VIPeR: Visual Incremental Place Recognition with Adaptive Mining and
  Lifelong Learning",,"Visual place recognition (VPR) is an essential component of many autonomous
and augmented/virtual reality systems. It enables the systems to robustly
localize themselves in large-scale environments. Existing VPR methods
demonstrate attractive performance at the cost of heavy pre-training and
limited generalizability. When deployed in unseen environments, these methods
exhibit significant performance drops. Targeting this issue, we present VIPeR,
a novel approach for visual incremental place recognition with the ability to
adapt to new environments while retaining the performance of previous
environments. We first introduce an adaptive mining strategy that balances the
performance within a single environment and the generalizability across
multiple environments. Then, to prevent catastrophic forgetting in lifelong
learning, we draw inspiration from human memory systems and design a novel
memory bank for our VIPeR. Our memory bank contains a sensory memory, a working
memory and a long-term memory, with the first two focusing on the current
environment and the last one for all previously visited environments.
Additionally, we propose a probabilistic knowledge distillation to explicitly
safeguard the previously learned knowledge. We evaluate our proposed VIPeR on
three large-scale datasets, namely Oxford Robotcar, Nordland, and TartanAir.
For comparison, we first set a baseline performance with naive finetuning.
Then, several more recent lifelong learning methods are compared. Our VIPeR
achieves better performance in almost all aspects with the biggest improvement
of 13.65% in average performance.",cs.CV cs.RO,2024-07-31
"Dancing in Chains: Reconciling Instruction Following and Faithfulness in
  Language Models",,"Modern language models (LMs) need to follow human instructions while being
faithful; yet, they often fail to achieve both. Here, we provide concrete
evidence of a trade-off between instruction following (i.e., follow open-ended
instructions) and faithfulness (i.e., ground responses in given context) when
training LMs with these objectives. For instance, fine-tuning LLaMA-7B on
instruction following datasets renders it less faithful. Conversely,
instruction-tuned Vicuna-7B shows degraded performance at following
instructions when further optimized on tasks that require contextual grounding.
One common remedy is multi-task learning (MTL) with data mixing, yet it remains
far from achieving a synergic outcome. We propose a simple yet effective method
that relies on Rejection Sampling for Continued Self-instruction Tuning
(ReSet), which significantly outperforms vanilla MTL. Surprisingly, we find
that less is more, as training ReSet with high-quality, yet substantially
smaller data (three-fold less) yields superior results. Our findings offer a
better understanding of objective discrepancies in alignment training of LMs.",cs.CL,2024-07-31
"FTuner: A Fast Dynamic Shape Tensors Program Auto-Tuner for Deep
  Learning Compilers",,"Many artificial intelligence models process input data of different lengths
and resolutions, making the shape of the tensors dynamic. The performance of
these models depends on the shape of the tensors, which makes it difficult to
optimize the tensors before the model runs. There are two common solutions to
this problem. The first is to add useless data to the input to match a
pre-optimized tensor library. The second is to use small basic tensors to
create a tensor that is closest in size to the input data and then tune it to
minimize padding. However, this second solution can be time-consuming.
  This paper proposes a new technique for deep learning compilers called
FTuner. Instead of using a large design space or training a cost model, we use
an abstract computational unit called the uKernel to patch together small,
various-sized tensors to match the shape of the input tensor. We determine the
shape of the uKernel using an analytic hardware information model. Experiments
show that the FTuner can achieve comparable operators and end-to-end
performance to vendor libraries and achieves 3\% speedup on existing auto-tuner
with the model-training compiler while reducing tuning time by two orders of
magnitude.",cs.LG cs.DC,2024-07-31
Generalized Tampered Scene Text Detection in the era of Generative AI,,"The rapid advancements of generative AI have fueled the potential of
generative text image editing while simultaneously escalating the threat of
misinformation spreading. However, existing forensics methods struggle to
detect unseen forgery types that they have not been trained on, leaving the
development of a model capable of generalized detection of tampered scene text
as an unresolved issue. To tackle this, we propose a novel task: open-set
tampered scene text detection, which evaluates forensics models on their
ability to identify both seen and previously unseen forgery types. We have
curated a comprehensive, high-quality dataset, featuring the texts tampered by
eight text editing models, to thoroughly assess the open-set generalization
capabilities. Further, we introduce a novel and effective pre-training paradigm
that subtly alters the texture of selected texts within an image and trains the
model to identify these regions. This approach not only mitigates the scarcity
of high-quality training data but also enhances models' fine-grained perception
and open-set generalization abilities. Additionally, we present DAF, a novel
framework that improves open-set generalization by distinguishing between the
features of authentic and tampered text, rather than focusing solely on the
tampered text's features. Our extensive experiments validate the remarkable
efficacy of our methods. For example, our zero-shot performance can even beat
the previous state-of-the-art full-shot model by a large margin. Our dataset
and code will be open-source.",cs.CV,2024-07-31
Deformable 3D Shape Diffusion Model,,"The Gaussian diffusion model, initially designed for image generation, has
recently been adapted for 3D point cloud generation. However, these adaptations
have not fully considered the intrinsic geometric characteristics of 3D shapes,
thereby constraining the diffusion model's potential for 3D shape manipulation.
To address this limitation, we introduce a novel deformable 3D shape diffusion
model that facilitates comprehensive 3D shape manipulation, including point
cloud generation, mesh deformation, and facial animation. Our approach
innovatively incorporates a differential deformation kernel, which deconstructs
the generation of geometric structures into successive non-rigid deformation
stages. By leveraging a probabilistic diffusion model to simulate this
step-by-step process, our method provides a versatile and efficient solution
for a wide range of applications, spanning from graphics rendering to facial
expression animation. Empirical evidence highlights the effectiveness of our
approach, demonstrating state-of-the-art performance in point cloud generation
and competitive results in mesh deformation. Additionally, extensive visual
demonstrations reveal the significant potential of our approach for practical
applications. Our method presents a unique pathway for advancing 3D shape
manipulation and unlocking new opportunities in the realm of virtual reality.",cs.GR cs.AI,2024-07-31
Chat-like Asserts Prediction with the Support of Large Language Model,,"Unit testing is an essential component of software testing, with the assert
statements playing an important role in determining whether the tested function
operates as expected. Although research has explored automated test case
generation, generating meaningful assert statements remains an ongoing
challenge. While several studies have investigated assert statement generation
in Java, limited work addresses this task in popular dynamically-typed
programming languages like Python. In this paper, we introduce Chat-like
execution-based Asserts Prediction (\tool), a novel Large Language Model-based
approach for generating meaningful assert statements for Python projects. \tool
utilizes the persona, Chain-of-Thought, and one-shot learning techniques in the
prompt design, and conducts rounds of communication with LLM and Python
interpreter to generate meaningful assert statements. We also present a Python
assert statement dataset mined from GitHub. Our evaluation demonstrates that
\tool achieves 64.7\% accuracy for single assert statement generation and 62\%
for overall assert statement generation, outperforming the existing approaches.
We also analyze the mismatched assert statements, which may still share the
same functionality and discuss the potential help \tool could offer to the
automated Python unit test generation. The findings indicate that \tool has the
potential to benefit the SE community through more practical usage scenarios.",cs.SE,2024-07-31
ABCDE: Application-Based Cluster Diff Evals,,"This paper considers the problem of evaluating clusterings of very large
populations of items. Given two clusterings, namely a Baseline clustering and
an Experiment clustering, the tasks are twofold: 1) characterize their
differences, and 2) determine which clustering is better. ABCDE is a novel
evaluation technique for accomplishing that. It aims to be practical: it allows
items to have associated importance values that are application-specific, it is
frugal in its use of human judgements when determining which clustering is
better, and it can report metrics for arbitrary slices of items, thereby
facilitating understanding and debugging. The approach to measuring the delta
in the clustering quality is novel: instead of trying to construct an expensive
ground truth up front and evaluating the each clustering with respect to that,
where the ground truth must effectively pre-anticipate clustering changes,
ABCDE samples questions for judgement on the basis of the actual diffs between
the clusterings. ABCDE builds upon the pointwise metrics for clustering
evaluation, which make the ABCDE metrics intuitive and simple to understand.
The mathematical elegance of the pointwise metrics equip ABCDE with rigorous
yet practical ways to explore the clustering diffs and to estimate the quality
delta.",cs.IR,2024-07-31
"Analyzing the impact of semantic LoD3 building models on image-based
  vehicle localization",,"Numerous navigation applications rely on data from global navigation
satellite systems (GNSS), even though their accuracy is compromised in urban
areas, posing a significant challenge, particularly for precise autonomous car
localization. Extensive research has focused on enhancing localization accuracy
by integrating various sensor types to address this issue. This paper
introduces a novel approach for car localization, leveraging image features
that correspond with highly detailed semantic 3D building models. The core
concept involves augmenting positioning accuracy by incorporating prior
geometric and semantic knowledge into calculations. The work assesses outcomes
using Level of Detail 2 (LoD2) and Level of Detail 3 (LoD3) models, analyzing
whether facade-enriched models yield superior accuracy. This comprehensive
analysis encompasses diverse methods, including off-the-shelf feature matching
and deep learning, facilitating thorough discussion. Our experiments
corroborate that LoD3 enables detecting up to 69\% more features than using
LoD2 models. We believe that this study will contribute to the research of
enhancing positioning accuracy in GNSS-denied urban canyons. It also shows a
practical application of under-explored LoD3 building models on map-based car
positioning.",cs.CV cs.RO,2024-07-31
Transient anisotropic kernel for probabilistic learning on manifolds,,"PLoM (Probabilistic Learning on Manifolds) is a method introduced in 2016 for
handling small training datasets by projecting an It\^o equation from a
stochastic dissipative Hamiltonian dynamical system, acting as the MCMC
generator, for which the KDE-estimated probability measure with the training
dataset is the invariant measure. PLoM performs a projection on a reduced-order
vector basis related to the training dataset, using the diffusion maps (DMAPS)
basis constructed with a time-independent isotropic kernel. In this paper, we
propose a new ISDE projection vector basis built from a transient anisotropic
kernel, providing an alternative to the DMAPS basis to improve statistical
surrogates for stochastic manifolds with heterogeneous data. The construction
ensures that for times near the initial time, the DMAPS basis coincides with
the transient basis. For larger times, the differences between the two bases
are characterized by the angle of their spanned vector subspaces. The optimal
instant yielding the optimal transient basis is determined using an estimation
of mutual information from Information Theory, which is normalized by the
entropy estimation to account for the effects of the number of realizations
used in the estimations. Consequently, this new vector basis better represents
statistical dependencies in the learned probability measure for any dimension.
Three applications with varying levels of statistical complexity and data
heterogeneity validate the proposed theory, showing that the transient
anisotropic kernel improves the learned probability measure.",stat.ML cs.LG,2024-07-31
"A Plug-and-Play Method for Rare Human-Object Interactions Detection by
  Bridging Domain Gap",,"Human-object interactions (HOI) detection aims at capturing human-object
pairs in images and corresponding actions. It is an important step toward
high-level visual reasoning and scene understanding. However, due to the
natural bias from the real world, existing methods mostly struggle with rare
human-object pairs and lead to sub-optimal results. Recently, with the
development of the generative model, a straightforward approach is to construct
a more balanced dataset based on a group of supplementary samples.
Unfortunately, there is a significant domain gap between the generated data and
the original data, and simply merging the generated images into the original
dataset cannot significantly boost the performance. To alleviate the above
problem, we present a novel model-agnostic framework called
\textbf{C}ontext-\textbf{E}nhanced \textbf{F}eature \textbf{A}lignment (CEFA)
module, which can effectively align the generated data with the original data
at the feature level and bridge the domain gap. Specifically, CEFA consists of
a feature alignment module and a context enhancement module. On one hand,
considering the crucial role of human-object pairs information in HOI tasks,
the feature alignment module aligns the human-object pairs by aggregating
instance information. On the other hand, to mitigate the issue of losing
important context information caused by the traditional discriminator-style
alignment method, we employ a context-enhanced image reconstruction module to
improve the model's learning ability of contextual cues. Extensive experiments
have shown that our method can serve as a plug-and-play module to improve the
detection performance of HOI models on rare
categories\footnote{https://github.com/LijunZhang01/CEFA}.",cs.CV,2024-07-31
"MLLM Is a Strong Reranker: Advancing Multimodal Retrieval-augmented
  Generation via Knowledge-enhanced Reranking and Noise-injected Training",,"Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in processing and generating content across multiple data
modalities, including text, images, audio, and video. However, a significant
drawback of MLLMs is their reliance on static training data, leading to
outdated information and limited contextual awareness. This static nature
hampers their ability to provide accurate, up-to-date responses, particularly
in dynamic or rapidly evolving contexts. Integrating Multimodal
Retrieval-augmented Generation (Multimodal RAG) offers a promising solution,
but the system would inevitably encounter the multi-granularity noisy
correspondence (MNC) problem, which involves two types of noise: coarse-grained
(query-caption) and fine-grained (query-image). This noise hinders accurate
retrieval and generation. In this work, we propose \textbf{RagLLaVA}, a novel
framework with knowledge-enhanced reranking and noise-injected training, to
address these limitations. We instruction-tune the MLLM with a simple yet
effective instruction template to induce its ranking ability and serve it as a
reranker to precisely filter the top-k retrieved images. For generation, we
inject visual noise during training at the data and token levels to enhance the
generator's robustness. Extensive experiments are conducted on the subsets of
two datasets that require retrieving and reasoning over images to answer a
given query. Our results demonstrate the superiority of RagLLaVA in retrieving
accurately and generating robustly. Code and models are available at
https://github.com/IDEA-FinAI/RagLLaVA.",cs.AI cs.CL cs.LG,2024-07-31
"QuestGen: Effectiveness of Question Generation Methods for Fact-Checking
  Applications",,"Verifying fact-checking claims poses a significant challenge, even for
humans. Recent approaches have demonstrated that decomposing claims into
relevant questions to gather evidence enhances the efficiency of the
fact-checking process. In this paper, we provide empirical evidence showing
that this question decomposition can be effectively automated. We demonstrate
that smaller generative models, fine-tuned for the question generation task
using data augmentation from various datasets, outperform large language models
by up to 8%. Surprisingly, in some cases, the evidence retrieved using
machine-generated questions proves to be significantly more effective for
fact-checking than that obtained from human-written questions. We also perform
manual evaluation of the decomposed questions to assess the quality of the
questions generated.",cs.CL,2024-07-31
"Improving Faithfulness of Large Language Models in Summarization via
  Sliding Generation and Self-Consistency",,"Despite large language models (LLMs) have demonstrated impressive performance
in various tasks, they are still suffering from the factual inconsistency
problem called hallucinations. For instance, LLMs occasionally generate content
that diverges from source article, and prefer to extract information that
appears at the beginning and end of the context, especially in long document
summarization. Inspired by these findings, we propose to improve the
faithfulness of LLMs in summarization by impelling them to process the entire
article more fairly and faithfully. We present a novel summary generation
strategy, namely SliSum, which exploits the ideas of sliding windows and
self-consistency. Specifically, SliSum divides the source article into
overlapping windows, and utilizes LLM to generate local summaries for the
content in the windows. Finally, SliSum aggregates all local summaries using
clustering and majority voting algorithm to produce more faithful summary of
entire article. Extensive experiments demonstrate that SliSum significantly
improves the faithfulness of diverse LLMs including LLaMA-2, Claude-2 and
GPT-3.5 in both short and long text summarization, while maintaining their
fluency and informativeness and without additional fine-tuning and resources.
We further conduct qualitative and quantitative studies to investigate why
SliSum works and impacts of hyperparameters in SliSum on performance.",cs.CL cs.AI,2024-07-31
"Accelerating Image Super-Resolution Networks with Pixel-Level
  Classification",,"In recent times, the need for effective super-resolution (SR) techniques has
surged, especially for large-scale images ranging 2K to 8K resolutions. For
DNN-based SISR, decomposing images into overlapping patches is typically
necessary due to computational constraints. In such patch-decomposing scheme,
one can allocate computational resources differently based on each patch's
difficulty to further improve efficiency while maintaining SR performance.
However, this approach has a limitation: computational resources is uniformly
allocated within a patch, leading to lower efficiency when the patch contain
pixels with varying levels of restoration difficulty. To address the issue, we
propose the Pixel-level Classifier for Single Image Super-Resolution (PCSR), a
novel method designed to distribute computational resources adaptively at the
pixel level. A PCSR model comprises a backbone, a pixel-level classifier, and a
set of pixel-level upsamplers with varying capacities. The pixel-level
classifier assigns each pixel to an appropriate upsampler based on its
restoration difficulty, thereby optimizing computational resource usage. Our
method allows for performance and computational cost balance during inference
without re-training. Our experiments demonstrate PCSR's advantage over existing
patch-distributing methods in PSNR-FLOP trade-offs across different backbone
models and benchmarks. The code is available at
https://github.com/3587jjh/PCSR.",cs.CV,2024-07-31
"Navigating Beyond Instructions: Vision-and-Language Navigation in
  Obstructed Environments",,"Real-world navigation often involves dealing with unexpected obstructions
such as closed doors, moved objects, and unpredictable entities. However,
mainstream Vision-and-Language Navigation (VLN) tasks typically assume
instructions perfectly align with the fixed and predefined navigation graphs
without any obstructions. This assumption overlooks potential discrepancies in
actual navigation graphs and given instructions, which can cause major failures
for both indoor and outdoor agents. To address this issue, we integrate diverse
obstructions into the R2R dataset by modifying both the navigation graphs and
visual observations, introducing an innovative dataset and task, R2R with
UNexpected Obstructions (R2R-UNO). R2R-UNO contains various types and numbers
of path obstructions to generate instruction-reality mismatches for VLN
research. Experiments on R2R-UNO reveal that state-of-the-art VLN methods
inevitably encounter significant challenges when facing such mismatches,
indicating that they rigidly follow instructions rather than navigate
adaptively. Therefore, we propose a novel method called ObVLN (Obstructed VLN),
which includes a curriculum training strategy and virtual graph construction to
help agents effectively adapt to obstructed environments. Empirical results
show that ObVLN not only maintains robust performance in unobstructed scenarios
but also achieves a substantial performance advantage with unexpected
obstructions.",cs.RO cs.CL cs.CV,2024-07-31
"TinyChirp: Bird Song Recognition Using TinyML Models on Low-power
  Wireless Acoustic Sensors",,"Monitoring biodiversity at scale is challenging. Detecting and identifying
species in fine grained taxonomies requires highly accurate machine learning
(ML) methods. Training such models requires large high quality data sets. And
deploying these models to low power devices requires novel compression
techniques and model architectures. While species classification methods have
profited from novel data sets and advances in ML methods, in particular neural
networks, deploying these state of the art models to low power devices remains
difficult. Here we present a comprehensive empirical comparison of various
tinyML neural network architectures and compression techniques for species
classification. We focus on the example of bird song detection, more concretely
a data set curated for studying the corn bunting bird species. The data set is
released along with all code and experiments of this study. In our experiments
we compare predictive performance, memory and time complexity of classical
spectrogram based methods and recent approaches operating on raw audio signal.
Our results indicate that individual bird species can be robustly detected with
relatively simple architectures that can be readily deployed to low power
devices.",cs.LG cs.AI cs.SD eess.AS eess.SP,2024-07-31
RF Power Transmission for Self-sustaining Miniaturized IoT Devices,,"Radio Frequency (RF) wireless power transfer is a promising technology that
has the potential to constantly power small Internet of Things (IoT) devices,
enabling even battery-less systems and reducing their maintenance requirements.
However, to achieve this ambitious goal, carefully designed RF energy
harvesting (EH) systems are needed to minimize the conversion losses and the
conversion efficiency of the limited power. For intelligent internet of things
sensors and devices, which often have non-constant power requirements, an
additional power management stage with energy storage is needed to temporarily
provide a higher power output than the power being harvested. This paper
proposes an RF wireless power energy conversion system for miniaturized IoT
composed of an impedance matching network, a rectifier, and power management
with energy storage. The proposed sub-system has been experimentally validated
and achieved an overall power conversion efficiency (PCE) of over 30 % for an
input power of -10 dBm and a peak efficiency of 57 % at 3 dBm.",eess.SY cs.SY eess.SP,2024-07-31
"KemenkeuGPT: Leveraging a Large Language Model on Indonesia's Government
  Financial Data and Regulations to Enhance Decision Making",,"Data is crucial for evidence-based policymaking and enhancing public
services, including those at the Ministry of Finance of the Republic of
Indonesia. However, the complexity and dynamic nature of governmental financial
data and regulations can hinder decision-making. This study investigates the
potential of Large Language Models (LLMs) to address these challenges, focusing
on Indonesia's financial data and regulations. While LLMs are effective in the
financial sector, their use in the public sector in Indonesia is unexplored.
This study undertakes an iterative process to develop KemenkeuGPT using the
LangChain with Retrieval-Augmented Generation (RAG), prompt engineering and
fine-tuning. The dataset from 2003 to 2023 was collected from the Ministry of
Finance, Statistics Indonesia and the International Monetary Fund (IMF).
Surveys and interviews with Ministry officials informed, enhanced and
fine-tuned the model. We evaluated the model using human feedback, LLM-based
evaluation and benchmarking. The model's accuracy improved from 35% to 61%,
with correctness increasing from 48% to 64%. The Retrieval-Augmented Generation
Assessment (RAGAS) framework showed that KemenkeuGPT achieved 44% correctness
with 73% faithfulness, 40% precision and 60% recall, outperforming several
other base models. An interview with an expert from the Ministry of Finance
indicated that KemenkeuGPT has the potential to become an essential tool for
decision-making. These results are expected to improve with continuous human
feedback.",cs.AI,2024-07-31
"Multi-agent Assessment with QoS Enhancement for HD Map Updates in a
  Vehicular Network",,"Reinforcement Learning (RL) algorithms have been used to address the
challenging problems in the offloading process of vehicular ad hoc networks
(VANET). More recently, they have been utilized to improve the dissemination of
high-definition (HD) Maps. Nevertheless, implementing solutions such as deep
Q-learning (DQN) and Actor-critic at the autonomous vehicle (AV) may lead to an
increase in the computational load, causing a heavy burden on the computational
devices and higher costs. Moreover, their implementation might raise
compatibility issues between technologies due to the required modifications to
the standards. Therefore, in this paper, we assess the scalability of an
application utilizing a Q-learning single-agent solution in a distributed
multi-agent environment. This application improves the network performance by
taking advantage of a smaller state, and action space whilst using a
multi-agent approach. The proposed solution is extensively evaluated with
different test cases involving reward function considering individual or
overall network performance, number of agents, and centralized and distributed
learning comparison. The experimental results demonstrate that the time
latencies of our proposed solution conducted in voice, video, HD Map, and
best-effort cases have significant improvements, with 40.4%, 36%, 43%, and 12%
respectively, compared to the performances with the single-agent approach.",cs.AI cs.LG,2024-07-31
"MarvelOVD: Marrying Object Recognition and Vision-Language Models for
  Robust Open-Vocabulary Object Detection",,"Learning from pseudo-labels that generated with VLMs~(Vision Language Models)
has been shown as a promising solution to assist open vocabulary detection
(OVD) in recent studies. However, due to the domain gap between VLM and
vision-detection tasks, pseudo-labels produced by the VLMs are prone to be
noisy, while the training design of the detector further amplifies the bias. In
this work, we investigate the root cause of VLMs' biased prediction under the
OVD context. Our observations lead to a simple yet effective paradigm, coded
MarvelOVD, that generates significantly better training targets and optimizes
the learning procedure in an online manner by marrying the capability of the
detector with the vision-language model. Our key insight is that the detector
itself can act as a strong auxiliary guidance to accommodate VLM's inability of
understanding both the ``background'' and the context of a proposal within the
image. Based on it, we greatly purify the noisy pseudo-labels via Online Mining
and propose Adaptive Reweighting to effectively suppress the biased training
boxes that are not well aligned with the target object. In addition, we also
identify a neglected ``base-novel-conflict'' problem and introduce stratified
label assignments to prevent it. Extensive experiments on COCO and LVIS
datasets demonstrate that our method outperforms the other state-of-the-arts by
significant margins. Codes are available at https://github.com/wkfdb/MarvelOVD",cs.CV,2024-07-31
"Deep Learning-Based Longitudinal Prediction of Childhood Myopia
  Progression Using Fundus Image Sequences and Baseline Refraction Data",,"Childhood myopia constitutes a significant global health concern. It exhibits
an escalating prevalence and has the potential to evolve into severe,
irreversible conditions that detrimentally impact familial well-being and
create substantial economic costs. Contemporary research underscores the
importance of precisely predicting myopia progression to enable timely and
effective interventions, thereby averting severe visual impairment in children.
Such predictions predominantly rely on subjective clinical assessments, which
are inherently biased and resource-intensive, thus hindering their widespread
application. In this study, we introduce a novel, high-accuracy method for
quantitatively predicting the myopic trajectory and myopia risk in children
using only fundus images and baseline refraction data. This approach was
validated through a six-year longitudinal study of 3,408 children in Henan,
utilizing 16,211 fundus images and corresponding refractive data. Our method
based on deep learning demonstrated predictive accuracy with an error margin of
0.311D per year and AUC scores of 0.944 and 0.995 for forecasting the risks of
developing myopia and high myopia, respectively. These findings confirm the
utility of our model in supporting early intervention strategies and in
significantly reducing healthcare costs, particularly by obviating the need for
additional metadata and repeated consultations. Furthermore, our method was
designed to rely only on fundus images and refractive error data, without the
need for meta data or multiple inquiries from doctors, strongly reducing the
associated medical costs and facilitating large-scale screening. Our model can
even provide good predictions based on only a single time measurement.
Consequently, the proposed method is an important means to reduce medical
inequities caused by economic disparities.",cs.CV cs.AI,2024-07-31
An Invertible State Space for Process Trees,,"Process models are, like event data, first-class citizens in most process
mining approaches. Several process modeling formalisms have been proposed and
used, e.g., Petri nets, BPMN, and process trees. Despite their frequent use,
little research addresses the formal properties of process trees and the
corresponding potential to improve the efficiency of solving common
computational problems. Therefore, in this paper, we propose an invertible
state space definition for process trees and demonstrate that the corresponding
state space graph is isomorphic to the state space graph of the tree's inverse.
Our result supports the development of novel, time-efficient, decomposition
strategies for applications of process trees. Our experiments confirm that our
state space definition allows for the adoption of bidirectional state space
search, which significantly improves the overall performance of state space
searches.",cs.DS cs.AI,2024-07-31
Fine-gained Zero-shot Video Sampling,,"Incorporating a temporal dimension into pretrained image diffusion models for
video generation is a prevalent approach. However, this method is
computationally demanding and necessitates large-scale video datasets. More
critically, the heterogeneity between image and video datasets often results in
catastrophic forgetting of the image expertise. Recent attempts to directly
extract video snippets from image diffusion models have somewhat mitigated
these problems. Nevertheless, these methods can only generate brief video clips
with simple movements and fail to capture fine-grained motion or non-grid
deformation. In this paper, we propose a novel Zero-Shot video Sampling
algorithm, denoted as $\mathcal{ZS}^2$, capable of directly sampling
high-quality video clips from existing image synthesis methods, such as Stable
Diffusion, without any training or optimization. Specifically, $\mathcal{ZS}^2$
utilizes the dependency noise model and temporal momentum attention to ensure
content consistency and animation coherence, respectively. This ability enables
it to excel in related tasks, such as conditional and context-specialized video
generation and instruction-guided video editing. Experimental results
demonstrate that $\mathcal{ZS}^2$ achieves state-of-the-art performance in
zero-shot video generation, occasionally outperforming recent supervised
methods.
  Homepage: \url{https://densechen.github.io/zss/}.",cs.CV cs.AI,2024-07-31
"On the Problem of Text-To-Speech Model Selection for Synthetic Data
  Generation in Automatic Speech Recognition",,"The rapid development of neural text-to-speech (TTS) systems enabled its
usage in other areas of natural language processing such as automatic speech
recognition (ASR) or spoken language translation (SLT). Due to the large number
of different TTS architectures and their extensions, selecting which TTS
systems to use for synthetic data creation is not an easy task. We use the
comparison of five different TTS decoder architectures in the scope of
synthetic data generation to show the impact on CTC-based speech recognition
training. We compare the recognition results to computable metrics like NISQA
MOS and intelligibility, finding that there are no clear relations to the ASR
performance. We also observe that for data generation auto-regressive decoding
performs better than non-autoregressive decoding, and propose an approach to
quantify TTS generalization capabilities.",cs.CL cs.LG cs.SD eess.AS,2024-07-31
"Designing Harvesting Tools for Olive Trees: Methodological Reflections
  on Exploring and Incorporating Plant Perspectives in the Early Stages of
  Design Process",,"Sustainability-focused design research is witnessing a change in approach
with the emergence of More-than-human Design (MTHD), which challenges
human-centered thinking by incorporating nonhuman perspectives into the design
process. However, implementing MTHD presents challenges for design researchers
and practitioners, such as understanding non-verbal species. Despite the
techniques developed to facilitate such an understanding (e.g. contact zone),
the growing literature on MTHD lacks studies reflecting on how these techniques
are utilized in the design process. In this paper, we present a case study on
designing olive harvesting tools from a MTH lens, where designers used contact
zone, plant interviews, plant persona, and experience map to explore the
perspectives of olive trees and incorporate them into ideas in collaboration
with farmers and agricultural engineers. The results indicate the significance
of reconsidering decentralization in MTHD from the standpoint of entanglements
among techniques and incorporating various knowledge types to manage tensions
arising from perspective shifts.",cs.HC,2024-07-31
"Breaking the Hourglass Phenomenon of Residual Quantization: Enhancing
  the Upper Bound of Generative Retrieval",,"Generative retrieval (GR) has emerged as a transformative paradigm in search
and recommender systems, leveraging numeric-based identifier representations to
enhance efficiency and generalization. Notably, methods like TIGER employing
Residual Quantization-based Semantic Identifiers (RQ-SID), have shown
significant promise in e-commerce scenarios by effectively managing item IDs.
However, a critical issue termed the ""\textbf{Hourglass}"" phenomenon, occurs in
RQ-SID, where intermediate codebook tokens become overly concentrated,
hindering the full utilization of generative retrieval methods. This paper
analyses and addresses this problem by identifying data sparsity and
long-tailed distribution as the primary causes. Through comprehensive
experiments and detailed ablation studies, we analyze the impact of these
factors on codebook utilization and data distribution. Our findings reveal that
the ""Hourglass"" phenomenon substantially impacts the performance of RQ-SID in
generative retrieval. We propose effective solutions to mitigate this issue,
thereby significantly enhancing the effectiveness of generative retrieval in
real-world E-commerce applications.",cs.IR cs.AI,2024-07-31
"Maverick: Efficient and Accurate Coreference Resolution Defying Recent
  Trends",,"Large autoregressive generative models have emerged as the cornerstone for
achieving the highest performance across several Natural Language Processing
tasks. However, the urge to attain superior results has, at times, led to the
premature replacement of carefully designed task-specific approaches without
exhaustive experimentation. The Coreference Resolution task is no exception;
all recent state-of-the-art solutions adopt large generative autoregressive
models that outperform encoder-based discriminative systems. In this work,we
challenge this recent trend by introducing Maverick, a carefully designed - yet
simple - pipeline, which enables running a state-of-the-art Coreference
Resolution system within the constraints of an academic budget, outperforming
models with up to 13 billion parameters with as few as 500 million parameters.
Maverick achieves state-of-the-art performance on the CoNLL-2012 benchmark,
training with up to 0.006x the memory resources and obtaining a 170x faster
inference compared to previous state-of-the-art systems. We extensively
validate the robustness of the Maverick framework with an array of diverse
experiments, reporting improvements over prior systems in data-scarce,
long-document, and out-of-domain settings. We release our code and models for
research purposes at https://github.com/SapienzaNLP/maverick-coref.",cs.CL cs.AI,2024-07-31
"Explainable and Controllable Motion Curve Guided Cardiac Ultrasound
  Video Generation",,"Echocardiography video is a primary modality for diagnosing heart diseases,
but the limited data poses challenges for both clinical teaching and machine
learning training. Recently, video generative models have emerged as a
promising strategy to alleviate this issue. However, previous methods often
relied on holistic conditions during generation, hindering the flexible
movement control over specific cardiac structures. In this context, we propose
an explainable and controllable method for echocardiography video generation,
taking an initial frame and a motion curve as guidance. Our contributions are
three-fold. First, we extract motion information from each heart substructure
to construct motion curves, enabling the diffusion model to synthesize
customized echocardiography videos by modifying these curves. Second, we
propose the structure-to-motion alignment module, which can map semantic
features onto motion curves across cardiac structures. Third, The
position-aware attention mechanism is designed to enhance video consistency
utilizing Gaussian masks with structural position information. Extensive
experiments on three echocardiography datasets show that our method outperforms
others regarding fidelity and consistency. The full code will be released at
https://github.com/mlmi-2024-72/ECM.",eess.IV cs.AI cs.CV cs.LG,2024-07-31
Generative Expressive Conversational Speech Synthesis,,"Conversational Speech Synthesis (CSS) aims to express a target utterance with
the proper speaking style in a user-agent conversation setting. Existing CSS
methods employ effective multi-modal context modeling techniques to achieve
empathy understanding and expression. However, they often need to design
complex network architectures and meticulously optimize the modules within
them. In addition, due to the limitations of small-scale datasets containing
scripted recording styles, they often fail to simulate real natural
conversational styles. To address the above issues, we propose a novel
generative expressive CSS system, termed GPT-Talker.We transform the multimodal
information of the multi-turn dialogue history into discrete token sequences
and seamlessly integrate them to form a comprehensive user-agent dialogue
context. Leveraging the power of GPT, we predict the token sequence, that
includes both semantic and style knowledge, of response for the agent. After
that, the expressive conversational speech is synthesized by the
conversation-enriched VITS to deliver feedback to the user.Furthermore, we
propose a large-scale Natural CSS Dataset called NCSSD, that includes both
naturally recorded conversational speech in improvised styles and dialogues
extracted from TV shows. It encompasses both Chinese and English languages,
with a total duration of 236 hours.We conducted comprehensive experiments on
the reliability of the NCSSD and the effectiveness of our GPT-Talker. Both
subjective and objective evaluations demonstrate that our model outperforms
other state-of-the-art CSS systems significantly in terms of naturalness and
expressiveness. The Code, Dataset, and Pre-trained Model are available at:
https://github.com/AI-S2-Lab/GPT-Talker.",cs.CL cs.SD eess.AS,2024-07-31
Towards Automated Continuous Security Compliance,,"Context: Continuous Software Engineering is increasingly adopted in highly
regulated domains, raising the need for continuous compliance. Adherence to
especially security regulations -- a major concern in highly regulated domains
-- renders Continuous Security Compliance of high relevance to industry and
research.
  Problem: One key barrier to adopting continuous software engineering in the
industry is the resource-intensive and error-prone nature of traditional manual
security compliance activities. Automation promises to be advantageous.
However, continuous security compliance is under-researched, precluding an
effective adoption.
  Contribution: We have initiated a long-term research project with our
industry partner to address these issues. In this manuscript, we make three
contributions: (1) We provide a precise definition of the term continuous
security compliance aligning with the state-of-art, (2) elaborate a preliminary
overview of challenges in the field of automated continuous security compliance
through a tertiary literature study, and (3) present a research roadmap to
address those challenges via automated continuous security compliance.",cs.SE,2024-07-31
"Mitral Regurgitation Recogniton based on Unsupervised
  Out-of-Distribution Detection with Residual Diffusion Amplification",,"Mitral regurgitation (MR) is a serious heart valve disease. Early and
accurate diagnosis of MR via ultrasound video is critical for timely clinical
decision-making and surgical intervention. However, manual MR diagnosis heavily
relies on the operator's experience, which may cause misdiagnosis and
inter-observer variability. Since MR data is limited and has large intra-class
variability, we propose an unsupervised out-of-distribution (OOD) detection
method to identify MR rather than building a deep classifier. To our knowledge,
we are the first to explore OOD in MR ultrasound videos. Our method consists of
a feature extractor, a feature reconstruction model, and a residual
accumulation amplification algorithm. The feature extractor obtains features
from the video clips and feeds them into the feature reconstruction model to
restore the original features. The residual accumulation amplification
algorithm then iteratively performs noise feature reconstruction, amplifying
the reconstructed error of OOD features. This algorithm is straightforward yet
efficient and can seamlessly integrate as a plug-and-play component in
reconstruction-based OOD detection methods. We validated the proposed method on
a large ultrasound dataset containing 893 non-MR and 267 MR videos.
Experimental results show that our OOD detection method can effectively
identify MR samples.",cs.CV,2024-07-31
MaskUno: Switch-Split Block For Enhancing Instance Segmentation,,"Instance segmentation is an advanced form of image segmentation which, beyond
traditional segmentation, requires identifying individual instances of
repeating objects in a scene. Mask R-CNN is the most common architecture for
instance segmentation, and improvements to this architecture include steps such
as benefiting from bounding box refinements, adding semantics, or backbone
enhancements. In all the proposed variations to date, the problem of competing
kernels (each class aims to maximize its own accuracy) persists when models try
to synchronously learn numerous classes. In this paper, we propose mitigating
this problem by replacing mask prediction with a Switch-Split block that
processes refined ROIs, classifies them, and assigns them to specialized mask
predictors. We name the method MaskUno and test it on various models from the
literature, which are then trained on multiple classes using the benchmark COCO
dataset. An increase in the mean Average Precision (mAP) of 2.03% was observed
for the high-performing DetectoRS when trained on 80 classes. MaskUno proved to
enhance the mAP of instance segmentation models regardless of the number and
typ",cs.CV cs.AI,2024-07-31
"DIABLO: A 6-DoF Wheeled Bipedal Robot Composed Entirely of Direct-Drive
  Joints",,"Wheeled bipedal robots offer the advantages of both wheeled and legged
robots, combining the ability to traverse a wide range of terrains and
environments with high efficiency. However, the conventional approach in
existing wheeled bipedal robots involves motor-driven joints with high-ratio
gearboxes. While this approach provides specific benefits, it also presents
several challenges, including increased mechanical complexity, efficiency
losses, noise, vibrations, and higher maintenance and lubrication requirements.
Addressing the aforementioned concerns, we developed a direct-drive wheeled
bipedal robot called DIABLO, which eliminates the use of gearboxes entirely.
Our robotic system is simplified as a second-order inverted pendulum, and we
have designed an LQR-based balance controller to ensure stability.
Additionally, we implemented comprehensive motion controller, including yaw,
split-angle, height, and roll controllers. Through expriments in simulations
and real-world prototype, we have demonstrated that our platform achieves
satisfactory performance.",cs.RO,2024-07-31
"H-Watch: An Open, Connected Platform for AI-Enhanced COVID19 Infection
  Symptoms Monitoring and Contact Tracing",,"The novel COVID-19 disease has been declared a pandemic event. Early
detection of infection symptoms and contact tracing are playing a vital role in
containing COVID-19 spread. As demonstrated by recent literature, multi-sensor
and connected wearable devices might enable symptom detection and help tracing
contacts, while also acquiring useful epidemiological information. This paper
presents the design and implementation of a fully open-source wearable platform
called H-Watch. It has been designed to include several sensors for COVID-19
early detection, multi-radio for wireless transmission and tracking, a
microcontroller for processing data on-board, and finally, an energy harvester
to extend the battery lifetime. Experimental results demonstrated only 5.9 mW
of average power consumption, leading to a lifetime of 9 days on a small watch
battery. Finally, all the hardware and the software, including a machine
learning on MCU toolkit, are provided open-source, allowing the research
community to build and use the H-Watch.",eess.SY cs.SY,2024-07-31
"Root Cause Analysis Of Productivity Losses In Manufacturing Systems
  Utilizing Ensemble Machine Learning",,"In today's rapidly evolving landscape of automation and manufacturing
systems, the efficient resolution of productivity losses is paramount. This
study introduces a data-driven ensemble approach, utilizing the cyclic
multivariate time series data from binary sensors and signals from Programmable
Logic Controllers (PLCs) within these systems. The objective is to
automatically analyze productivity losses per cycle and pinpoint their root
causes by assigning the loss to a system element. The ensemble approach
introduced in this publication integrates various methods, including
information theory and machine learning behavior models, to provide a robust
analysis for each production cycle. To expedite the resolution of productivity
losses and ensure short response times, stream processing becomes a necessity.
Addressing this, the approach is implemented as data-stream analysis and can be
transferred to batch processing, seamlessly integrating into existing systems
without the need for extensive historical data analysis. This method has two
positive effects. Firstly, the result of the analysis ensures that the period
of lower productivity is reduced by identifying the likely root cause of the
productivity loss. Secondly, these results are more reliable due to the
ensemble approach and therefore avoid dependency on technical experts. The
approach is validated using a semi-automated welding manufacturing system, an
injection molding automation system, and a synthetically generated test PLC
dataset. The results demonstrate the method's efficacy in offering a
data-driven understanding of process behavior and mark an advancement in
autonomous manufacturing system analysis.",cs.LG,2024-07-31
"FSSC: Federated Learning of Transformer Neural Networks for Semantic
  Image Communication",,"In this paper, we address the problem of image semantic communication in a
multi-user deployment scenario and propose a federated learning (FL) strategy
for a Swin Transformer-based semantic communication system (FSSC). Firstly, we
demonstrate that the adoption of a Swin Transformer for joint source-channel
coding (JSCC) effectively extracts semantic information in the communication
system. Next, the FL framework is introduced to collaboratively learn a global
model by aggregating local model parameters, rather than directly sharing
clients' data. This approach enhances user privacy protection and reduces the
workload on the server or mobile edge. Simulation evaluations indicate that our
method outperforms the typical JSCC algorithm and traditional separate-based
communication algorithms. Particularly after integrating local semantics, the
global aggregation model has further increased the Peak Signal-to-Noise Ratio
(PSNR) by more than 2dB, thoroughly proving the effectiveness of our algorithm.",cs.AI cs.LG eess.IV,2024-07-31
"Machine Learning In-Sensors: Computation-enabled Intelligent Sensors For
  Next Generation of IoT",,"Smart sensors are an emerging technology that allows combining the data
acquisition with the elaboration directly on the Edge device, very close to the
sensors. To push this concept to the extreme, technology companies are
proposing a new generation of sensors allowing to move the intelligence from
the edge host device, typically a microcontroller, directly to the
ultra-low-power sensor itself, in order to further reduce the miniaturization,
cost and energy efficiency. This paper evaluates the capabilities of a novel
and promising solution from STMicroelectronics. The presence of a floating
point unit and an accelerator for binary neural networks provide capabilities
for in-sensor feature extraction and machine learning. We propose a comparison
of full-precision and binary neural networks for activity recognition with
accelerometer data generated by the sensor itself. Experimental results have
demonstrated that the sensor can achieve an inference performance of 10.7
cycles/MAC, comparable to a Cortex-M4-based microcontroller, with
full-precision networks, and up to 1.5 cycles/MAC with large binary models for
low latency inference, with an average energy consumption of only 90
$\mu$J/inference with the core running at 5 MHz.",eess.SP cs.AR cs.SY eess.SY,2024-07-31
PEAR: Phrase-Based Hand-Object Interaction Anticipation,,"First-person hand-object interaction anticipation aims to predict the
interaction process over a forthcoming period based on current scenes and
prompts. This capability is crucial for embodied intelligence and human-robot
collaboration. The complete interaction process involves both pre-contact
interaction intention (i.e., hand motion trends and interaction hotspots) and
post-contact interaction manipulation (i.e., manipulation trajectories and hand
poses with contact). Existing research typically anticipates only interaction
intention while neglecting manipulation, resulting in incomplete predictions
and an increased likelihood of intention errors due to the lack of manipulation
constraints. To address this, we propose a novel model, PEAR (Phrase-Based
Hand-Object Interaction Anticipation), which jointly anticipates interaction
intention and manipulation. To handle uncertainties in the interaction process,
we employ a twofold approach. Firstly, we perform cross-alignment of verbs,
nouns, and images to reduce the diversity of hand movement patterns and object
functional attributes, thereby mitigating intention uncertainty. Secondly, we
establish bidirectional constraints between intention and manipulation using
dynamic integration and residual connections, ensuring consistency among
elements and thus overcoming manipulation uncertainty. To rigorously evaluate
the performance of the proposed model, we collect a new task-relevant dataset,
EGO-HOIP, with comprehensive annotations. Extensive experimental results
demonstrate the superiority of our method.",cs.CV,2024-07-31
"Interpreting and learning voice commands with a Large Language Model for
  a robot system",,"Robots are increasingly common in industry and daily life, such as in nursing
homes where they can assist staff. A key challenge is developing intuitive
interfaces for easy communication. The use of Large Language Models (LLMs) like
GPT-4 has enhanced robot capabilities, allowing for real-time interaction and
decision-making. This integration improves robots' adaptability and
functionality. This project focuses on merging LLMs with databases to improve
decision-making and enable knowledge acquisition for request interpretation
problems.",cs.RO cs.CL cs.NE,2024-07-31
Kuramoto oscillators in random networks,,"By means of numerical analysis conducted with the aid of the computer, the
collective synchronization of coupled phase oscillators in the Kuramoto model
in the connected regime of random networks of various sizes is studied. The
oscillators synchronize and achieve phase coherence, and this process is not
significantly affected by the level of connectivity of the network. If the
probability that two oscillators are coupled is around the network connectivity
threshold synchronization still occurs, although in a more attenuated way. If
the size of the network is sufficiently large the oscillators have a phase
transition.",cs.NI nlin.CD,2024-07-31
"Learning Effective Representations for Retrieval Using Self-Distillation
  with Adaptive Relevance Margins",,"Representation-based retrieval models, so-called biencoders, estimate the
relevance of a document to a query by calculating the similarity of their
respective embeddings. Current state-of-the-art biencoders are trained using an
expensive training regime involving knowledge distillation from a teacher model
and batch-sampling. Instead of relying on a teacher model, we contribute a
novel parameter-free loss function for self-supervision that exploits the
pre-trained language modeling capabilities of the encoder model as a training
signal, eliminating the need for batch sampling by performing implicit hard
negative mining. We investigate the capabilities of our proposed approach
through extensive ablation studies, demonstrating that self-distillation can
match the effectiveness of teacher distillation using only 13.5% of the data,
while offering a speedup in training time between 3x and 15x compared to
parametrized losses. Code and data is made openly available.",cs.IR,2024-07-31
"Expanding the Medical Decathlon dataset: segmentation of colon and
  colorectal cancer from computed tomography images",,"Colorectal cancer is the third-most common cancer in the Western Hemisphere.
The segmentation of colorectal and colorectal cancer by computed tomography is
an urgent problem in medicine. Indeed, a system capable of solving this problem
will enable the detection of colorectal cancer at early stages of the disease,
facilitate the search for pathology by the radiologist, and significantly
accelerate the process of diagnosing the disease. However, scientific
publications on medical image processing mostly use closed, non-public data.
This paper presents an extension of the Medical Decathlon dataset with
colorectal markups in order to improve the quality of segmentation algorithms.
An experienced radiologist validated the data, categorized it into subsets by
quality, and published it in the public domain. Based on the obtained results,
we trained neural network models of the UNet architecture with 5-part
cross-validation and achieved a Dice metric quality of $0.6988 \pm 0.3$. The
published markups will improve the quality of colorectal cancer detection and
simplify the radiologist's job for study description.",eess.IV cs.AI cs.CV,2024-07-31
"A Simple Low-bit Quantization Framework for Video Snapshot Compressive
  Imaging",,"Video Snapshot Compressive Imaging (SCI) aims to use a low-speed 2D camera to
capture high-speed scene as snapshot compressed measurements, followed by a
reconstruction algorithm to reconstruct the high-speed video frames.
State-of-the-art (SOTA) deep learning-based algorithms have achieved impressive
performance, yet with heavy computational workload. Network quantization is a
promising way to reduce computational cost. However, a direct low-bit
quantization will bring large performance drop. To address this challenge, in
this paper, we propose a simple low-bit quantization framework (dubbed Q-SCI)
for the end-to-end deep learning-based video SCI reconstruction methods which
usually consist of a feature extraction, feature enhancement, and video
reconstruction module. Specifically, we first design a high-quality feature
extraction module and a precise video reconstruction module to extract and
propagate high-quality features in the low-bit quantized model. In addition, to
alleviate the information distortion of the Transformer branch in the quantized
feature enhancement module, we introduce a shift operation on the query and key
distributions to further bridge the performance gap. Comprehensive experimental
results manifest that our Q-SCI framework can achieve superior performance,
e.g., 4-bit quantized EfficientSCI-S derived by our Q-SCI framework can
theoretically accelerate the real-valued EfficientSCI-S by 7.8X with only 2.3%
performance gap on the simulation testing datasets. Code is available at
https://github.com/mcao92/QuantizedSCI.",cs.CV,2024-07-31
"PhysFlow: Skin tone transfer for remote heart rate estimation through
  conditional normalizing flows",,"In recent years, deep learning methods have shown impressive results for
camera-based remote physiological signal estimation, clearly surpassing
traditional methods. However, the performance and generalization ability of
Deep Neural Networks heavily depends on rich training data truly representing
different factors of variation encountered in real applications. Unfortunately,
many current remote photoplethysmography (rPPG) datasets lack diversity,
particularly in darker skin tones, leading to biased performance of existing
rPPG approaches. To mitigate this bias, we introduce PhysFlow, a novel method
for augmenting skin diversity in remote heart rate estimation using conditional
normalizing flows. PhysFlow adopts end-to-end training optimization, enabling
simultaneous training of supervised rPPG approaches on both original and
generated data. Additionally, we condition our model using CIELAB color space
skin features directly extracted from the facial videos without the need for
skin-tone labels. We validate PhysFlow on publicly available datasets,
UCLA-rPPG and MMPD, demonstrating reduced heart rate error, particularly in
dark skin tones. Furthermore, we demonstrate its versatility and adaptability
across different data-driven rPPG methods.",cs.CV,2024-07-31
The Impacts of AI Avatar Appearance and Disclosure on User Motivation,,"This study examines the influence of perceived AI features on user motivation
in virtual interactions. AI avatars, being disclosed as being an AI, or
embodying specific genders, could be used in user-AI interactions. Leveraging
insights from AI and avatar research, we explore how AI disclosure and gender
affect user motivation. We conducted a game-based experiment involving over
72,500 participants who solved search problems alone or with an AI companion.
Different groups experienced varying AI appearances and disclosures. We
measured play intensity. Results revealed that the presence of another avatar
led to less intense play compared to solo play. Disclosure of the avatar as AI
heightened effort intensity compared to non-disclosed AI companions.
Additionally, a masculine AI appearance reduced effort intensity.",cs.HC cs.AI cs.CY,2024-07-31
"Tabular Data Augmentation for Machine Learning: Progress and Prospects
  of Embracing Generative AI",,"Machine learning (ML) on tabular data is ubiquitous, yet obtaining abundant
high-quality tabular data for model training remains a significant obstacle.
Numerous works have focused on tabular data augmentation (TDA) to enhance the
original table with additional data, thereby improving downstream ML tasks.
Recently, there has been a growing interest in leveraging the capabilities of
generative AI for TDA. Therefore, we believe it is time to provide a
comprehensive review of the progress and future prospects of TDA, with a
particular emphasis on the trending generative AI. Specifically, we present an
architectural view of the TDA pipeline, comprising three main procedures:
pre-augmentation, augmentation, and post-augmentation. Pre-augmentation
encompasses preparation tasks that facilitate subsequent TDA, including error
handling, table annotation, table simplification, table representation, table
indexing, table navigation, schema matching, and entity matching. Augmentation
systematically analyzes current TDA methods, categorized into retrieval-based
methods, which retrieve external data, and generation-based methods, which
generate synthetic data. We further subdivide these methods based on the
granularity of the augmentation process at the row, column, cell, and table
levels. Post-augmentation focuses on the datasets, evaluation and optimization
aspects of TDA. We also summarize current trends and future directions for TDA,
highlighting promising opportunities in the era of generative AI. In addition,
the accompanying papers and related resources are continuously updated and
maintained in the GitHub repository at
https://github.com/SuDIS-ZJU/awesome-tabular-data-augmentation to reflect
ongoing advancements in the field.",cs.LG cs.AI cs.DB,2024-07-31
"Skeleton-Based Action Recognition with Spatial-Structural Graph
  Convolution",,"Human Activity Recognition (HAR) is a field of study that focuses on
identifying and classifying human activities. Skeleton-based Human Activity
Recognition has received much attention in recent years, where Graph
Convolutional Network (GCN) based method is widely used and has achieved
remarkable results. However, the representation of skeleton data and the issue
of over-smoothing in GCN still need to be studied. 1). Compared to central
nodes, edge nodes can only aggregate limited neighbor information, and
different edge nodes of the human body are always structurally related.
However, the information from edge nodes is crucial for fine-grained activity
recognition. 2). The Graph Convolutional Network suffers from a significant
over-smoothing issue, causing nodes to become increasingly similar as the
number of network layers increases. Based on these two ideas, we propose a
two-stream graph convolution method called Spatial-Structural GCN (SpSt-GCN).
Spatial GCN performs information aggregation based on the topological structure
of the human body, and structural GCN performs differentiation based on the
similarity of edge node sequences. The spatial connection is fixed, and the
human skeleton naturally maintains this topology regardless of the actions
performed by humans. However, the structural connection is dynamic and depends
on the type of movement the human body is performing. Based on this idea, we
also propose an entirely data-driven structural connection, which greatly
increases flexibility. We evaluate our method on two large-scale datasets,
i.e., NTU RGB+D and NTU RGB+D 120. The proposed method achieves good results
while being efficient.",cs.CV cs.AI,2024-07-31
"Can LLMs ""Reason"" in Music? An Evaluation of LLMs' Capability of Music
  Understanding and Generation",,"Symbolic Music, akin to language, can be encoded in discrete symbols. Recent
research has extended the application of large language models (LLMs) such as
GPT-4 and Llama2 to the symbolic music domain including understanding and
generation. Yet scant research explores the details of how these LLMs perform
on advanced music understanding and conditioned generation, especially from the
multi-step reasoning perspective, which is a critical aspect in the
conditioned, editable, and interactive human-computer co-creation process. This
study conducts a thorough investigation of LLMs' capability and limitations in
symbolic music processing. We identify that current LLMs exhibit poor
performance in song-level multi-step music reasoning, and typically fail to
leverage learned music knowledge when addressing complex musical tasks. An
analysis of LLMs' responses highlights distinctly their pros and cons. Our
findings suggest achieving advanced musical capability is not intrinsically
obtained by LLMs, and future research should focus more on bridging the gap
between music knowledge and reasoning, to improve the co-creation experience
for musicians.",cs.SD cs.CL cs.MM eess.AS,2024-07-31
Long-Term Forecasts of Failures in Wind Turbines,,"We collect papers forecasting wind turbine failures at least two days in
advance. We examine the prediction time, methods, failed components, and
dataset size. We investigate the effect of using standard SCADA data and data
from additional sensors, such as those measuring vibration. We observe a
positive correlation between dataset size and prediction time. In the
considered cases, one may roughly expect a forecast for at least two days using
a dataset of one turbine year and a forecast for two hundred days from a
dataset of a hundred turbine years.",eess.SY cs.SY,2024-07-31
"ControlMLLM: Training-Free Visual Prompt Learning for Multimodal Large
  Language Models",,"In this work, we propose a training-free method to inject visual referring
into Multimodal Large Language Models (MLLMs) through learnable visual token
optimization. We observe the relationship between text prompt tokens and visual
tokens in MLLMs, where attention layers model the connection between them. Our
approach involves adjusting visual tokens from the MLP output during inference,
controlling which text prompt tokens attend to which visual tokens. We optimize
a learnable visual token based on an energy function, enhancing the strength of
referential regions in the attention map. This enables detailed region
description and reasoning without the need for substantial training costs or
model retraining. Our method offers a promising direction for integrating
referential abilities into MLLMs. Our method support referring with box, mask,
scribble and point. The results demonstrate that our method exhibits
controllability and interpretability.",cs.CV,2024-07-31
Probabilistic Scoring Lists for Interpretable Machine Learning,,"A scoring system is a simple decision model that checks a set of features,
adds a certain number of points to a total score for each feature that is
satisfied, and finally makes a decision by comparing the total score to a
threshold. Scoring systems have a long history of active use in safety-critical
domains such as healthcare and justice, where they provide guidance for making
objective and accurate decisions. Given their genuine interpretability, the
idea of learning scoring systems from data is obviously appealing from the
perspective of explainable AI. In this paper, we propose a practically
motivated extension of scoring systems called probabilistic scoring lists
(PSL), as well as a method for learning PSLs from data. Instead of making a
deterministic decision, a PSL represents uncertainty in the form of probability
distributions, or, more generally, probability intervals. Moreover, in the
spirit of decision lists, a PSL evaluates features one by one and stops as soon
as a decision can be made with enough confidence. To evaluate our approach, we
conduct a case study in the medical domain.",cs.LG,2024-07-31
"Tracing Intricate Cues in Dialogue: Joint Graph Structure and Sentiment
  Dynamics for Multimodal Emotion Recognition",,"Multimodal emotion recognition in conversation (MERC) has garnered
substantial research attention recently. Existing MERC methods face several
challenges: (1) they fail to fully harness direct inter-modal cues, possibly
leading to less-than-thorough cross-modal modeling; (2) they concurrently
extract information from the same and different modalities at each network
layer, potentially triggering conflicts from the fusion of multi-source data;
(3) they lack the agility required to detect dynamic sentimental changes,
perhaps resulting in inaccurate classification of utterances with abrupt
sentiment shifts. To address these issues, a novel approach named GraphSmile is
proposed for tracking intricate emotional cues in multimodal dialogues.
GraphSmile comprises two key components, i.e., GSF and SDP modules. GSF
ingeniously leverages graph structures to alternately assimilate inter-modal
and intra-modal emotional dependencies layer by layer, adequately capturing
cross-modal cues while effectively circumventing fusion conflicts. SDP is an
auxiliary task to explicitly delineate the sentiment dynamics between
utterances, promoting the model's ability to distinguish sentimental
discrepancies. Furthermore, GraphSmile is effortlessly applied to multimodal
sentiment analysis in conversation (MSAC), forging a unified multimodal
affective model capable of executing MERC and MSAC tasks. Empirical results on
multiple benchmarks demonstrate that GraphSmile can handle complex emotional
and sentimental patterns, significantly outperforming baseline models.",cs.CL,2024-07-31
Locomotion Dynamics of an Underactuated Three-Link Robotic Vehicle,,"The wheeled three-link snake robot is a well-known example of an
underactuated system modelled using nonholonomic constraints, preventing
lateral slippage (skid) of the wheels. A kinematically controlled configuration
assumes that both joint angles are directly prescribed as phase-shifted
periodic input. In another configuration of the robot, only one joint is
periodically actuated while the second joint is passively governed by a
visco-elastic torsion spring. In our work, we constructed the two
configurations of the wheeled robot and conducted motion experiments under
different actuation inputs. Analysis of the motion tracking measurements
reveals a significant amount of wheels' skid, in contrast to the assumptions
used in standard nonholonomic models. Therefore, we propose modified dynamic
models which include wheels' skid and viscous friction forces, as well as
rolling resistance. After parameter fitting, these dynamic models reach good
agreement with the motion measurements, including effects of input's frequency
on the mean speed and net displacement per period. This illustrates the
importance of incorporating wheels' skid and friction into the system's model.",cs.RO,2024-07-31
Robust Lossy Audio Compression Identification,,"Previous research contributions on blind lossy compression identification
report near perfect performance metrics on their test set, across a variety of
codecs and bit rates. However, we show that such results can be deceptive and
may not accurately represent true ability of the system to tackle the task at
hand. In this article, we present an investigation into the robustness and
generalisation capability of a lossy audio identification model. Our
contributions are as follows. (1) We show the lack of robustness to codec
parameter variations of a model equivalent to prior art. In particular, when
naively training a lossy compression detection model on a dataset of music
recordings processed with a range of codecs and their lossless counterparts, we
obtain near perfect performance metrics on the held-out test set, but severely
degraded performance on lossy tracks produced with codec parameters not seen in
training. (2) We propose and show the effectiveness of an improved training
strategy to significantly increase the robustness and generalisation capability
of the model beyond codec configurations seen during training. Namely we apply
a random mask to the input spectrogram to encourage the model not to rely
solely on the training set's codec cutoff frequency.",cs.SD eess.AS,2024-07-31
Black box meta-learning intrinsic rewards for sparse-reward environments,,"Despite the successes and progress of deep reinforcement learning over the
last decade, several challenges remain that hinder its broader application.
Some fundamental aspects to improve include data efficiency, generalization
capability, and ability to learn in sparse-reward environments, which often
require human-designed dense rewards. Meta-learning has emerged as a promising
approach to address these issues by optimizing components of the learning
algorithm to meet desired characteristics. Additionally, a different line of
work has extensively studied the use of intrinsic rewards to enhance the
exploration capabilities of algorithms. This work investigates how
meta-learning can improve the training signal received by RL agents. The focus
is on meta-learning intrinsic rewards under a framework that doesn't rely on
the use of meta-gradients. We analyze and compare this approach to the use of
extrinsic rewards and a meta-learned advantage function. The developed
algorithms are evaluated on distributions of continuous control tasks with both
parametric and non-parametric variations, and with only sparse rewards
accessible for the evaluation tasks.",cs.LG,2024-07-31
"CXSimulator: A User Behavior Simulation using LLM Embeddings for
  Web-Marketing Campaign Assessment",,"This paper presents the Customer Experience (CX) Simulator, a novel framework
designed to assess the effects of untested web-marketing campaigns through user
behavior simulations. The proposed framework leverages large language models
(LLMs) to represent various events in a user's behavioral history, such as
viewing an item, applying a coupon, or purchasing an item, as semantic
embedding vectors. We train a model to predict transitions between events from
their LLM embeddings, which can even generalize to unseen events by learning
from diverse training data. In web-marketing applications, we leverage this
transition prediction model to simulate how users might react differently when
new campaigns or products are presented to them. This allows us to eliminate
the need for costly online testing and enhance the marketers' abilities to
reveal insights. Our numerical evaluation and user study, utilizing BigQuery
Public Datasets from the Google Merchandise Store, demonstrate the
effectiveness of our framework.",cs.LG cs.SY eess.SY,2024-07-31
Conditioned Prompt-Optimization for Continual Deepfake Detection,,"The rapid advancement of generative models has significantly enhanced the
realism and customization of digital content creation. The increasing power of
these tools, coupled with their ease of access, fuels the creation of
photorealistic fake content, termed deepfakes, that raises substantial concerns
about their potential misuse. In response, there has been notable progress in
developing detection mechanisms to identify content produced by these advanced
systems. However, existing methods often struggle to adapt to the continuously
evolving landscape of deepfake generation. This paper introduces Prompt2Guard,
a novel solution for exemplar-free continual deepfake detection of images, that
leverages Vision-Language Models (VLMs) and domain-specific multimodal prompts.
Compared to previous VLM-based approaches that are either bounded by prompt
selection accuracy or necessitate multiple forward passes, we leverage a
prediction ensembling technique with read-only prompts. Read-only prompts do
not interact with VLMs internal representation, mitigating the need for
multiple forward passes. Thus, we enhance efficiency and accuracy in detecting
generated content. Additionally, our method exploits a text-prompt conditioning
tailored to deepfake detection, which we demonstrate is beneficial in our
setting. We evaluate Prompt2Guard on CDDB-Hard, a continual deepfake detection
benchmark composed of five deepfake detection datasets spanning multiple
domains and generators, achieving a new state-of-the-art. Additionally, our
results underscore the effectiveness of our approach in addressing the
challenges posed by continual deepfake detection, paving the way for more
robust and adaptable solutions in deepfake detection.",cs.CV,2024-07-31
"Operator-based semantics for choice programs: is choosing losing? (full
  version)",,"Choice constructs are an important part of the language of logic programming,
yet the study of their semantics has been a challenging task. So far, only
two-valued semantics have been studied, and the different proposals for such
semantics have not been compared in a principled way. In this paper, an
operator-based framework allow for the definition and comparison of different
semantics in a principled way is proposed.",cs.AI cs.LO,2024-07-31
"Self-Sovereign Identity for Consented and Content-Based Access to
  Medical Records using Blockchain",,"Electronic Health Records (EHRs) and Medical Data are classified as personal
data in every privacy law, meaning that any related service that includes
processing such data must come with full security, confidentiality, privacy and
accountability. Solutions for health data management, as in storing it, sharing
and processing it, are emerging quickly and were significantly boosted by the
Covid-19 pandemic that created a need to move things online. EHRs makes a
crucial part of digital identity data, and the same digital identity trends --
as in self sovereign identity powered by decentralized ledger technologies like
Blockchain, are being researched or implemented in contexts managing digital
interactions between health facilities, patients and health professionals. In
this paper, we propose a blockchain-based solution enabling secure exchange of
EHRs between different parties powered by a self-sovereign identity (SSI)
wallet and decentralized identifiers. We also make use of a consortium IPFS
network for off-chain storage and attribute-based encryption (ABE) to ensure
data confidentiality and integrity. Through our solution, we grant users full
control over their medical data, and enable them to securely share it in total
confidentiality over secure communication channels between user wallets using
encryption. We also use DIDs for better user privacy and limit any possible
correlations or identification by using pairwise DIDs. Overall, combining this
set of technologies guarantees secure exchange of EHRs, secure storage and
management along with by-design features inherited from the technological
stack.",cs.CR cs.ET,2024-07-31
"Generative Sentiment Analysis via Latent Category Distribution and
  Constrained Decoding",,"Fine-grained sentiment analysis involves extracting and organizing sentiment
elements from textual data. However, existing approaches often overlook issues
of category semantic inclusion and overlap, as well as inherent structural
patterns within the target sequence. This study introduces a generative
sentiment analysis model. To address the challenges related to category
semantic inclusion and overlap, a latent category distribution variable is
introduced. By reconstructing the input of a variational autoencoder, the model
learns the intensity of the relationship between categories and text, thereby
improving sequence generation. Additionally, a trie data structure and
constrained decoding strategy are utilized to exploit structural patterns,
which in turn reduces the search space and regularizes the generation process.
Experimental results on the Restaurant-ACOS and Laptop-ACOS datasets
demonstrate a significant performance improvement compared to baseline models.
Ablation experiments further confirm the effectiveness of latent category
distribution and constrained decoding strategy.",cs.CL cs.AI,2024-07-31
"Algorithmic methods of finite discrete structures. Topological graph
  drawing (part I)",,"Modern methods of graph theory describe a graph up to isomorphism, which
makes it difficult to create mathematical models for visualizing graph drawings
on a plane. The topological drawing of the planar part of a graph allows
representing the planarization process by algebraic methods, without making any
geometric constructions on the plane. Constructing a rotation of graph vertices
solves two most important problems of graph theory simultaneously: the problem
of testing a graph for planarity and the problem of constructing a topological
drawing of a planar graph. It is shown that the problem of constructing a
drawing of a non-planar graph can be reduced to the problem of constructing a
drawing of a planar graph, taking into account the introduction of additional
vertices characterizing the intersection of edges. Naturally, the development
of such a mathematical structure will make it possible to solve the following
important problems of graph theory: testing the planarity of a graph,
identifying the largest planar subgraph of a graph, determining the thickness
of a graph, obtaining a graph with a minimum number of intersections, etc.",math.CO cs.DM,2024-07-31
"Multi-agent reinforcement learning for the control of three-dimensional
  Rayleigh-B\'enard convection",,"Deep reinforcement learning (DRL) has found application in numerous use-cases
pertaining to flow control. Multi-agent RL (MARL), a variant of DRL, has shown
to be more effective than single-agent RL in controlling flows exhibiting
locality and translational invariance. We present, for the first time, an
implementation of MARL-based control of three-dimensional Rayleigh-B\'enard
convection (RBC). Control is executed by modifying the temperature distribution
along the bottom wall divided into multiple control segments, each of which
acts as an independent agent. Two regimes of RBC are considered at Rayleigh
numbers $\mathrm{Ra}=500$ and $750$. Evaluation of the learned control policy
reveals a reduction in convection intensity by $23.5\%$ and $8.7\%$ at
$\mathrm{Ra}=500$ and $750$, respectively. The MARL controller converts
irregularly shaped convective patterns to regular straight rolls with lower
convection that resemble flow in a relatively more stable regime. We draw
comparisons with proportional control at both $\mathrm{Ra}$ and show that MARL
is able to outperform the proportional controller. The learned control strategy
is complex, featuring different non-linear segment-wise actuator delays and
actuation magnitudes. We also perform successful evaluations on a larger domain
than used for training, demonstrating that the invariant property of MARL
allows direct transfer of the learnt policy.",physics.flu-dyn cs.LG,2024-07-31
TRGR: Transmissive RIS-aided Gait Recognition Through Walls,,"Gait recognition with radio frequency (RF) signals enables many potential
applications requiring accurate identification. However, current systems
require individuals to be within a line-of-sight (LOS) environment and struggle
with low signal-to-noise ratio (SNR) when signals traverse concrete and thick
walls. To address these challenges, we present TRGR, a novel transmissive
reconfigurable intelligent surface (RIS)-aided gait recognition system. TRGR
can recognize human identities through walls using only the magnitude
measurements of channel state information (CSI) from a pair of transceivers.
Specifically, by leveraging transmissive RIS alongside a configuration
alternating optimization algorithm, TRGR enhances wall penetration and signal
quality, enabling accurate gait recognition. Furthermore, a residual
convolution network (RCNN) is proposed as the backbone network to learn robust
human information. Experimental results confirm the efficacy of transmissive
RIS, highlighting the significant potential of transmissive RIS in enhancing
RF-based gait recognition systems. Extensive experiment results show that TRGR
achieves an average accuracy of 97.88\% in identifying persons when signals
traverse concrete walls, demonstrating the effectiveness and robustness of
TRGR.",cs.AI,2024-07-31
"Analysis of Functional Insufficiencies and Triggering Conditions to
  Improve the SOTIF of an MPC-based Trajectory Planner",,"Automated and autonomous driving has made a significant technological leap
over the past decade. In this process, the complexity of algorithms used for
vehicle control has grown significantly. Model Predictive Control (MPC) is a
prominent example, which has gained enormous popularity and is now widely used
for vehicle motion planning and control. However, safety concerns constrain its
practical application, especially since traditional procedures of functional
safety (FS), with its universal standard ISO26262, reach their limits.
Concomitantly, the new aspect of safety-of-the-intended-function (SOTIF) has
moved into the center of attention, whose standard, ISO21448, has only been
released in 2022. Thus, experience with SOTIF is low and few case studies are
available in industry and research. Hence this paper aims to make two main
contributions: (1) an analysis of the SOTIF for a generic MPC-based trajectory
planner and (2) an interpretation and concrete application of the generic
procedures described in ISO21448 for determining functional insufficiencies
(FIs) and triggering conditions (TCs). Particular novelties of the paper
include an approach for the out-of-context development of SOTIF-related
elements (SOTIF-EooC), a compilation of important FIs and TCs for a MPC-based
trajectory planner, and an optimized safety concept based on the identified FIs
and TCs for the MPC-based trajectory planner.",eess.SY cs.RO cs.SE cs.SY eess.SP,2024-07-31
Vision and Contact based Optimal Control for Autonomous Trocar Docking,,"Future operating theatres will be equipped with robots to perform various
surgical tasks including, for example, endoscope control. Human-in-the-loop
supervisory control architectures where the surgeon selects from several
autonomous sequences is already being successfully applied in preclinical
tests. Inserting an endoscope into a trocar or introducer is a key step for
every keyhole surgical procedure -- hereafter we will only refer to this device
as a ""trocar"". Our goal is to develop a controller for autonomous trocar
docking.
  Autonomous trocar docking is a version of the peg-in-hole problem. Extensive
work in the robotics literature addresses this problem. The peg-in-hole problem
has been widely studied in the context of assembly where, typically, the hole
is considered static and rigid to interaction. In our case, however, the trocar
is not fixed and responds to interaction. We consider a variety of surgical
procedures where surgeons will utilize contact between the endoscope and trocar
in order to complete the insertion successfully. To the best of our knowledge,
we have not found literature that explores this particular generalization of
the problem directly.
  Our primary contribution in this work is an optimal control formulation for
automated trocar docking. We use a nonlinear optimization program to model the
task, minimizing a cost function subject to constraints to find optimal joint
configurations. The controller incorporates a geometric model for insertion and
a force-feedback (FF) term to ensure patient safety by preventing excessive
interaction forces with the trocar. Experiments, demonstrated on a real
hardware lab setup, validate the approach. Our method successfully achieves
trocar insertion on our real robot lab setup, and simulation trials demonstrate
its ability to reduce interaction forces.",cs.RO,2024-07-31
"PMoE: Progressive Mixture of Experts with Asymmetric Transformer for
  Continual Learning",,"Large Language Models (LLMs) encounter significant challenges in continual
learning due to catastrophic forgetting, where new information overwrites
previously acquired knowledge. This limitation leads to substantial
environmental and economic waste. In this study, we introduce the PMoE,
Progressive Mixture of Experts with Asymmetric Transformer, which aims to
minimize forgetting by utilizing an asymmetric design with shallow layers
dedicated to general knowledge and deep layers for new knowledge. PMoE
incorporates progressively added experts in deep layers and a router that
allocates new knowledge to the appropriate experts efficiently. The router,
positioned adjacent to the deep layers, utilizes deep features aggregating
consolidated information. This enables the router to perform efficiently,
allocating new knowledge to the appropriate experts, which progressively
increase in the deep layers. Extensive experiments on TRACE datasets and
general language understanding datasets demonstrate that the proposed PMoE
outperforms previous state-of-the-art approaches.",cs.CL cs.AI,2024-07-31
"Multi-Site Class-Incremental Learning with Weighted Experts in
  Echocardiography",,"Building an echocardiography view classifier that maintains performance in
real-life cases requires diverse multi-site data, and frequent updates with
newly available data to mitigate model drift. Simply fine-tuning on new
datasets results in ""catastrophic forgetting"", and cannot adapt to variations
of view labels between sites. Alternatively, collecting all data on a single
server and re-training may not be feasible as data sharing agreements may
restrict image transfer, or datasets may only become available at different
times. Furthermore, time and cost associated with re-training grows with every
new dataset. We propose a class-incremental learning method which learns an
expert network for each dataset, and combines all expert networks with a score
fusion model. The influence of ``unqualified experts'' is minimised by
weighting each contribution with a learnt in-distribution score. These weights
promote transparency as the contribution of each expert is known during
inference. Instead of using the original images, we use learned features from
each dataset, which are easier to share and raise fewer licensing and privacy
concerns. We validate our work on six datasets from multiple sites,
demonstrating significant reductions in training time while improving view
classification performance.",cs.CV cs.AI,2024-07-31
"Algorithmic methods of finite discrete structures. Topological graph
  drawing (part II)",,"A visualized graph is a powerful tool for data analysis and synthesis tasks.
In this case, the task of visualization constitutes not only in displaying
vertices and edges according to the graph representation, but also in ensuring
that the result is visually simple and comprehensible for a human. Thus, the
visualization process involves solving several problems, one of which is the
problem of constructing a topological drawing of a planar part of a non-planar
graph with a minimum number of removed edges. In this manuscript, we consider a
mathematical model for representing the topological drawing of a graph, which
is based on methods of the theory of vertex rotation with the induction of
simple cycles that satisfy the Mac Lane planarity criterion. It is shown that
the topological drawing of a non-planar graph can be constructed on the basis
of a selected planar part of the graph. The topological model of a graph
drawing allows us to reduce the brute-force enumeration problem of identifying
a plane graph to a discrete optimization problem - searching for a subset of
the set of isometric cycles of the graph that satisfy the zero value of the Mac
Lane's functional. To isolate the planar part of the graph, a new computational
method has been developed based on linear algebra and the algebra of structural
numbers. The proposed method has polynomial computational complexity.",math.CO cs.DM,2024-07-31
A Performance Study of LLM-Generated Code on Leetcode,,"This study evaluates the efficiency of code generation by Large Language
Models (LLMs) and measures their performance against human-crafted solutions
using a dataset from Leetcode. We compare 18 LLMs, considering factors such as
model temperature and success rate, and their impact on code performance. This
research introduces a novel method for measuring and comparing the speed of
LLM-generated code, revealing that LLMs produce code with comparable
performance, irrespective of the adopted LLM. We also find that LLMs are
capable of generating code that is, on average, more efficient than the code
written by humans. The paper further discusses the use of Leetcode as a
benchmarking dataset, the limitations imposed by potential data contamination,
and the platform's measurement reliability. We believe that our findings
contribute to a better understanding of LLM capabilities in code generation and
set the stage for future optimizations in the field.",cs.SE cs.AI,2024-07-31
Voxel Scene Graph for Intracranial Hemorrhage,,"Patients with Intracranial Hemorrhage (ICH) face a potentially
life-threatening condition, and patient-centered individualized treatment
remains challenging due to possible clinical complications. Deep-Learning-based
methods can efficiently analyze the routinely acquired head CTs to support the
clinical decision-making. The majority of early work focuses on the detection
and segmentation of ICH, but do not model the complex relations between ICH and
adjacent brain structures. In this work, we design a tailored object detection
method for ICH, which we unite with segmentation-grounded Scene Graph
Generation (SGG) methods to learn a holistic representation of the clinical
cerebral scene. To the best of our knowledge, this is the first application of
SGG for 3D voxel images. We evaluate our method on two head-CT datasets and
demonstrate that our model can recall up to 74% of clinically relevant
relations. This work lays the foundation towards SGG for 3D voxel data. The
generated Scene Graphs can already provide insights for the clinician, but are
also valuable for all downstream tasks as a compact and interpretable
representation.",cs.CV cs.AI,2024-07-31
"InScope: A New Real-world 3D Infrastructure-side Collaborative
  Perception Dataset for Open Traffic Scenarios",,"Perception systems of autonomous vehicles are susceptible to occlusion,
especially when examined from a vehicle-centric perspective. Such occlusion can
lead to overlooked object detections, e.g., larger vehicles such as trucks or
buses may create blind spots where cyclists or pedestrians could be obscured,
accentuating the safety concerns associated with such perception system
limitations. To mitigate these challenges, the vehicle-to-everything (V2X)
paradigm suggests employing an infrastructure-side perception system (IPS) to
complement autonomous vehicles with a broader perceptual scope. Nevertheless,
the scarcity of real-world 3D infrastructure-side datasets constrains the
advancement of V2X technologies. To bridge these gaps, this paper introduces a
new 3D infrastructure-side collaborative perception dataset, abbreviated as
inscope. Notably, InScope is the first dataset dedicated to addressing
occlusion challenges by strategically deploying multiple-position Light
Detection and Ranging (LiDAR) systems on the infrastructure side. Specifically,
InScope encapsulates a 20-day capture duration with 303 tracking trajectories
and 187,787 3D bounding boxes annotated by experts. Through analysis of
benchmarks, four different benchmarks are presented for open traffic scenarios,
including collaborative 3D object detection, multisource data fusion, data
domain transfer, and 3D multiobject tracking tasks. Additionally, a new metric
is designed to quantify the impact of occlusion, facilitating the evaluation of
detection degradation ratios among various algorithms. The Experimental
findings showcase the enhanced performance of leveraging InScope to assist in
detecting and tracking 3D multiobjects in real-world scenarios, particularly in
tracking obscured, small, and distant objects. The dataset and benchmarks are
available at https://github.com/xf-zh/InScope.",cs.CV,2024-07-31
Adaptive Mix for Semi-Supervised Medical Image Segmentation,,"Mix-up is a key technique for consistency regularization-based
semi-supervised learning methods, generating strong-perturbed samples for
strong-weak pseudo-supervision. Existing mix-up operations are performed either
randomly or with predefined rules, such as replacing low-confidence patches
with high-confidence ones. The former lacks control over the perturbation
degree, leading to overfitting on randomly perturbed samples, while the latter
tends to generate images with trivial perturbations, both of which limit the
effectiveness of consistency learning. This paper aims to answer the following
question: How can image mix-up perturbation be adaptively performed during
training? To this end, we propose an Adaptive Mix algorithm (AdaMix) for image
mix-up in a self-paced learning manner. Given that, in general, a model's
performance gradually improves during training, AdaMix is equipped with a
self-paced curriculum that, in the initial training stage, provides relatively
simple perturbed samples and then gradually increases the difficulty of
perturbed images by adaptively controlling the perturbation degree based on the
model's learning state estimated by a self-paced regularize. We develop three
frameworks with our AdaMix, i.e., AdaMix-ST, AdaMix-MT, and AdaMix-CT, for
semi-supervised medical image segmentation. Extensive experiments on three
public datasets, including both 2D and 3D modalities, show that the proposed
frameworks are capable of achieving superior performance. For example, compared
with the state-of-the-art, AdaMix-CT achieves relative improvements of 2.62% in
Dice and 48.25% in average surface distance on the ACDC dataset with 10%
labeled data. The results demonstrate that mix-up operations with dynamically
adjusted perturbation strength based on the segmentation model's state can
significantly enhance the effectiveness of consistency regularization.",cs.CV,2024-07-31
"Measuring What Matters: Intrinsic Distance Preservation as a Robust
  Metric for Embedding Quality",,"Unsupervised embeddings are fundamental to numerous machine learning
applications, yet their evaluation remains a challenging task. Traditional
assessment methods often rely on extrinsic variables, such as performance in
downstream tasks, which can introduce confounding factors and mask the true
quality of embeddings. This paper introduces the Intrinsic Distance
Preservation Evaluation (IDPE) method, a novel approach for assessing embedding
quality based on the preservation of Mahalanobis distances between data points
in the original and embedded spaces. We demonstrate the limitations of
extrinsic evaluation methods through a simple example, highlighting how they
can lead to misleading conclusions about embedding quality. IDPE addresses
these issues by providing a task-independent measure of how well embeddings
preserve the intrinsic structure of the original data. Our method leverages
efficient similarity search techniques to make it applicable to large-scale
datasets. We compare IDPE with established intrinsic metrics like
trustworthiness and continuity, as well as extrinsic metrics such as Average
Rank and Mean Reciprocal Rank. Our results show that IDPE offers a more
comprehensive and reliable assessment of embedding quality across various
scenarios. We evaluate PCA and t-SNE embeddings using IDPE, revealing insights
into their performance that are not captured by traditional metrics. This work
contributes to the field by providing a robust, efficient, and interpretable
method for embedding evaluation. IDPE's focus on intrinsic properties offers a
valuable tool for researchers and practitioners seeking to develop and assess
high-quality embeddings for diverse machine learning applications.",cs.LG cs.AI,2024-07-31
Simpler Optimal Sorting from a Directed Acyclic Graph,,"Fredman proposed in 1976 the following algorithmic problem: Given are a
ground set $X$, some partial order $P$ over $X$, and some comparison oracle
$O_L$ that specifies a linear order $L$ over $X$ that extends $P$. A query to
$O_L$ has as input distinct $x, x' \in X$ and outputs whether $x <_L x'$ or
vice versa. If we denote by $e(P)$ the number of linear extensions of $P$, then
$\log e(P)$ is a worst-case lower bound on the number of queries needed to
output the sorted order of $X$.
  Fredman did not specify in what form the partial order is given. Haeupler,
Hlad\'ik, Iacono, Rozhon, Tarjan, and T\v{e}tek ('24) propose to assume as
input a directed acyclic graph, $G$, with $m$ edges and $n=|X|$ vertices.
Denote by $P_G$ the partial order induced by $G$. Algorithmic performance is
measured in running time and the number of queries used, where they use
$\Theta(m + n + \log e(P_G))$ time and $\Theta(\log e(P_G))$ queries to output
$X$ in its sorted order. Their algorithm is worst-case optimal in terms of
running time and queries, both. Their algorithm combines topological sorting
with heapsort, and uses sophisticated data structures (including a recent type
of heap with a working-set bound). Their analysis relies upon sophisticated
counting arguments using entropy, recursively defined sets defined over the run
of their algorithm, and vertices in the graph that they identify as bottlenecks
for sorting.
  In this paper, we do away with sophistication. We show that when the input is
a directed acyclic graph then the problem admits a simple solution using
$\Theta(m + n + \log e(P_G))$ time and $\Theta(\log e(P_G))$ queries.
Especially our proofs are much simpler as we avoid the usage of advanced
charging arguments and data structures, and instead rely upon two brief
observations.",cs.DS,2024-07-31
"Does the Source of a Warning Matter? Examining the Effectiveness of
  Veracity Warning Labels Across Warners",,"In this study, we conducted an online, between-subjects experiment (N =
2,049) to better understand the impact of warning label sources on information
trust and sharing intentions. Across four warners (the social media platform,
other social media users, Artificial Intelligence (AI), and fact checkers), we
found that all significantly decreased trust in false information relative to
control, but warnings from AI were modestly more effective. All warners
significantly decreased the sharing intentions of false information, except
warnings from other social media users. AI was again the most effective. These
results were moderated by prior trust in media and the information itself. Most
noteworthy, we found that warning labels from AI were significantly more
effective than all other warning labels for participants who reported a low
trust in news organizations, while warnings from AI were no more effective than
any other warning label for participants who reported a high trust in news
organizations.",cs.CY cs.HC cs.SI,2024-07-31
"LLM-for-X: Application-agnostic Integration of Large Language Models to
  Support Personal Writing Workflows",,"To enhance productivity and to streamline workflows, there is a growing trend
to embed large language model (LLM) functionality into applications, from
browser-based web apps to native apps that run on personal computers. Here, we
introduce LLM-for-X, a system-wide shortcut layer that seamlessly augments any
application with LLM services through a lightweight popup dialog. Our native
layer seamlessly connects front-end applications to popular LLM backends, such
as ChatGPT and Gemini, using their uniform chat front-ends as the programming
interface or their custom API calls. We demonstrate the benefits of LLM-for-X
across a wide variety of applications, including Microsoft Office, VSCode, and
Adobe Acrobat as well as popular web apps such as Overleaf. In our evaluation,
we compared LLM-for-X with ChatGPT's web interface in a series of tasks,
showing that our approach can provide users with quick, efficient, and
easy-to-use LLM assistance without context switching to support writing and
reading tasks that is agnostic of the specific application.",cs.HC,2024-07-31
Stable Rank and Intrinsic Dimension of Real and Complex Matrices,,"We compare the properties of the stable rank and intrinsic dimension of real
and complex matrices to those of the classical rank. Basic proofs and examples
illustrate that the stable rank does not satisfy any of the fundamental rank
properties, while the intrinsic dimension satisfies a few. In particular, the
stable rank and intrinsic dimension of a submatrix can exceed those of the
original matrix; adding a Hermitian positive semi-definite matrix can lower the
intrinsic dimension of the sum; and multiplication by a nonsingular matrix can
drastically change the stable rank and the intrinsic dimension. We generalize
the concept of stable rank to the p-stable in a Schatten p-norm, thereby
unifying the concepts of stable rank and intrinsic dimension: The stable rank
is the 2-stable rank, while the intrinsic dimension is the 1-stable rank of a
Hermitian positive semi-definite matrix. We derive sum and product inequalities
for the pth root of the p-stable rank, and show that it is well-conditioned in
the norm-wise absolute sense. The conditioning improves if the matrix and the
perturbation are Hermitian positive semi-definite.",math.NA cs.NA,2024-07-31
Precomputing approach for a two-scale phase transition model,,"In this study, we employ analytical and numerical techniques to examine a
phase transition model with moving boundaries. The model displays two relevant
spatial scales pointing out to a macroscopic phase and a microscopic phase,
interacting on disjoint inclusions. The shrinkage or the growth of the
inclusions is governed by a modified Gibbs-Thomson law depending on the
macroscopic temperature, but without accessing curvature information. We use
the Hanzawa transformation to transform the problem onto a fixed reference
domain. Then a fixed-point argument is employed to demonstrate the
well-posedness of the system for a finite time interval. Due to the model's
nonlinearities and the macroscopic parameters, which are given by differential
equations that depend on the size of the inclusions, the problem is
computationally expensive to solve numerically. We introduce a precomputing
approach that solves multiple cell problems in an offline phase and uses an
interpolation scheme afterward to determine the needed parameters.
Additionally, we propose a semi-implicit time-stepping method to resolve the
nonlinearity of the problem. We investigate the errors of both the precomputing
and time-stepping procedures and verify the theoretical results via numerical
simulations.",math.NA cs.NA math.AP,2024-07-31
Evaluating SAM2's Role in Camouflaged Object Detection: From SAM to SAM2,,"The Segment Anything Model (SAM), introduced by Meta AI Research as a generic
object segmentation model, quickly garnered widespread attention and
significantly influenced the academic community. To extend its application to
video, Meta further develops Segment Anything Model 2 (SAM2), a unified model
capable of both video and image segmentation. SAM2 shows notable improvements
over its predecessor in terms of applicable domains, promptable segmentation
accuracy, and running speed. However, this report reveals a decline in SAM2's
ability to perceive different objects in images without prompts in its auto
mode, compared to SAM. Specifically, we employ the challenging task of
camouflaged object detection to assess this performance decrease, hoping to
inspire further exploration of the SAM model family by researchers. The results
of this paper are provided in \url{https://github.com/luckybird1994/SAMCOD}.",cs.CV,2024-07-31
"Robust Simultaneous Multislice MRI Reconstruction Using Deep Generative
  Priors",,"Simultaneous multislice (SMS) imaging is a powerful technique for
accelerating magnetic resonance imaging (MRI) acquisitions. However, SMS
reconstruction remains challenging due to the complex signal interactions
between and within the excited slices. This study presents a robust SMS MRI
reconstruction method using deep generative priors. Starting from Gaussian
noise, we leverage denoising diffusion probabilistic models (DDPM) to gradually
recover the individual slices through reverse diffusion iterations while
imposing data consistency from the measured k-space under readout concatenation
framework. The posterior sampling procedure is designed such that the DDPM
training can be performed on single-slice images without special adjustments
for SMS tasks. Additionally, our method integrates a low-frequency enhancement
(LFE) module to address a practical issue that SMS-accelerated fast spin echo
(FSE) and echo-planar imaging (EPI) sequences cannot easily embed
autocalibration signals. Extensive experiments demonstrate that our approach
consistently outperforms existing methods and generalizes well to unseen
datasets. The code is available at https://github.com/Solor-pikachu/ROGER after
the review process.",eess.IV cs.AI cs.CV eess.SP physics.med-ph,2024-07-31
"Higher order quantum reservoir computing for non-intrusive reduced-order
  models",,"Forecasting dynamical systems is of importance to numerous real-world
applications. When possible, dynamical systems forecasts are constructed based
on first-principles-based models such as through the use of differential
equations. When these equations are unknown, non-intrusive techniques must be
utilized to build predictive models from data alone. Machine learning (ML)
methods have recently been used for such tasks. Moreover, ML methods provide
the added advantage of significant reductions in time-to-solution for
predictions in contrast with first-principle based models. However, many
state-of-the-art ML-based methods for forecasting rely on neural networks,
which may be expensive to train and necessitate requirements for large amounts
of memory. In this work, we propose a quantum mechanics inspired ML modeling
strategy for learning nonlinear dynamical systems that provides data-driven
forecasts for complex dynamical systems with reduced training time and memory
costs. This approach, denoted the quantum reservoir computing technique (QRC),
is a hybrid quantum-classical framework employing an ensemble of interconnected
small quantum systems via classical linear feedback connections. By mapping the
dynamical state to a suitable quantum representation amenable to unitary
operations, QRC is able to predict complex nonlinear dynamical systems in a
stable and accurate manner. We demonstrate the efficacy of this framework
through benchmark forecasts of the NOAA Optimal Interpolation Sea Surface
Temperature dataset and compare the performance of QRC to other ML methods.",cs.LG math.DS physics.comp-ph physics.flu-dyn,2024-07-31
"A {\tau} Matrix Based Approximate Inverse Preconditioning for Tempered
  Fractional Diffusion Equations",,"Tempered fractional diffusion equations are a crucial class of equations
widely applied in many physical fields. In this paper, the Crank-Nicolson
method and the tempered weighted and shifts Gr\""unwald formula are firstly
applied to discretize the tempered fractional diffusion equations. We then
obtain that the coefficient matrix of the discretized system has the structure
of the sum of the identity matrix and a diagonal matrix multiplied by a
symmetric positive definite(SPD) Toeplitz matrix. Based on the properties of
SPD Toeplitz matrices, we use $\tau$ matrix approximate it and then propose a
novel approximate inverse preconditioner to approximate the coefficient matrix.
The $\tau$ matrix based approximate inverse preconditioner can be efficiently
computed using the discrete sine transform(DST). In spectral analysis, the
eigenvalues of the preconditioned coefficient matrix are clustered around 1,
ensuring fast convergence of Krylov subspace methods with the new
preconditioner. Finally, numerical experiments demonstrate the effectiveness of
the proposed preconditioner.",math.NA cs.NA,2024-07-31
"MicroMIL: Graph-based Contextual Multiple Instance Learning for Patient
  Diagnosis Using Microscopy Images",,"Current histopathology research has primarily focused on using whole-slide
images (WSIs) produced by scanners with weakly-supervised multiple instance
learning (MIL). However, WSIs are costly, memory-intensive, and require
extensive analysis time. As an alternative, microscopy-based analysis offers
cost and memory efficiency, though microscopy images face issues with unknown
absolute positions and redundant images due to multiple captures from the
subjective perspectives of pathologists. To this end, we introduce MicroMIL, a
weakly-supervised MIL framework specifically built to address these challenges
by dynamically clustering images using deep cluster embedding (DCE) and Gumbel
Softmax for representative image extraction. Graph edges are then constructed
from the upper triangular similarity matrix, with nodes connected to their most
similar neighbors, and a graph neural network (GNN) is utilized to capture
local and diverse areas of contextual information. Unlike existing graph-based
MIL methods designed for WSIs that require absolute positions, MicroMIL
efficiently handles the graph edges without this need. Extensive evaluations on
real-world colon cancer (Seegene) and public BreakHis datasets demonstrate that
MicroMIL outperforms state-of-the-art (SOTA) methods, offering a robust and
efficient solution for patient diagnosis using microscopy images. The code is
available at https://anonymous.4open.science/r/MicroMIL-6C7C",cs.CV,2024-07-31
"Ironing the Graphs: Toward a Correct Geometric Analysis of Large-Scale
  Graphs",,"Graph embedding approaches attempt to project graphs into geometric entities,
i.e, manifolds. The idea is that the geometric properties of the projected
manifolds are helpful in the inference of graph properties. However, if the
choice of the embedding manifold is incorrectly performed, it can lead to
incorrect geometric inference. In this paper, we argue that the classical
embedding techniques cannot lead to correct geometric interpretation as they
miss the curvature at each point, of manifold. We advocate that for doing
correct geometric interpretation the embedding of graph should be done over
regular constant curvature manifolds. To this end, we present an embedding
approach, the discrete Ricci flow graph embedding (dRfge) based on the discrete
Ricci flow that adapts the distance between nodes in a graph so that the graph
can be embedded onto a constant curvature manifold that is homogeneous and
isotropic, i.e., all directions are equivalent and distances comparable,
resulting in correct geometric interpretations. A major contribution of this
paper is that for the first time, we prove the convergence of discrete Ricci
flow to a constant curvature and stable distance metrics over the edges. A
drawback of using the discrete Ricci flow is the high computational complexity
that prevented its usage in large-scale graph analysis. Another contribution of
this paper is a new algorithmic solution that makes it feasible to calculate
the Ricci flow for graphs of up to 50k nodes, and beyond. The intuitions behind
the discrete Ricci flow make it possible to obtain new insights into the
structure of large-scale graphs. We demonstrate this through a case study on
analyzing the internet connectivity structure between countries at the BGP
level.",cs.CG cs.LG,2024-07-31
"Enhancing Partially Spoofed Audio Localization with Boundary-aware
  Attention Mechanism",,"The task of partially spoofed audio localization aims to accurately determine
audio authenticity at a frame level. Although some works have achieved
encouraging results, utilizing boundary information within a single model
remains an unexplored research topic. In this work, we propose a novel method
called Boundary-aware Attention Mechanism (BAM). Specifically, it consists of
two core modules: Boundary Enhancement and Boundary Frame-wise Attention. The
former assembles the intra-frame and inter-frame information to extract
discriminative boundary features that are subsequently used for boundary
position detection and authenticity decision, while the latter leverages
boundary prediction results to explicitly control the feature interaction
between frames, which achieves effective discrimination between real and fake
frames. Experimental results on PartialSpoof database demonstrate our proposed
method achieves the best performance. The code is available at
https://github.com/media-sec-lab/BAM.",cs.SD cs.AI eess.AS,2024-07-31
"Maintaining $k$-MinHash Signatures over Fully-Dynamic Data Streams with
  Recovery",,"We consider the task of performing Jaccard similarity queries over a large
collection of items that are dynamically updated according to a streaming input
model. An item here is a subset of a large universe $U$ of elements. A
well-studied approach to address this important problem in data mining is to
design fast-similarity data sketches. In this paper, we focus on global
solutions for this problem, i.e., a single data structure which is able to
answer both Similarity Estimation and All-Candidate Pairs queries, while also
dynamically managing an arbitrary, online sequence of element insertions and
deletions received in input.
  We introduce and provide an in-depth analysis of a dynamic, buffered version
of the well-known $k$-MinHash sketch. This buffered version better manages
critical update operations thus significantly reducing the number of times the
sketch needs to be rebuilt from scratch using expensive recovery queries. We
prove that the buffered $k$-MinHash uses $O(k \log |U|)$ memory words per
subset and that its amortized update time per insertion/deletion is $O(k \log
|U|)$ with high probability. Moreover, our data structure can return the
$k$-MinHash signature of any subset in $O(k)$ time, and this signature is
exactly the same signature that would be computed from scratch (and thus the
quality of the signature is the same as the one guaranteed by the static
$k$-MinHash).
  Analytical and experimental comparisons with the other, state-of-the-art
global solutions for this problem given in [Bury et al.,WSDM'18] show that the
buffered $k$-MinHash turns out to be competitive in a wide and relevant range
of the online input parameters.",cs.DS,2024-07-31
"Between the AI and Me: Analysing Listeners' Perspectives on AI- and
  Human-Composed Progressive Metal Music",,"Generative AI models have recently blossomed, significantly impacting
artistic and musical traditions. Research investigating how humans interact
with and deem these models is therefore crucial. Through a listening and
reflection study, we explore participants' perspectives on AI- vs
human-generated progressive metal, in symbolic format, using rock music as a
control group. AI-generated examples were produced by ProgGP, a
Transformer-based model. We propose a mixed methods approach to assess the
effects of generation type (human vs. AI), genre (progressive metal vs. rock),
and curation process (random vs. cherry-picked). This combines quantitative
feedback on genre congruence, preference, creativity, consistency, playability,
humanness, and repeatability, and qualitative feedback to provide insights into
listeners' experiences. A total of 32 progressive metal fans completed the
study. Our findings validate the use of fine-tuning to achieve genre-specific
specialization in AI music generation, as listeners could distinguish between
AI-generated rock and progressive metal. Despite some AI-generated excerpts
receiving similar ratings to human music, listeners exhibited a preference for
human compositions. Thematic analysis identified key features for genre and AI
vs. human distinctions. Finally, we consider the ethical implications of our
work in promoting musical data diversity within MIR research by focusing on an
under-explored genre.",cs.SD cs.AI cs.HC eess.AS,2024-07-31
EZSR: Event-based Zero-Shot Recognition,,"This paper studies zero-shot object recognition using event camera data.
Guided by CLIP, which is pre-trained on RGB images, existing approaches achieve
zero-shot object recognition by maximizing embedding similarities between event
data encoded by an event encoder and RGB images encoded by the CLIP image
encoder. Alternatively, several methods learn RGB frame reconstructions from
event data for the CLIP image encoder. However, these approaches often result
in suboptimal zero-shot performance.
  This study develops an event encoder without relying on additional
reconstruction networks. We theoretically analyze the performance bottlenecks
of previous approaches: global similarity-based objective (i.e., maximizing the
embedding similarities) cause semantic misalignments between the learned event
embedding space and the CLIP text embedding space due to the degree of freedom.
To mitigate the issue, we explore a scalar-wise regularization strategy.
Furthermore, to scale up the number of events and RGB data pairs for training,
we also propose a pipeline for synthesizing event data from static RGB images.
  Experimentally, our data synthesis strategy exhibits an attractive scaling
property, and our method achieves superior zero-shot object recognition
performance on extensive standard benchmark datasets, even compared with past
supervised learning approaches. For example, we achieve 47.84% zero-shot
accuracy on the N-ImageNet dataset.",cs.CV,2024-07-31
Interactive Diagrams for Software Documentation,,"Getting acquainted with a large codebase can be a daunting task for software
developers, both new and seasoned. The description of a codebase and its
development should be the purpose of its documentation. However, software
documentation, if it exists at all, is usually textual and accompanied only by
simple static diagrams. It is also time-consuming to maintain manually. Even an
API reference, which can be generated automatically from the codebase itself,
has many drawbacks. It is limited to what it can extract from the codebase, is
cumbersome to navigate, and fails to capture the interwoven nature of code. We
explore an alternative approach centered around a node-link diagram
representing the structure of a codebase. The diagram is interactive and
filterable, providing details on demand. It is designed for automation, relying
on static analysis of the codebase, and thus produces results quickly and
offers a viable alternative to missing or outdated documentation. To evaluate
this approach, we implemented a prototype named Helveg that is able to analyze
and visualize C# code. Testing with five professional programmers provided
feedback on the approach's benefits and challenges, which we discuss in detail.",cs.SE,2024-07-31
"Extended Fiducial Inference: Toward an Automated Process of Statistical
  Inference",,"While fiducial inference was widely considered a big blunder by R.A. Fisher,
the goal he initially set --`inferring the uncertainty of model parameters on
the basis of observations' -- has been continually pursued by many
statisticians. To this end, we develop a new statistical inference method
called extended Fiducial inference (EFI). The new method achieves the goal of
fiducial inference by leveraging advanced statistical computing techniques
while remaining scalable for big data. EFI involves jointly imputing random
errors realized in observations using stochastic gradient Markov chain Monte
Carlo and estimating the inverse function using a sparse deep neural network
(DNN). The consistency of the sparse DNN estimator ensures that the uncertainty
embedded in observations is properly propagated to model parameters through the
estimated inverse function, thereby validating downstream statistical
inference. Compared to frequentist and Bayesian methods, EFI offers significant
advantages in parameter estimation and hypothesis testing. Specifically, EFI
provides higher fidelity in parameter estimation, especially when outliers are
present in the observations; and eliminates the need for theoretical reference
distributions in hypothesis testing, thereby automating the statistical
inference process. EFI also provides an innovative framework for
semi-supervised learning.",stat.ML cs.LG math.ST stat.TH,2024-07-31
"Grid-Based Decompositions for Spatial Data under Local Differential
  Privacy",,"Local differential privacy (LDP) has recently emerged as a popular privacy
standard. With the growing popularity of LDP, several recent works have applied
LDP to spatial data, and grid-based decompositions have been a common building
block in the collection and analysis of spatial data under DP and LDP. In this
paper, we study three grid-based decomposition methods for spatial data under
LDP: Uniform Grid (UG), PrivAG, and AAG. UG is a static approach that consists
of equal-sized cells. To enable data-dependent decomposition, PrivAG was
proposed by Yang et al. as the most recent adaptive grid method. To advance the
state-of-the-art in adaptive grids, in this paper we propose the Advanced
Adaptive Grid (AAG) method. For each grid cell, following the intuition that
the cell's intra-cell density distribution will be affected by its neighbors,
AAG performs uneven cell divisions depending on the neighboring cells'
densities. We experimentally compare UG, PrivAG, and AAG using three real-world
location datasets, varying privacy budgets, and query sizes. Results show that
AAG provides higher utility than PrivAG, demonstrating the superiority of our
proposed approach. Furthermore, UG's performance is heavily dependent on the
choice of grid size. When the grid size is chosen optimally in UG, AAG still
beats UG for small queries, but UG beats AAG for large (coarse-grained)
queries.",cs.CR,2024-07-31
"REPS: Recycling Entropies for Packet Spraying to Adaptively Explore
  Paths and Mitigate Failures",,"Most existing datacenter transport protocols rely on in-order packet
delivery, a design choice rooted in legacy systems and simplicity. However,
advancements in technology, such as RDMA, have made it feasible to relax this
requirement, allowing for more effective use of modern datacenter topologies
like FatTree and Dragonfly. The rise of AI/ML workloads underscores the
necessity for enhanced link utilization, a challenge for single-path load
balancers due to issues like ECMP collisions.
  In this paper, we introduce REPS, a novel per-packet traffic load-balancing
algorithm that integrates seamlessly with existing congestion control
mechanisms. REPS reroutes packets around congested hotspots and unreliable or
failing links with remarkable simplicity and minimal state requirements.
  Our evaluation demonstrates that REPS significantly outperforms traditional
packet spraying and other state-of-the-art solutions in datacenter networks,
offering substantial improvements in performance and link utilization.",cs.NI,2024-07-31
"TAROT: Task-Oriented Authorship Obfuscation Using Policy Optimization
  Methods",,"Authorship obfuscation aims to disguise the identity of an author within a
text by altering the writing style, vocabulary, syntax, and other linguistic
features associated with the text author. This alteration needs to balance
privacy and utility. While strong obfuscation techniques can effectively hide
the author's identity, they often degrade the quality and usefulness of the
text for its intended purpose. Conversely, maintaining high utility tends to
provide insufficient privacy, making it easier for an adversary to de-anonymize
the author. Thus, achieving an optimal trade-off between these two conflicting
objectives is crucial. In this paper, we propose TAROT: Task-Oriented
Authorship Obfuscation Using Policy Optimization, a new unsupervised authorship
obfuscation method whose goal is to optimize the privacy-utility trade-off by
regenerating the entire text considering its downstream utility. Our approach
leverages policy optimization as a fine-tuning paradigm over small language
models in order to rewrite texts by preserving author identity and downstream
task utility. We show that our approach largely reduce the accuracy of
attackers while preserving utility. We make our code and models publicly
available.",cs.CL,2024-07-31
"RoadFormer+: Delivering RGB-X Scene Parsing through Scale-Aware
  Information Decoupling and Advanced Heterogeneous Feature Fusion",,"Task-specific data-fusion networks have marked considerable achievements in
urban scene parsing. Among these networks, our recently proposed RoadFormer
successfully extracts heterogeneous features from RGB images and surface normal
maps and fuses these features through attention mechanisms, demonstrating
compelling efficacy in RGB-Normal road scene parsing. However, its performance
significantly deteriorates when handling other types/sources of data or
performing more universal, all-category scene parsing tasks. To overcome these
limitations, this study introduces RoadFormer+, an efficient, robust, and
adaptable model capable of effectively fusing RGB-X data, where ``X'',
represents additional types/modalities of data such as depth, thermal, surface
normal, and polarization. Specifically, we propose a novel hybrid feature
decoupling encoder to extract heterogeneous features and decouple them into
global and local components. These decoupled features are then fused through a
dual-branch multi-scale heterogeneous feature fusion block, which employs
parallel Transformer attentions and convolutional neural network modules to
merge multi-scale features across different scales and receptive fields. The
fused features are subsequently fed into a decoder to generate the final
semantic predictions. Notably, our proposed RoadFormer+ ranks first on the
KITTI Road benchmark and achieves state-of-the-art performance in mean
intersection over union on the Cityscapes, MFNet, FMB, and ZJU datasets.
Moreover, it reduces the number of learnable parameters by 65\% compared to
RoadFormer. Our source code will be publicly available at
mias.group/RoadFormerPlus.",cs.CV,2024-07-31
"Lexicase-based Selection Methods with Down-sampling for Symbolic
  Regression Problems: Overview and Benchmark",,"In recent years, several new lexicase-based selection variants have emerged
due to the success of standard lexicase selection in various application
domains. For symbolic regression problems, variants that use an
epsilon-threshold or batches of training cases, among others, have led to
performance improvements. Lately, especially variants that combine lexicase
selection and down-sampling strategies have received a lot of attention. This
paper evaluates random as well as informed down-sampling in combination with
the relevant lexicase-based selection methods on a wide range of symbolic
regression problems. In contrast to most work, we not only compare the methods
over a given evaluation budget, but also over a given time as time is usually
limited in practice. We find that for a given evaluation budget,
epsilon-lexicase selection in combination with random or informed down-sampling
outperforms all other methods. Only for a rather long running time of 24h, the
best performing method is tournament selection in combination with informed
down-sampling. If the given running time is very short, lexicase variants using
batches of training cases perform best.",cs.NE,2024-07-31
"Zero-Shot Cross-Domain Dialogue State Tracking via Dual Low-Rank
  Adaptation",,"Zero-shot dialogue state tracking (DST) seeks to enable dialogue systems to
transition to unfamiliar domains without manual annotation or extensive
retraining. Prior research has approached this objective by embedding prompts
into language models (LMs). Common methodologies include integrating prompts at
the input layer or introducing learnable variables at each transformer layer.
Nonetheless, each strategy exhibits inherent limitations. Prompts integrated at
the input layer risk underutilization, with their impact potentially
diminishing across successive transformer layers. Conversely, the addition of
learnable variables to each layer can complicate the training process and
increase inference latency. To tackle the issues mentioned above, this paper
proposes Dual Low-Rank Adaptation (DualLoRA), a plug-and-play architecture
designed for zero-shot DST. DualLoRA incorporates two distinct Low-Rank
Adaptation (LoRA) components, targeting both dialogue context processing and
prompt optimization, to ensure the comprehensive influence of prompts
throughout the transformer model layers. This is achieved without incurring
additional inference latency, showcasing an efficient integration into existing
architectures. Through rigorous evaluation on the MultiWOZ and SGD datasets,
DualLoRA demonstrates notable improvements across multiple domains,
outperforming traditional baseline methods in zero-shot settings. Our code is
accessible at: \url{https://github.com/suntea233/DualLoRA}.",cs.CL,2024-07-31
"MART: MultiscAle Relational Transformer Networks for Multi-agent
  Trajectory Prediction",,"Multi-agent trajectory prediction is crucial to autonomous driving and
understanding the surrounding environment. Learning-based approaches for
multi-agent trajectory prediction, such as primarily relying on graph neural
networks, graph transformers, and hypergraph neural networks, have demonstrated
outstanding performance on real-world datasets in recent years. However, the
hypergraph transformer-based method for trajectory prediction is yet to be
explored. Therefore, we present a MultiscAle Relational Transformer (MART)
network for multi-agent trajectory prediction. MART is a hypergraph transformer
architecture to consider individual and group behaviors in transformer
machinery. The core module of MART is the encoder, which comprises a Pair-wise
Relational Transformer (PRT) and a Hyper Relational Transformer (HRT). The
encoder extends the capabilities of a relational transformer by introducing
HRT, which integrates hyperedge features into the transformer mechanism,
promoting attention weights to focus on group-wise relations. In addition, we
propose an Adaptive Group Estimator (AGE) designed to infer complex group
relations in real-world environments. Extensive experiments on three real-world
datasets (NBA, SDD, and ETH-UCY) demonstrate that our method achieves
state-of-the-art performance, enhancing ADE/FDE by 3.9%/11.8% on the NBA
dataset. Code is available at https://github.com/gist-ailab/MART.",cs.LG,2024-07-31
"Quality Control for Radiology Report Generation Models via Auxiliary
  Auditing Components",,"Automation of medical image interpretation could alleviate bottlenecks in
diagnostic workflows, and has become of particular interest in recent years due
to advancements in natural language processing. Great strides have been made
towards automated radiology report generation via AI, yet ensuring clinical
accuracy in generated reports is a significant challenge, hindering deployment
of such methods in clinical practice. In this work we propose a quality control
framework for assessing the reliability of AI-generated radiology reports with
respect to semantics of diagnostic importance using modular auxiliary auditing
components (AC). Evaluating our pipeline on the MIMIC-CXR dataset, our findings
show that incorporating ACs in the form of disease-classifiers can enable
auditing that identifies more reliable reports, resulting in higher F1 scores
compared to unfiltered generated reports. Additionally, leveraging the
confidence of the AC labels further improves the audit's effectiveness.",cs.AI cs.CV,2024-07-31
"Lyapunov weights to convey the meaning of time in physics-informed
  neural networks",,"Time is not a dimension as the others. In Physics-Informed Neural Networks
(PINN) several proposals attempted to adapt the time sampling or time weighting
to take into account the specifics of this special dimension. But these
proposals are not principled and need guidance to be used. We explain here
theoretically why the Lyapunov exponents give actionable insights and propose a
weighting scheme to automatically adapt to chaotic, periodic or stable
dynamics. We characterize theoretically the best weighting scheme under
computational constraints as a cumulative exponential integral of the local
Lyapunov exponent estimators and show that it performs well in practice under
the regimes mentioned above.",cs.LG cs.AI cs.NA math.NA,2024-07-31
"Towards Achieving Human Parity on End-to-end Simultaneous Speech
  Translation via LLM Agent",,"In this paper, we present Cross Language Agent -- Simultaneous
Interpretation, CLASI, a high-quality and human-like Simultaneous Speech
Translation (SiST) System. Inspired by professional human interpreters, we
utilize a novel data-driven read-write strategy to balance the translation
quality and latency. To address the challenge of translating in-domain
terminologies, CLASI employs a multi-modal retrieving module to obtain relevant
information to augment the translation. Supported by LLMs, our approach can
generate error-tolerated translation by considering the input audio, historical
context, and retrieved information. Experimental results show that our system
outperforms other systems by significant margins. Aligned with professional
human interpreters, we evaluate CLASI with a better human evaluation metric,
valid information proportion (VIP), which measures the amount of information
that can be successfully conveyed to the listeners. In the real-world
scenarios, where the speeches are often disfluent, informal, and unclear, CLASI
achieves VIP of 81.3% and 78.0% for Chinese-to-English and English-to-Chinese
translation directions, respectively. In contrast, state-of-the-art commercial
or open-source systems only achieve 35.4% and 41.6%. On the extremely hard
dataset, where other systems achieve under 13% VIP, CLASI can still achieve 70%
VIP.",cs.CL cs.SD eess.AS,2024-07-31
Human interaction classifier for LLM based chatbot,,"This study investigates different approaches to classify human interactions
in an artificial intelligence-based environment, specifically for Applus+
IDIADA's intelligent agent AIDA. The main objective is to develop a classifier
that accurately identifies the type of interaction received (Conversation,
Services, or Document Translation) to direct requests to the appropriate
channel and provide a more specialized and efficient service. Various models
are compared, including LLM-based classifiers, KNN using Titan and Cohere
embeddings, SVM, and artificial neural networks. Results show that SVM and ANN
models with Cohere embeddings achieve the best overall performance, with
superior F1 scores and faster execution times compared to LLM-based approaches.
The study concludes that the SVM model with Cohere embeddings is the most
suitable option for classifying human interactions in the AIDA environment,
offering an optimal balance between accuracy and computational efficiency.",cs.AI,2024-07-31
Spatial Transformer Network YOLO Model for Agricultural Object Detection,,"Object detection plays a crucial role in the field of computer vision by
autonomously identifying and locating objects of interest. The You Only Look
Once (YOLO) model is an effective single-shot detector. However, YOLO faces
challenges in cluttered or partially occluded scenes and can struggle with
small, low-contrast objects. We propose a new method that integrates spatial
transformer networks (STNs) into YOLO to improve performance. The proposed
STN-YOLO aims to enhance the model's effectiveness by focusing on important
areas of the image and improving the spatial invariance of the model before the
detection process. Our proposed method improved object detection performance
both qualitatively and quantitatively. We explore the impact of different
localization networks within the STN module as well as the robustness of the
model across different spatial transformations. We apply the STN-YOLO on
benchmark datasets for Agricultural object detection as well as a new dataset
from a state-of-the-art plant phenotyping greenhouse facility. Our code and
dataset are publicly available.",cs.CV cs.AI cs.LG,2024-07-31
MTA-CLIP: Language-Guided Semantic Segmentation with Mask-Text Alignment,,"Recent approaches have shown that large-scale vision-language models such as
CLIP can improve semantic segmentation performance. These methods typically aim
for pixel-level vision-language alignment, but often rely on low resolution
image features from CLIP, resulting in class ambiguities along boundaries.
Moreover, the global scene representations in CLIP text embeddings do not
directly correlate with the local and detailed pixel-level features, making
meaningful alignment more difficult. To address these limitations, we introduce
MTA-CLIP, a novel framework employing mask-level vision-language alignment.
Specifically, we first propose Mask-Text Decoder that enhances the mask
representations using rich textual data with the CLIP language model.
Subsequently, it aligns mask representations with text embeddings using
Mask-to-Text Contrastive Learning. Furthermore, we introduce MaskText Prompt
Learning, utilizing multiple context-specific prompts for text embeddings to
capture diverse class representations across masks. Overall, MTA-CLIP achieves
state-of-the-art, surpassing prior works by an average of 2.8% and 1.3% on on
standard benchmark datasets, ADE20k and Cityscapes, respectively.",cs.CV,2024-07-31
Comgra: A Tool for Analyzing and Debugging Neural Networks,,"Neural Networks are notoriously difficult to inspect. We introduce comgra, an
open source python library for use with PyTorch. Comgra extracts data about the
internal activations of a model and organizes it in a GUI (graphical user
interface). It can show both summary statistics and individual data points,
compare early and late stages of training, focus on individual samples of
interest, and visualize the flow of the gradient through the network. This
makes it possible to inspect the model's behavior from many different angles
and save time by rapidly testing different hypotheses without having to rerun
it. Comgra has applications for debugging, neural architecture design, and
mechanistic interpretability. We publish our library through Python Package
Index (PyPI) and provide code, documentation, and tutorials at
https://github.com/FlorianDietz/comgra.",cs.LG,2024-07-31
Beat this! Accurate beat tracking without DBN postprocessing,,"We propose a system for tracking beats and downbeats with two objectives:
generality across a diverse music range, and high accuracy. We achieve
generality by training on multiple datasets -- including solo instrument
recordings, pieces with time signature changes, and classical music with high
tempo variations -- and by removing the commonly used Dynamic Bayesian Network
(DBN) postprocessing, which introduces constraints on the meter and tempo. For
high accuracy, among other improvements, we develop a loss function tolerant to
small time shifts of annotations, and an architecture alternating convolutions
with transformers either over frequency or time. Our system surpasses the
current state of the art in F1 score despite using no DBN. However, it can
still fail, especially for difficult and underrepresented genres, and performs
worse on continuity metrics, so we publish our model, code, and preprocessed
datasets, and invite others to beat this.",cs.SD cs.LG eess.AS,2024-07-31
"Defending Jailbreak Attack in VLMs via Cross-modality Information
  Detector",,"Vision Language Models (VLMs) extend the capacity of LLMs to comprehensively
understand vision information, achieving remarkable performance in many
vision-centric tasks. Despite that, recent studies have shown that these models
are susceptible to jailbreak attacks, which refer to an exploitative technique
where malicious users can break the safety alignment of the target model and
generate misleading and harmful answers. This potential threat is caused by
both the inherent vulnerabilities of LLM and the larger attack scope introduced
by vision input. To enhance the security of VLMs against jailbreak attacks,
researchers have developed various defense techniques. However, these methods
either require modifications to the model's internal structure or demand
significant computational resources during the inference phase. Multimodal
information is a double-edged sword. While it increases the risk of attacks, it
also provides additional data that can enhance safeguards. Inspired by this, we
propose $\underline{\textbf{C}}$ross-modality
$\underline{\textbf{I}}$nformation
$\underline{\textbf{DE}}$tecto$\underline{\textbf{R}}$ ($\textit{CIDER})$, a
plug-and-play jailbreaking detector designed to identify maliciously perturbed
image inputs, utilizing the cross-modal similarity between harmful queries and
adversarial images. This simple yet effective cross-modality information
detector, $\textit{CIDER}$, is independent of the target VLMs and requires less
computation cost. Extensive experimental results demonstrate the effectiveness
and efficiency of $\textit{CIDER}$, as well as its transferability to both
white-box and black-box VLMs.",cs.CL,2024-07-31
Towards Error Correction for Computing in Racetrack Memory,,"Computing-in-memory (CIM) promises to alleviate the Von Neumann bottleneck
and accelerate data-intensive applications. Depending on the underlying
technology and configuration, CIM enables implementing compute primitives in
place, such as multiplication, search operations, and bulk bitwise logic
operations. Emerging nonvolatile memory technologies such as spintronic
Racetrack memory (RTM) promise not only unprecedented density but also
significant parallelism through CIM. However, most CIM designs, including those
based on RTM, exhibit high fault rates. Existing error correction codes (ECC)
are not homomorphic over bitwise operations such as AND and OR, and hence
cannot protect against CIM faults. This paper proposes CIRM-ECC, a technique to
protect spintronic RTMs against CIM faults. At the core of CIRM-ECC, we use a
recently proposed RTM-based CIM approach and leverage its peripheral circuitry
to our implement our novel ECC codes. We show that CIRM-ECC can be applied to
single-bit Hamming codes as well as multi-bit BCH codes.",cs.AR,2024-07-31
"A State-of-the-Art Review of Computational Models for Analyzing
  Longitudinal Wearable Sensor Data in Healthcare",,"Wearable devices are increasingly used as tools for biomedical research, as
the continuous stream of behavioral and physiological data they collect can
provide insights about our health in everyday contexts. Long-term tracking,
defined in the timescale of months of year, can provide insights of patterns
and changes as indicators of health changes. These insights can make medicine
and healthcare more predictive, preventive, personalized, and participative
(The 4P's). However, the challenges in modeling, understanding and processing
longitudinal data are a significant barrier to their adoption in research
studies and clinical settings. In this paper, we review and discuss three
models used to make sense of longitudinal data: routines, rhythms and stability
metrics. We present the challenges associated with the processing and analysis
of longitudinal wearable sensor data, with a special focus on how to handle the
different temporal dynamics at various granularities. We then discuss current
limitations and identify directions for future work. This review is essential
to the advancement of computational modeling and analysis of longitudinal
sensor data for pervasive healthcare.",cs.HC cs.LG,2024-07-31
"An Explainable Vision Transformer with Transfer Learning Combined with
  Support Vector Machine Based Efficient Drought Stress Identification",,"Early detection of drought stress is critical for taking timely measures for
reducing crop loss before the drought impact becomes irreversible. The subtle
phenotypical and physiological changes in response to drought stress are
captured by non-invasive imaging techniques and these imaging data serve as
valuable resource for machine learning methods to identify drought stress.
While convolutional neural networks (CNNs) are in wide use, vision transformers
(ViTs) present a promising alternative in capturing long-range dependencies and
intricate spatial relationships, thereby enhancing the detection of subtle
indicators of drought stress. We propose an explainable deep learning pipeline
that leverages the power of ViTs for drought stress detection in potato crops
using aerial imagery. We applied two distinct approaches: a synergistic
combination of ViT and support vector machine (SVM), where ViT extracts
intricate spatial features from aerial images, and SVM classifies the crops as
stressed or healthy and an end-to-end approach using a dedicated classification
layer within ViT to directly detect drought stress. Our key findings explain
the ViT model's decision-making process by visualizing attention maps. These
maps highlight the specific spatial features within the aerial images that the
ViT model focuses as the drought stress signature. Our findings demonstrate
that the proposed methods not only achieve high accuracy in drought stress
identification but also shedding light on the diverse subtle plant features
associated with drought stress. This offers a robust and interpretable solution
for drought stress monitoring for farmers to undertake informed decisions for
improved crop management.",cs.CV cs.AI cs.ET cs.LG,2024-07-31
Stable Sparse Operator Inference for Nonlinear Structural Dynamics,,"Structural dynamics models with nonlinear stiffness appear, for example, when
analyzing systems with nonlinear material behavior or undergoing large
deformations. For complex systems, these models become too large for real-time
applications or multi-query workflows. Hence, model reduction is needed.
However, the mathematical operators of these models are often not available
since, as is common in industry practice, the models are constructed using
commercial simulation software. In this work, we propose an operator
inference-based approach aimed at inferring, from data generated by the
simulation model, reduced-order models (ROMs) of structural dynamics systems
with stiffness terms represented by polynomials of arbitrary degree. To ensure
physically meaningful models, we impose constraints on the inference such that
the model is guaranteed to exhibit stability properties. Convexity of the
optimization problem associated with the inference is maintained by applying a
sum-of-squares relaxation to the polynomial term. To further reduce the size of
the ROM and improve numerical conditioning of the inference, we also propose a
novel clustering-based sparsification of the polynomial term. We validate the
proposed method on several numerical examples, including a representative 3D
Finite Element Model (FEM) of a steel piston rod.",math.NA cs.NA,2024-07-31
Synthetic Simplicity: Unveiling Bias in Medical Data Augmentation,,"Synthetic data is becoming increasingly integral in data-scarce fields such
as medical imaging, serving as a substitute for real data. However, its
inherent statistical characteristics can significantly impact downstream tasks,
potentially compromising deployment performance. In this study, we empirically
investigate this issue and uncover a critical phenomenon: downstream neural
networks often exploit spurious distinctions between real and synthetic data
when there is a strong correlation between the data source and the task label.
This exploitation manifests as \textit{simplicity bias}, where models overly
rely on superficial features rather than genuine task-related complexities.
Through principled experiments, we demonstrate that the source of data (real
vs.\ synthetic) can introduce spurious correlating factors leading to poor
performance during deployment when the correlation is absent. We first
demonstrate this vulnerability on a digit classification task, where the model
spuriously utilizes the source of data instead of the digit to provide an
inference. We provide further evidence of this phenomenon in a medical imaging
problem related to cardiac view classification in echocardiograms, particularly
distinguishing between 2-chamber and 4-chamber views. Given the increasing role
of utilizing synthetic datasets, we hope that our experiments serve as
effective guidelines for the utilization of synthetic datasets in model
training.",cs.CV cs.AI,2024-07-31
"Pedestrian Inertial Navigation: An Overview of Model and Data-Driven
  Approaches",,"The task of indoor positioning is fundamental to several applications,
including navigation, healthcare, location-based services, and security. An
emerging field is inertial navigation for pedestrians, which relies only on
inertial sensors for positioning. In this paper, we present inertial pedestrian
navigation models and learning approaches. Among these, are methods and
algorithms for shoe-mounted inertial sensors and pedestrian dead reckoning
(PDR) with unconstrained inertial sensors. We also address three categories of
data-driven PDR strategies: activity-assisted, hybrid approaches, and
learning-based frameworks.",cs.RO eess.SP,2024-07-31
"The Pascal Matrix, Commuting Tridiagonal Operators and Fourier Algebras",,"We consider the (symmetric) Pascal matrix, in its finite and infinite
versions, and prove the existence of symmetric tridiagonal matrices commuting
with it by giving explicit expressions for these commuting matrices. This is
achieved by studying the associated Fourier algebra, which as a byproduct,
allows us to show that all the linear relations of a certain general form for
the entries of the Pascal matrix arise from only three basic relations. We also
show that pairs of eigenvectors of the tridiagonal matrix define a natural
eigenbasis for the binomial transform. Lastly, we show that the commuting
tridiagonal matrices provide a numerically stable means of diagonalizing the
Pascal matrix.",math.SP cs.NA math.NA,2024-07-31
Expressive Whole-Body 3D Gaussian Avatar,,"Facial expression and hand motions are necessary to express our emotions and
interact with the world. Nevertheless, most of the 3D human avatars modeled
from a casually captured video only support body motions without facial
expressions and hand motions.In this work, we present ExAvatar, an expressive
whole-body 3D human avatar learned from a short monocular video. We design
ExAvatar as a combination of the whole-body parametric mesh model (SMPL-X) and
3D Gaussian Splatting (3DGS). The main challenges are 1) a limited diversity of
facial expressions and poses in the video and 2) the absence of 3D
observations, such as 3D scans and RGBD images. The limited diversity in the
video makes animations with novel facial expressions and poses non-trivial. In
addition, the absence of 3D observations could cause significant ambiguity in
human parts that are not observed in the video, which can result in noticeable
artifacts under novel motions. To address them, we introduce our hybrid
representation of the mesh and 3D Gaussians. Our hybrid representation treats
each 3D Gaussian as a vertex on the surface with pre-defined connectivity
information (i.e., triangle faces) between them following the mesh topology of
SMPL-X. It makes our ExAvatar animatable with novel facial expressions by
driven by the facial expression space of SMPL-X. In addition, by using
connectivity-based regularizers, we significantly reduce artifacts in novel
facial expressions and poses.",cs.CV,2024-07-31
"Dynamic Object Queries for Transformer-based Incremental Object
  Detection",,"Incremental object detection (IOD) aims to sequentially learn new classes,
while maintaining the capability to locate and identify old ones. As the
training data arrives with annotations only with new classes, IOD suffers from
catastrophic forgetting. Prior methodologies mainly tackle the forgetting issue
through knowledge distillation and exemplar replay, ignoring the conflict
between limited model capacity and increasing knowledge. In this paper, we
explore \textit{dynamic object queries} for incremental object detection built
on Transformer architecture. We propose the \textbf{Dy}namic object
\textbf{Q}uery-based \textbf{DE}tection \textbf{TR}ansformer (DyQ-DETR), which
incrementally expands the model representation ability to achieve
stability-plasticity tradeoff. First, a new set of learnable object queries are
fed into the decoder to represent new classes. These new object queries are
aggregated with those from previous phases to adapt both old and new knowledge
well. Second, we propose the isolated bipartite matching for object queries in
different phases, based on disentangled self-attention. The interaction among
the object queries at different phases is eliminated to reduce inter-class
confusion. Thanks to the separate supervision and computation over object
queries, we further present the risk-balanced partial calibration for effective
exemplar replay. Extensive experiments demonstrate that DyQ-DETR significantly
surpasses the state-of-the-art methods, with limited parameter overhead. Code
will be made publicly available.",cs.CV cs.AI,2024-07-31
"Explainable Artificial Intelligence for Quantifying Interfering and
  High-Risk Behaviors in Autism Spectrum Disorder in a Real-World Classroom
  Environment Using Privacy-Preserving Video Analysis",,"Rapid identification and accurate documentation of interfering and high-risk
behaviors in ASD, such as aggression, self-injury, disruption, and restricted
repetitive behaviors, are important in daily classroom environments for
tracking intervention effectiveness and allocating appropriate resources to
manage care needs. However, having a staff dedicated solely to observing is
costly and uncommon in most educational settings. Recently, multiple research
studies have explored developing automated, continuous, and objective tools
using machine learning models to quantify behaviors in ASD. However, the
majority of the work was conducted under a controlled environment and has not
been validated for real-world conditions. In this work, we demonstrate that the
latest advances in video-based group activity recognition techniques can
quantify behaviors in ASD in real-world activities in classroom environments
while preserving privacy. Our explainable model could detect the episode of
problem behaviors with a 77% F1-score and capture distinctive behavior features
in different types of behaviors in ASD. To the best of our knowledge, this is
the first work that shows the promise of objectively quantifying behaviors in
ASD in a real-world environment, which is an important step toward the
development of a practical tool that can ease the burden of data collection for
classroom staff.",cs.CV,2024-07-31
"Long-Term Energy Management for Microgrid with Hybrid Hydrogen-Battery
  Energy Storage: A Prediction-Free Coordinated Optimization Framework",,"This paper studies the long-term energy management of a microgrid
coordinating hybrid hydrogen-battery energy storage. We develop an approximate
semi-empirical hydrogen storage model to accurately capture the power-dependent
efficiency of hydrogen storage. We introduce a prediction-free two-stage
coordinated optimization framework, which generates the annual state-of-charge
(SoC) reference for hydrogen storage offline. During online operation, it
updates the SoC reference online using kernel regression and makes operation
decisions based on the proposed adaptive virtual-queue-based online convex
optimization (OCO) algorithm. We innovatively incorporate penalty terms for
long-term pattern tracking and expert-tracking for step size updates. We
provide theoretical proof to show that the proposed OCO algorithm achieves a
sublinear bound of dynamic regret without using prediction information.
Numerical studies based on the Elia and North China datasets show that the
proposed framework significantly outperforms the existing online optimization
approaches by reducing the operational costs and loss of load by around 30% and
80%, respectively. These benefits can be further enhanced with optimized
settings for the penalty coefficient and step size of OCO, as well as more
historical references.",math.OC cs.SY eess.SY,2024-07-31
Hyper-parameter tuning for text guided image editing,,"The test-time finetuning text-guided image editing method, Forgedit, is
capable of tackling general and complex image editing problems given only the
input image itself and the target text prompt. During finetuning stage, using
the same set of finetuning hyper-paramters every time for every given image,
Forgedit remembers and understands the input image in 30 seconds. During
editing stage, the workflow of Forgedit might seem complicated. However, in
fact, the editing process of Forgedit is not more complex than previous SOTA
Imagic, yet completely solves the overfitting problem of Imagic. In this paper,
we will elaborate the workflow of Forgedit editing stage with examples. We will
show how to tune the hyper-parameters in an efficient way to obtain ideal
editing results.",cs.CV,2024-07-31
Tora: Trajectory-oriented Diffusion Transformer for Video Generation,,"Recent advancements in Diffusion Transformer (DiT) have demonstrated
remarkable proficiency in producing high-quality video content. Nonetheless,
the potential of transformer-based diffusion models for effectively generating
videos with controllable motion remains an area of limited exploration. This
paper introduces Tora, the first trajectory-oriented DiT framework that
integrates textual, visual, and trajectory conditions concurrently for video
generation. Specifically, Tora consists of a Trajectory Extractor~(TE), a
Spatial-Temporal DiT, and a Motion-guidance Fuser~(MGF). The TE encodes
arbitrary trajectories into hierarchical spacetime motion patches with a 3D
video compression network. The MGF integrates the motion patches into the DiT
blocks to generate consistent videos following trajectories. Our design aligns
seamlessly with DiT's scalability, allowing precise control of video content's
dynamics with diverse durations, aspect ratios, and resolutions. Extensive
experiments demonstrate Tora's excellence in achieving high motion fidelity,
while also meticulously simulating the movement of the physical world. Page can
be found at https://ali-videoai.github.io/tora_video.",cs.CV,2024-07-31
"Tree-Cotree-Based Tearing and Interconnecting for 3D Magnetostatics: A
  Dual-Primal Approach",,"The simulation of electromagnetic devices with complex geometries and
large-scale discrete systems benefits from advanced computational methods like
IsoGeometric Analysis and Domain Decomposition. In this paper, we employ both
concepts in an Isogeometric Tearing and Interconnecting method to enable the
use of parallel computations for magnetostatic problems. We address the
underlying non-uniqueness by using a graph-theoretic approach, the tree-cotree
decomposition. The classical tree-cotree gauging is adapted to be feasible for
parallelization, which requires that all local subsystems are uniquely
solvable. Our contribution consists of an explicit algorithm for constructing
compatible trees and combining it with a dual-primal approach to enable
parallelization. The correctness of the proposed approach is proved and
verified by numerical experiments, showing its accuracy, scalability and
optimal convergence.",cs.CE cs.NA math.NA,2024-07-31
"CEAR: Automatic construction of a knowledge graph of chemical entities
  and roles from scientific literature",,"Ontologies are formal representations of knowledge in specific domains that
provide a structured framework for organizing and understanding complex
information. Creating ontologies, however, is a complex and time-consuming
endeavor. ChEBI is a well-known ontology in the field of chemistry, which
provides a comprehensive resource for defining chemical entities and their
properties. However, it covers only a small fraction of the rapidly growing
knowledge in chemistry and does not provide references to the scientific
literature. To address this, we propose a methodology that involves augmenting
existing annotated text corpora with knowledge from Chebi and fine-tuning a
large language model (LLM) to recognize chemical entities and their roles in
scientific text. Our experiments demonstrate the effectiveness of our approach.
By combining ontological knowledge and the language understanding capabilities
of LLMs, we achieve high precision and recall rates in identifying both the
chemical entities and roles in scientific literature. Furthermore, we extract
them from a set of 8,000 ChemRxiv articles, and apply a second LLM to create a
knowledge graph (KG) of chemical entities and roles (CEAR), which provides
complementary information to ChEBI, and can help to extend it.",cs.AI,2024-07-31
Adaptive Retrieval-Augmented Generation for Conversational Systems,,"Despite the success of integrating large language models into the development
of conversational systems, many studies have shown the effectiveness of
retrieving and augmenting external knowledge for informative responses. Hence,
many existing studies commonly assume the always need for Retrieval Augmented
Generation (RAG) in a conversational system without explicit control. This
raises a research question about such a necessity. In this study, we propose to
investigate the need for each turn of system response to be augmented with
external knowledge. In particular, by leveraging human judgements on the binary
choice of adaptive augmentation, we develop RAGate, a gating model, which
models conversation context and relevant inputs to predict if a conversational
system requires RAG for improved responses. We conduct extensive experiments on
devising and applying RAGate to conversational models and well-rounded analyses
of different conversational scenarios. Our experimental results and analysis
indicate the effective application of RAGate in RAG-based conversational
systems in identifying system responses for appropriate RAG with high-quality
responses and a high generation confidence. This study also identifies the
correlation between the generation's confidence level and the relevance of the
augmented knowledge.",cs.CL cs.IR,2024-07-31
"UMMAN: Unsupervised Multi-graph Merge Adversarial Network for Disease
  Prediction Based on Intestinal Flora",,"The abundance of intestinal flora is closely related to human diseases, but
diseases are not caused by a single gut microbe. Instead, they result from the
complex interplay of numerous microbial entities. This intricate and implicit
connection among gut microbes poses a significant challenge for disease
prediction using abundance information from OTU data. Recently, several methods
have shown potential in predicting corresponding diseases. However, these
methods fail to learn the inner association among gut microbes from different
hosts, leading to unsatisfactory performance. In this paper, we present a novel
architecture, Unsupervised Multi-graph Merge Adversarial Network (UMMAN). UMMAN
can obtain the embeddings of nodes in the Multi-Graph in an unsupervised
scenario, so that it helps learn the multiplex association. Our method is the
first to combine Graph Neural Network with the task of intestinal flora disease
prediction. We employ complex relation-types to construct the Original-Graph
and disrupt the relationships among nodes to generate corresponding
Shuffled-Graph. We introduce the Node Feature Global Integration (NFGI) module
to represent the global features of the graph. Furthermore, we design a joint
loss comprising adversarial loss and hybrid attention loss to ensure that the
real graph embedding aligns closely with the Original-Graph and diverges from
the Shuffled-Graph. Comprehensive experiments on five classical OTU gut
microbiome datasets demonstrate the effectiveness and stability of our method.
(We will release our code soon.)",cs.AI q-bio.QM,2024-07-31
Assessing the State of AI Policy,,"The deployment of artificial intelligence (AI) applications has accelerated
rapidly. AI enabled technologies are facing the public in many ways including
infrastructure, consumer products and home applications. Because many of these
technologies present risks either in the form of physical injury, or bias,
potentially yielding unfair outcomes, policy makers must consider the need for
oversight. Most policymakers, however, lack the technical knowledge to judge
whether an emerging AI technology is safe, effective, and requires oversight,
therefore policy makers must depend on expert opinion. But policymakers are
better served when, in addition to expert opinion, they have some general
understanding of existing guidelines and regulations. This work provides an
overview [the landscape] of AI legislation and directives at the international,
U.S. state, city and federal levels. It also reviews relevant business
standards, and technical society initiatives. Then an overlap and gap analysis
are performed resulting in a reference guide that includes recommendations and
guidance for future policy making.",cs.AI cs.CY,2024-07-31
"Detecting, Explaining, and Mitigating Memorization in Diffusion Models",,"Recent breakthroughs in diffusion models have exhibited exceptional
image-generation capabilities. However, studies show that some outputs are
merely replications of training data. Such replications present potential legal
challenges for model owners, especially when the generated content contains
proprietary information. In this work, we introduce a straightforward yet
effective method for detecting memorized prompts by inspecting the magnitude of
text-conditional predictions. Our proposed method seamlessly integrates without
disrupting sampling algorithms, and delivers high accuracy even at the first
generation step, with a single generation per prompt. Building on our detection
strategy, we unveil an explainable approach that shows the contribution of
individual words or tokens to memorization. This offers an interactive medium
for users to adjust their prompts. Moreover, we propose two strategies i.e., to
mitigate memorization by leveraging the magnitude of text-conditional
predictions, either through minimization during inference or filtering during
training. These proposed strategies effectively counteract memorization while
maintaining high-generation quality. Code is available at
https://github.com/YuxinWenRick/diffusion_memorization.",cs.CV,2024-07-31
Open-Vocabulary Audio-Visual Semantic Segmentation,,"Audio-visual semantic segmentation (AVSS) aims to segment and classify
sounding objects in videos with acoustic cues. However, most approaches operate
on the close-set assumption and only identify pre-defined categories from
training data, lacking the generalization ability to detect novel categories in
practical applications. In this paper, we introduce a new task: open-vocabulary
audio-visual semantic segmentation, extending AVSS task to open-world scenarios
beyond the annotated label space. This is a more challenging task that requires
recognizing all categories, even those that have never been seen nor heard
during training. Moreover, we propose the first open-vocabulary AVSS framework,
OV-AVSS, which mainly consists of two parts: 1) a universal sound source
localization module to perform audio-visual fusion and locate all potential
sounding objects and 2) an open-vocabulary classification module to predict
categories with the help of the prior knowledge from large-scale pre-trained
vision-language models. To properly evaluate the open-vocabulary AVSS, we split
zero-shot training and testing subsets based on the AVSBench-semantic
benchmark, namely AVSBench-OV. Extensive experiments demonstrate the strong
segmentation and zero-shot generalization ability of our model on all
categories. On the AVSBench-OV dataset, OV-AVSS achieves 55.43% mIoU on base
categories and 29.14% mIoU on novel categories, exceeding the state-of-the-art
zero-shot method by 41.88%/20.61% and open-vocabulary method by 10.2%/11.6%.
The code is available at https://github.com/ruohaoguo/ovavss.",cs.MM cs.AI,2024-07-31
"Commutativity and spectral properties for a general class of
  Szasz-Mirakjan-Durrmeyer operators",,"In this paper we present commutativity results for a general class of
Szasz-Mirakjan-Durrmeyer type operators and associated differential operators
and investigate their eigenfunctions.",math.NA cs.NA,2024-07-31
Artificial Intelligence Approaches for Energy Efficiency: A Review,,"United Nations set Sustainable Development Goals and this paper focuses on
7th (Affordable and Clean Energy), 9th (Industries, Innovation and
Infrastructure), and 13th (Climate Action) goals. Climate change is a major
concern in our society; for this reason, a current global objective is to
reduce energy waste. This work summarizes all main approaches towards energy
efficiency using Artificial Intelligence with a particular focus on multi-agent
systems to create smart buildings. It mentions the tight relationship between
AI, especially IoT, and Big Data. It explains the application of AI to anomaly
detection in smart buildings and a possible classification of Intelligent
Energy Management Systems: Direct and Indirect. Finally, some drawbacks of AI
approaches and some possible future research focuses are proposed.",cs.AI,2024-07-31
"ParLS-PBO: A Parallel Local Search Solver for Pseudo Boolean
  Optimization",,"As a broadly applied technique in numerous optimization problems, recently,
local search has been employed to solve Pseudo-Boolean Optimization (PBO)
problem. A representative local search solver for PBO is LSPBO. In this paper,
firstly, we improve LSPBO by a dynamic scoring mechanism, which dynamically
strikes a balance between score on hard constraints and score on the objective
function.
  Moreover, on top of this improved LSPBO , we develop the first parallel local
search PBO solver. The main idea is to share good solutions among different
threads to guide the search, by maintaining a pool of feasible solutions. For
evaluating solutions when updating the pool, we propose a function that
considers both the solution quality and the diversity of the pool. Furthermore,
we calculate the polarity density in the pool to enhance the scoring function
of local search. Our empirical experiments show clear benefits of the proposed
parallel approach, making it competitive with the parallel version of the
famous commercial solver Gurobi.",cs.AI,2024-07-31
"On the Zero-Error Capacity of Semantic Channels with Input and Output
  Memories",,"This paper investigates the zero-error capacity of channels with memory.
Motivated by the nuanced requirements of semantic communication that
incorporate memory, we advance the classical enlightened dictator channel by
introducing a new category known as the semantic channel. We analyze the
zero-error capacity of the semantic channel using a comprehensive framework
that accommodates multiple input and output memories. Our approach reveals a
more sophisticated and detailed model compared to the classical memory
channels, highlighting the impact of memory on achieving error-free
communication.",cs.IT math.IT,2024-07-31
"Human-Machine Co-Adaptation for Robot-Assisted Rehabilitation via
  Dual-Agent Multiple Model Reinforcement Learning (DAMMRL)",,"This study introduces a novel approach to robot-assisted ankle rehabilitation
by proposing a Dual-Agent Multiple Model Reinforcement Learning (DAMMRL)
framework, leveraging multiple model adaptive control (MMAC) and co-adaptive
control strategies. In robot-assisted rehabilitation, one of the key challenges
is modelling human behaviour due to the complexity of human cognition and
physiological systems. Traditional single-model approaches often fail to
capture the dynamics of human-machine interactions. Our research employs a
multiple model strategy, using simple sub-models to approximate complex human
responses during rehabilitation tasks, tailored to varying levels of patient
incapacity. The proposed system's versatility is demonstrated in real
experiments and simulated environments. Feasibility and potential were
evaluated with 13 healthy young subjects, yielding promising results that
affirm the anticipated benefits of the approach. This study not only introduces
a new paradigm for robot-assisted ankle rehabilitation but also opens the way
for future research in adaptive, patient-centred therapeutic interventions.",cs.RO,2024-07-31
"Unifying Event-based Flow, Stereo and Depth Estimation via Feature
  Similarity Matching",,"As an emerging vision sensor, the event camera has gained popularity in
various vision tasks such as optical flow estimation, stereo matching, and
depth estimation due to its high-speed, sparse, and asynchronous event streams.
Unlike traditional approaches that use specialized architectures for each
specific task, we propose a unified framework, EventMatch, that reformulates
these tasks as an event-based dense correspondence matching problem, allowing
them to be solved with a single model by directly comparing feature
similarities. By utilizing a shared feature similarities module, which
integrates knowledge from other event flows via temporal or spatial
interactions, and distinct task heads, our network can concurrently perform
optical flow estimation from temporal inputs (e.g., two segments of event
streams in the temporal domain) and stereo matching from spatial inputs (e.g.,
two segments of event streams from different viewpoints in the spatial domain).
Moreover, we further demonstrate that our unified model inherently supports
cross-task transfer since the architecture and parameters are shared across
tasks. Without the need for retraining on each task, our model can effectively
handle both optical flow and disparity estimation simultaneously. The
experiment conducted on the DSEC benchmark demonstrates that our model exhibits
superior performance in both optical flow and disparity estimation tasks,
outperforming existing state-of-the-art methods. Our unified approach not only
advances event-based models but also opens new possibilities for cross-task
transfer and inter-task fusion in both spatial and temporal dimensions. Our
code will be available later.",cs.CV,2024-07-31
"Leveraging Self-Supervised Learning for Fetal Cardiac Planes
  Classification using Ultrasound Scan Videos",,"Self-supervised learning (SSL) methods are popular since they can address
situations with limited annotated data by directly utilising the underlying
data distribution. However, the adoption of such methods is not explored enough
in ultrasound (US) imaging, especially for fetal assessment. We investigate the
potential of dual-encoder SSL in utilizing unlabelled US video data to improve
the performance of challenging downstream Standard Fetal Cardiac Planes (SFCP)
classification using limited labelled 2D US images. We study 7 SSL approaches
based on reconstruction, contrastive loss, distillation, and information theory
and evaluate them extensively on a large private US dataset. Our observations
and findings are consolidated from more than 500 downstream training
experiments under different settings. Our primary observation shows that for
SSL training, the variance of the dataset is more crucial than its size because
it allows the model to learn generalisable representations, which improve the
performance of downstream tasks. Overall, the BarlowTwins method shows robust
performance, irrespective of the training settings and data variations, when
used as an initialisation for downstream tasks. Notably, full fine-tuning with
1% of labelled data outperforms ImageNet initialisation by 12% in F1-score and
outperforms other SSL initialisations by at least 4% in F1-score, thus making
it a promising candidate for transfer learning from US video to image data.",eess.IV cs.AI cs.CV cs.LG,2024-07-31
"A Federated Learning-Friendly Approach for Parameter-Efficient
  Fine-Tuning of SAM in 3D Segmentation",,"Adapting foundation models for medical image analysis requires finetuning
them on a considerable amount of data because of extreme distribution shifts
between natural (source) data used for pretraining and medical (target) data.
However, collecting task-specific medical data for such finetuning at a central
location raises many privacy concerns. Although Federated learning (FL)
provides an effective means for training on private decentralized data,
communication costs in federating large foundation models can quickly become a
significant bottleneck, impacting the solution's scalability. In this work, we
address this problem of efficient communication while ensuring effective
learning in FL by combining the strengths of Parameter-Efficient Fine-tuning
(PEFT) with FL. Specifically, we study plug-and-play Low-Rank Adapters (LoRA)
in a federated manner to adapt the Segment Anything Model (SAM) for 3D medical
image segmentation. Unlike prior works that utilize LoRA and finetune the
entire decoder, we critically analyze the contribution of each granular
component of SAM on finetuning performance. Thus, we identify specific layers
to be federated that are very efficient in terms of communication cost while
producing on-par accuracy. Our experiments show that retaining the parameters
of the SAM model (including most of the decoder) in their original state during
adaptation is beneficial because fine-tuning on small datasets tends to distort
the inherent capabilities of the underlying foundation model. On Fed-KiTS, our
approach decreases communication cost (~48x) compared to full fine-tuning while
increasing performance (~6% Dice score) in 3D segmentation tasks. Our approach
performs similar to SAMed while achieving ~2.8x reduction in communication and
parameters to be finetuned. We further validate our approach with experiments
on Fed-IXI and Prostate MRI datasets.",cs.CV cs.AI cs.LG eess.IV,2024-07-31
Contrastive Factor Analysis,,"Factor analysis, often regarded as a Bayesian variant of matrix
factorization, offers superior capabilities in capturing uncertainty, modeling
complex dependencies, and ensuring robustness. As the deep learning era
arrives, factor analysis is receiving less and less attention due to their
limited expressive ability. On the contrary, contrastive learning has emerged
as a potent technique with demonstrated efficacy in unsupervised
representational learning. While the two methods are different paradigms,
recent theoretical analysis has revealed the mathematical equivalence between
contrastive learning and matrix factorization, providing a potential
possibility for factor analysis combined with contrastive learning. Motivated
by the interconnectedness of contrastive learning, matrix factorization, and
factor analysis, this paper introduces a novel Contrastive Factor Analysis
framework, aiming to leverage factor analysis's advantageous properties within
the realm of contrastive learning. To further leverage the interpretability
properties of non-negative factor analysis, which can learn disentangled
representations, contrastive factor analysis is extended to a non-negative
version. Finally, extensive experimental validation showcases the efficacy of
the proposed contrastive (non-negative) factor analysis methodology across
multiple key properties, including expressiveness, robustness,
interpretability, and accurate uncertainty estimation.",cs.LG cs.AI cs.CV,2024-07-31
"HGOE: Hybrid External and Internal Graph Outlier Exposure for Graph
  Out-of-Distribution Detection",,"With the progressive advancements in deep graph learning, out-of-distribution
(OOD) detection for graph data has emerged as a critical challenge. While the
efficacy of auxiliary datasets in enhancing OOD detection has been extensively
studied for image and text data, such approaches have not yet been explored for
graph data. Unlike Euclidean data, graph data exhibits greater diversity but
lower robustness to perturbations, complicating the integration of outliers. To
tackle these challenges, we propose the introduction of \textbf{H}ybrid
External and Internal \textbf{G}raph \textbf{O}utlier \textbf{E}xposure (HGOE)
to improve graph OOD detection performance. Our framework involves using
realistic external graph data from various domains and synthesizing internal
outliers within ID subgroups to address the poor robustness and presence of OOD
samples within the ID class. Furthermore, we develop a boundary-aware OE loss
that adaptively assigns weights to outliers, maximizing the use of high-quality
OOD samples while minimizing the impact of low-quality ones. Our proposed HGOE
framework is model-agnostic and designed to enhance the effectiveness of
existing graph OOD detection models. Experimental results demonstrate that our
HGOE framework can significantly improve the performance of existing OOD
detection models across all 8 real datasets.",cs.LG cs.AI,2024-07-31
"Assessing the Reliability Benefits of Energy Storage as a Transmission
  Asset",,"Utilizing energy storage solutions to reduce the need for traditional
transmission investments has been recognized by system planners and supported
by federal policies in recent years. This work demonstrates the need for
detailed reliability assessment for quantitative comparison of the reliability
benefits of energy storage and traditional transmission investments. First, a
mixed-integer linear programming expansion planning model considering candidate
transmission lines and storage technologies is solved to find the least-cost
investment decisions. Next, operations under the resulting system configuration
are simulated in a probabilistic reliability assessment which accounts for
weather-dependent forced outages. The outcome of this work, when applied to
TPPs, is to further equalize the consideration of energy storage compared to
traditional transmission assets by capturing the value of storage for system
reliability.",eess.SY cs.SY,2024-07-31
Diagnostic Runtime Monitoring with Martingales,,"Machine learning systems deployed in safety-critical robotics settings must
be robust to distribution shifts. However, system designers must understand the
cause of a distribution shift in order to implement the appropriate
intervention or mitigation strategy and prevent system failure. In this paper,
we present a novel framework for diagnosing distribution shifts in a streaming
fashion by deploying multiple stochastic martingales simultaneously. We show
that knowledge of the underlying cause of a distribution shift can lead to
proper interventions over the lifecycle of a deployed system. Our experimental
framework can easily be adapted to different types of distribution shifts,
models, and datasets. We find that our method outperforms existing work on
diagnosing distribution shifts in terms of speed, accuracy, and flexibility,
and validate the efficiency of our model in both simulated and live hardware
settings.",cs.RO cs.LG,2024-07-31
Learning Video Context as Interleaved Multimodal Sequences,,"Narrative videos, such as movies, pose significant challenges in video
understanding due to their rich contexts (characters, dialogues, storylines)
and diverse demands (identify who, relationship, and reason). In this paper, we
introduce MovieSeq, a multimodal language model developed to address the wide
range of challenges in understanding video contexts. Our core idea is to
represent videos as interleaved multimodal sequences (including images, plots,
videos, and subtitles), either by linking external knowledge databases or using
offline models (such as whisper for subtitles). Through instruction-tuning,
this approach empowers the language model to interact with videos using
interleaved multimodal instructions. For example, instead of solely relying on
video as input, we jointly provide character photos alongside their names and
dialogues, allowing the model to associate these elements and generate more
comprehensive responses. To demonstrate its effectiveness, we validate
MovieSeq's performance on six datasets (LVU, MAD, Movienet, CMD, TVC, MovieQA)
across five settings (video classification, audio description, video-text
retrieval, video captioning, and video question-answering). The code will be
public at https://github.com/showlab/MovieSeq.",cs.CV cs.MM,2024-07-31
MOSAIC: Multimodal Multistakeholder-aware Visual Art Recommendation,,"Visual art (VA) recommendation is complex, as it has to consider the
interests of users (e.g. museum visitors) and other stakeholders (e.g. museum
curators). We study how to effectively account for key stakeholders in VA
recommendations while also considering user-centred measures such as novelty,
serendipity, and diversity. We propose MOSAIC, a novel multimodal
multistakeholder-aware approach using state-of-the-art CLIP and BLIP backbone
architectures and two joint optimisation objectives: popularity and
representative selection of paintings across different categories. We conducted
an offline evaluation using preferences elicited from 213 users followed by a
user study with 100 crowdworkers. We found a strong effect of popularity, which
was positively perceived by users, and a minimal effect of representativeness.
MOSAIC's impact extends beyond visitors, benefiting various art stakeholders.
Its user-centric approach has broader applicability, offering advancements for
content recommendation across domains that require considering multiple
stakeholders.",cs.IR,2024-07-31
Optimal price signal generation for demand-side energy management,,"Renewable Energy Sources play a key role in smart energy systems. To achieve
100% renewable energy, utilizing the flexibility potential on the demand side
becomes the cost-efficient option to balance the grid. However, it is not
trivial to exploit these available capacities and flexibility options
profitably. The amount of available flexibility is a complex and time-varying
function of the price signal and weather forecasts. In this work, we use a
Flexibility Function to represent the relationship between the price signal and
the demand and investigate optimization problems for the price signal
computation. Consequently, this study considers the higher and lower levels in
the hierarchy from the markets to appliances, households, and districts. This
paper investigates optimal price generation via the Flexibility Function and
studies its employment in controller design for demand-side management, its
capability to provide ancillary services for balancing throughout the Smart
Energy Operating System, and its effect on the physical level performance.
Sequential and simultaneous approaches for computing the price signal, along
with various cost functions are analyzed and compared. Simulation results
demonstrate the generated price/penalty signal and its employment in a model
predictive controller.",eess.SY cs.SY,2024-07-31
ReplanVLM: Replanning Robotic Tasks with Visual Language Models,,"Large language models (LLMs) have gained increasing popularity in robotic
task planning due to their exceptional abilities in text analytics and
generation, as well as their broad knowledge of the world. However, they fall
short in decoding visual cues. LLMs have limited direct perception of the
world, which leads to a deficient grasp of the current state of the world. By
contrast, the emergence of visual language models (VLMs) fills this gap by
integrating visual perception modules, which can enhance the autonomy of
robotic task planning. Despite these advancements, VLMs still face challenges,
such as the potential for task execution errors, even when provided with
accurate instructions. To address such issues, this paper proposes a ReplanVLM
framework for robotic task planning. In this study, we focus on error
correction interventions. An internal error correction mechanism and an
external error correction mechanism are presented to correct errors under
corresponding phases. A replan strategy is developed to replan tasks or correct
error codes when task execution fails. Experimental results on real robots and
in simulation environments have demonstrated the superiority of the proposed
framework, with higher success rates and robust error correction capabilities
in open-world tasks. Videos of our experiments are available at
https://youtu.be/NPk2pWKazJc.",cs.RO,2024-07-31
"A Waveguide Port Boundary Condition based on approximation space
  restriction for Finite Element Analysis",,"A Waveguide Port Boundary Condition (WPBC) based on the restriction of the
approximation space is presented in the context of Finite Element Analysis. As
well as reducing the computational domain in the same manner as the traditional
WPBC, the proposed scheme further reduces the degrees of freedom at the
waveguide ports, simplifies the implementation and seamlessly provides
post-processing results such as the reflected power in each waveguide mode. The
boundary condition is thoroughly derived, and numerical examples are used as a
support for the discussion on topics such as the needed number of modes to be
employed at a waveguide port. Finally, a nanograting-based plasmonic sensor is
analysed to illustrate further possibilities of the scheme.",math.NA cs.NA,2024-07-31
"Paying More Attention to Image: A Training-Free Method for Alleviating
  Hallucination in LVLMs",,"Existing Large Vision-Language Models (LVLMs) primarily align image features
of vision encoder with Large Language Models (LLMs) to leverage their superior
text generation capabilities. However, the scale disparity between vision
encoder and language model may led to LLMs assuming a predominant role in
multi-modal comprehension. This imbalance in LVLMs may result in the instances
of hallucinatory. Concretely, LVLMs may generate consistent descriptions with
or without visual input, indicating that certain outputs are influenced solely
by context text. We refer to this phenomenon as ""text inertia."" To counteract
this issue, we introduce a training-free algorithm to find an equilibrium point
between image comprehension and language inference. Specifically, we adaptively
involve adjusting and amplifying the attention weights assigned to image
tokens, thereby granting greater prominence to visual elements. Meanwhile, we
subtract the logits of multi-modal inputs from ones of pure text input, which
can help LVLMs be not biased towards LLMs. By enhancing images tokens and
reducing the stubborn output of LLM, we can let LVLM pay more attention to
images, towards alleviating text inertia and reducing the hallucination in
LVLMs. Our extensive experiments shows that this method substantially reduces
the frequency of hallucinatory outputs in various LVLMs in terms of different
metrics. Project page is available at https://lalbj.github.io/projects/PAI/.",cs.CV,2024-07-31
"RainMamba: Enhanced Locality Learning with State Space Models for Video
  Deraining",,"The outdoor vision systems are frequently contaminated by rain streaks and
raindrops, which significantly degenerate the performance of visual tasks and
multimedia applications. The nature of videos exhibits redundant temporal cues
for rain removal with higher stability. Traditional video deraining methods
heavily rely on optical flow estimation and kernel-based manners, which have a
limited receptive field. Yet, transformer architectures, while enabling
long-term dependencies, bring about a significant increase in computational
complexity. Recently, the linear-complexity operator of the state space models
(SSMs) has contrarily facilitated efficient long-term temporal modeling, which
is crucial for rain streaks and raindrops removal in videos. Unexpectedly, its
uni-dimensional sequential process on videos destroys the local correlations
across the spatio-temporal dimension by distancing adjacent pixels. To address
this, we present an improved SSMs-based video deraining network (RainMamba)
with a novel Hilbert scanning mechanism to better capture sequence-level local
information. We also introduce a difference-guided dynamic contrastive locality
learning strategy to enhance the patch-level self-similarity learning ability
of the proposed network. Extensive experiments on four synthesized video
deraining datasets and real-world rainy videos demonstrate the superiority of
our network in the removal of rain streaks and raindrops.",cs.CV,2024-07-31
"Tulip Agent -- Enabling LLM-Based Agents to Solve Tasks Using Large Tool
  Libraries",,"We introduce tulip agent, an architecture for autonomous LLM-based agents
with Create, Read, Update, and Delete access to a tool library containing a
potentially large number of tools. In contrast to state-of-the-art
implementations, tulip agent does not encode the descriptions of all available
tools in the system prompt, which counts against the model's context window, or
embed the entire prompt for retrieving suitable tools. Instead, the tulip agent
can recursively search for suitable tools in its extensible tool library,
implemented exemplarily as a vector store. The tulip agent architecture
significantly reduces inference costs, allows using even large tool libraries,
and enables the agent to adapt and extend its set of tools. We evaluate the
architecture with several ablation studies in a mathematics context and
demonstrate its generalizability with an application to robotics. A reference
implementation and the benchmark are available at
github.com/HRI-EU/tulip_agent.",cs.AI cs.RO,2024-07-31
Berkeley Humanoid: A Research Platform for Learning-based Control,,"We introduce Berkeley Humanoid, a reliable and low-cost mid-scale humanoid
research platform for learning-based control. Our lightweight, in-house-built
robot is designed specifically for learning algorithms with low simulation
complexity, anthropomorphic motion, and high reliability against falls. The
robot's narrow sim-to-real gap enables agile and robust locomotion across
various terrains in outdoor environments, achieved with a simple reinforcement
learning controller using light domain randomization. Furthermore, we
demonstrate the robot traversing for hundreds of meters, walking on a steep
unpaved trail, and hopping with single and double legs as a testimony to its
high performance in dynamical walking. Capable of omnidirectional locomotion
and withstanding large perturbations with a compact setup, our system aims for
scalable, sim-to-real deployment of learning-based humanoid systems. Please
check http://berkeley-humanoid.com for more details.",cs.RO,2024-07-31
The Llama 3 Herd of Models,,"Modern artificial intelligence (AI) systems are powered by foundation models.
This paper presents a new set of foundation models, called Llama 3. It is a
herd of language models that natively support multilinguality, coding,
reasoning, and tool usage. Our largest model is a dense Transformer with 405B
parameters and a context window of up to 128K tokens. This paper presents an
extensive empirical evaluation of Llama 3. We find that Llama 3 delivers
comparable quality to leading language models such as GPT-4 on a plethora of
tasks. We publicly release Llama 3, including pre-trained and post-trained
versions of the 405B parameter language model and our Llama Guard 3 model for
input and output safety. The paper also presents the results of experiments in
which we integrate image, video, and speech capabilities into Llama 3 via a
compositional approach. We observe this approach performs competitively with
the state-of-the-art on image, video, and speech recognition tasks. The
resulting models are not yet being broadly released as they are still under
development.",cs.AI cs.CL cs.CV,2024-07-31
Robust Restaking Networks,,"We study the risks of validator reuse across multiple services in a restaking
protocol. We characterize the robust security of a restaking network as a
function of the buffer between the costs and profits from attacks. For example,
our results imply that if attack costs always exceed attack profits by 10\%,
then a sudden loss of .1\% of the overall stake (e.g., due to a software error)
cannot result in the ultimate loss of more than 1.1\% of the overall stake. We
also provide local analogs of these overcollateralization conditions and robust
security guarantees that apply specifically for a target service or coalition
of services. All of our bounds on worst-case stake loss are the best possible.
Finally, we bound the maximum-possible length of a cascade of attacks.
  Our results suggest measures of robustness that could be exposed to the
participants in a restaking protocol. We also suggest polynomial-time
computable sufficient conditions that can proxy for these measures.",cs.GT cs.DS,2024-07-31
Large Language Monkeys: Scaling Inference Compute with Repeated Sampling,,"Scaling the amount of compute used to train language models has dramatically
improved their capabilities. However, when it comes to inference, we often
limit the amount of compute to only one attempt per problem. Here, we explore
inference compute as another axis for scaling by increasing the number of
generated samples. Across multiple tasks and models, we observe that coverage -
the fraction of problems solved by any attempt - scales with the number of
samples over four orders of magnitude. In domains like coding and formal
proofs, where all answers can be automatically verified, these increases in
coverage directly translate into improved performance. When we apply repeated
sampling to SWE-bench Lite, the fraction of issues solved with
DeepSeek-V2-Coder-Instruct increases from 15.9% with one sample to 56% with 250
samples, outperforming the single-attempt state-of-the-art of 43% which uses
more capable frontier models. Moreover, using current API pricing, amplifying
the cheaper DeepSeek model with five samples is more cost-effective and solves
more issues than paying a premium for one sample from GPT-4o or Claude 3.5
Sonnet. Interestingly, the relationship between coverage and the number of
samples is often log-linear and can be modelled with an exponentiated power
law, suggesting the existence of inference-time scaling laws. Finally, we find
that identifying correct samples out of many generations remains an important
direction for future research in domains without automatic verifiers. When
solving math word problems from GSM8K and MATH, coverage with Llama-3 models
grows to over 95% with 10,000 samples. However, common methods to pick correct
solutions from a sample collection, such as majority voting or reward models,
plateau beyond several hundred samples and fail to fully scale with the sample
budget.",cs.LG cs.AI,2024-07-31
Vision-Language Model Based Handwriting Verification,,"Handwriting Verification is a critical in document forensics. Deep learning
based approaches often face skepticism from forensic document examiners due to
their lack of explainability and reliance on extensive training data and
handcrafted features. This paper explores using Vision Language Models (VLMs),
such as OpenAI's GPT-4o and Google's PaliGemma, to address these challenges. By
leveraging their Visual Question Answering capabilities and 0-shot
Chain-of-Thought (CoT) reasoning, our goal is to provide clear,
human-understandable explanations for model decisions. Our experiments on the
CEDAR handwriting dataset demonstrate that VLMs offer enhanced
interpretability, reduce the need for large training datasets, and adapt better
to diverse handwriting styles. However, results show that the CNN-based
ResNet-18 architecture outperforms the 0-shot CoT prompt engineering approach
with GPT-4o (Accuracy: 70%) and supervised fine-tuned PaliGemma (Accuracy:
71%), achieving an accuracy of 84% on the CEDAR AND dataset. These findings
highlight the potential of VLMs in generating human-interpretable decisions
while underscoring the need for further advancements to match the performance
of specialized deep learning models.",cs.CV cs.AI cs.CL cs.LG,2024-07-31
Deep Learning for Options Trading: An End-To-End Approach,,"We introduce a novel approach to options trading strategies using a highly
scalable and data-driven machine learning algorithm. In contrast to traditional
approaches that often require specifications of underlying market dynamics or
assumptions on an option pricing model, our models depart fundamentally from
the need for these prerequisites, directly learning non-trivial mappings from
market data to optimal trading signals. Backtesting on more than a decade of
option contracts for equities listed on the S&P 100, we demonstrate that deep
learning models trained according to our end-to-end approach exhibit
significant improvements in risk-adjusted performance over existing rules-based
trading strategies. We find that incorporating turnover regularization into the
models leads to further performance enhancements at prohibitively high levels
of transaction costs.",q-fin.PM cs.LG q-fin.CP q-fin.TR,2024-07-31
Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?,,"As artificial intelligence systems grow more powerful, there has been
increasing interest in ""AI safety"" research to address emerging and future
risks. However, the field of AI safety remains poorly defined and
inconsistently measured, leading to confusion about how researchers can
contribute. This lack of clarity is compounded by the unclear relationship
between AI safety benchmarks and upstream general capabilities (e.g., general
knowledge and reasoning). To address these issues, we conduct a comprehensive
meta-analysis of AI safety benchmarks, empirically analyzing their correlation
with general capabilities across dozens of models and providing a survey of
existing directions in AI safety. Our findings reveal that many safety
benchmarks highly correlate with upstream model capabilities, potentially
enabling ""safetywashing"" -- where capability improvements are misrepresented as
safety advancements. Based on these findings, we propose an empirical
foundation for developing more meaningful safety metrics and define AI safety
in a machine learning research context as a set of clearly delineated research
goals that are empirically separable from generic capabilities advancements. In
doing so, we aim to provide a more rigorous framework for AI safety research,
advancing the science of safety evaluations and clarifying the path towards
measurable progress.",cs.LG cs.AI cs.CL cs.CY,2024-07-31
"Generalized Out-of-Distribution Detection and Beyond in Vision Language
  Model Era: A Survey",,"Detecting out-of-distribution (OOD) samples is crucial for ensuring the
safety of machine learning systems and has shaped the field of OOD detection.
Meanwhile, several other problems are closely related to OOD detection,
including anomaly detection (AD), novelty detection (ND), open set recognition
(OSR), and outlier detection (OD). To unify these problems, a generalized OOD
detection framework was proposed, taxonomically categorizing these five
problems. However, Vision Language Models (VLMs) such as CLIP have
significantly changed the paradigm and blurred the boundaries between these
fields, again confusing researchers. In this survey, we first present a
generalized OOD detection v2, encapsulating the evolution of AD, ND, OSR, OOD
detection, and OD in the VLM era. Our framework reveals that, with some field
inactivity and integration, the demanding challenges have become OOD detection
and AD. In addition, we also highlight the significant shift in the definition,
problem settings, and benchmarks; we thus feature a comprehensive review of the
methodology for OOD detection, including the discussion over other related
tasks to clarify their relationship to OOD detection. Finally, we explore the
advancements in the emerging Large Vision Language Model (LVLM) era, such as
GPT-4V. We conclude this survey with open challenges and future directions.",cs.CV cs.AI cs.LG,2024-07-31
Replication in Visual Diffusion Models: A Survey and Outlook,,"Visual diffusion models have revolutionized the field of creative AI,
producing high-quality and diverse content. However, they inevitably memorize
training images or videos, subsequently replicating their concepts, content, or
styles during inference. This phenomenon raises significant concerns about
privacy, security, and copyright within generated outputs. In this survey, we
provide the first comprehensive review of replication in visual diffusion
models, marking a novel contribution to the field by systematically
categorizing the existing studies into unveiling, understanding, and mitigating
this phenomenon. Specifically, unveiling mainly refers to the methods used to
detect replication instances. Understanding involves analyzing the underlying
mechanisms and factors that contribute to this phenomenon. Mitigation focuses
on developing strategies to reduce or eliminate replication. Beyond these
aspects, we also review papers focusing on its real-world influence. For
instance, in the context of healthcare, replication is critically worrying due
to privacy concerns related to patient data. Finally, the paper concludes with
a discussion of the ongoing challenges, such as the difficulty in detecting and
benchmarking replication, and outlines future directions including the
development of more robust mitigation techniques. By synthesizing insights from
diverse studies, this paper aims to equip researchers and practitioners with a
deeper understanding at the intersection between AI technology and social good.
We release this project at
https://github.com/WangWenhao0716/Awesome-Diffusion-Replication.",cs.CV cs.AI cs.CY,2024-07-07
"Evaluating Transfer Learning in Deep Learning Models for Classification
  on a Custom Wildlife Dataset: Can YOLOv8 Surpass Other Architectures?",,"Biodiversity plays a crucial role in maintaining the balance of the
ecosystem. However, poaching and unintentional human activities contribute to
the decline in the population of many species. Hence, active monitoring is
required to preserve these endangered species. Current human-led monitoring
techniques are prone to errors and are labor-intensive. Therefore, we study the
application of deep learning methods like Convolutional Neural Networks (CNNs)
and transfer learning, which can aid in automating the process of monitoring
endangered species. For this, we create our custom dataset utilizing
trustworthy online databases like iNaturalist and ZooChat. To choose the best
model for our use case, we compare the performance of different architectures
like DenseNet, ResNet, VGGNet, and YOLOv8 on the custom wildlife dataset.
Transfer learning reduces training time by freezing the pre-trained weights and
replacing only the output layer with custom, fully connected layers designed
for our dataset. Our results indicate that YOLOv8 performs better, achieving a
training accuracy of 97.39 % and an F1 score of 96.50 %, surpassing other
models. Our findings suggest that integrating YOLOv8 into conservation efforts
could revolutionize wildlife monitoring with its high accuracy and efficiency,
potentially transforming how endangered species are monitored and protected
worldwide.",cs.CV cs.AI,2024-07-10
Handling Numeric Expressions in Automatic Speech Recognition,,"This paper addresses the problem of correctly formatting numeric expressions
in automatic speech recognition (ASR) transcripts. This is challenging since
the expected transcript format depends on the context, e.g., 1945 (year) vs.
19:45 (timestamp). We compare cascaded and end-to-end approaches to recognize
and format numeric expression, such as years, timestamps, currency amounts, and
quantities. For the end-to-end approach we employed a data generation strategy
using a large language model (LLM) together with a text to speech (TTS) model
to generate adaptation data. The results on our test dataset show that while
approaches based on LLMs perform well on recognizing formatted numeric
expressions, adapted end-to-end models offer competitive performance with the
advantage of lower latency and inference cost.",eess.AS cs.AI cs.CL,2024-07-18
"Framework for Curating Speech Datasets and Evaluating ASR Systems: A
  Case Study for Polish",,"Speech datasets available in the public domain are often underutilized
because of challenges in discoverability and interoperability. A comprehensive
framework has been designed to survey, catalog, and curate available speech
datasets, which allows replicable evaluation of automatic speech recognition
(ASR) systems. A case study focused on the Polish language was conducted; the
framework was applied to curate more than 24 datasets and evaluate 25
combinations of ASR systems and models. This research constitutes the most
extensive comparison to date of both commercial and free ASR systems for the
Polish language. It draws insights from 600 system-model-test set evaluations,
marking a significant advancement in both scale and comprehensiveness. The
results of surveys and performance comparisons are available as interactive
dashboards (https://huggingface.co/spaces/amu-cai/pl-asr-leaderboard) along
with curated datasets (https://huggingface.co/datasets/amu-cai/pl-asr-bigos-v2,
https://huggingface.co/datasets/pelcra/pl-asr-pelcra-for-bigos) and the open
challenge call (https://poleval.pl/tasks/task3). Tools used for evaluation are
open-sourced (https://github.com/goodmike31/pl-asr-bigos-tools), facilitating
replication and adaptation for other languages, as well as continuous expansion
with new datasets and systems.",eess.AS cs.AI cs.CL cs.LG cs.SD,2024-07-18
Synthetic Time Series for Anomaly Detection in Cloud Microservices,,"This paper proposes a framework for time series generation built to
investigate anomaly detection in cloud microservices. In the field of cloud
computing, ensuring the reliability of microservices is of paramount concern
and yet a remarkably challenging task. Despite the large amount of research in
this area, validation of anomaly detection algorithms in realistic environments
is difficult to achieve. To address this challenge, we propose a framework to
mimic the complex time series patterns representative of both normal and
anomalous cloud microservices behaviors. We detail the pipeline implementation
that allows deployment and management of microservices as well as the
theoretical approach required to generate anomalies. Two datasets generated
using the proposed framework have been made publicly available through GitHub.",cs.DC cs.LG,2024-07-21
"ScaleLLM: A Resource-Frugal LLM Serving Framework by Optimizing
  End-to-End Efficiency",,"Large language models (LLMs) have surged in popularity and are extensively
used in commercial applications, where the efficiency of model serving is
crucial for the user experience. Most current research focuses on optimizing
individual sub-procedures, e.g. local inference and communication, however,
there is no comprehensive framework that provides a holistic system view for
optimizing LLM serving in an end-to-end manner. In this work, we conduct a
detailed analysis to identify major bottlenecks that impact end-to-end latency
in LLM serving systems. Our analysis reveals that a comprehensive LLM serving
endpoint must address a series of efficiency bottlenecks that extend beyond LLM
inference. We then propose ScaleLLM, an optimized system for resource-efficient
LLM serving. Our extensive experiments reveal that with 64 concurrent requests,
ScaleLLM achieves a 4.3x speed up over vLLM and outperforms state-of-the-arts
with 1.5x higher throughput.",cs.DC cs.LG,2024-07-23
"Optimization of Energy Consumption Forecasting in Puno using Parallel
  Computing and ARIMA Models: An Innovative Approach to Big Data Processing",,"This research presents an innovative use of parallel computing with the ARIMA
(AutoRegressive Integrated Moving Average) model to forecast energy consumption
in Peru's Puno region. The study conducts a thorough and multifaceted analysis,
focusing on the execution speed, prediction accuracy, and scalability of both
sequential and parallel implementations. A significant emphasis is placed on
efficiently managing large datasets. The findings demonstrate notable
improvements in computational efficiency and data processing capabilities
through the parallel approach, all while maintaining the accuracy and integrity
of predictions. This new method provides a versatile and reliable solution for
real-time predictive analysis and enhances energy resource management, which is
particularly crucial for developing areas. In addition to highlighting the
technical advantages of parallel computing in this field, the study explores
its practical impacts on energy planning and sustainable development in regions
like Puno.",cs.DC stat.CO stat.ML,2024-07-27
Towards a Universal Method for Meaningful Signal Detection,,"It is known that human speech and certain animal vocalizations can convey
meaningful content because we can decipher the content that a given utterance
does convey. This paper explores an alternative approach to determining whether
a signal is meaningful, one that analyzes only the signal itself and is
independent of what the conveyed meaning might be. We devise a method that
takes a waveform as input and outputs a score indicating its degree of
`meaningfulness`. We cluster contiguous portions of the input to minimize the
total description length, and then take the length of the code of the assigned
cluster labels as meaningfulness score. We evaluate our method empirically,
against several baselines, and show that it is the only one to give a high
score to human speech in various languages and with various speakers, a
moderate score to animal vocalizations from birds and orcas, and a low score to
ambient noise from various sources.",eess.AS cs.CL cs.SD,2024-07-28
"An efficient implementation of parallel simulated annealing algorithm in
  GPUs",,"In this work we propose a highly optimized version of a simulated annealing
(SA) algorithm adapted to the more recently developed Graphic Processor Units
(GPUs). The programming has been carried out with CUDA toolkit, specially
designed for Nvidia GPUs. For this purpose, efficient versions of SA have been
first analyzed and adapted to GPUs. Thus, an appropriate sequential SA
algorithm has been developed as a starting point. Next, a straightforward
asynchronous parallel version has been implemented and then a specific and more
efficient synchronous version has been developed. A wide appropriate benchmark
to illustrate the performance properties of the implementation has been
considered. Among all tests, a classical sample problem provided by the
minimization of the normalized Schwefel function has been selected to compare
the behavior of the sequential, asynchronous, and synchronous versions, the
last one being more advantageous in terms of balance between convergence,
accuracy, and computational cost. Also, the implementation of a hybrid method
combining SA with a local minimizer method has been developed. Note that the
generic feature of the SA algorithm allows its application in a wide set of
real problems arising in a large variety of fields, such as biology, physics,
engineering, finance, and industrial processes.",cs.DC math.OC,2024-07-30
WebApp1K: A Practical Code-Generation Benchmark for Web App Development,,"We introduce WebApp1K, a practical code-generation benchmark to measure LLM
ability to develop web apps. This benchmark aims to calibrate LLM output and
aid the models to progressively improve code correctness and functionality. The
benchmark is lightweight and easy to run. We present the initial version of
WebApp1K, and share our findings of running the benchmark against the latest
frontier LLMs. First, open source LLMs deliver impressive performance, closely
trailing behind GPT-4o and Claude 3.5. Second, model size has strong
correlation with code correctness. Third, no prompting techniques have been
found to lift performance either universally to all models, or significantly to
a single model.",cs.SE cs.AI,2024-07-30
"Deceptive AI systems that give explanations are more convincing than
  honest AI systems and can amplify belief in misinformation",,"Advanced Artificial Intelligence (AI) systems, specifically large language
models (LLMs), have the capability to generate not just misinformation, but
also deceptive explanations that can justify and propagate false information
and erode trust in the truth. We examined the impact of deceptive AI generated
explanations on individuals' beliefs in a pre-registered online experiment with
23,840 observations from 1,192 participants. We found that in addition to being
more persuasive than accurate and honest explanations, AI-generated deceptive
explanations can significantly amplify belief in false news headlines and
undermine true ones as compared to AI systems that simply classify the headline
incorrectly as being true/false. Moreover, our results show that personal
factors such as cognitive reflection and trust in AI do not necessarily protect
individuals from these effects caused by deceptive AI generated explanations.
Instead, our results show that the logical validity of AI generated deceptive
explanations, that is whether the explanation has a causal effect on the
truthfulness of the AI's classification, plays a critical role in countering
their persuasiveness - with logically invalid explanations being deemed less
credible. This underscores the importance of teaching logical reasoning and
critical thinking skills to identify logically invalid arguments, fostering
greater resilience against advanced AI-driven misinformation.",cs.AI cs.CY,2024-07-31
Need of AI in Modern Education: in the Eyes of Explainable AI (xAI),,"Modern Education is not \textit{Modern} without AI. However, AI's complex
nature makes understanding and fixing problems challenging. Research worldwide
shows that a parent's income greatly influences a child's education. This led
us to explore how AI, especially complex models, makes important decisions
using Explainable AI tools. Our research uncovered many complexities linked to
parental income and offered reasonable explanations for these decisions.
However, we also found biases in AI that go against what we want from AI in
education: clear transparency and equal access for everyone. These biases can
impact families and children's schooling, highlighting the need for better AI
solutions that offer fair opportunities to all. This chapter tries to shed
light on the complex ways AI operates, especially concerning biases. These are
the foundational steps towards better educational policies, which include using
AI in ways that are more reliable, accountable, and beneficial for everyone
involved.",cs.AI,2024-07-31
A New Horizon of Data Communication through Quantum Entanglement,,"By the blessing of our existing data communication system, we can communicate
or share our information with each other in every nook and corner of the world
within some few seconds but there are some limitations in our traditional data
communication system. Every day we are trying to overcome these limitations and
improve our systems for better performance. Among them some problems may not be
resolvable, for the reason of very basic or root dependencies of physics. In
this paper, we have clarified some main drawbacks in our traditional
communication system and provided a conceptual model to overcome these issues
by using mystic Quantum Entanglement theorem rather than classical or modern
physics phenomenon. In the end, we introduced a possible Quantum circuit
diagram and Quantum network architecture for end-to-end data communication. It
is predicted that through this hypothetical model data can be transmitted
faster than light and it will be 100% real time between any distances without
any kinds of traditional communication medium that are being used to date.",quant-ph cs.NI,2024-07-31
"A New Type of Foundation Model Based on Recordings of People's Emotions
  and Physiology",,"Foundation models have had a big impact in recent years and billions of
dollars are being invested in them in the current AI boom. The more popular
ones, such as Chat-GPT, are trained on large amounts of data from the Internet,
and then reinforcement learning, RAG, prompt engineering and cognitive
modelling are used to fine-tune and augment their behavior. This technology has
been used to create models of individual people, such as Caryn Marjorie.
However, these chatbots are not based on people's actual emotional and
physiological responses to their environment, so they are, at best,
surface-level approximations to the characters they are imitating. This paper
describes how a new type of foundation model - a first-person foundation model
- could be created from recordings of what a person sees and hears as well as
their emotional and physiological reactions to these stimuli. A first-person
foundation model would map environmental stimuli to a person's emotional and
physiological states, and map a person's emotional and physiological states to
their behavior. First-person foundation models have many exciting applications,
including a new type of recommendation engine, personal assistants, generative
adversarial networks, dating and recruitment. To obtain training data for a
first-person foundation model, we have developed a recording rig that captures
what the wearer is seeing and hearing as well as their emotional and
physiological states. This novel source of data could help to address the
shortage of new data for building the next generation of foundation models.",cs.AI cs.LG,2024-07-31
"Enhanced Fault Detection and Cause Identification Using Integrated
  Attention Mechanism",,"This study introduces a novel methodology for fault detection and cause
identification within the Tennessee Eastman Process (TEP) by integrating a
Bidirectional Long Short-Term Memory (BiLSTM) neural network with an Integrated
Attention Mechanism (IAM). The IAM combines the strengths of scaled dot product
attention, residual attention, and dynamic attention to capture intricate
patterns and dependencies crucial for TEP fault detection. Initially, the
attention mechanism extracts important features from the input data, enhancing
the model's interpretability and relevance. The BiLSTM network processes these
features bidirectionally to capture long-range dependencies, and the IAM
further refines the output, leading to improved fault detection results.
Simulation results demonstrate the efficacy of this approach, showcasing
superior performance in accuracy, false alarm rate, and misclassification rate
compared to existing methods. This methodology provides a robust and
interpretable solution for fault detection and diagnosis in the TEP,
highlighting its potential for industrial applications.",cs.AI eess.SP,2024-07-31
"MIMNet: Multi-Interest Meta Network with Multi-Granularity Target-Guided
  Attention for Cross-domain Recommendation",,"Cross-domain recommendation (CDR) plays a critical role in alleviating the
sparsity and cold-start problem and substantially boosting the performance of
recommender systems. Existing CDR methods prefer to either learn a common
preference bridge shared by all users or a personalized preference bridge
tailored for each user to transfer user preference from the source domain to
the target domain. Although these methods significantly improve the
recommendation performance, there are still some limitations. First, these
methods usually assume a user only has a unique interest, while ignoring the
fact that a user may interact with items with different interest preferences.
Second, they learn transformed preference representation mainly relies on the
source domain signals, while neglecting the rich information available in the
target domain. To handle these issues, in this paper, we propose a novel method
named Multi-interest Meta Network with Multi-granularity Target-guided
Attention (MIMNet) for cross-domain recommendation. To be specific, we employ
the capsule network to learn user multiple interests in the source domain,
which will be fed into a meta network to generate multiple interest-level
preference bridges. Then, we transfer user representations from the source
domain to the target domain based on these multi-interest bridges. In addition,
we introduce both fine-grained and coarse-grained target signals to aggregate
user transformed interest-level representations by incorporating a novel
multi-granularity target-guided attention network. We conduct extensive
experimental results on three real-world CDR tasks, and the results show that
our proposed approach MIMNet consistently outperforms all baseline methods. The
source code of MIMNet is released at https://github.com/marqu22/MIMNet.",cs.IR,2024-07-31
"Barlow Twins Deep Neural Network for Advanced 1D Drug-Target Interaction
  Prediction",,"Accurate prediction of drug-target interactions is critical for advancing
drug discovery. By reducing time and cost, machine learning and deep learning
can accelerate this discovery process. Our approach utilises the powerful
Barlow Twins architecture for feature-extraction while considering the
structure of the target protein, achieving state-of-the-art predictive
performance against multiple established benchmarks. The use of gradient
boosting machine as the underlying predictor ensures fast and efficient
predictions without the need for large computational resources. In addition, we
further benchmarked new baselines against existing methods. Together, these
innovations improve the efficiency and effectiveness of drug-target interaction
predictions, providing robust tools for accelerating drug development and
deepening the understanding of molecular interactions.",q-bio.BM cs.AI cs.LG,2024-07-31
"Con4m: Context-aware Consistency Learning Framework for Segmented Time
  Series Classification",,"Time Series Classification (TSC) encompasses two settings: classifying entire
sequences or classifying segmented subsequences. The raw time series for
segmented TSC usually contain Multiple classes with Varying Duration of each
class (MVD). Therefore, the characteristics of MVD pose unique challenges for
segmented TSC, yet have been largely overlooked by existing works.
Specifically, there exists a natural temporal dependency between consecutive
instances (segments) to be classified within MVD. However, mainstream TSC
models rely on the assumption of independent and identically distributed
(i.i.d.), focusing on independently modeling each segment. Additionally,
annotators with varying expertise may provide inconsistent boundary labels,
leading to unstable performance of noise-free TSC models. To address these
challenges, we first formally demonstrate that valuable contextual information
enhances the discriminative power of classification instances. Leveraging the
contextual priors of MVD at both the data and label levels, we propose a novel
consistency learning framework Con4m, which effectively utilizes contextual
information more conducive to discriminating consecutive segments in segmented
TSC tasks, while harmonizing inconsistent boundary labels for training.
Extensive experiments across multiple datasets validate the effectiveness of
Con4m in handling segmented TSC tasks on MVD.",cs.AI,2024-07-31
"Ponder: Online Prediction of Task Memory Requirements for Scientific
  Workflows",,"Scientific workflows are used to analyze large amounts of data. These
workflows comprise numerous tasks, many of which are executed repeatedly,
running the same custom program on different inputs. Users specify resource
allocations for each task, which must be sufficient for all inputs to prevent
task failures. As a result, task memory allocations tend to be overly
conservative, wasting precious cluster resources, limiting overall parallelism,
and increasing workflow makespan.
  In this paper, we first benchmark a state-of-the-art method on four real-life
workflows from the nf-core workflow repository. This analysis reveals that
certain assumptions underlying current prediction methods, which typically were
evaluated only on simulated workflows, cannot generally be confirmed for real
workflows and executions. We then present Ponder, a new online task-sizing
strategy that considers and chooses between different methods to cater to
different memory demand patterns. We implemented Ponder for Nextflow and made
the code publicly available. In an experimental evaluation that also considers
the impact of memory predictions on scheduling, Ponder improves Memory
Allocation Quality on average by 71.0% and makespan by 21.8% in comparison to a
state-of-the-art method. Moreover, Ponder produces 93.8% fewer task failures.",cs.DC,2024-07-31
"A User Study Method on Healthy Participants for Assessing an Assistive
  Wearable Robot Utilising EMG Sensing",,"Hand-wearable robots, specifically exoskeletons, are designed to aid hands in
daily activities, playing a crucial role in post-stroke rehabilitation and
assisting the elderly. Our contribution to this field is a textile robotic
glove with integrated actuators. These actuators, powered by pneumatic
pressure, guide the user's hand to a desired position. Crafted from textile
materials, our soft robotic glove prioritizes safety, lightweight construction,
and user comfort. Utilizing the ruffles technique, integrated actuators
guarantee high performance in blocking force and bending effectiveness. Here,
we present a participant study confirming the effectiveness of our robotic
device on a healthy participant group, exploiting EMG sensing.",cs.RO,2024-07-31
"Algorithms for Collaborative Machine Learning under Statistical
  Heterogeneity",,"Learning from distributed data without accessing them is undoubtedly a
challenging and non-trivial task. Nevertheless, the necessity for distributed
training of a statistical model has been increasing, due to the privacy
concerns of local data owners and the cost in centralizing the massively
distributed data. Federated learning (FL) is currently the de facto standard of
training a machine learning model across heterogeneous data owners, without
leaving the raw data out of local silos. Nevertheless, several challenges must
be addressed in order for FL to be more practical in reality. Among these
challenges, the statistical heterogeneity problem is the most significant and
requires immediate attention. From the main objective of FL, three major
factors can be considered as starting points -- \textit{parameter},
textit{mixing coefficient}, and \textit{local data distributions}. In alignment
with the components, this dissertation is organized into three parts. In
Chapter II, a novel personalization method, \texttt{SuPerFed}, inspired by the
mode-connectivity is introduced. In Chapter III, an adaptive decision-making
algorithm, \texttt{AAggFF}, is introduced for inducing uniform performance
distributions in participating clients, which is realized by online convex
optimization framework. Finally, in Chapter IV, a collaborative synthetic data
generation method, \texttt{FedEvg}, is introduced, leveraging the flexibility
and compositionality of an energy-based modeling approach. Taken together, all
of these approaches provide practical solutions to mitigate the statistical
heterogeneity problem in data-decentralized settings, paving the way for
distributed systems and applications using collaborative machine learning
methods.",stat.ML cs.DC cs.LG,2024-07-31
"Areas of Improvement for Autonomous Vehicles: A Machine Learning
  Analysis of Disengagement Reports",,"Since 2014, the California Department of Motor Vehicles (CDMV) has compiled
information from manufacturers of autonomous vehicles (AVs) regarding factors
that lead to the disengagement from autonomous driving mode in these vehicles.
These disengagement reports (DRs) contain information detailing whether the AV
disengaged from autonomous mode due to technology failure, manual override, or
other factors during driving tests. This paper presents a machine learning (ML)
based analysis of the information from the 2023 DRs. We use a natural language
processing (NLP) approach to extract important information from the description
of a disengagement, and use the k-Means clustering algorithm to group report
entries together. The cluster frequency is then analyzed, and each cluster is
manually categorized based on the factors leading to disengagement. We discuss
findings from previous years' DRs, and provide our own analysis to identify
areas of improvement for AVs.",cs.AI,2024-07-31
Temporal Subspace Clustering for Molecular Dynamics Data,,"We introduce MOSCITO (MOlecular Dynamics Subspace Clustering with Temporal
Observance), a subspace clustering for molecular dynamics data. MOSCITO groups
those timesteps of a molecular dynamics trajectory together into clusters in
which the molecule has similar conformations. In contrast to state-of-the-art
methods, MOSCITO takes advantage of sequential relationships found in time
series data. Unlike existing work, MOSCITO does not need a two-step procedure
with tedious post-processing, but directly models essential properties of the
data. Interpreting clusters as Markov states allows us to evaluate the
clustering performance based on the resulting Markov state models. In
experiments on 60 trajectories and 4 different proteins, we show that the
performance of MOSCITO achieves state-of-the-art performance in a novel
single-step method. Moreover, by modeling temporal aspects, MOSCITO obtains
better segmentation of trajectories, especially for small numbers of clusters.",cs.LG cs.IR physics.chem-ph,2024-07-31
"GOProteinGNN: Leveraging Protein Knowledge Graphs for Protein
  Representation Learning",,"Proteins play a vital role in biological processes and are indispensable for
living organisms. Accurate representation of proteins is crucial, especially in
drug development. Recently, there has been a notable increase in interest in
utilizing machine learning and deep learning techniques for unsupervised
learning of protein representations. However, these approaches often focus
solely on the amino acid sequence of proteins and lack factual knowledge about
proteins and their interactions, thus limiting their performance. In this
study, we present GOProteinGNN, a novel architecture that enhances protein
language models by integrating protein knowledge graph information during the
creation of amino acid level representations. Our approach allows for the
integration of information at both the individual amino acid level and the
entire protein level, enabling a comprehensive and effective learning process
through graph-based learning. By doing so, we can capture complex relationships
and dependencies between proteins and their functional annotations, resulting
in more robust and contextually enriched protein representations. Unlike
previous fusion methods, GOProteinGNN uniquely learns the entire protein
knowledge graph during training, which allows it to capture broader relational
nuances and dependencies beyond mere triplets as done in previous work. We
perform a comprehensive evaluation on several downstream tasks demonstrating
that GOProteinGNN consistently outperforms previous methods, showcasing its
effectiveness and establishing it as a state-of-the-art solution for protein
representation learning.",q-bio.BM cs.LG,2024-07-31
TASI Lectures on Physics for Machine Learning,,"These notes are based on lectures I gave at TASI 2024 on Physics for Machine
Learning. The focus is on neural network theory, organized according to network
expressivity, statistics, and dynamics. I present classic results such as the
universal approximation theorem and neural network / Gaussian process
correspondence, and also more recent results such as the neural tangent kernel,
feature learning with the maximal update parameterization, and
Kolmogorov-Arnold networks. The exposition on neural network theory emphasizes
a field theoretic perspective familiar to theoretical physicists. I elaborate
on connections between the two, including a neural network approach to field
theory.",hep-th cs.LG hep-ph,2024-07-31
Localized Gaussian Splatting Editing with Contextual Awareness,,"Recent text-guided generation of individual 3D object has achieved great
success using diffusion priors. However, these methods are not suitable for
object insertion and replacement tasks as they do not consider the background,
leading to illumination mismatches within the environment. To bridge the gap,
we introduce an illumination-aware 3D scene editing pipeline for 3D Gaussian
Splatting (3DGS) representation. Our key observation is that inpainting by the
state-of-the-art conditional 2D diffusion model is consistent with background
in lighting. To leverage the prior knowledge from the well-trained diffusion
models for 3D object generation, our approach employs a coarse-to-fine
objection optimization pipeline with inpainted views. In the first coarse step,
we achieve image-to-3D lifting given an ideal inpainted view. The process
employs 3D-aware diffusion prior from a view-conditioned diffusion model, which
preserves illumination present in the conditioning image. To acquire an ideal
inpainted image, we introduce an Anchor View Proposal (AVP) algorithm to find a
single view that best represents the scene illumination in target region. In
the second Texture Enhancement step, we introduce a novel Depth-guided
Inpainting Score Distillation Sampling (DI-SDS), which enhances geometry and
texture details with the inpainting diffusion prior, beyond the scope of the
3D-aware diffusion prior knowledge in the first coarse step. DI-SDS not only
provides fine-grained texture enhancement, but also urges optimization to
respect scene lighting. Our approach efficiently achieves local editing with
global illumination consistency without explicitly modeling light transport. We
demonstrate robustness of our method by evaluating editing in real scenes
containing explicit highlight and shadows, and compare against the
state-of-the-art text-to-3D editing methods.",cs.CV,2024-07-31
"Approximating Rayleigh Scattering in Exoplanetary Atmospheres using
  Physics-informed Neural Networks (PINNs)",,"This research introduces an innovative application of physics-informed neural
networks (PINNs) to tackle the intricate challenges of radiative transfer (RT)
modeling in exoplanetary atmospheres, with a special focus on efficiently
handling scattering phenomena. Traditional RT models often simplify scattering
as absorption, leading to inaccuracies. Our approach utilizes PINNs, noted for
their ability to incorporate the governing differential equations of RT
directly into their loss function, thus offering a more precise yet potentially
fast modeling technique. The core of our method involves the development of a
parameterized PINN tailored for a modified RT equation, enhancing its
adaptability to various atmospheric scenarios. We focus on RT in transiting
exoplanet atmospheres using a simplified 1D isothermal model with
pressure-dependent coefficients for absorption and Rayleigh scattering. In
scenarios of pure absorption, the PINN demonstrates its effectiveness in
predicting transmission spectra for diverse absorption profiles. For Rayleigh
scattering, the network successfully computes the RT equation, addressing both
direct and diffuse stellar light components. While our preliminary results with
simplified models are promising, indicating the potential of PINNs in improving
RT calculations, we acknowledge the errors stemming from our approximations as
well as the challenges in applying this technique to more complex atmospheric
conditions. Specifically, extending our approach to atmospheres with intricate
temperature-pressure profiles and varying scattering properties, such as those
introduced by clouds and hazes, remains a significant area for future
development.",astro-ph.EP astro-ph.IM cs.LG cs.NE,2024-07-31
Designing Beyond Current Conceptualizations of Spaceflight Experiences,,"The potential future democratization of spaceflight reveals a need for design
of experiences that extend beyond our current conceptualization of spaceflight.
Research on career astronauts indicates that transformative experiences occur
during spaceflight despite the physiological and psychological stressors
involved. This phenomenon allows us to envision a future where such profound
experiences are accessible to diverse spaceflight participants. In this
position paper, we advocate for acknowledging how design decisions made at the
genesis of commercial spaceflight might impact space travelers of this
speculative future. In proposing salutogenesis as an orienting topic, a
potential design framework, and as a metric for spaceflight participant
experience, we offer a call to action for the broader experience design
community to engage with the design of profound experiences for spaceflight
participants.",cs.HC,2024-07-31
Execution Semantics of Behavior Trees in Robotic Applications,,"This document aims at describing, in a suitably precise and unambiguous
though informal way, the execution semantics of Behavior Trees as used in
Robotics applications, with particular attention to the Halt semantics.",cs.RO cs.AI,2024-07-31
"From Attributes to Natural Language: A Survey and Foresight on
  Text-based Person Re-identification",,"Text-based person re-identification (Re-ID) is a challenging topic in the
field of complex multimodal analysis, its ultimate aim is to recognize specific
pedestrians by scrutinizing attributes/natural language descriptions. Despite
the wide range of applicable areas such as security surveillance, video
retrieval, person tracking, and social media analytics, there is a notable
absence of comprehensive reviews dedicated to summarizing the text-based person
Re-ID from a technical perspective. To address this gap, we propose to
introduce a taxonomy spanning Evaluation, Strategy, Architecture, and
Optimization dimensions, providing a comprehensive survey of the text-based
person Re-ID task. We start by laying the groundwork for text-based person
Re-ID, elucidating fundamental concepts related to attribute/natural
language-based identification. Then a thorough examination of existing
benchmark datasets and metrics is presented. Subsequently, we further delve
into prevalent feature extraction strategies employed in text-based person
Re-ID research, followed by a concise summary of common network architectures
within the domain. Prevalent loss functions utilized for model optimization and
modality alignment in text-based person Re-ID are also scrutinized. To
conclude, we offer a concise summary of our findings, pinpointing challenges in
text-based person Re-ID. In response to these challenges, we outline potential
avenues for future open-set text-based person Re-ID and present a baseline
architecture for text-based pedestrian image generation-guided
re-identification(TBPGR).",cs.CV cs.AI,2024-07-31
"Adaptive Transit Signal Priority based on Deep Reinforcement Learning
  and Connected Vehicles in a Traffic Microsimulation Environment",,"Model free reinforcement learning (RL) provides a potential alternative to
earlier formulations of adaptive transit signal priority (TSP) algorithms based
on mathematical programming that require complex and nonlinear objective
functions. This study extends RL - based traffic control to include TSP. Using
a microscopic simulation environment and connected vehicle data, the study
develops and tests a TSP event-based RL agent that assumes control from another
developed RL - based general traffic signal controller. The TSP agent assumes
control when transit buses enter the dedicated short-range communication (DSRC)
zone of the intersection. This agent is shown to reduce the bus travel time by
about 21%, with marginal impacts to general traffic at a saturation rate of
0.95. The TSP agent also shows slightly better bus travel time compared to
actuated signal control with TSP. The architecture of the agent and simulation
is selected considering the need to improve simulation run time efficiency.",cs.LG,2024-07-31
"ReLiK: Retrieve and LinK, Fast and Accurate Entity Linking and Relation
  Extraction on an Academic Budget",,"Entity Linking (EL) and Relation Extraction (RE) are fundamental tasks in
Natural Language Processing, serving as critical components in a wide range of
applications. In this paper, we propose ReLiK, a Retriever-Reader architecture
for both EL and RE, where, given an input text, the Retriever module undertakes
the identification of candidate entities or relations that could potentially
appear within the text. Subsequently, the Reader module is tasked to discern
the pertinent retrieved entities or relations and establish their alignment
with the corresponding textual spans. Notably, we put forward an innovative
input representation that incorporates the candidate entities or relations
alongside the text, making it possible to link entities or extract relations in
a single forward pass and to fully leverage pre-trained language models
contextualization capabilities, in contrast with previous
Retriever-Reader-based methods, which require a forward pass for each
candidate. Our formulation of EL and RE achieves state-of-the-art performance
in both in-domain and out-of-domain benchmarks while using academic budget
training and with up to 40x inference speed compared to competitors. Finally,
we show how our architecture can be used seamlessly for Information Extraction
(cIE), i.e. EL + RE, and setting a new state of the art by employing a shared
Reader that simultaneously extracts entities and relations.",cs.CL cs.AI,2024-07-31
WAS: Dataset and Methods for Artistic Text Segmentation,,"Accurate text segmentation results are crucial for text-related generative
tasks, such as text image generation, text editing, text removal, and text
style transfer. Recently, some scene text segmentation methods have made
significant progress in segmenting regular text. However, these methods perform
poorly in scenarios containing artistic text. Therefore, this paper focuses on
the more challenging task of artistic text segmentation and constructs a real
artistic text segmentation dataset. One challenge of the task is that the local
stroke shapes of artistic text are changeable with diversity and complexity. We
propose a decoder with the layer-wise momentum query to prevent the model from
ignoring stroke regions of special shapes. Another challenge is the complexity
of the global topological structure. We further design a skeleton-assisted head
to guide the model to focus on the global structure. Additionally, to enhance
the generalization performance of the text segmentation model, we propose a
strategy for training data synthesis, based on the large multi-modal model and
the diffusion model. Experimental results show that our proposed method and
synthetic dataset can significantly enhance the performance of artistic text
segmentation and achieve state-of-the-art results on other public datasets.",cs.CV cs.AI,2024-07-31
Back to the Continuous Attractor,,"Continuous attractors offer a unique class of solutions for storing
continuous-valued variables in recurrent system states for indefinitely long
time intervals. Unfortunately, continuous attractors suffer from severe
structural instability in general--they are destroyed by most infinitesimal
changes of the dynamical law that defines them. This fragility limits their
utility especially in biological systems as their recurrent dynamics are
subject to constant perturbations. We observe that the bifurcations from
continuous attractors in theoretical neuroscience models display various
structurally stable forms. Although their asymptotic behaviors to maintain
memory are categorically distinct, their finite-time behaviors are similar. We
build on the persistent manifold theory to explain the commonalities between
bifurcations from and approximations of continuous attractors. Fast-slow
decomposition analysis uncovers the persistent manifold that survives the
seemingly destructive bifurcation. Moreover, recurrent neural networks trained
on analog memory tasks display approximate continuous attractors with predicted
slow manifold structures. Therefore, continuous attractors are functionally
robust and remain useful as a universal analogy for understanding analog
memory.",q-bio.NC cs.NE nlin.AO,2024-07-31
"Automated Sperm Morphology Analysis Based on Instance-Aware Part
  Segmentation",,"Traditional sperm morphology analysis is based on tedious manual annotation.
Automated morphology analysis of a high number of sperm requires accurate
segmentation of each sperm part and quantitative morphology evaluation.
State-of-the-art instance-aware part segmentation networks follow a
""detect-then-segment"" paradigm. However, due to sperm's slim shape, their
segmentation suffers from large context loss and feature distortion due to
bounding box cropping and resizing during ROI Align. Moreover, morphology
measurement of sperm tail is demanding because of the long and curved shape and
its uneven width. This paper presents automated techniques to measure sperm
morphology parameters automatically and quantitatively. A novel attention-based
instance-aware part segmentation network is designed to reconstruct lost
contexts outside bounding boxes and to fix distorted features, by refining
preliminary segmented masks through merging features extracted by feature
pyramid network. An automated centerline-based tail morphology measurement
method is also proposed, in which an outlier filtering method and endpoint
detection algorithm are designed to accurately reconstruct tail endpoints.
Experimental results demonstrate that the proposed network outperformed the
state-of-the-art top-down RP-R-CNN by 9.2% [AP]_vol^p, and the proposed
automated tail morphology measurement method achieved high measurement
accuracies of 95.34%,96.39%,91.2% for length, width and curvature,
respectively.",cs.CV,2024-07-31
"Measuring Progress in Dictionary Learning for Language Model
  Interpretability with Board Game Models",,"What latent features are encoded in language model (LM) representations?
Recent work on training sparse autoencoders (SAEs) to disentangle interpretable
features in LM representations has shown significant promise. However,
evaluating the quality of these SAEs is difficult because we lack a
ground-truth collection of interpretable features that we expect good SAEs to
recover. We thus propose to measure progress in interpretable dictionary
learning by working in the setting of LMs trained on chess and Othello
transcripts. These settings carry natural collections of interpretable features
-- for example, ""there is a knight on F3"" -- which we leverage into
$\textit{supervised}$ metrics for SAE quality. To guide progress in
interpretable dictionary learning, we introduce a new SAE training technique,
$\textit{p-annealing}$, which improves performance on prior unsupervised
metrics as well as our new metrics.",cs.LG cs.AI cs.CL,2024-07-31
"Certifying Robustness of Learning-Based Keypoint Detection and Pose
  Estimation Methods",,"This work addresses the certification of the local robustness of vision-based
two-stage 6D object pose estimation. The two-stage method for object pose
estimation achieves superior accuracy by first employing deep neural
network-driven keypoint regression and then applying a Perspective-n-Point
(PnP) technique. Despite advancements, the certification of these methods'
robustness remains scarce. This research aims to fill this gap with a focus on
their local robustness on the system level--the capacity to maintain robust
estimations amidst semantic input perturbations. The core idea is to transform
the certification of local robustness into neural network verification for
classification tasks. The challenge is to develop model, input, and output
specifications that align with off-the-shelf verification tools. To facilitate
verification, we modify the keypoint detection model by substituting nonlinear
operations with those more amenable to the verification processes. Instead of
injecting random noise into images, as is common, we employ a convex hull
representation of images as input specifications to more accurately depict
semantic perturbations. Furthermore, by conducting a sensitivity analysis, we
propagate the robustness criteria from pose to keypoint accuracy, and then
formulating an optimal error threshold allocation problem that allows for the
setting of a maximally permissible keypoint deviation thresholds. Viewing each
pixel as an individual class, these thresholds result in linear,
classification-akin output specifications. Under certain conditions, we
demonstrate that the main components of our certification framework are both
sound and complete, and validate its effects through extensive evaluations on
realistic perturbations. To our knowledge, this is the first study to certify
the robustness of large-scale, keypoint-based pose estimation given images in
real-world scenarios.",cs.CV cs.LG cs.RO cs.SY eess.SY,2024-07-31
A Course Shared Task on Evaluating LLM Output for Clinical Questions,,"This paper presents a shared task that we organized at the Foundations of
Language Technology (FoLT) course in 2023/2024 at the Technical University of
Darmstadt, which focuses on evaluating the output of Large Language Models
(LLMs) in generating harmful answers to health-related clinical questions. We
describe the task design considerations and report the feedback we received
from the students. We expect the task and the findings reported in this paper
to be relevant for instructors teaching natural language processing (NLP) and
designing course assignments.",cs.CL,2024-07-31
Semantic Codebook Learning for Dynamic Recommendation Models,,"Dynamic sequential recommendation (DSR) can generate model parameters based
on user behavior to improve the personalization of sequential recommendation
under various user preferences. However, it faces the challenges of large
parameter search space and sparse and noisy user-item interactions, which
reduces the applicability of the generated model parameters. The Semantic
Codebook Learning for Dynamic Recommendation Models (SOLID) framework presents
a significant advancement in DSR by effectively tackling these challenges. By
transforming item sequences into semantic sequences and employing a dual
parameter model, SOLID compresses the parameter generation search space and
leverages homogeneity within the recommendation system. The introduction of the
semantic metacode and semantic codebook, which stores disentangled item
representations, ensures robust and accurate parameter generation. Extensive
experiments demonstrates that SOLID consistently outperforms existing DSR,
delivering more accurate, stable, and robust recommendations.",cs.IR cs.AI cs.MM cs.SI,2024-07-31
Revisiting Monte Carlo Strength Evaluation,,"The Monte Carlo method, proposed by Dell'Amico and Filippone, estimates a
password's rank within a probabilistic model for password generation, i.e., it
determines the password's strength according to this model. We propose several
ideas to improve the precision or speed of the estimation. Through experimental
tests, we demonstrate that improved sampling can yield slightly better
precision. Moreover, additional precomputation results in faster estimations
with a modest increase in memory usage.",cs.CR,2024-07-31
Vera Verto: Multimodal Hijacking Attack,,"The increasing cost of training machine learning (ML) models has led to the
inclusion of new parties to the training pipeline, such as users who contribute
training data and companies that provide computing resources. This involvement
of such new parties in the ML training process has introduced new attack
surfaces for an adversary to exploit. A recent attack in this domain is the
model hijacking attack, whereby an adversary hijacks a victim model to
implement their own -- possibly malicious -- hijacking tasks. However, the
scope of the model hijacking attack is so far limited to the
homogeneous-modality tasks. In this paper, we transform the model hijacking
attack into a more general multimodal setting, where the hijacking and original
tasks are performed on data of different modalities. Specifically, we focus on
the setting where an adversary implements a natural language processing (NLP)
hijacking task into an image classification model. To mount the attack, we
propose a novel encoder-decoder based framework, namely the Blender, which
relies on advanced image and language models. Experimental results show that
our modal hijacking attack achieves strong performances in different settings.
For instance, our attack achieves 94%, 94%, and 95% attack success rate when
using the Sogou news dataset to hijack STL10, CIFAR-10, and MNIST classifiers.",cs.CR cs.LG,2024-07-31
"Sampling strategies for expectation values within the Herman--Kluk
  approximation",,"When computing quantum-mechanical observables, the ``curse of
dimensionality'' limits the naive approach that uses the quantum-mechanical
wavefunction. The semiclassical Herman--Kluk propagator mitigates this curse by
employing a grid-free ansatz to evaluate the expectation values of these
observables. Here, we investigate quadrature techniques for this
high-dimensional and highly oscillatory propagator. In particular, we analyze
Monte Carlo quadratures using three different initial sampling approaches. The
first two, based either on the Husimi density or its square root, are
independent of the observable whereas the third approach, which is new,
incorporates the observable in the sampling to minimize the variance of the
Monte Carlo integrand at the initial time. We prove sufficient conditions for
the convergence of the Monte Carlo estimators and provide convergence error
estimates. The analytical results are validated by numerical experiments in
various dimensions on a harmonic oscillator and on a Henon-Heiles potential
with an increasing degree of anharmonicity.",math.NA cs.NA,2024-07-31
"Distributionally Robust Optimization as a Scalable Framework to
  Characterize Extreme Value Distributions",,"The goal of this paper is to develop distributionally robust optimization
(DRO) estimators, specifically for multidimensional Extreme Value Theory (EVT)
statistics. EVT supports using semi-parametric models called max-stable
distributions built from spatial Poisson point processes. While powerful, these
models are only asymptotically valid for large samples. However, since extreme
data is by definition scarce, the potential for model misspecification error is
inherent to these applications, thus DRO estimators are natural. In order to
mitigate over-conservative estimates while enhancing out-of-sample performance,
we study DRO estimators informed by semi-parametric max-stable constraints in
the space of point processes. We study both tractable convex formulations for
some problems of interest (e.g. CVaR) and more general neural network based
estimators. Both approaches are validated using synthetically generated data,
recovering prescribed characteristics, and verifying the efficacy of the
proposed techniques. Additionally, the proposed method is applied to a real
data set of financial returns for comparison to a previous analysis. We
established the proposed model as a novel formulation in the multivariate EVT
domain, and innovative with respect to performance when compared to relevant
alternate proposals.",stat.ML cs.AI cs.LG q-fin.RM,2024-07-31
"LSTM-Based Net Load Forecasting for Wind and Solar Power-Equipped
  Microgrids",,"The rising integration of variable renewable energy sources (RES), like solar
and wind power, introduces considerable uncertainty in grid operations and
energy management. Effective forecasting models are essential for grid
operators to anticipate the net load - the difference between consumer
electrical demand and renewable power generation. This paper proposes a deep
learning (DL) model based on long short-term memory (LSTM) networks for net
load forecasting in renewable-based microgrids, considering both solar and wind
power. The model's architecture is detailed, and its performance is evaluated
using a residential microgrid test case based on a typical meteorological year
(TMY) dataset. The results demonstrate the effectiveness of the proposed
LSTM-based DL model in predicting the net load, showcasing its potential for
enhancing energy management in renewable-based microgrids.",eess.SY cs.SY,2024-07-31
"Correcting Negative Bias in Large Language Models through Negative
  Attention Score Alignment",,"A binary decision task, like yes-no questions or answer verification,
reflects a significant real-world scenario such as where users look for
confirmation about the correctness of their decisions on specific issues. In
this work, we observe that language models exhibit a negative bias in the
binary decisions of complex reasoning tasks. Based on our observations and the
rationale about attention-based model dynamics, we propose a negative attention
score (NAS) to systematically and quantitatively formulate negative bias. Based
on NAS, we identify attention heads that attend to negative tokens provided in
the instructions as answer candidate of binary decisions, regardless of the
question in the prompt, and validate their association with the negative bias.
Additionally, we propose the negative attention score alignment (NASA) method,
which is a parameter-efficient fine-tuning technique to address the extracted
negatively biased attention heads. Experimental results from various domains of
reasoning tasks and large model search space demonstrate that NASA
significantly reduces the gap between precision and recall caused by negative
bias while preserving their generalization abilities. Our codes are available
at \url{https://github.com/ysw1021/NASA}.",cs.CL cs.AI,2024-07-31
Multiway Alignment of Political Attitudes,,"The related concepts of partisan belief systems, issue alignment, and
partisan sorting are central to our understanding of politics. These phenomena
have been studied using measures of alignment between pairs of topics, or how
much individuals' attitudes toward a topic reveal about their attitudes toward
another topic. We introduce a higher-order measure that extends the assessment
of alignment beyond pairs of topics by quantifying the amount of information
individuals' opinions on one topic reveal about a set of topics simultaneously.
Our multiway alignment measure indicates how much individuals' opinions on
multiple topics align into a single ideological divide. Applying this approach
to legislative voting behavior reveals that parliamentary systems typically
exhibit similar multiway alignment characteristics, but can change in response
to shifting intergroup dynamics. In American National Election Studies surveys,
our approach reveals a growing significance of party identification together
with a consistent rise in multiway alignment over time. Similarly, the growing
multiway alignment among topical issues in Finnish online discussions suggests
a trend towards a more ideologically driven political landscape. Our case
studies demonstrate that the multiway alignment measure is a versatile tool for
understanding societal polarization and partisan belief systems across diverse
domains.",cs.SI physics.soc-ph stat.AP,2024-07-31
Distributed In-Context Learning under Non-IID Among Clients,,"Advancements in large language models (LLMs) have shown their effectiveness
in multiple complicated natural language reasoning tasks. A key challenge
remains in adapting these models efficiently to new or unfamiliar tasks.
In-context learning (ICL) provides a promising solution for few-shot adaptation
by retrieving a set of data points relevant to a query, called in-context
examples (ICE), from a training dataset and providing them during the inference
as context. Most existing studies utilize a centralized training dataset, yet
many real-world datasets may be distributed among multiple clients, and remote
data retrieval can be associated with costs. Especially when the client data
are non-identical independent distributions (non-IID), retrieving from clients
a proper set of ICEs needed for a test query presents critical challenges. In
this paper, we first show that in this challenging setting, test queries will
have different preferences among clients because of non-IIDness, and equal
contribution often leads to suboptimal performance. We then introduce a novel
approach to tackle the distributed non-IID ICL problem when a data usage budget
is present. The principle is that each client's proper contribution (budget)
should be designed according to the preference of each query for that client.
Our approach uses a data-driven manner to allocate a budget for each client,
tailored to each test query. Through extensive empirical studies on diverse
datasets, our framework demonstrates superior performance relative to competing
baselines.",cs.CL cs.AI,2024-07-31
"Formal Ethical Obligations in Reinforcement Learning Agents:
  Verification and Policy Updates",,"When designing agents for operation in uncertain environments, designers need
tools to automatically reason about what agents ought to do, how that conflicts
with what is actually happening, and how a policy might be modified to remove
the conflict. These obligations include ethical and social obligations,
permissions and prohibitions, which constrain how the agent achieves its
mission and executes its policy. We propose a new deontic logic, Expected Act
Utilitarian deontic logic, for enabling this reasoning at design time: for
specifying and verifying the agent's strategic obligations, then modifying its
policy from a reference policy to meet those obligations. Unlike approaches
that work at the reward level, working at the logical level increases the
transparency of the trade-offs. We introduce two algorithms: one for
model-checking whether an RL agent has the right strategic obligations, and one
for modifying a reference decision policy to make it meet obligations expressed
in our logic. We illustrate our algorithms on DAC-MDPs which accurately
abstract neural decision policies, and on toy gridworld environments.",cs.AI cs.LO,2024-07-31
"StyleRF-VolVis: Style Transfer of Neural Radiance Fields for Expressive
  Volume Visualization",,"In volume visualization, visualization synthesis has attracted much attention
due to its ability to generate novel visualizations without following the
conventional rendering pipeline. However, existing solutions based on
generative adversarial networks often require many training images and take
significant training time. Still, issues such as low quality, consistency, and
flexibility persist. This paper introduces StyleRF-VolVis, an innovative style
transfer framework for expressive volume visualization (VolVis) via neural
radiance field (NeRF). The expressiveness of StyleRF-VolVis is upheld by its
ability to accurately separate the underlying scene geometry (i.e., content)
and color appearance (i.e., style), conveniently modify color, opacity, and
lighting of the original rendering while maintaining visual content consistency
across the views, and effectively transfer arbitrary styles from reference
images to the reconstructed 3D scene. To achieve these, we design a base NeRF
model for scene geometry extraction, a palette color network to classify
regions of the radiance field for photorealistic editing, and an unrestricted
color network to lift the color palette constraint via knowledge distillation
for non-photorealistic editing. We demonstrate the superior quality,
consistency, and flexibility of StyleRF-VolVis by experimenting with various
volume rendering scenes and reference images and comparing StyleRF-VolVis
against other image-based (AdaIN), video-based (ReReVST), and NeRF-based (ARF
and SNeRF) style rendering solutions.",cs.GR cs.AI cs.CV,2024-07-31
Moderating Group Conversation Dynamics with Social Robots,,"This research investigates the impact of social robot participation in group
conversations and assesses the effectiveness of various addressing policies.
The study involved 300 participants, divided into groups of four, interacting
with a humanoid robot serving as the moderator. The robot utilized conversation
data to determine the most appropriate speaker to address. The findings
indicate that the robot's addressing policy significantly influenced
conversation dynamics, resulting in more balanced attention to each participant
and a reduction in subgroup formation.",cs.RO cs.AI,2024-07-31
Understanding Feedback Mechanisms in Machine Learning Jupyter Notebooks,,"The machine learning development lifecycle is characterized by iterative and
exploratory processes that rely on feedback mechanisms to ensure data and model
integrity. Despite the critical role of feedback in machine learning
engineering, no prior research has been conducted to identify and understand
these mechanisms. To address this knowledge gap, we mine 297.8 thousand Jupyter
notebooks and analyse 2.3 million code cells. We identify three key feedback
mechanisms -- assertions, print statements and last cell statements -- and
further categorize them into implicit and explicit forms of feedback. Our
findings reveal extensive use of implicit feedback for critical design
decisions and the relatively limited adoption of explicit feedback mechanisms.
By conducting detailed case studies with selected feedback instances, we
uncover the potential for automated validation of critical assumptions in ML
workflows using assertions. Finally, this study underscores the need for
improved documentation, and provides practical recommendations on how existing
feedback mechanisms in the ML development workflow can be effectively used to
mitigate technical debt and enhance reproducibility.",cs.SE,2024-07-31
"Measuring Falseness in News Articles based on Concealment and
  Overstatement",,"This research investigates the extent of misinformation in certain
journalistic articles by introducing a novel measurement tool to assess the
degrees of falsity. It aims to measure misinformation using two metrics
(concealment and overstatement) to explore how information is interpreted as
false. This should help examine how articles containing partly true and partly
false information can potentially harm readers, as they are more challenging to
identify than completely fabricated information. In this study, the full story
provided by the fact-checking website serves as a standardized source of
information for comparing differences between fake and real news. The result
suggests that false news has greater concealment and overstatement, due to
longer and more complex new stories being shortened and ambiguously phrased.
While there are no major distinctions among categories of politics science and
civics, it demonstrates that misinformation lacks crucial details while
simultaneously containing more redundant words. Hence, news articles containing
partial falsity, categorized as misinformation, can deceive inattentive readers
who lack background knowledge. Hopefully, this approach instigates future
fact-checkers, journalists, and the readers to secure high quality articles for
a resilient information environment.",cs.CY,2024-07-31
"Generative Learning of the Solution of Parametric Partial Differential
  Equations Using Guided Diffusion Models and Virtual Observations",,"We introduce a generative learning framework to model high-dimensional
parametric systems using gradient guidance and virtual observations. We
consider systems described by Partial Differential Equations (PDEs) discretized
with structured or unstructured grids. The framework integrates multi-level
information to generate high fidelity time sequences of the system dynamics. We
demonstrate the effectiveness and versatility of our framework with two case
studies in incompressible, two dimensional, low Reynolds cylinder flow on an
unstructured mesh and incompressible turbulent channel flow on a structured
mesh, both parameterized by the Reynolds number. Our results illustrate the
framework's robustness and ability to generate accurate flow sequences across
various parameter settings, significantly reducing computational costs allowing
for efficient forecasting and reconstruction of flow dynamics.",cs.LG physics.comp-ph physics.flu-dyn,2024-07-31
"Hierarchical Conditioning of Diffusion Models Using Tree-of-Life for
  Studying Species Evolution",,"A central problem in biology is to understand how organisms evolve and adapt
to their environment by acquiring variations in the observable characteristics
or traits of species across the tree of life. With the growing availability of
large-scale image repositories in biology and recent advances in generative
modeling, there is an opportunity to accelerate the discovery of evolutionary
traits automatically from images. Toward this goal, we introduce
Phylo-Diffusion, a novel framework for conditioning diffusion models with
phylogenetic knowledge represented in the form of HIERarchical Embeddings
(HIER-Embeds). We also propose two new experiments for perturbing the embedding
space of Phylo-Diffusion: trait masking and trait swapping, inspired by
counterpart experiments of gene knockout and gene editing/swapping. Our work
represents a novel methodological advance in generative modeling to structure
the embedding space of diffusion models using tree-based knowledge. Our work
also opens a new chapter of research in evolutionary biology by using
generative models to visualize evolutionary changes directly from images. We
empirically demonstrate the usefulness of Phylo-Diffusion in capturing
meaningful trait variations for fishes and birds, revealing novel insights
about the biological mechanisms of their evolution.",q-bio.PE cs.CV cs.LG,2024-07-31
A Taxonomy of Stereotype Content in Large Language Models,,"This study introduces a taxonomy of stereotype content in contemporary large
language models (LLMs). We prompt ChatGPT 3.5, Llama 3, and Mixtral 8x7B, three
powerful and widely used LLMs, for the characteristics associated with 87
social categories (e.g., gender, race, occupations). We identify 14 stereotype
dimensions (e.g., Morality, Ability, Health, Beliefs, Emotions), accounting for
~90% of LLM stereotype associations. Warmth and Competence facets were the most
frequent content, but all other dimensions were significantly prevalent.
Stereotypes were more positive in LLMs (vs. humans), but there was significant
variability across categories and dimensions. Finally, the taxonomy predicted
the LLMs' internal evaluations of social categories (e.g., how
positively/negatively the categories were represented), supporting the
relevance of a multidimensional taxonomy for characterizing LLM stereotypes.
Our findings suggest that high-dimensional human stereotypes are reflected in
LLMs and must be considered in AI auditing and debiasing to minimize
unidentified harms from reliance in low-dimensional views of bias in LLMs.",cs.CY cs.AI cs.CL cs.LG,2024-07-31
Review of Explainable Graph-Based Recommender Systems,,"Explainability of recommender systems has become essential to ensure users'
trust and satisfaction. Various types of explainable recommender systems have
been proposed including explainable graph-based recommender systems. This
review paper discusses state-of-the-art approaches of these systems and
categorizes them based on three aspects: learning methods, explaining methods,
and explanation types. It also explores the commonly used datasets,
explainability evaluation methods, and future directions of this research area.
Compared with the existing review papers, this paper focuses on explainability
based on graphs and covers the topics required for developing novel explainable
graph-based recommender systems.",cs.IR cs.AI cs.LG,2024-07-31
Finch: Prompt-guided Key-Value Cache Compression,,"Recent large language model applications, such as Retrieval-Augmented
Generation and chatbots, have led to an increased need to process longer input
contexts. However, this requirement is hampered by inherent limitations.
Architecturally, models are constrained by a context window defined during
training. Additionally, processing extensive texts requires substantial GPU
memory. We propose a novel approach, Finch, to compress the input context by
leveraging the pre-trained model weights of the self-attention. Given a prompt
and a long text, Finch iteratively identifies the most relevant Key (K) and
Value (V) pairs over chunks of the text conditioned on the prompt. Only such
pairs are stored in the KV cache, which, within the space constrained by the
context window, ultimately contains a compressed version of the long text. Our
proposal enables models to consume large inputs even with high compression (up
to 93x) while preserving semantic integrity without the need for fine-tuning.",cs.AI,2024-07-31
"Strike the Balance: On-the-Fly Uncertainty based User Interactions for
  Long-Term Video Object Segmentation",,"In this paper, we introduce a variant of video object segmentation (VOS) that
bridges interactive and semi-automatic approaches, termed Lazy Video Object
Segmentation (ziVOS). In contrast, to both tasks, which handle video object
segmentation in an off-line manner (i.e., pre-recorded sequences), we propose
through ziVOS to target online recorded sequences. Here, we strive to strike a
balance between performance and robustness for long-term scenarios by
soliciting user feedback's on-the-fly during the segmentation process. Hence,
we aim to maximize the tracking duration of an object of interest, while
requiring minimal user corrections to maintain tracking over an extended
period. We propose a competitive baseline, i.e., Lazy-XMem, as a reference for
future works in ziVOS. Our proposed approach uses an uncertainty estimation of
the tracking state to determine whether a user interaction is necessary to
refine the model's prediction. To quantitatively assess the performance of our
method and the user's workload, we introduce complementary metrics alongside
those already established in the field. We evaluate our approach using the
recently introduced LVOS dataset, which offers numerous long-term videos. Our
code is publicly available at https://github.com/Vujas-Eteph/LazyXMem.",cs.CV cs.HC cs.LG,2024-07-31
CREW: Facilitating Human-AI Teaming Research,,"With the increasing deployment of artificial intelligence (AI) technologies,
the potential of humans working with AI agents has been growing at a great
speed. Human-AI teaming is an important paradigm for studying various aspects
when humans and AI agents work together. The unique aspect of Human-AI teaming
research is the need to jointly study humans and AI agents, demanding
multidisciplinary research efforts from machine learning to human-computer
interaction, robotics, cognitive science, neuroscience, psychology, social
science, and complex systems. However, existing platforms for Human-AI teaming
research are limited, often supporting oversimplified scenarios and a single
task, or specifically focusing on either human-teaming research or multi-agent
AI algorithms. We introduce CREW, a platform to facilitate Human-AI teaming
research and engage collaborations from multiple scientific disciplines, with a
strong emphasis on human involvement. It includes pre-built tasks for cognitive
studies and Human-AI teaming with expandable potentials from our modular
design. Following conventional cognitive neuroscience research, CREW also
supports multimodal human physiological signal recording for behavior analysis.
Moreover, CREW benchmarks real-time human-guided reinforcement learning agents
using state-of-the-art algorithms and well-tuned baselines. With CREW, we were
able to conduct 50 human subject studies within a week to verify the
effectiveness of our benchmark.",cs.HC cs.AI cs.LG,2024-07-31
Adapting Skills to Novel Grasps: A Self-Supervised Approach,,"In this paper, we study the problem of adapting manipulation trajectories
involving grasped objects (e.g. tools) defined for a single grasp pose to novel
grasp poses. A common approach to address this is to define a new trajectory
for each possible grasp explicitly, but this is highly inefficient. Instead, we
propose a method to adapt such trajectories directly while only requiring a
period of self-supervised data collection, during which a camera observes the
robot's end-effector moving with the object rigidly grasped. Importantly, our
method requires no prior knowledge of the grasped object (such as a 3D CAD
model), it can work with RGB images, depth images, or both, and it requires no
camera calibration. Through a series of real-world experiments involving 1360
evaluations, we find that self-supervised RGB data consistently outperforms
alternatives that rely on depth images including several state-of-the-art pose
estimation methods. Compared to the best-performing baseline, our method
results in an average of 28.5% higher success rate when adapting manipulation
trajectories to novel grasps on several everyday tasks. Videos of the
experiments are available on our webpage at
https://www.robot-learning.uk/adapting-skills",cs.RO cs.LG,2024-07-31
"CC-SAM: SAM with Cross-feature Attention and Context for Ultrasound
  Image Segmentation",,"The Segment Anything Model (SAM) has achieved remarkable successes in the
realm of natural image segmentation, but its deployment in the medical imaging
sphere has encountered challenges. Specifically, the model struggles with
medical images that feature low contrast, faint boundaries, intricate
morphologies, and small-sized objects. To address these challenges and enhance
SAM's performance in the medical domain, we introduce a comprehensive
modification. Firstly, we incorporate a frozen Convolutional Neural Network
(CNN) branch as an image encoder, which synergizes with SAM's original Vision
Transformer (ViT) encoder through a novel variational attention fusion module.
This integration bolsters the model's capability to capture local spatial
information, which is often paramount in medical imagery. Moreover, to further
optimize SAM for medical imaging, we introduce feature and position adapters
within the ViT branch, refining the encoder's representations. We see that
compared to current prompting strategies to fine-tune SAM for ultrasound
medical segmentation, the use of text descriptions that serve as text prompts
for SAM helps significantly improve the performance. Leveraging ChatGPT's
natural language understanding capabilities, we generate prompts that offer
contextual information and guidance to SAM, enabling it to better understand
the nuances of ultrasound medical images and improve its segmentation accuracy.
Our method, in its entirety, represents a significant stride towards making
universal image segmentation models more adaptable and efficient in the medical
domain.",cs.CV,2024-07-31
"S-SYNTH: Knowledge-Based, Synthetic Generation of Skin Images",,"Development of artificial intelligence (AI) techniques in medical imaging
requires access to large-scale and diverse datasets for training and
evaluation. In dermatology, obtaining such datasets remains challenging due to
significant variations in patient populations, illumination conditions, and
acquisition system characteristics. In this work, we propose S-SYNTH, the first
knowledge-based, adaptable open-source skin simulation framework to rapidly
generate synthetic skin, 3D models and digitally rendered images, using an
anatomically inspired multi-layer, multi-component skin and growing lesion
model. The skin model allows for controlled variation in skin appearance, such
as skin color, presence of hair, lesion shape, and blood fraction among other
parameters. We use this framework to study the effect of possible variations on
the development and evaluation of AI models for skin lesion segmentation, and
show that results obtained using synthetic data follow similar comparative
trends as real dermatologic images, while mitigating biases and limitations
from existing datasets including small dataset size, lack of diversity, and
underrepresentation.",cs.CV cs.AI,2024-07-31
Combining audio control and style transfer using latent diffusion,,"Deep generative models are now able to synthesize high-quality audio signals,
shifting the critical aspect in their development from audio quality to control
capabilities. Although text-to-music generation is getting largely adopted by
the general public, explicit control and example-based style transfer are more
adequate modalities to capture the intents of artists and musicians.
  In this paper, we aim to unify explicit control and style transfer within a
single model by separating local and global information to capture musical
structure and timbre respectively. To do so, we leverage the capabilities of
diffusion autoencoders to extract semantic features, in order to build two
representation spaces. We enforce disentanglement between those spaces using an
adversarial criterion and a two-stage training strategy. Our resulting model
can generate audio matching a timbre target, while specifying structure either
with explicit controls or through another audio example. We evaluate our model
on one-shot timbre transfer and MIDI-to-audio tasks on instrumental recordings
and show that we outperform existing baselines in terms of audio quality and
target fidelity. Furthermore, we show that our method can generate cover
versions of complete musical pieces by transferring rhythmic and melodic
content to the style of a target audio in a different genre.",cs.SD cs.LG eess.AS stat.ML,2024-07-31
"Automated Software Vulnerability Static Code Analysis Using Generative
  Pre-Trained Transformer Models",,"Generative Pre-Trained Transformer models have been shown to be surprisingly
effective at a variety of natural language processing tasks -- including
generating computer code. We evaluate the effectiveness of open source GPT
models for the task of automatic identification of the presence of vulnerable
code syntax (specifically targeting C and C++ source code). This task is
evaluated on a selection of 36 source code examples from the NIST SARD dataset,
which are specifically curated to not contain natural English that indicates
the presence, or lack thereof, of a particular vulnerability. The NIST SARD
source code dataset contains identified vulnerable lines of source code that
are examples of one out of the 839 distinct Common Weakness Enumerations (CWE),
allowing for exact quantification of the GPT output classification error rate.
A total of 5 GPT models are evaluated, using 10 different inference
temperatures and 100 repetitions at each setting, resulting in 5,000 GPT
queries per vulnerable source code analyzed. Ultimately, we find that the GPT
models that we evaluated are not suitable for fully automated vulnerability
scanning because the false positive and false negative rates are too high to
likely be useful in practice. However, we do find that the GPT models perform
surprisingly well at automated vulnerability detection for some of the test
cases, in particular surpassing random sampling, and being able to identify the
exact lines of code that are vulnerable albeit at a low success rate. The best
performing GPT model result found was Llama-2-70b-chat-hf with inference
temperature of 0.1 applied to NIST SARD test case 149165 (which is an example
of a buffer overflow vulnerability), which had a binary classification recall
score of 1.0 and a precision of 1.0 for correctly and uniquely identifying the
vulnerable line of code and the correct CWE number.",cs.CR cs.AI cs.CL cs.LG,2024-07-31
"UnPaSt: unsupervised patient stratification by differentially expressed
  biclusters in omics data",,"Most complex diseases, including cancer and non-malignant diseases like
asthma, have distinct molecular subtypes that require distinct clinical
approaches. However, existing computational patient stratification methods have
been benchmarked almost exclusively on cancer omics data and only perform well
when mutually exclusive subtypes can be characterized by many biomarkers. Here,
we contribute with a massive evaluation attempt, quantitatively exploring the
power of 22 unsupervised patient stratification methods using both, simulated
and real transcriptome data. From this experience, we developed UnPaSt
(https://apps.cosy.bio/unpast/) optimizing unsupervised patient stratification,
working even with only a limited number of subtype-predictive biomarkers. We
evaluated all 23 methods on real-world breast cancer and asthma transcriptomics
data. Although many methods reliably detected major breast cancer subtypes,
only few identified Th2-high asthma, and UnPaSt significantly outperformed its
closest competitors in both test datasets. Essentially, we showed that UnPaSt
can detect many biologically insightful and reproducible patterns in omic
datasets.",cs.LG q-bio.GN,2024-07-31
OmniParser for Pure Vision Based GUI Agent,,"The recent success of large vision language models shows great potential in
driving the agent system operating on user interfaces. However, we argue that
the power multimodal models like GPT-4V as a general agent on multiple
operating systems across different applications is largely underestimated due
to the lack of a robust screen parsing technique capable of: 1) reliably
identifying interactable icons within the user interface, and 2) understanding
the semantics of various elements in a screenshot and accurately associate the
intended action with the corresponding region on the screen. To fill these
gaps, we introduce \textsc{OmniParser}, a comprehensive method for parsing user
interface screenshots into structured elements, which significantly enhances
the ability of GPT-4V to generate actions that can be accurately grounded in
the corresponding regions of the interface. We first curated an interactable
icon detection dataset using popular webpages and an icon description dataset.
These datasets were utilized to fine-tune specialized models: a detection model
to parse interactable regions on the screen and a caption model to extract the
functional semantics of the detected elements. \textsc{OmniParser}
significantly improves GPT-4V's performance on ScreenSpot benchmark. And on
Mind2Web and AITW benchmark, \textsc{OmniParser} with screenshot only input
outperforms the GPT-4V baselines requiring additional information outside of
screenshot.",cs.CV cs.AI cs.CL cs.LG,2024-07-31
"Sentence-wise Speech Summarization: Task, Datasets, and End-to-End
  Modeling with LM Knowledge Distillation",,"This paper introduces a novel approach called sentence-wise speech
summarization (Sen-SSum), which generates text summaries from a spoken document
in a sentence-by-sentence manner. Sen-SSum combines the real-time processing of
automatic speech recognition (ASR) with the conciseness of speech
summarization. To explore this approach, we present two datasets for Sen-SSum:
Mega-SSum and CSJ-SSum. Using these datasets, our study evaluates two types of
Transformer-based models: 1) cascade models that combine ASR and strong text
summarization models, and 2) end-to-end (E2E) models that directly convert
speech into a text summary. While E2E models are appealing to develop
compute-efficient models, they perform worse than cascade models. Therefore, we
propose knowledge distillation for E2E models using pseudo-summaries generated
by the cascade models. Our experiments show that this proposed knowledge
distillation effectively improves the performance of the E2E model on both
datasets.",cs.CL eess.AS,2024-07-31
"Prognosis of COVID-19 using Artificial Intelligence: A Systematic Review
  and Meta-analysis",,"Purpose: Artificial intelligence (AI) techniques have been extensively
utilized for diagnosing and prognosis of several diseases in recent years. This
study identifies, appraises and synthesizes published studies on the use of AI
for the prognosis of COVID-19. Method: Electronic search was performed using
Medline, Google Scholar, Scopus, Embase, Cochrane and ProQuest. Studies that
examined machine learning or deep learning methods to determine the prognosis
of COVID-19 using CT or chest X-ray images were included. Polled sensitivity,
specificity area under the curve and diagnostic odds ratio were calculated.
Result: A total of 36 articles were included; various prognosis-related issues,
including disease severity, mechanical ventilation or admission to the
intensive care unit and mortality, were investigated. Several AI models and
architectures were employed, such as the Siamense model, support vector
machine, Random Forest , eXtreme Gradient Boosting, and convolutional neural
networks. The models achieved 71%, 88% and 67% sensitivity for mortality,
severity assessment and need for ventilation, respectively. The specificity of
69%, 89% and 89% were reported for the aforementioned variables. Conclusion:
Based on the included articles, machine learning and deep learning methods used
for the prognosis of COVID-19 patients using radiomic features from CT or CXR
images can help clinicians manage patients and allocate resources more
effectively. These studies also demonstrate that combining patient demographic,
clinical data, laboratory tests and radiomic features improves model
performances.",physics.med-ph cs.LG,2024-07-31
"A Prior Embedding-Driven Architecture for Long Distance Blind Iris
  Recognition",,"Blind iris images, which result from unknown degradation during the process
of iris recognition at long distances, often lead to decreased iris recognition
rates. Currently, little existing literature offers a solution to this problem.
In response, we propose a prior embedding-driven architecture for long distance
blind iris recognition. We first proposed a blind iris image restoration
network called Iris-PPRGAN. To effectively restore the texture of the blind
iris, Iris-PPRGAN includes a Generative Adversarial Network (GAN) used as a
Prior Decoder, and a DNN used as the encoder. To extract iris features more
efficiently, we then proposed a robust iris classifier by modifying the
bottleneck module of InsightFace, which called Insight-Iris. A low-quality
blind iris image is first restored by Iris-PPRGAN, then the restored iris image
undergoes recognition via Insight-Iris. Experimental results on the public
CASIA-Iris-distance dataset demonstrate that our proposed method significantly
superior results to state-of-the-art blind iris restoration methods both
quantitatively and qualitatively, Specifically, the recognition rate for
long-distance blind iris images reaches 90% after processing with our methods,
representing an improvement of approximately ten percentage points compared to
images without restoration.",cs.CV cs.AI,2024-07-31
"Penzai + Treescope: A Toolkit for Interpreting, Visualizing, and Editing
  Models As Data",,"Much of today's machine learning research involves interpreting, modifying or
visualizing models after they are trained. I present Penzai, a neural network
library designed to simplify model manipulation by representing models as
simple data structures, and Treescope, an interactive pretty-printer and array
visualizer that can visualize both model inputs/outputs and the models
themselves. Penzai models are built using declarative combinators that expose
the model forward pass in the structure of the model object itself, and use
named axes to ensure each operation is semantically meaningful. With Penzai's
tree-editing selector system, users can both insert and replace model
components, allowing them to intervene on intermediate values or make other
edits to the model structure. Users can then get immediate feedback by
visualizing the modified model with Treescope. I describe the motivation and
main features of Penzai and Treescope, and discuss how treating the model as
data enables a variety of analyses and interventions to be implemented as
data-structure transformations, without requiring model designers to add
explicit hooks.",cs.LG,2024-07-31
"Well-conditioned dipole-type method of fundamental solutions: derivation
  and its mathematical analysis",,"In this paper, we examine the dipole-type method of fundamental solutions,
which can be conceptualized as a discretization of the ""singularity-removed""
double-layer potential. We present a method for removing the
ill-conditionality, which was previously considered a significant challenge,
and provide a mathematical analysis in the context of disk regions. Moreover,
we extend the proposed method to the general Jordan region using conformal
mapping, demonstrating the efficacy of the proposed method through numerical
experiments.",math.NA cs.NA,2024-07-31
"Large Language Model (LLM)-enabled In-context Learning for Wireless
  Network Optimization: A Case Study of Power Control",,"Large language model (LLM) has recently been considered a promising technique
for many fields. This work explores LLM-based wireless network optimization via
in-context learning. To showcase the potential of LLM technologies, we consider
the base station (BS) power control as a case study, a fundamental but crucial
technique that is widely investigated in wireless networks. Different from
existing machine learning (ML) methods, our proposed in-context learning
algorithm relies on LLM's inference capabilities. It avoids the complexity of
tedious model training and hyper-parameter fine-tuning, which is a well-known
bottleneck of many ML algorithms. Specifically, the proposed algorithm first
describes the target task via formatted natural language, and then designs the
in-context learning framework and demonstration examples. After that, it
considers two cases, namely discrete-state and continuous-state problems, and
proposes state-based and ranking-based methods to select appropriate examples
for these two cases, respectively. Finally, the simulations demonstrate that
the proposed algorithm can achieve comparable performance as conventional deep
reinforcement learning (DRL) techniques without dedicated model training or
fine-tuning. Such an efficient and low-complexity approach has great potential
for future wireless network optimization.",eess.SY cs.SY,2024-07-31
Clutter-Aware Spill-Free Liquid Transport via Learned Dynamics,,"In this work, we present a novel algorithm to perform spill-free handling of
open-top liquid-filled containers that operates in cluttered environments. By
allowing liquid-filled containers to be tilted at higher angles and enabling
motion along all axes of end-effector orientation, our work extends the
reachable space and enhances maneuverability around obstacles, broadening the
range of feasible scenarios. Our key contributions include: i) generating
spill-free paths through the use of RRT* with an informed sampler that
leverages container properties to avoid spill-inducing states (such as an
upside-down container), ii) parameterizing the resulting path to generate
spill-free trajectories through the implementation of a time parameterization
algorithm, coupled with a transformer-based machine-learning model capable of
classifying trajectories as spill-free or not. We validate our approach in
real-world, obstacle-rich task settings using containers of various shapes and
fill levels and demonstrate an extended solution space that is at least 3x
larger than an existing approach.",cs.RO,2024-07-31
Load Balancing in Federated Learning,,"Federated Learning (FL) is a decentralized machine learning framework that
enables learning from data distributed across multiple remote devices,
enhancing communication efficiency and data privacy. Due to limited
communication resources, a scheduling policy is often applied to select a
subset of devices for participation in each FL round. The scheduling process
confronts significant challenges due to the need for fair workload
distribution, efficient resource utilization, scalability in environments with
numerous edge devices, and statistically heterogeneous data across devices.
This paper proposes a load metric for scheduling policies based on the Age of
Information and addresses the above challenges by minimizing the load metric
variance across the clients. Furthermore, a decentralized Markov scheduling
policy is presented, that ensures a balanced workload distribution while
eliminating the management overhead irrespective of the network size due to
independent client decision-making. We establish the optimal parameters of the
Markov chain model and validate our approach through simulations. The results
demonstrate that reducing the load metric variance not only promotes fairness
and improves operational efficiency, but also enhances the convergence rate of
the learning models.",cs.LG cs.IT math.IT,2024-07-31
Persistent de Rham-Hodge Laplacians in the Eulerian representation,,"Recently, topological data analysis (TDA) has become a trending topic in data
science and engineering. However, the key technique of TDA, i.e., persistent
homology, is defined on point cloud data, which restricts its scope. In this
work, we propose persistent de Rham-Hodge Laplacian, or persistent Hodge
Laplacian (PHL) for abbreviation, for the TDA on manifolds with boundaries, or
volumetric data. Specifically, we extended the evolutionary de Rham-Hodge
theory from the Lagrangian formulation to the Eulerian formulation via
structure-persevering Cartesian grids, and extended the persistent Laplacian on
point clouds to persistent (de Rham-)Hodge Laplacian on nested families of
manifolds with appropriate boundary conditions. The proposed PHL facilitates
the machine learning and deep learning prediction of volumetric data. For a
proof-of-principle application of the proposed PHL, we propose a persistent
Hodge Laplacian learning (PHLL) algorithm for data on manifolds or volumetric
data. To this end, we showcase the PHLL prediction of protein-ligand binding
affinities in two benchmark datasets. Our numerical experiments highlight the
power and promise of PHLL.",math.DG cs.LG,2024-07-31
"multiGradICON: A Foundation Model for Multimodal Medical Image
  Registration",,"Modern medical image registration approaches predict deformations using deep
networks. These approaches achieve state-of-the-art (SOTA) registration
accuracy and are generally fast. However, deep learning (DL) approaches are, in
contrast to conventional non-deep-learning-based approaches, anatomy-specific.
Recently, a universal deep registration approach, uniGradICON, has been
proposed. However, uniGradICON focuses on monomodal image registration. In this
work, we therefore develop multiGradICON as a first step towards universal
*multimodal* medical image registration. Specifically, we show that 1) we can
train a DL registration model that is suitable for monomodal *and* multimodal
registration; 2) loss function randomization can increase multimodal
registration accuracy; and 3) training a model with multimodal data helps
multimodal generalization. Our code and the multiGradICON model are available
at https://github.com/uncbiag/uniGradICON.",eess.IV cs.CV,2024-07-31
"Age of Information Analysis for Multi-Priority Queue and NOMA Enabled
  C-V2X in IoV",,"As development Internet-of-Vehicles (IoV) technology and demand for
Intelligent Transportation Systems (ITS) increase, there is a growing need for
real-time data and communication by vehicle users. Traditional request-based
methods face challenges such as latency and bandwidth limitations. Mode 4 in
Connected Vehicle-to-Everything (C-V2X) addresses latency and overhead issues
through autonomous resource selection. However, Semi-Persistent Scheduling
(SPS) based on distributed sensing may lead to increased collision.
Non-Orthogonal Multiple Access (NOMA) can alleviate the problem of reduced
packet reception probability due to collisions. Moreover, the concept of Age of
Information (AoI) is introduced as a comprehensive metric reflecting
reliability and latency performance, analyzing the impact of NOMA on C-V2X
communication system. AoI indicates the time a message spends in both local
waiting and transmission processes. In C-V2X, waiting process can be extended
to queuing process, influenced by packet generation rate and Resource
Reservation Interval (RRI). The transmission process is mainly affected by
transmission delay and success rate. In C-V2X, a smaller selection window (SW)
limits the number of available resources for vehicles, resulting in higher
collision rates with increased number of vehicles. SW is generally equal to
RRI, which not only affects AoI in queuing process but also AoI in the
transmission process. Therefore, this paper proposes an AoI estimation method
based on multi-priority data type queues and considers the influence of NOMA on
the AoI generated in both processes in C-V2X system under different RRI
conditions. This work aims to gain a better performance of C-V2X system
comparing with some known algorithms.",cs.NI cs.PF,2024-07-31
Scaling and assigning resources on ion trap QCCD architectures,,"Ion trap technologies have earned significant attention as potential
candidates for quantum information processing due to their long decoherence
times and precise manipulation of individual qubits, distinguishing them from
other candidates in the field of quantum technologies. However, scalability
remains a challenge, as introducing additional qubits into a trap increases
noise and heating effects, consequently decreasing operational fidelity.
Trapped-ion Quantum Charge-Coupled Device (QCCD) architectures have addressed
this limitation by interconnecting multiple traps and employing ion shuttling
mechanisms to transfer ions among traps. This new architectural design requires
the development of novel compilation techniques for quantum algorithms, which
efficiently allocate and route qubits, and schedule operations. The aim of a
compiler is to minimize ion movements and, therefore, reduce the execution time
of the circuit to achieve a higher fidelity.
  In this paper, we propose a novel approach for initial qubit placement,
demonstrating enhancements of up to 50\% compared to prior methods.
Furthermore, we conduct a scalability analysis on two distinct QCCD topologies:
a 1D-linear array and a ring structure. Additionally, we evaluate the impact of
the excess capacity -- i.e. the number of free spaces within a trap -- on the
algorithm performance.",quant-ph cs.ET,2024-07-31
Finding a Shortest $M$-link Path in a Monge Directed Acyclic Graph,,"A Monge directed acyclic graph (DAG) $G$ on the nodes $1,2,\cdots,N$ has
edges $\left( i,j\right) $ for $1\leq i<j\leq N$ carrying submodular
edge-lengths. Finding a shortest $M$-link path from $1$ to $N$ in $G$ for any
given $1<M<N-1$ has many applications. In this paper, we give a
contract-and-conquer algorithm for this problem which runs in $O\left(
\sqrt{NM\left( N-M\right) \log\left( N-M\right) }\right) $ time and $O\left(
N\right) $ space. It is the first $o\left( NM\right) $-time algorithm with
linear space complexity, and its time complexity decreases with $M$ when $M\geq
N/2$. In contrast, all previous strongly polynomial algorithms have running
time growing with $M$. For both $O\left( poly\left( \log N\right) \right) $ and
$N-O\left( poly\left( \log N\right) \right) $ regimes of $M$, our algorithm has
running time $O\left( N\cdot poly\left( \log N\right) \right) $, which
partially answers an open question rased in \cite{AST94} affirmatively.",cs.DS,2024-07-31
"Invariant Discovery of Features Across Multiple Length Scales:
  Applications in Microscopy and Autonomous Materials Characterization",,"Physical imaging is a foundational characterization method in areas from
condensed matter physics and chemistry to astronomy and spans length scales
from atomic to universe. Images encapsulate crucial data regarding atomic
bonding, materials microstructures, and dynamic phenomena such as
microstructural evolution and turbulence, among other phenomena. The challenge
lies in effectively extracting and interpreting this information. Variational
Autoencoders (VAEs) have emerged as powerful tools for identifying underlying
factors of variation in image data, providing a systematic approach to
distilling meaningful patterns from complex datasets. However, a significant
hurdle in their application is the definition and selection of appropriate
descriptors reflecting local structure. Here we introduce the scale-invariant
VAE approach (SI-VAE) based on the progressive training of the VAE with the
descriptors sampled at different length scales. The SI-VAE allows the discovery
of the length scale dependent factors of variation in the system. Here, we
illustrate this approach using the ferroelectric domain images and generalize
it to the movies of the electron-beam induced phenomena in graphene and
topography evolution across combinatorial libraries. This approach can further
be used to initialize the decision making in automated experiments including
structure-property discovery and can be applied across a broad range of imaging
methods. This approach is universal and can be applied to any spatially
resolved data including both experimental imaging studies and simulations, and
can be particularly useful for exploration of phenomena such as turbulence,
scale-invariant transformation fronts, etc.",physics.comp-ph cond-mat.mtrl-sci cs.LG physics.app-ph,2024-07-31
"CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph
  Neural Network Training with Communication Reduction",,"Graph neural network training is mainly categorized into mini-batch and
full-batch training methods. The mini-batch training method samples subgraphs
from the original graph in each iteration. This sampling operation introduces
extra computation overhead and reduces the training accuracy. Meanwhile, the
full-batch training method calculates the features and corresponding gradients
of all vertices in each iteration, and therefore has higher convergence
accuracy. However, in the distributed cluster, frequent remote accesses of
vertex features and gradients lead to huge communication overhead, thus
restricting the overall training efficiency.
  In this paper, we introduce the cached-based distributed full-batch graph
neural network training framework (CDFGNN). We propose the adaptive cache
mechanism to reduce the remote vertex access by caching the historical features
and gradients of neighbor vertices. Besides, we further optimize the
communication overhead by quantifying the messages and designing the graph
partition algorithm for the hierarchical communication architecture.
Experiments show that the adaptive cache mechanism reduces remote vertex
accesses by 63.14% on average. Combined with communication quantization and
hierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed
full-batch training frameworks by 30.39% in our experiments. Our results
indicate that CDFGNN has great potential in accelerating distributed full-batch
GNN training tasks.",cs.DC cs.LG,2024-07-31
Solving cluster moment relaxation with hierarchical matrix,,"Convex relaxation methods are powerful tools for studying the lowest energy
of many-body problems. By relaxing the representability conditions for
marginals to a set of local constraints, along with a global semidefinite
constraint, a polynomial-time solvable semidefinite program (SDP) that provides
a lower bound for the energy can be derived. In this paper, we propose
accelerating the solution of such an SDP relaxation by imposing a hierarchical
structure on the positive semidefinite (PSD) primal and dual variables.
Furthermore, these matrices can be updated efficiently using the algebra of the
compressed representations within an augmented Lagrangian method. We achieve
quadratic and even near-linear time per-iteration complexity. Through
experimentation on the quantum transverse field Ising model, we showcase the
capability of our approach to provide a sufficiently accurate lower bound for
the exact ground-state energy.",math.OC cs.NA math.NA,2024-07-31
Empirical Bayes Linked Matrix Decomposition,,"Data for several applications in diverse fields can be represented as
multiple matrices that are linked across rows or columns. This is particularly
common in molecular biomedical research, in which multiple molecular ""omics""
technologies may capture different feature sets (e.g., corresponding to rows in
a matrix) and/or different sample populations (corresponding to columns). This
has motivated a large body of work on integrative matrix factorization
approaches that identify and decompose low-dimensional signal that is shared
across multiple matrices or specific to a given matrix. We propose an empirical
variational Bayesian approach to this problem that has several advantages over
existing techniques, including the flexibility to accommodate shared signal
over any number of row or column sets (i.e., bidimensional integration), an
intuitive model-based objective function that yields appropriate shrinkage for
the inferred signals, and a relatively efficient estimation algorithm with no
tuning parameters. A general result establishes conditions for the uniqueness
of the underlying decomposition for a broad family of methods that includes the
proposed approach. For scenarios with missing data, we describe an associated
iterative imputation approach that is novel for the single-matrix context and a
powerful approach for ""blockwise"" imputation (in which an entire row or column
is missing) in various linked matrix contexts. Extensive simulations show that
the method performs very well under different scenarios with respect to
recovering underlying low-rank signal, accurately decomposing shared and
specific signals, and accurately imputing missing data. The approach is applied
to gene expression and miRNA data from breast cancer tissue and normal breast
tissue, for which it gives an informative decomposition of variation and
outperforms alternative strategies for missing data imputation.",stat.ML cs.LG stat.ME,2024-07-31
Anytime Trust Rating Dynamics in a Human-Robot Interaction Task,,"Objective We model factors contributing to rating timing for a
single-dimensional, any-time trust in robotics measure.
  Background Many studies view trust as a slow-changing value after subjects
complete a trial or at regular intervals. Trust is a multifaceted concept that
can be measured simultaneously with a human-robot interaction.
  Method 65 subjects commanded a remote robot arm in a simulated space station.
The robot picked and placed stowage commanded by the subject, but the robot's
performance varied from trial to trial. Subjects rated their trust on a
non-obtrusive trust slider at any time throughout the experiment.
  Results A Cox Proportional Hazards Model described the time it took subjects
to rate their trust in the robot. A retrospective survey indicated that
subjects based their trust on the robot's performance or outcome of the task.
Strong covariates representing the task's state reflected this in the model.
  Conclusion Trust and robot task performance contributed little to the timing
of the trust rating. The subjects' exit survey responses aligned with the
assumption that the robot's task progress was the main reason for the timing of
their trust rating.
  Application Measuring trust in a human-robot interaction task should take as
little attention away from the task as possible. This trust rating technique
lays the groundwork for single-dimensional trust queries that probe estimated
human action.",cs.HC,2024-07-31
Multiple Greedy Quasi-Newton Methods for Saddle Point Problems,,"This paper introduces the Multiple Greedy Quasi-Newton (MGSR1-SP) method, a
novel approach to solving strongly-convex-strongly-concave (SCSC) saddle point
problems. Our method enhances the approximation of the squared indefinite
Hessian matrix inherent in these problems, significantly improving both
stability and efficiency through iterative greedy updates. We provide a
thorough theoretical analysis of MGSR1-SP, demonstrating its linear-quadratic
convergence rate. Numerical experiments conducted on AUC maximization and
adversarial debiasing problems, compared with state-of-the-art algorithms,
underscore our method's enhanced convergence rate. These results affirm the
potential of MGSR1-SP to improve performance across a broad spectrum of machine
learning applications where efficient and accurate Hessian approximations are
crucial.",cs.AI,2024-07-31
A Survey on the Applications of Zero-Knowledge Proofs,,"Zero-knowledge proofs (ZKPs) represent a revolutionary advance in
computational integrity and privacy technology, enabling the secure and private
exchange of information without revealing underlying private data. ZKPs have
unique advantages in terms of universality and minimal security assumptions
when compared to other privacy-sensitive computational methods for distributed
systems, such as homomorphic encryption and secure multiparty computation.
Their application spans multiple domains, from enhancing privacy in blockchain
to facilitating confidential verification of computational tasks. This survey
starts with a high-level overview of the technical workings of ZKPs with a
focus on an increasingly relevant subset of ZKPs called zk-SNARKS. While there
have been prior surveys on the algorithmic and theoretical aspects of ZKPs, our
work is distinguished by providing a broader view of practical aspects and
describing many recently-developed use cases of ZKPs across various domains.
These application domains span blockchain privacy, scaling, storage, and
interoperability, as well as non-blockchain applications like voting,
authentication, timelocks, and machine learning. Aimed at both practitioners
and researchers, the survey also covers foundational components and
infrastructure such as zero-knowledge virtual machines (zkVM), domain-specific
languages (DSLs), supporting libraries, frameworks, and protocols. We conclude
with a discussion on future directions, positioning ZKPs as pivotal in the
advancement of cryptographic practices and digital privacy across many
applications.",cs.CR cs.CC,2024-07-31
"Enhanced Structured State Space Models via Grouped FIR Filtering and
  Attention Sink Mechanisms",,"Structured State Space Models (SSMs) have emerged as compelling alternatives
to Transformer architectures, offering linear-time complexity and superior
performance in various sequence modeling tasks. Despite their advantages, SSMs
like the original Mamba-2 face training difficulties due to the sensitivities
introduced by the extended series of recurrent matrix multiplications. In this
paper, we propose an advanced architecture that mitigates these challenges by
decomposing A-multiplications into multiple groups and optimizing positional
encoding through Grouped Finite Impulse Response (FIR) filtering. This new
structure, denoted as Grouped FIR-enhanced SSM (GFSSM), employs semiseparable
matrices for efficient computation. Furthermore, inspired by the ""attention
sink"" phenomenon identified in streaming language models, we incorporate a
similar mechanism to enhance the stability and performance of our model over
extended sequences. Our approach further bridges the gap between SSMs and
Transformer architectures, offering a viable path forward for scalable and
high-performing sequence modeling.",cs.CL cs.LG,2024-07-31
"Joint Vehicle Connection and Beamforming Optimization in Digital Twin
  Assisted Integrated Sensing and Communication Vehicular Networks",,"This paper introduces an approach to harness digital twin (DT) technology in
the realm of integrated sensing and communications (ISAC) in the
sixth-generation (6G) Internet-of-everything (IoE) applications. We consider
moving targets in a vehicular network and use DT to track and predict the
motion of the vehicles. After predicting the location of the vehicle at the
next time slot, the DT designs the assignment and beamforming for each vehicle.
The real time sensing information is then utilized to update and refine the DT,
enabling further processing and decision-making. This model incorporates a
dynamic Kalman gain, which is updated at each time slot based on the received
echo signals. The state representation encompasses both vehicle motion
information and the error matrix, with the posterior Cram\'er-Rao bound (PCRB)
employed to assess sensing accuracy. We consider a network with two roadside
units (RSUs), and the vehicles need to be allocated to one of them. To optimize
the overall transmission rate while maintaining an acceptable sensing accuracy,
an optimization problem is formulated. Since it is generally hard to solve the
original problem, Lagrange multipliers and fractional programming are employed
to simplify this optimization problem. To solve the simplified problem, this
paper introduces both greedy and heuristic algorithms through optimizing both
vehicle assignments and predictive beamforming. The optimized results are then
transferred back to the real space for ISAC applications. Recognizing the
computational complexity of the greedy and heuristic algorithms, a
bidirectional long short-term memory (LSTM)-based recurrent neural network
(RNN) is proposed for efficient beamforming design within the DT. Simulation
results demonstrate the effectiveness of the DT-based ISAC network.",eess.SY cs.SY,2024-07-31
"Task-Adapter: Task-specific Adaptation of Image Models for Few-shot
  Action Recognition",,"Existing works in few-shot action recognition mostly fine-tune a pre-trained
image model and design sophisticated temporal alignment modules at feature
level. However, simply fully fine-tuning the pre-trained model could cause
overfitting due to the scarcity of video samples. Additionally, we argue that
the exploration of task-specific information is insufficient when relying
solely on well extracted abstract features. In this work, we propose a simple
but effective task-specific adaptation method (Task-Adapter) for few-shot
action recognition. By introducing the proposed Task-Adapter into the last
several layers of the backbone and keeping the parameters of the original
pre-trained model frozen, we mitigate the overfitting problem caused by full
fine-tuning and advance the task-specific mechanism into the process of feature
extraction. In each Task-Adapter, we reuse the frozen self-attention layer to
perform task-specific self-attention across different videos within the given
task to capture both distinctive information among classes and shared
information within classes, which facilitates task-specific adaptation and
enhances subsequent metric measurement between the query feature and support
prototypes. Experimental results consistently demonstrate the effectiveness of
our proposed Task-Adapter on four standard few-shot action recognition
datasets. Especially on temporal challenging SSv2 dataset, our method
outperforms the state-of-the-art methods by a large margin.",cs.CV,2024-07-31
"Discovering Car-following Dynamics from Trajectory Data through Deep
  Learning",,"This study aims to discover the governing mathematical expressions of
car-following dynamics from trajectory data directly using deep learning
techniques. We propose an expression exploration framework based on deep
symbolic regression (DSR) integrated with a variable intersection selection
(VIS) method to find variable combinations that encourage interpretable and
parsimonious mathematical expressions. In the exploration learning process, two
penalty terms are added to improve the reward function: (i) a complexity
penalty to regulate the complexity of the explored expressions to be
parsimonious, and (ii) a variable interaction penalty to encourage the
expression exploration to focus on variable combinations that can best describe
the data. We show the performance of the proposed method to learn several
car-following dynamics models and discuss its limitations and future research
directions.",cs.LG,2024-07-31
Saving Money for Analytical Workloads in the Cloud,,"As users migrate their analytical workloads to cloud databases, it is
becoming just as important to reduce monetary costs as it is to optimize query
runtime. In the cloud, a query is billed based on either its compute time or
the amount of data it processes. We observe that analytical queries are either
compute- or IO-bound and each query type executes cheaper in a different
pricing model. We exploit this opportunity and propose methods to build cheaper
execution plans across pricing models that complete within user-defined runtime
constraints. We implement these methods and produce execution plans spanning
multiple pricing models that reduce the monetary cost for workloads by as much
as 56%. We reduce individual query costs by as much as 90%. The prices chosen
by cloud vendors for cloud services also impact savings opportunities. To study
this effect, we simulate our proposed methods with different cloud prices and
observe that multi-cloud savings are robust to changes in cloud vendor prices.
These results indicate the massive opportunity to save money by executing
workloads across multiple pricing models.",cs.DB,2024-07-31
LoopSparseGS: Loop Based Sparse-View Friendly Gaussian Splatting,,"Despite the photorealistic novel view synthesis (NVS) performance achieved by
the original 3D Gaussian splatting (3DGS), its rendering quality significantly
degrades with sparse input views. This performance drop is mainly caused by the
limited number of initial points generated from the sparse input, insufficient
supervision during the training process, and inadequate regularization of the
oversized Gaussian ellipsoids. To handle these issues, we propose the
LoopSparseGS, a loop-based 3DGS framework for the sparse novel view synthesis
task. In specific, we propose a loop-based Progressive Gaussian Initialization
(PGI) strategy that could iteratively densify the initialized point cloud using
the rendered pseudo images during the training process. Then, the sparse and
reliable depth from the Structure from Motion, and the window-based dense
monocular depth are leveraged to provide precise geometric supervision via the
proposed Depth-alignment Regularization (DAR). Additionally, we introduce a
novel Sparse-friendly Sampling (SFS) strategy to handle oversized Gaussian
ellipsoids leading to large pixel errors. Comprehensive experiments on four
datasets demonstrate that LoopSparseGS outperforms existing state-of-the-art
methods for sparse-input novel view synthesis, across indoor, outdoor, and
object-level scenes with various image resolutions.",cs.CV,2024-07-31
Revocable Backdoor for Deep Model Trading,,"Deep models are being applied in numerous fields and have become a new
important digital product. Meanwhile, previous studies have shown that deep
models are vulnerable to backdoor attacks, in which compromised models return
attacker-desired results when a trigger appears. Backdoor attacks severely
break the trust-worthiness of deep models. In this paper, we turn this weakness
of deep models into a strength, and propose a novel revocable backdoor and deep
model trading scenario. Specifically, we aim to compromise deep models without
degrading their performance, meanwhile, we can easily detoxify poisoned models
without re-training the models. We design specific mask matrices to manage the
internal feature maps of the models. These mask matrices can be used to
deactivate the backdoors. The revocable backdoor can be adopted in the deep
model trading scenario. Sellers train models with revocable backdoors as a
trial version. Buyers pay a deposit to sellers and obtain a trial version of
the deep model. If buyers are satisfied with the trial version, they pay a
final payment to sellers and sellers send mask matrices to buyers to withdraw
revocable backdoors. We demonstrate the feasibility and robustness of our
revocable backdoor by various datasets and network architectures.",cs.CR cs.CV,2024-07-31
Mobility-Aware Federated Self-supervised Learning in Vehicular Network,,"Federated Learning (FL) is an advanced distributed machine learning approach,
that protects the privacy of each vehicle by allowing the model to be trained
on multiple devices simultaneously without the need to upload all data to a
road side unit (RSU). This enables FL to handle scenarios with sensitive or
widely distributed data. However, in these fields, it is well known that the
labeling costs can be a significant expense, and models relying on labels are
not suitable for these rapidly evolving fields especially in vehicular
networks, or mobile internet of things (MIoT), where new data emerges
constantly. To handle this issue, the self-supervised learning paves the way
for training without labels. Additionally, for vehicles with high velocity,
owing to blurred images, simple aggregation not only impacts the accuracy of
the aggregated model but also reduces the convergence speed of FL. This paper
proposes a FL algorithm based on image blur level to aggregation, called
FLSimCo, which does not require labels and serves as a pre-training stage for
self-supervised learning in the vehicular environment. Simulation results
demonstrate that the proposed algorithm exhibits fast and stable convergence.",cs.LG cs.NI,2024-07-31
"RoCo:Robust Collaborative Perception By Iterative Object Matching and
  Pose Adjustment",,"Collaborative autonomous driving with multiple vehicles usually requires the
data fusion from multiple modalities. To ensure effective fusion, the data from
each individual modality shall maintain a reasonably high quality. However, in
collaborative perception, the quality of object detection based on a modality
is highly sensitive to the relative pose errors among the agents. It leads to
feature misalignment and significantly reduces collaborative performance. To
address this issue, we propose RoCo, a novel unsupervised framework to conduct
iterative object matching and agent pose adjustment. To the best of our
knowledge, our work is the first to model the pose correction problem in
collaborative perception as an object matching task, which reliably associates
common objects detected by different agents. On top of this, we propose a graph
optimization process to adjust the agent poses by minimizing the alignment
errors of the associated objects, and the object matching is re-done based on
the adjusted agent poses. This process is carried out iteratively until
convergence. Experimental study on both simulated and real-world datasets
demonstrates that the proposed framework RoCo consistently outperforms existing
relevant methods in terms of the collaborative object detection performance,
and exhibits highly desired robustness when the pose information of agents is
with high-level noise. Ablation studies are also provided to show the impact of
its key parameters and components. The code is released at
https://github.com/HuangZhe885/RoCo.",cs.AI,2024-07-31
Improving Image De-raining Using Reference-Guided Transformers,,"Image de-raining is a critical task in computer vision to improve visibility
and enhance the robustness of outdoor vision systems. While recent advances in
de-raining methods have achieved remarkable performance, the challenge remains
to produce high-quality and visually pleasing de-rained results. In this paper,
we present a reference-guided de-raining filter, a transformer network that
enhances de-raining results using a reference clean image as guidance. We
leverage the capabilities of the proposed module to further refine the images
de-rained by existing methods. We validate our method on three datasets and
show that our module can improve the performance of existing prior-based,
CNN-based, and transformer-based approaches.",cs.CV,2024-07-31
"Clover-2: Accurate Inference for Regressive Lightweight Speculative
  Decoding",,"Large Language Models (LLMs) frequently suffer from inefficiencies, largely
attributable to the discord between the requirements of auto-regressive
decoding and the architecture of contemporary GPUs. Recently, regressive
lightweight speculative decoding has garnered attention for its notable
efficiency improvements in text generation tasks. This approach utilizes a
lightweight regressive draft model, like a Recurrent Neural Network (RNN) or a
single transformer decoder layer, leveraging sequential information to
iteratively predict potential tokens. Specifically, RNN draft models are
computationally economical but tend to deliver lower accuracy, while attention
decoder layer models exhibit the opposite traits. This paper presents Clover-2,
an advanced iteration of Clover, an RNN-based draft model designed to achieve
comparable accuracy to that of attention decoder layer models while maintaining
minimal computational overhead. Clover-2 enhances the model architecture and
incorporates knowledge distillation to increase Clover's accuracy and improve
overall efficiency. We conducted experiments using the open-source Vicuna 7B
and LLaMA3-Instruct 8B models. The results demonstrate that Clover-2 surpasses
existing methods across various model architectures, showcasing its efficacy
and robustness.",cs.CL cs.AI cs.LG,2024-07-31
3D U-KAN Implementation for Multi-modal MRI Brain Tumor Segmentation,,"We explore the application of U-KAN, a U-Net based network enhanced with
Kolmogorov-Arnold Network (KAN) layers, for 3D brain tumor segmentation using
multi-modal MRI data. We adapt the original 2D U-KAN model to the 3D task, and
introduce a variant called UKAN-SE, which incorporates Squeeze-and-Excitation
modules for global attention. We compare the performance of U-KAN and UKAN-SE
against existing methods such as U-Net, Attention U-Net, and Swin UNETR, using
the BraTS 2024 dataset. Our results show that U-KAN and UKAN-SE, with
approximately 10.6 million parameters, achieve exceptional efficiency,
requiring only about 1/4 of the training time of U-Net and Attention U-Net, and
1/6 that of Swin UNETR, while surpassing these models across most evaluation
metrics. Notably, UKAN-SE slightly outperforms U-KAN.",eess.IV cs.CV,2024-08-01
"QUITO: Accelerating Long-Context Reasoning through Query-Guided Context
  Compression",,"In-context learning (ICL) capabilities are foundational to the success of
large language models (LLMs). Recently, context compression has attracted
growing interest since it can largely reduce reasoning complexities and
computation costs of LLMs. In this paper, we introduce a novel Query-gUIded
aTtention cOmpression (QUITO) method, which leverages attention of the question
over the contexts to filter useless information. Specifically, we take a
trigger token to calculate the attention distribution of the context in
response to the question. Based on the distribution, we propose three different
filtering methods to satisfy the budget constraints of the context length. We
evaluate the QUITO using two widely-used datasets, namely, NaturalQuestions and
ASQA. Experimental results demonstrate that QUITO significantly outperforms
established baselines across various datasets and downstream LLMs, underscoring
its effectiveness. Our code is available at
https://github.com/Wenshansilvia/attention_compressor.",cs.CL cs.AI,2024-08-01
"High Performance Im2win and Direct Convolutions using Three Tensor
  Layouts on SIMD Architectures",,"Convolution is the core component within deep neural networks and it is
computationally intensive and time consuming. Tensor data layouts significantly
impact convolution operations in terms of memory access and computational
efficiency. Yet, there is still a lack of comprehensive performance
characterization on data layouts on SIMD architectures concerning convolution
methods. This paper proposes three novel data layouts for im2win convolution:
NHWC, CHWN, and CHWN8, and introduces a set of general optimization techniques
for both direct and im2win convolutions. We compare the optimized im2win
convolution with the direct convolution and PyTorch's im2col-based convolution
across the aforementioned layouts on SIMD machines. The experiments
demonstrated that the im2win convolution with the new NHWC layout achieved up
to 355% performance speedup over NCHW layout. Our optimizations also
significantly improve the performance of both im2win and direct convolutions.
Our optimized im2win and direct convolutions achieved up to 95% and 94% of
machine's theoretical peak performance, respectively.",cs.LG cs.AI cs.NE,2024-08-01
DMESA: Densely Matching Everything by Segmenting Anything,,"We propose MESA and DMESA as novel feature matching methods, which utilize
Segment Anything Model (SAM) to effectively mitigate matching redundancy. The
key insight of our methods is to establish implicit-semantic area matching
prior to point matching, based on advanced image understanding of SAM. Then,
informative area matches with consistent internal semantic are able to undergo
dense feature comparison, facilitating precise inside-area point matching.
Specifically, MESA adopts a sparse matching framework and first obtains
candidate areas from SAM results through a novel Area Graph (AG). Then, area
matching among the candidates is formulated as graph energy minimization and
solved by graphical models derived from AG. To address the efficiency issue of
MESA, we further propose DMESA as its dense counterpart, applying a dense
matching framework. After candidate areas are identified by AG, DMESA
establishes area matches through generating dense matching distributions. The
distributions are produced from off-the-shelf patch matching utilizing the
Gaussian Mixture Model and refined via the Expectation Maximization. With less
repetitive computation, DMESA showcases a speed improvement of nearly five
times compared to MESA, while maintaining competitive accuracy. Our methods are
extensively evaluated on five datasets encompassing indoor and outdoor scenes.
The results illustrate consistent performance improvements from our methods for
five distinct point matching baselines across all datasets. Furthermore, our
methods exhibit promise generalization and improved robustness against image
resolution variations. The code is publicly available at
https://github.com/Easonyesheng/A2PM-MESA.",cs.CV,2024-08-01
Towards Scalable GPU-Accelerated SNN Training via Temporal Fusion,,"Drawing on the intricate structures of the brain, Spiking Neural Networks
(SNNs) emerge as a transformative development in artificial intelligence,
closely emulating the complex dynamics of biological neural networks. While
SNNs show promising efficiency on specialized sparse-computational hardware,
their practical training often relies on conventional GPUs. This reliance
frequently leads to extended computation times when contrasted with traditional
Artificial Neural Networks (ANNs), presenting significant hurdles for advancing
SNN research. To navigate this challenge, we present a novel temporal fusion
method, specifically designed to expedite the propagation dynamics of SNNs on
GPU platforms, which serves as an enhancement to the current significant
approaches for handling deep learning tasks with SNNs. This method underwent
thorough validation through extensive experiments in both authentic training
scenarios and idealized conditions, confirming its efficacy and adaptability
for single and multi-GPU systems. Benchmarked against various existing SNN
libraries/implementations, our method achieved accelerations ranging from
$5\times$ to $40\times$ on NVIDIA A100 GPUs. Publicly available experimental
codes can be found at https://github.com/EMI-Group/snn-temporal-fusion.",cs.AI cs.DC,2024-08-01
Navigating Text-to-Image Generative Bias across Indic Languages,,"This research investigates biases in text-to-image (TTI) models for the Indic
languages widely spoken across India. It evaluates and compares the generative
performance and cultural relevance of leading TTI models in these languages
against their performance in English. Using the proposed IndicTTI benchmark, we
comprehensively assess the performance of 30 Indic languages with two
open-source diffusion models and two commercial generation APIs. The primary
objective of this benchmark is to evaluate the support for Indic languages in
these models and identify areas needing improvement. Given the linguistic
diversity of 30 languages spoken by over 1.4 billion people, this benchmark
aims to provide a detailed and insightful analysis of TTI models' effectiveness
within the Indic linguistic landscape. The data and code for the IndicTTI
benchmark can be accessed at
https://iab-rubric.org/resources/other-databases/indictti.",cs.CL cs.CV,2024-08-01
"Bailing-TTS: Chinese Dialectal Speech Synthesis Towards Human-like
  Spontaneous Representation",,"Large-scale text-to-speech (TTS) models have made significant progress
recently.However, they still fall short in the generation of Chinese dialectal
speech. Toaddress this, we propose Bailing-TTS, a family of large-scale TTS
models capable of generating high-quality Chinese dialectal speech. Bailing-TTS
serves as a foundation model for Chinese dialectal speech generation. First,
continual semi-supervised learning is proposed to facilitate the alignment of
text tokens and speech tokens. Second, the Chinese dialectal representation
learning is developed using a specific transformer architecture and multi-stage
training processes. With the proposed design of novel network architecture and
corresponding strategy, Bailing-TTS is able to generate Chinese dialectal
speech from text effectively and efficiently. Experiments demonstrate that
Bailing-TTS generates Chinese dialectal speech towards human-like spontaneous
representation. Readers are encouraged to listen to demos at
\url{https://c9412600.github.io/bltts_tech_report/index.html}.",cs.CL cs.SD eess.AS,2024-08-01
"Diff3DETR:Agent-based Diffusion Model for Semi-supervised 3D Object
  Detection",,"3D object detection is essential for understanding 3D scenes. Contemporary
techniques often require extensive annotated training data, yet obtaining
point-wise annotations for point clouds is time-consuming and laborious. Recent
developments in semi-supervised methods seek to mitigate this problem by
employing a teacher-student framework to generate pseudo-labels for unlabeled
point clouds. However, these pseudo-labels frequently suffer from insufficient
diversity and inferior quality. To overcome these hurdles, we introduce an
Agent-based Diffusion Model for Semi-supervised 3D Object Detection
(Diff3DETR). Specifically, an agent-based object query generator is designed to
produce object queries that effectively adapt to dynamic scenes while striking
a balance between sampling locations and content embedding. Additionally, a
box-aware denoising module utilizes the DDIM denoising process and the
long-range attention in the transformer decoder to refine bounding boxes
incrementally. Extensive experiments on ScanNet and SUN RGB-D datasets
demonstrate that Diff3DETR outperforms state-of-the-art semi-supervised 3D
object detection methods.",cs.CV,2024-08-01
Gradient Harmonization in Unsupervised Domain Adaptation,,"Unsupervised domain adaptation (UDA) intends to transfer knowledge from a
labeled source domain to an unlabeled target domain. Many current methods focus
on learning feature representations that are both discriminative for
classification and invariant across domains by simultaneously optimizing domain
alignment and classification tasks. However, these methods often overlook a
crucial challenge: the inherent conflict between these two tasks during
gradient-based optimization. In this paper, we delve into this issue and
introduce two effective solutions known as Gradient Harmonization, including GH
and GH++, to mitigate the conflict between domain alignment and classification
tasks. GH operates by altering the gradient angle between different tasks from
an obtuse angle to an acute angle, thus resolving the conflict and trade-offing
the two tasks in a coordinated manner. Yet, this would cause both tasks to
deviate from their original optimization directions. We thus further propose an
improved version, GH++, which adjusts the gradient angle between tasks from an
obtuse angle to a vertical angle. This not only eliminates the conflict but
also minimizes deviation from the original gradient directions. Finally, for
optimization convenience and efficiency, we evolve the gradient harmonization
strategies into a dynamically weighted loss function using an integral operator
on the harmonized gradient. Notably, GH/GH++ are orthogonal to UDA and can be
seamlessly integrated into most existing UDA models. Theoretical insights and
experimental analyses demonstrate that the proposed approaches not only enhance
popular UDA baselines but also improve recent state-of-the-art models.",cs.CV cs.AI cs.LG,2024-08-01
Multi-Modal Parameter-Efficient Fine-tuning via Graph Neural Network,,"With the advent of the era of foundation models, pre-training and fine-tuning
have become common paradigms. Recently, parameter-efficient fine-tuning has
garnered widespread attention due to its better balance between the number of
learnable parameters and performance. However, some current parameter-efficient
fine-tuning methods only model a single modality and lack the utilization of
structural knowledge in downstream tasks. To address this issue, this paper
proposes a multi-modal parameter-efficient fine-tuning method based on graph
networks. Each image is fed into a multi-modal large language model (MLLM) to
generate a text description. The image and its corresponding text description
are then processed by a frozen image encoder and text encoder to generate image
features and text features, respectively. A graph is constructed based on the
similarity of the multi-modal feature nodes, and knowledge and relationships
relevant to these features are extracted from each node. Additionally, Elastic
Weight Consolidation (EWC) regularization is incorporated into the loss
function to mitigate the problem of forgetting during task learning. The
proposed model achieves test accuracies on the OxfordPets, Flowers102, and
Food101 datasets that improve by 4.45%, 2.92%, and 0.23%, respectively. The
code is available at https://github.com/yunche0/GA-Net/tree/master.",cs.CV cs.AI,2024-08-01
Everything We Hear: Towards Tackling Misinformation in Podcasts,,"Advances in generative AI, the proliferation of large multimodal models
(LMMs), and democratized open access to these technologies have direct
implications for the production and diffusion of misinformation. In this
prequel, we address tackling misinformation in the unique and increasingly
popular context of podcasts. The rise of podcasts as a popular medium for
disseminating information across diverse topics necessitates a proactive
strategy to combat the spread of misinformation. Inspired by the proven
effectiveness of \textit{auditory alerts} in contexts like collision alerts for
drivers and error pings in mobile phones, our work envisions the application of
auditory alerts as an effective tool to tackle misinformation in podcasts. We
propose the integration of suitable auditory alerts to notify listeners of
potential misinformation within the podcasts they are listening to, in
real-time and without hampering listening experiences. We identify several
opportunities and challenges in this path and aim to provoke novel
conversations around instruments, methods, and measures to tackle
misinformation in podcasts.",cs.HC,2024-08-01
Gradient Flow Decoding,,"This paper presents the Gradient Flow (GF) decoding for LDPC codes. GF
decoding, a continuous-time methodology based on gradient flow, employs a
potential energy function associated with bipolar codewords of LDPC codes. The
decoding process of the GF decoding is concisely defined by an ordinary
differential equation and thus it is well suited to an analog circuit
implementation. We experimentally demonstrate that the decoding performance of
the GF decoding for AWGN channels is comparable to that of the multi-bit mode
gradient descent bit flipping algorithm. We further introduce the negative
log-likelihood function of the channel for generalizing the GF decoding. The
proposed method is shown to be tensor-computable, which means that the gradient
of the objective function can be evaluated with the combination of basic tensor
computations. This characteristic is well-suited to emerging AI accelerators,
potentially applicable in wireless signal processing. The paper assesses the
decoding performance of the generalized GF decoding in LDPC-coded MIMO
channels. Our numerical experiments reveal that the decoding performance rivals
that of established techniques like MMSE + BP. Furthermore, an exploration of
score-based channel learning for capturing statistical properties is also
provided.",cs.IT math.IT,2024-08-01
"RDP: Ranked Differential Privacy for Facial Feature Protection in
  Multiscale Sparsified Subspace",,"With the widespread sharing of personal face images in applications' public
databases, face recognition systems faces real threat of being breached by
potential adversaries who are able to access users' face images and use them to
intrude the face recognition systems. In this paper, we propose a novel privacy
protection method in the multiscale sparsified feature subspaces to protect
sensitive facial features, by taking care of the influence or weight ranked
feature coefficients on the privacy budget, named ""Ranked Differential Privacy
(RDP)"". After the multiscale feature decomposition, the lightweight Laplacian
noise is added to the dimension-reduced sparsified feature coefficients
according to the geometric superposition method. Then, we rigorously prove that
the RDP satisfies Differential Privacy. After that, the nonlinear Lagrange
Multiplier (LM) method is formulated for the constraint optimization problem of
maximizing the utility of the visualization quality protected face images with
sanitizing noise, under a given facial features privacy budget. Then, two
methods are proposed to solve the nonlinear LM problem and obtain the optimal
noise scale parameters: 1) the analytical Normalization Approximation (NA)
method with identical average noise scale parameter for real-time online
applications; and 2) the LM optimization Gradient Descent (LMGD) numerical
method to obtain the nonlinear solution through iterative updating for more
accurate offline applications. Experimental results on two real-world datasets
show that our proposed RDP outperforms other state-of-the-art methods: at a
privacy budget of 0.2, the PSNR (Peak Signal-to-Noise Ratio) of the RDP is
about ~10 dB higher than (10 times as high as) the highest PSNR of all compared
methods.",cs.CV cs.IR,2024-08-01
"Contrastive Graph Representation Learning with Adversarial Cross-view
  Reconstruction and Information Bottleneck",,"Graph Neural Networks (GNNs) have received extensive research attention due
to their powerful information aggregation capabilities. Despite the success of
GNNs, most of them suffer from the popularity bias issue in a graph caused by a
small number of popular categories. Additionally, real graph datasets always
contain incorrect node labels, which hinders GNNs from learning effective node
representations. Graph contrastive learning (GCL) has been shown to be
effective in solving the above problems for node classification tasks. Most
existing GCL methods are implemented by randomly removing edges and nodes to
create multiple contrasting views, and then maximizing the mutual information
(MI) between these contrasting views to improve the node feature
representation. However, maximizing the mutual information between multiple
contrasting views may lead the model to learn some redundant information
irrelevant to the node classification task. To tackle this issue, we propose an
effective Contrastive Graph Representation Learning with Adversarial Cross-view
Reconstruction and Information Bottleneck (CGRL) for node classification, which
can adaptively learn to mask the nodes and edges in the graph to obtain the
optimal graph structure representation. Furthermore, we innovatively introduce
the information bottleneck theory into GCLs to remove redundant information in
multiple contrasting views while retaining as much information as possible
about node classification. Moreover, we add noise perturbations to the original
views and reconstruct the augmented views by constructing adversarial views to
improve the robustness of node feature representation. Extensive experiments on
real-world public datasets demonstrate that our method significantly
outperforms existing state-of-the-art algorithms.",cs.LG cs.AI,2024-08-01
"Head360: Learning a Parametric 3D Full-Head for Free-View Synthesis in
  360{\deg}",,"Creating a 360{\deg} parametric model of a human head is a very challenging
task. While recent advancements have demonstrated the efficacy of leveraging
synthetic data for building such parametric head models, their performance
remains inadequate in crucial areas such as expression-driven animation,
hairstyle editing, and text-based modifications. In this paper, we build a
dataset of artist-designed high-fidelity human heads and propose to create a
novel parametric 360{\deg} renderable parametric head model from it. Our scheme
decouples the facial motion/shape and facial appearance, which are represented
by a classic parametric 3D mesh model and an attached neural texture,
respectively. We further propose a training method for decompositing hairstyle
and facial appearance, allowing free-swapping of the hairstyle. A novel
inversion fitting method is presented based on single image input with high
generalization and fidelity. To the best of our knowledge, our model is the
first parametric 3D full-head that achieves 360{\deg} free-view synthesis,
image-based fitting, appearance editing, and animation within a single model.
Experiments show that facial motions and appearances are well disentangled in
the parametric space, leading to SOTA performance in rendering and animating
quality. The code and SynHead100 dataset are released at
https://nju-3dv.github.io/projects/Head360.",cs.CV,2024-08-01
"EmoTalk3D: High-Fidelity Free-View Synthesis of Emotional 3D Talking
  Head",,"We present a novel approach for synthesizing 3D talking heads with
controllable emotion, featuring enhanced lip synchronization and rendering
quality. Despite significant progress in the field, prior methods still suffer
from multi-view consistency and a lack of emotional expressiveness. To address
these issues, we collect EmoTalk3D dataset with calibrated multi-view videos,
emotional annotations, and per-frame 3D geometry. By training on the EmoTalk3D
dataset, we propose a \textit{`Speech-to-Geometry-to-Appearance'} mapping
framework that first predicts faithful 3D geometry sequence from the audio
features, then the appearance of a 3D talking head represented by 4D Gaussians
is synthesized from the predicted geometry. The appearance is further
disentangled into canonical and dynamic Gaussians, learned from multi-view
videos, and fused to render free-view talking head animation. Moreover, our
model enables controllable emotion in the generated talking heads and can be
rendered in wide-range views. Our method exhibits improved rendering quality
and stability in lip motion generation while capturing dynamic facial details
such as wrinkles and subtle expressions. Experiments demonstrate the
effectiveness of our approach in generating high-fidelity and
emotion-controllable 3D talking heads. The code and EmoTalk3D dataset are
released at https://nju-3dv.github.io/projects/EmoTalk3D.",cs.CV,2024-08-01
Tails Tell Tales: Chapter-Wide Manga Transcriptions with Character Names,,"Enabling engagement of manga by visually impaired individuals presents a
significant challenge due to its inherently visual nature. With the goal of
fostering accessibility, this paper aims to generate a dialogue transcript of a
complete manga chapter, entirely automatically, with a particular emphasis on
ensuring narrative consistency. This entails identifying (i) what is being
said, i.e., detecting the texts on each page and classifying them into
essential vs non-essential, and (ii) who is saying it, i.e., attributing each
dialogue to its speaker, while ensuring the same characters are named
consistently throughout the chapter.
  To this end, we introduce: (i) Magiv2, a model that is capable of generating
high-quality chapter-wide manga transcripts with named characters and
significantly higher precision in speaker diarisation over prior works; (ii) an
extension of the PopManga evaluation dataset, which now includes annotations
for speech-bubble tail boxes, associations of text to corresponding tails,
classifications of text as essential or non-essential, and the identity for
each character box; and (iii) a new character bank dataset, which comprises
over 11K characters from 76 manga series, featuring 11.5K exemplar character
images in total, as well as a list of chapters in which they appear. The code,
trained model, and both datasets can be found at:
https://github.com/ragavsachdeva/magi",cs.CV,2024-08-01
Towards Flexible Evaluation for Generative Visual Question Answering,,"Throughout rapid development of multimodal large language models, a crucial
ingredient is a fair and accurate evaluation of their multimodal comprehension
abilities. Although Visual Question Answering (VQA) could serve as a developed
test field, limitations of VQA evaluation, like the inflexible pattern of Exact
Match, have hindered MLLMs from demonstrating their real capability and
discourage rich responses. Therefore, this paper proposes the use of
semantics-based evaluators for assessing unconstrained open-ended responses on
VQA datasets. As characteristics of VQA have made such evaluation significantly
different than the traditional Semantic Textual Similarity (STS) task, to
systematically analyze the behaviour and compare the performance of various
evaluators including LLM-based ones, we proposes three key properties, i.e.,
Alignment, Consistency and Generalization, and a corresponding dataset
Assessing VQA Evaluators (AVE) to facilitate analysis. In addition, this paper
proposes a Semantically Flexible VQA Evaluator (SFVE) with meticulous design
based on the unique features of VQA evaluation. Experimental results verify the
feasibility of model-based VQA evaluation and effectiveness of the proposed
evaluator that surpasses existing semantic evaluators by a large margin. The
proposed training scheme generalizes to both the BERT-like encoders and
decoder-only LLM.",cs.CV cs.MM,2024-08-01
"Neural Octahedral Field: Octahedral prior for simultaneous smoothing and
  sharp edge regularization",,"Neural implicit representation, the parameterization of distance function as
a coordinate neural field, has emerged as a promising lead in tackling surface
reconstruction from unoriented point clouds. To enforce consistent orientation,
existing methods focus on regularizing the gradient of the distance function,
such as constraining it to be of the unit norm, minimizing its divergence, or
aligning it with the eigenvector of Hessian that corresponds to zero
eigenvalue. However, under the presence of large scanning noise, they tend to
either overfit the noise input or produce an excessively smooth reconstruction.
In this work, we propose to guide the surface reconstruction under a new
variant of neural field, the octahedral field, leveraging the spherical
harmonics representation of octahedral frames originated in the hexahedral
meshing. Such field automatically snaps to geometry features when constrained
to be smooth, and naturally preserves sharp angles when interpolated over
creases. By simultaneously fitting and smoothing the octahedral field alongside
the implicit geometry, it behaves analogously to bilateral filtering, resulting
in smooth reconstruction while preserving sharp edges. Despite being operated
purely pointwise, our method outperforms various traditional and neural
approaches across extensive experiments, and is very competitive with methods
that require normal and data priors. Our full implementation is available at:
https://github.com/Ankbzpx/frame-field.",cs.CV cs.GR,2024-08-01
"Constraint Energy Minimizing Generalized Multiscale Finite Element
  Method for Convection Diffusion Equations with Inhomogeneous Boundary
  Conditions",,"In this paper, we develop the constraint energy minimizing generalized
multiscale finite element method (CEM-GMsFEM) for convection-diffusion
equations with inhomogeneous Dirichlet, Neumann and Robin boundary conditions,
along with high-contrast coefficients. For time independent problems, boundary
correctors $\mathcal{D}^m$ and $\mathcal{N}^{m}$ for Dirichlet, Neumann, and
Robin conditions are designed. For time dependent problems, a scheme to update
the boundary correctors is formulated. Error analysis in both cases is given to
show the first-order convergence in energy norm with respect to the coarse mesh
size $H$ and second-order convergence in $L^2-$norm, as verified by numerical
examples, with which different finite difference schemes are compared for
temporal discretization. Nonlinear problems are also demonstrated in
combination with Strang splitting.",math.NA cs.NA,2024-08-01
"Leveraging Weak Cross-Modal Guidance for Coherence Modelling via
  Iterative Learning",,"Cross-modal coherence modeling is essential for intelligent systems to help
them organize and structure information, thereby understanding and creating
content of the physical world coherently like human-beings. Previous work on
cross-modal coherence modeling attempted to leverage the order information from
another modality to assist the coherence recovering of the target modality.
Despite of the effectiveness, labeled associated coherency information is not
always available and might be costly to acquire, making the cross-modal
guidance hard to leverage. To tackle this challenge, this paper explores a new
way to take advantage of cross-modal guidance without gold labels on coherency,
and proposes the Weak Cross-Modal Guided Ordering (WeGO) model. More
specifically, it leverages high-confidence predicted pairwise order in one
modality as reference information to guide the coherence modeling in another.
An iterative learning paradigm is further designed to jointly optimize the
coherence modeling in two modalities with selected guidance from each other.
The iterative cross-modal boosting also functions in inference to further
enhance coherence prediction in each modality. Experimental results on two
public datasets have demonstrated that the proposed method outperforms existing
methods for cross-modal coherence modeling tasks. Major technical modules have
been evaluated effective through ablation studies. Codes are available at:
\url{https://github.com/scvready123/IterWeGO}.",cs.MM cs.IR,2024-08-01
ABC Align: Large Language Model Alignment for Safety & Accuracy,,"Alignment of Large Language Models (LLMs) remains an unsolved problem. Human
preferences are highly distributed and can be captured at multiple levels of
abstraction, from the individual to diverse populations. Organisational
preferences, represented by standards and principles, are defined to mitigate
reputational risk or meet legislative obligations. In this paper, we present
ABC Align, a novel alignment methodology for LLMs that enables integration of
the standards and preferences of a large media organisation into the LLM
itself. We combine a set of data and methods that build on recent breakthroughs
in synthetic data generation, preference optimisation, and post-training model
quantisation. Our unified approach mitigates bias and improves accuracy, while
preserving reasoning capability, as measured against standard benchmarks.",cs.LG cs.AI cs.CL,2024-08-01
Online Computation of String Net Frequency,,"The net frequency (NF) of a string, of length $m$, in a text, of length $n$,
is the number of occurrences of the string in the text with unique left and
right extensions. Recently, Guo et al. [CPM 2024] showed that NF is
combinatorially interesting and how two key questions can be computed
efficiently in the offline setting. First, SINGLE-NF: reporting the NF of a
query string in an input text. Second, ALL-NF: reporting an occurrence and the
NF of each string of positive NF in an input text. For many applications,
however, facilitating these computations in an online manner is highly
desirable. We are the first to solve the above two problems in the online
setting, and we do so in optimal time, assuming, as is common, a constant-size
alphabet: SINGLE-NF in $O(m)$ time and ALL-NF in $O(n)$ time. Our results are
achieved by first designing new and simpler offline algorithms using suffix
trees, proving additional properties of NF, and exploiting Ukkonen's online
suffix tree construction algorithm and results on implicit node maintenance in
an implicit suffix tree by Breslauer and Italiano.",cs.DS,2024-08-01
"Discretizing Continuous Action Space with Unimodal Probability
  Distributions for On-Policy Reinforcement Learning",,"For on-policy reinforcement learning, discretizing action space for
continuous control can easily express multiple modes and is straightforward to
optimize. However, without considering the inherent ordering between the
discrete atomic actions, the explosion in the number of discrete actions can
possess undesired properties and induce a higher variance for the policy
gradient estimator. In this paper, we introduce a straightforward architecture
that addresses this issue by constraining the discrete policy to be unimodal
using Poisson probability distributions. This unimodal architecture can better
leverage the continuity in the underlying continuous action space using
explicit unimodal probability distributions. We conduct extensive experiments
to show that the discrete policy with the unimodal probability distribution
provides significantly faster convergence and higher performance for on-policy
reinforcement learning algorithms in challenging control tasks, especially in
highly complex tasks such as Humanoid. We provide theoretical analysis on the
variance of the policy gradient estimator, which suggests that our attentively
designed unimodal discrete policy can retain a lower variance and yield a
stable learning process.",cs.LG cs.AI,2024-08-01
Online Linear Programming with Batching,,"We study Online Linear Programming (OLP) with batching. The planning horizon
is cut into $K$ batches, and the decisions on customers arriving within a batch
can be delayed to the end of their associated batch. Compared with OLP without
batching, the ability to delay decisions brings better operational performance,
as measured by regret. Two research questions of interest are: (1) What is a
lower bound of the regret as a function of $K$? (2) What algorithms can achieve
the regret lower bound? These questions have been analyzed in the literature
when the distribution of the reward and the resource consumption of the
customers have finite support. By contrast, this paper analyzes these questions
when the conditional distribution of the reward given the resource consumption
is continuous, and we show the answers are different under this setting. When
there is only a single type of resource and the decision maker knows the total
number of customers, we propose an algorithm with a $O(\log K)$ regret upper
bound and provide a $\Omega(\log K)$ regret lower bound. We also propose
algorithms with $O(\log K)$ regret upper bound for the setting in which there
are multiple types of resource and the setting in which customers arrive
following a Poisson process. All these regret upper and lower bounds are
independent of the length of the planning horizon, and all the proposed
algorithms delay decisions on customers arriving in only the first and the last
batch. We also take customer impatience into consideration and establish a way
of selecting an appropriate batch size.",cs.LG math.OC,2024-08-01
"Translating Imaging to Genomics: Leveraging Transformers for Predictive
  Modeling",,"In this study, we present a novel approach for predicting genomic information
from medical imaging modalities using a transformer-based model. We aim to
bridge the gap between imaging and genomics data by leveraging transformer
networks, allowing for accurate genomic profile predictions from CT/MRI images.
Presently most studies rely on the use of whole slide images (WSI) for the
association, which are obtained via invasive methodologies. We propose using
only available CT/MRI images to predict genomic sequences. Our transformer
based approach is able to efficiently generate associations between multiple
sequences based on CT/MRI images alone. This work paves the way for the use of
non-invasive imaging modalities for precise and personalized healthcare,
allowing for a better understanding of diseases and treatment.",cs.CV,2024-08-01
Adversarial Text Rewriting for Text-aware Recommender Systems,,"Text-aware recommender systems incorporate rich textual features, such as
titles and descriptions, to generate item recommendations for users. The use of
textual features helps mitigate cold-start problems, and thus, such recommender
systems have attracted increased attention. However, we argue that the
dependency on item descriptions makes the recommender system vulnerable to
manipulation by adversarial sellers on e-commerce platforms. In this paper, we
explore the possibility of such manipulation by proposing a new text rewriting
framework to attack text-aware recommender systems. We show that the rewriting
attack can be exploited by sellers to unfairly uprank their products, even
though the adversarially rewritten descriptions are perceived as realistic by
human evaluators. Methodologically, we investigate two different variations to
carry out text rewriting attacks: (1) two-phase fine-tuning for greater attack
performance, and (2) in-context learning for higher text rewriting quality.
Experiments spanning 3 different datasets and 4 existing approaches demonstrate
that recommender systems exhibit vulnerability against the proposed text
rewriting attack. Our work adds to the existing literature around the
robustness of recommender systems, while highlighting a new dimension of
vulnerability in the age of large-scale automated text generation.",cs.IR cs.CR cs.LG cs.SI,2024-08-01
"ADBM: Adversarial diffusion bridge model for reliable adversarial
  purification",,"Recently Diffusion-based Purification (DiffPure) has been recognized as an
effective defense method against adversarial examples. However, we find
DiffPure which directly employs the original pre-trained diffusion models for
adversarial purification, to be suboptimal. This is due to an inherent
trade-off between noise purification performance and data recovery quality.
Additionally, the reliability of existing evaluations for DiffPure is
questionable, as they rely on weak adaptive attacks. In this work, we propose a
novel Adversarial Diffusion Bridge Model, termed ADBM. ADBM directly constructs
a reverse bridge from the diffused adversarial data back to its original clean
examples, enhancing the purification capabilities of the original diffusion
models. Through theoretical analysis and experimental validation across various
scenarios, ADBM has proven to be a superior and robust defense mechanism,
offering significant promise for practical applications.",cs.LG cs.AI cs.CV,2024-08-01
Condorcet's Jury Theorem with Abstention,,"The well-known Condorcet's Jury theorem posits that the majority rule selects
the best alternative among two available options with probability one, as the
population size increases to infinity. We study this result under an asymmetric
two-candidate setup, where supporters of both candidates may have different
participation costs.
  When the decision to abstain is fully rational i.e., when the vote pivotality
is the probability of a tie, the only equilibrium outcome is a trivial
equilibrium where all voters except those with zero voting cost, abstain. We
propose and analyze a more practical, boundedly rational model where voters
overestimate their pivotality, and show that under this model, non-trivial
equilibria emerge where the winning probability of both candidates is bounded
away from one.
  We show that when the pivotality estimate strongly depends on the margin of
victory, victory is not assured to any candidate in any non-trivial
equilibrium, regardless of population size and in contrast to Condorcet's
assertion. Whereas, under a weak dependence on margin, Condorcet's Jury theorem
is restored.",cs.GT cs.MA,2024-08-01
"A Novel Edge Laplacian-based Approach for Adaptive Formation Control of
  Uncertain Multi-agent Systems with Unified Relative Error Performance",,"For most existing prescribed performance formation control methods,
performance requirements are not directly imposed on the relative states
between agents but on the consensus error, which lacks a clear physical
interpretation of their solution. In this paper, we propose a novel adaptive
prescribed performance formation control strategy, capable of guaranteeing
prescribed performance on the relative errors, for uncertain high-order
multi-agent systems under a class of directed graphs. Due to the consideration
of performance constraints for relative errors, a coupled nonlinear interaction
term that contains global graphic information among agents is involved in the
error dynamics, leading to a fully distributed control design more difficult
and challenging. Here by proposing a series of nonlinear mappings and utilizing
the edge Laplacian along with Lyapunov stability theory, the presented
formation control scheme exhibits the following appealing features when
compared to existing results: 1) different performance requirements can be
guaranteed in a unified way by solely tuning the design parameters a priori,
without the need for control redesign and stability reanalysis under the
proposed fixed control protocol, making the design more user-friendly and the
implementation less demanding; 2) the complex and burdensome verification
process for the initial constraint, often encountered in existing prescribed
performance controls, is completely obviated if the performance requirements
are global; and 3) nonlinear interaction is completely decoupled and the
asymptotic stability of the formation manifold is ensured via using the
adaptive parameter estimate technique. Finally, simulations of various
performance behaviors are performed to show the efficiency of the theoretical
results.",eess.SY cs.SY,2024-08-01
Iterative Prototype Refinement for Ambiguous Speech Emotion Recognition,,"Recognizing emotions from speech is a daunting task due to the subtlety and
ambiguity of expressions. Traditional speech emotion recognition (SER) systems,
which typically rely on a singular, precise emotion label, struggle with this
complexity. Therefore, modeling the inherent ambiguity of emotions is an urgent
problem. In this paper, we propose an iterative prototype refinement framework
(IPR) for ambiguous SER. IPR comprises two interlinked components: contrastive
learning and class prototypes. The former provides an efficient way to obtain
high-quality representations of ambiguous samples. The latter are dynamically
updated based on ambiguous labels -- the similarity of the ambiguous data to
all prototypes. These refined embeddings yield precise pseudo labels, thus
reinforcing representation quality. Experimental evaluations conducted on the
IEMOCAP dataset validate the superior performance of IPR over state-of-the-art
methods, thus proving the effectiveness of our proposed method.",cs.SD eess.AS,2024-08-01
"Exploiting Preferences in Loss Functions for Sequential Recommendation
  via Weak Transitivity",,"A choice of optimization objective is immensely pivotal in the design of a
recommender system as it affects the general modeling process of a user's
intent from previous interactions. Existing approaches mainly adhere to three
categories of loss functions: pairwise, pointwise, and setwise loss functions.
Despite their effectiveness, a critical and common drawback of such objectives
is viewing the next observed item as a unique positive while considering all
remaining items equally negative. Such a binary label assignment is generally
limited to assuring a higher recommendation score of the positive item,
neglecting potential structures induced by varying preferences between other
unobserved items. To alleviate this issue, we propose a novel method that
extends original objectives to explicitly leverage the different levels of
preferences as relative orders between their scores. Finally, we demonstrate
the superior performance of our method compared to baseline objectives.",cs.LG cs.IR,2024-08-01
"Leveraging Virtual Reality Simulation to Engage Non-Disabled People in
  Reflection on Access Barriers for Disabled People",,"Disabled people experience many barriers in daily life, but non-disabled
people rarely pause to reflect and engage in joint action to advocate for
access. In this demo, we explore the potential of Virtual Reality (VR) to
sensitize non-disabled people to barriers in the built environment. We
contribute a VR simulation of a major traffic hub in Karlsruhe, Germany, and we
employ visual embellishments and animations to showcase barriers and potential
removal strategies. Through our work, we seek to engage users in conversation
on what kind of environment is accessible to whom, and what equitable
participation in society requires. Additionally, we aim to expand the
understanding of how VR technology can promote reflection through interactive
exploration.",cs.HC,2024-08-01
"OTAD: An Optimal Transport-Induced Robust Model for Agnostic Adversarial
  Attack",,"Deep neural networks (DNNs) are vulnerable to small adversarial perturbations
of the inputs, posing a significant challenge to their reliability and
robustness. Empirical methods such as adversarial training can defend against
particular attacks but remain vulnerable to more powerful attacks.
Alternatively, Lipschitz networks provide certified robustness to unseen
perturbations but lack sufficient expressive power. To harness the advantages
of both approaches, we design a novel two-step Optimal Transport induced
Adversarial Defense (OTAD) model that can fit the training data accurately
while preserving the local Lipschitz continuity. First, we train a DNN with a
regularizer derived from optimal transport theory, yielding a discrete optimal
transport map linking data to its features. By leveraging the map's inherent
regularity, we interpolate the map by solving the convex integration problem
(CIP) to guarantee the local Lipschitz property. OTAD is extensible to diverse
architectures of ResNet and Transformer, making it suitable for complex data.
For efficient computation, the CIP can be solved through training neural
networks. OTAD opens a novel avenue for developing reliable and secure deep
learning systems through the regularity of optimal transport maps. Empirical
results demonstrate that OTAD can outperform other robust models on diverse
datasets.",cs.LG cs.AI math.OC stat.ML,2024-08-01
"""Patriarchy Hurts Men Too."" Does Your Model Agree? A Discussion on
  Fairness Assumptions",,"The pipeline of a fair ML practitioner is generally divided into three
phases: 1) Selecting a fairness measure. 2) Choosing a model that minimizes
this measure. 3) Maximizing the model's performance on the data. In the context
of group fairness, this approach often obscures implicit assumptions about how
bias is introduced into the data. For instance, in binary classification, it is
often assumed that the best model, with equal fairness, is the one with better
performance. However, this belief already imposes specific properties on the
process that introduced bias. More precisely, we are already assuming that the
biasing process is a monotonic function of the fair scores, dependent solely on
the sensitive attribute. We formally prove this claim regarding several
implicit fairness assumptions. This leads, in our view, to two possible
conclusions: either the behavior of the biasing process is more complex than
mere monotonicity, which means we need to identify and reject our implicit
assumptions in order to develop models capable of tackling more complex
situations; or the bias introduced in the data behaves predictably, implying
that many of the developed models are superfluous.",cs.LG,2024-08-01
"DECIDER: Leveraging Foundation Model Priors for Improved Model Failure
  Detection and Explanation",,"Reliably detecting when a deployed machine learning model is likely to fail
on a given input is crucial for ensuring safe operation. In this work, we
propose DECIDER (Debiasing Classifiers to Identify Errors Reliably), a novel
approach that leverages priors from large language models (LLMs) and
vision-language models (VLMs) to detect failures in image classification
models. DECIDER utilizes LLMs to specify task-relevant core attributes and
constructs a ``debiased'' version of the classifier by aligning its visual
features to these core attributes using a VLM, and detects potential failure by
measuring disagreement between the original and debiased models. In addition to
proactively identifying samples on which the model would fail, DECIDER also
provides human-interpretable explanations for failure through a novel
attribute-ablation strategy. Through extensive experiments across diverse
benchmarks spanning subpopulation shifts (spurious correlations, class
imbalance) and covariate shifts (synthetic corruptions, domain shifts), DECIDER
consistently achieves state-of-the-art failure detection performance,
significantly outperforming baselines in terms of the overall Matthews
correlation coefficient as well as failure and success recall. Our codes can be
accessed at~\url{https://github.com/kowshikthopalli/DECIDER/}",cs.CV,2024-08-01
"Vision-based Wearable Steering Assistance for People with Impaired
  Vision in Jogging",,"Outdoor sports pose a challenge for people with impaired vision. The demand
for higher-speed mobility inspired us to develop a vision-based wearable
steering assistance. To ensure broad applicability, we focused on a
representative sports environment, the athletics track. Our efforts centered on
improving the speed and accuracy of perception, enhancing planning adaptability
for the real world, and providing swift and safe assistance for people with
impaired vision. In perception, we engineered a lightweight multitask network
capable of simultaneously detecting track lines and obstacles. Additionally,
due to the limitations of existing datasets for supporting multi-task detection
in athletics tracks, we diligently collected and annotated a new dataset (MAT)
containing 1000 images. In planning, we integrated the methods of sampling and
spline curves, addressing the planning challenges of curves. Meanwhile, we
utilized the positions of the track lines and obstacles as constraints to guide
people with impaired vision safely along the current track. Our system is
deployed on an embedded device, Jetson Orin NX. Through outdoor experiments, it
demonstrated adaptability in different sports scenarios, assisting users in
achieving free movement of 400-meter at an average speed of 1.34 m/s, meeting
the level of normal people in jogging. Our MAT dataset is publicly available
from https://github.com/snoopy-l/MAT",cs.CV cs.RO,2024-08-01
"DistillGrasp: Integrating Features Correlation with Knowledge
  Distillation for Depth Completion of Transparent Objects",,"Due to the visual properties of reflection and refraction, RGB-D cameras
cannot accurately capture the depth of transparent objects, leading to
incomplete depth maps. To fill in the missing points, recent studies tend to
explore new visual features and design complex networks to reconstruct the
depth, however, these approaches tremendously increase computation, and the
correlation of different visual features remains a problem. To this end, we
propose an efficient depth completion network named DistillGrasp which
distillates knowledge from the teacher branch to the student branch.
Specifically, in the teacher branch, we design a position correlation block
(PCB) that leverages RGB images as the query and key to search for the
corresponding values, guiding the model to establish correct correspondence
between two features and transfer it to the transparent areas. For the student
branch, we propose a consistent feature correlation module (CFCM) that retains
the reliable regions of RGB images and depth maps respectively according to the
consistency and adopts a CNN to capture the pairwise relationship for depth
completion. To avoid the student branch only learning regional features from
the teacher branch, we devise a distillation loss that not only considers the
distance loss but also the object structure and edge information. Extensive
experiments conducted on the ClearGrasp dataset manifest that our teacher
network outperforms state-of-the-art methods in terms of accuracy and
generalization, and the student network achieves competitive results with a
higher speed of 48 FPS. In addition, the significant improvement in a
real-world robotic grasping system illustrates the effectiveness and robustness
of our proposed system.",cs.CV cs.IR,2024-08-01
"MAARS: Multi-Rate Attack-Aware Randomized Scheduling for Securing
  Real-time Systems",,"Modern Cyber-Physical Systems (CPSs) consist of numerous control units
interconnected by communication networks. Each control unit executes multiple
safety-critical and non-critical tasks in real-time. Most of the
safety-critical tasks are executed with a fixed sampling period to ensure
deterministic timing behaviour that helps in its safety and performance
analysis. However, adversaries can exploit this deterministic behaviour of
safety-critical tasks to launch inference-based-based attacks on them. This
paper aims to prevent and minimize the possibility of such timing inference or
schedule-based attacks to compromise the control units. This is done by
switching between strategically chosen execution rates of the safety-critical
control tasks such that their performance remains unhampered. Thereafter, we
present a novel schedule vulnerability analysis methodology to switch between
valid schedules generated for these multiple periodicities of the control tasks
in run time. Utilizing these strategies, we introduce a novel Multi-Rate
Attack-Aware Randomized Scheduling (MAARS) framework for preemptive
fixed-priority schedulers that minimize the success rate of
timing-inference-based attacks on safety-critical real-time systems. To our
knowledge, this is the first work to propose a schedule randomization method
with attack awareness that preserves both the control and scheduling aspects.
The efficacy of the framework in terms of attack prevention is finally
evaluated on several automotive benchmarks in a Hardware-in-loop (HiL)
environment.",eess.SY cs.CR cs.OS cs.SY,2024-08-01
MuJoCo MPC for Humanoid Control: Evaluation on HumanoidBench,,"We tackle the recently introduced benchmark for whole-body humanoid control
HumanoidBench using MuJoCo MPC. We find that sparse reward functions of
HumanoidBench yield undesirable and unrealistic behaviors when optimized;
therefore, we propose a set of regularization terms that stabilize the robot
behavior across tasks. Current evaluations on a subset of tasks demonstrate
that our proposed reward function allows achieving the highest HumanoidBench
scores while maintaining realistic posture and smooth control signals. Our code
is publicly available and will become a part of MuJoCo MPC, enabling rapid
prototyping of robot behaviors.",cs.RO cs.AI cs.LG,2024-08-01
Interaural time difference loss for binaural target sound extraction,,"Binaural target sound extraction (TSE) aims to extract a desired sound from a
binaural mixture of arbitrary sounds while preserving the spatial cues of the
desired sound. Indeed, for many applications, the target sound signal and its
spatial cues carry important information about the sound source. Binaural TSE
can be realized with a neural network trained to output only the desired sound
given a binaural mixture and an embedding characterizing the desired sound
class as inputs. Conventional TSE systems are trained using signal-level
losses, which measure the difference between the extracted and reference
signals for the left and right channels. In this paper, we propose adding
explicit spatial losses to better preserve the spatial cues of the target
sound. In particular, we explore losses aiming at preserving the interaural
level (ILD), phase (IPD), and time differences (ITD). We show experimentally
that adding such spatial losses, particularly our newly proposed ITD loss,
helps preserve better spatial cues while maintaining the signal-level metrics.",cs.SD eess.AS,2024-08-01
"Neural Graph Matching for Video Retrieval in Large-Scale Video-driven
  E-commerce",,"With the rapid development of the short video industry, traditional
e-commerce has encountered a new paradigm, video-driven e-commerce, which
leverages attractive videos for product showcases and provides both video and
item services for users. Benefitting from the dynamic and visualized
introduction of items,video-driven e-commerce has shown huge potential in
stimulating consumer confidence and promoting sales. In this paper, we focus on
the video retrieval task, facing the following challenges: (1) Howto handle the
heterogeneities among users, items, and videos? (2)How to mine the
complementarity between items and videos for better user understanding? In this
paper, we first leverage the dual graph to model the co-existing of user-video
and user-item interactions in video-driven e-commerce and innovatively reduce
user preference understanding to a graph matching problem. To solve it, we
further propose a novel bi-level Graph Matching Network(GMN), which mainly
consists of node- and preference-level graph matching. Given a user, node-level
graph matching aims to match videos and items, while preference-level graph
matching aims to match multiple user preferences extracted from both videos and
items. Then the proposed GMN can generate and improve user embedding by
aggregating matched nodes or preferences from the dual graph in a bi-level
manner. Comprehensive experiments show the superiority of the proposed GMN with
significant improvements over state-of-the-art approaches (e.g., AUC+1.9% and
CTR+7.15%). We have developed it on a well-known video-driven e-commerce
platform, serving hundreds of millions of users every day",cs.LG cs.AI,2024-08-01
"Advancing Medical Image Segmentation: Morphology-Driven Learning with
  Diffusion Transformer",,"Understanding the morphological structure of medical images and precisely
segmenting the region of interest or abnormality is an important task that can
assist in diagnosis. However, the unique properties of medical imaging make
clear segmentation difficult, and the high cost and time-consuming task of
labeling leads to a coarse-grained representation of ground truth. Facing with
these problems, we propose a novel Diffusion Transformer Segmentation (DTS)
model for robust segmentation in the presence of noise. We propose an
alternative to the dominant Denoising U-Net encoder through experiments
applying a transformer architecture, which captures global dependency through
self-attention. Additionally, we propose k-neighbor label smoothing, reverse
boundary attention, and self-supervised learning with morphology-driven
learning to improve the ability to identify complex structures. Our model,
which analyzes the morphological representation of images, shows better results
than the previous models in various medical imaging modalities, including CT,
MRI, and lesion images.",cs.CV cs.AI,2024-08-01
"Securing the Diagnosis of Medical Imaging: An In-depth Analysis of
  AI-Resistant Attacks",,"Machine learning (ML) is a rapidly developing area of medicine that uses
significant resources to apply computer science and statistics to medical
issues. ML's proponents laud its capacity to handle vast, complicated, and
erratic medical data. It's common knowledge that attackers might cause
misclassification by deliberately creating inputs for machine learning
classifiers. Research on adversarial examples has been extensively conducted in
the field of computer vision applications. Healthcare systems are thought to be
highly difficult because of the security and life-or-death considerations they
include, and performance accuracy is very important. Recent arguments have
suggested that adversarial attacks could be made against medical image analysis
(MedIA) technologies because of the accompanying technology infrastructure and
powerful financial incentives. Since the diagnosis will be the basis for
important decisions, it is essential to assess how strong medical DNN tasks are
against adversarial attacks. Simple adversarial attacks have been taken into
account in several earlier studies. However, DNNs are susceptible to more risky
and realistic attacks. The present paper covers recent proposed adversarial
attack strategies against DNNs for medical imaging as well as countermeasures.
In this study, we review current techniques for adversarial imaging attacks,
detections. It also encompasses various facets of these techniques and offers
suggestions for the robustness of neural networks to be improved in the future.",cs.CR cs.AI eess.IV,2024-08-01
"A Simple Background Augmentation Method for Object Detection with
  Diffusion Model",,"In computer vision, it is well-known that a lack of data diversity will
impair model performance. In this study, we address the challenges of enhancing
the dataset diversity problem in order to benefit various downstream tasks such
as object detection and instance segmentation. We propose a simple yet
effective data augmentation approach by leveraging advancements in generative
models, specifically text-to-image synthesis technologies like Stable
Diffusion. Our method focuses on generating variations of labeled real images,
utilizing generative object and background augmentation via inpainting to
augment existing training data without the need for additional annotations. We
find that background augmentation, in particular, significantly improves the
models' robustness and generalization capabilities. We also investigate how to
adjust the prompt and mask to ensure the generated content comply with the
existing annotations. The efficacy of our augmentation techniques is validated
through comprehensive evaluations of the COCO dataset and several other key
object detection benchmarks, demonstrating notable enhancements in model
performance across diverse scenarios. This approach offers a promising solution
to the challenges of dataset enhancement, contributing to the development of
more accurate and robust computer vision models.",cs.CV cs.AI,2024-08-01
"Hierarchically Structured Neural Bones for Reconstructing Animatable
  Objects from Casual Videos",,"We propose a new framework for creating and easily manipulating 3D models of
arbitrary objects using casually captured videos. Our core ingredient is a
novel hierarchy deformation model, which captures motions of objects with a
tree-structured bones. Our hierarchy system decomposes motions based on the
granularity and reveals the correlations between parts without exploiting any
prior structural knowledge. We further propose to regularize the bones to be
positioned at the basis of motions, centers of parts, sufficiently covering
related surfaces of the part. This is achieved by our bone occupancy function,
which identifies whether a given 3D point is placed within the bone. Coupling
the proposed components, our framework offers several clear advantages: (1)
users can obtain animatable 3D models of the arbitrary objects in improved
quality from their casual videos, (2) users can manipulate 3D models in an
intuitive manner with minimal costs, and (3) users can interactively add or
delete control points as necessary. The experimental results demonstrate the
efficacy of our framework on diverse instances, in reconstruction quality,
interpretability and easier manipulation. Our code is available at
https://github.com/subin6/HSNB.",cs.CV,2024-08-01
Autonomous LLM-Enhanced Adversarial Attack for Text-to-Motion,,"Human motion generation driven by deep generative models has enabled
compelling applications, but the ability of text-to-motion (T2M) models to
produce realistic motions from text prompts raises security concerns if
exploited maliciously. Despite growing interest in T2M, few methods focus on
safeguarding these models against adversarial attacks, with existing work on
text-to-image models proving insufficient for the unique motion domain. In the
paper, we propose ALERT-Motion, an autonomous framework leveraging large
language models (LLMs) to craft targeted adversarial attacks against black-box
T2M models. Unlike prior methods modifying prompts through predefined rules,
ALERT-Motion uses LLMs' knowledge of human motion to autonomously generate
subtle yet powerful adversarial text descriptions. It comprises two key
modules: an adaptive dispatching module that constructs an LLM-based agent to
iteratively refine and search for adversarial prompts; and a multimodal
information contrastive module that extracts semantically relevant motion
information to guide the agent's search. Through this LLM-driven approach,
ALERT-Motion crafts adversarial prompts querying victim models to produce
outputs closely matching targeted motions, while avoiding obvious
perturbations. Evaluations across popular T2M models demonstrate ALERT-Motion's
superiority over previous methods, achieving higher attack success rates with
stealthier adversarial prompts. This pioneering work on T2M adversarial attacks
highlights the urgency of developing defensive measures as motion generation
technology advances, urging further research into safe and responsible
deployment.",cs.CV,2024-08-01
"Redefining Lexicographical Ordering: Optimizing Pauli String
  Decompositions for Quantum Compiling",,"In quantum computing, the efficient optimization of Pauli string
decompositions is a crucial aspect for the compilation of quantum circuits for
many applications, such as chemistry simulations and quantum machine learning.
In this paper, we propose a novel algorithm for the synthesis of trotterized
time-evolution operators that results in circuits with significantly fewer
gates than previous solutions. Our synthesis procedure takes the qubit
connectivity of a target quantum computer into account. As a result, the
generated quantum circuit does not require routing, and no additional CNOT
gates are needed to run the resulting circuit on a target device. We compare
our algorithm against Paulihedral and TKET, and show a significant improvement
for randomized circuits and different molecular ansatzes. We also investigate
the Trotter error introduced by our ordering of the terms in the Hamiltonian
versus default ordering and the ordering from the baseline methods and conclude
that our method on average does not increase the Trotter error.",quant-ph cs.DS cs.PL,2024-08-01
"DNTextSpotter: Arbitrary-Shaped Scene Text Spotting via Improved
  Denoising Training",,"More and more end-to-end text spotting methods based on Transformer
architecture have demonstrated superior performance. These methods utilize a
bipartite graph matching algorithm to perform one-to-one optimal matching
between predicted objects and actual objects. However, the instability of
bipartite graph matching can lead to inconsistent optimization targets, thereby
affecting the training performance of the model. Existing literature applies
denoising training to solve the problem of bipartite graph matching instability
in object detection tasks. Unfortunately, this denoising training method cannot
be directly applied to text spotting tasks, as these tasks need to perform
irregular shape detection tasks and more complex text recognition tasks than
classification. To address this issue, we propose a novel denoising training
method (DNTextSpotter) for arbitrary-shaped text spotting. Specifically, we
decompose the queries of the denoising part into noised positional queries and
noised content queries. We use the four Bezier control points of the Bezier
center curve to generate the noised positional queries. For the noised content
queries, considering that the output of the text in a fixed positional order is
not conducive to aligning position with content, we employ a masked character
sliding method to initialize noised content queries, thereby assisting in the
alignment of text content and position. To improve the model's perception of
the background, we further utilize an additional loss function for background
characters classification in the denoising training part.Although DNTextSpotter
is conceptually simple, it outperforms the state-of-the-art methods on four
benchmarks (Total-Text, SCUT-CTW1500, ICDAR15, and Inverse-Text), especially
yielding an improvement of 11.3% against the best approach in Inverse-Text
dataset.",cs.CV cs.AI,2024-08-01
"DeliLaw: A Chinese Legal Counselling System Based on a Large Language
  Model",,"Traditional legal retrieval systems designed to retrieve legal documents,
statutes, precedents, and other legal information are unable to give
satisfactory answers due to lack of semantic understanding of specific
questions. Large Language Models (LLMs) have achieved excellent results in a
variety of natural language processing tasks, which inspired us that we train a
LLM in the legal domain to help legal retrieval. However, in the Chinese legal
domain, due to the complexity of legal questions and the rigour of legal
articles, there is no legal large model with satisfactory practical application
yet. In this paper, we present DeliLaw, a Chinese legal counselling system
based on a large language model. DeliLaw integrates a legal retrieval module
and a case retrieval module to overcome the model hallucination. Users can
consult professional legal questions, search for legal articles and relevant
judgement cases, etc. on the DeliLaw system in a dialogue mode. In addition,
DeliLaw supports the use of English for counseling. we provide the address of
the system: https://data.delilegal.com/lawQuestion.",cs.CL,2024-08-01
Memorization Capacity for Additive Fine-Tuning with Small ReLU Networks,,"Fine-tuning large pre-trained models is a common practice in machine learning
applications, yet its mathematical analysis remains largely unexplored. In this
paper, we study fine-tuning through the lens of memorization capacity. Our new
measure, the Fine-Tuning Capacity (FTC), is defined as the maximum number of
samples a neural network can fine-tune, or equivalently, as the minimum number
of neurons ($m$) needed to arbitrarily change $N$ labels among $K$ samples
considered in the fine-tuning process. In essence, FTC extends the memorization
capacity concept to the fine-tuning scenario. We analyze FTC for the additive
fine-tuning scenario where the fine-tuned network is defined as the summation
of the frozen pre-trained network $f$ and a neural network $g$ (with $m$
neurons) designed for fine-tuning. When $g$ is a ReLU network with either 2 or
3 layers, we obtain tight upper and lower bounds on FTC; we show that $N$
samples can be fine-tuned with $m=\Theta(N)$ neurons for 2-layer networks, and
with $m=\Theta(\sqrt{N})$ neurons for 3-layer networks, no matter how large $K$
is. Our results recover the known memorization capacity results when $N = K$ as
a special case.",cs.LG stat.ML,2024-08-01
"High-Precision Self-Supervised Monocular Depth Estimation with
  Rich-Resource Prior",,"In the area of self-supervised monocular depth estimation, models that
utilize rich-resource inputs, such as high-resolution and multi-frame inputs,
typically achieve better performance than models that use ordinary single image
input. However, these rich-resource inputs may not always be available,
limiting the applicability of these methods in general scenarios. In this
paper, we propose Rich-resource Prior Depth estimator (RPrDepth), which only
requires single input image during the inference phase but can still produce
highly accurate depth estimations comparable to rich resource based methods.
Specifically, we treat rich-resource data as prior information and extract
features from it as reference features in an offline manner. When estimating
the depth for a single-image image, we search for similar pixels from the
rich-resource features and use them as prior information to estimate the depth.
Experimental results demonstrate that our model outperform other single-image
model and can achieve comparable or even better performance than models with
rich-resource inputs, only using low-resolution single-image input.",cs.CV,2024-08-01
Multimodal Fusion and Coherence Modeling for Video Topic Segmentation,,"The video topic segmentation (VTS) task segments videos into intelligible,
non-overlapping topics, facilitating efficient comprehension of video content
and quick access to specific content. VTS is also critical to various
downstream video understanding tasks. Traditional VTS methods using shallow
features or unsupervised approaches struggle to accurately discern the nuances
of topical transitions. Recently, supervised approaches have achieved superior
performance on video action or scene segmentation over unsupervised approaches.
In this work, we improve supervised VTS by thoroughly exploring multimodal
fusion and multimodal coherence modeling. Specifically, (1) we enhance
multimodal fusion by exploring different architectures using cross-attention
and mixture of experts. (2) To generally strengthen multimodality alignment and
fusion, we pre-train and fine-tune the model with multimodal contrastive
learning. (3) We propose a new pre-training task tailored for the VTS task, and
a novel fine-tuning task for enhancing multimodal coherence modeling for VTS.
We evaluate the proposed approaches on educational videos, in the form of
lectures, due to the vital role of topic segmentation of educational videos in
boosting learning experiences. Additionally, we introduce a large-scale Chinese
lecture video dataset to augment the existing English corpus, promoting further
research in VTS. Experiments on both English and Chinese lecture datasets
demonstrate that our model achieves superior VTS performance compared to
competitive unsupervised and supervised baselines.",cs.AI cs.CV eess.IV,2024-08-01
"DiM-Gesture: Co-Speech Gesture Generation with Adaptive Layer
  Normalization Mamba-2 framework",,"Speech-driven gesture generation is an emerging domain within virtual human
creation, where current methods predominantly utilize Transformer-based
architectures that necessitate extensive memory and are characterized by slow
inference speeds. In response to these limitations, we propose
\textit{DiM-Gestures}, a novel end-to-end generative model crafted to create
highly personalized 3D full-body gestures solely from raw speech audio,
employing Mamba-based architectures. This model integrates a Mamba-based fuzzy
feature extractor with a non-autoregressive Adaptive Layer Normalization
(AdaLN) Mamba-2 diffusion architecture. The extractor, leveraging a Mamba
framework and a WavLM pre-trained model, autonomously derives implicit,
continuous fuzzy features, which are then unified into a singular latent
feature. This feature is processed by the AdaLN Mamba-2, which implements a
uniform conditional mechanism across all tokens to robustly model the interplay
between the fuzzy features and the resultant gesture sequence. This innovative
approach guarantees high fidelity in gesture-speech synchronization while
maintaining the naturalness of the gestures. Employing a diffusion model for
training and inference, our framework has undergone extensive subjective and
objective evaluations on the ZEGGS and BEAT datasets. These assessments
substantiate our model's enhanced performance relative to contemporary
state-of-the-art methods, demonstrating competitive outcomes with the DiTs
architecture (Persona-Gestors) while optimizing memory usage and accelerating
inference speed.",cs.GR cs.AI cs.RO cs.SD,2024-08-01
Few-shot Defect Image Generation based on Consistency Modeling,,"Image generation can solve insufficient labeled data issues in defect
detection. Most defect generation methods are only trained on a single product
without considering the consistencies among multiple products, leading to poor
quality and diversity of generated results. To address these issues, we propose
DefectDiffu, a novel text-guided diffusion method to model both intra-product
background consistency and inter-product defect consistency across multiple
products and modulate the consistency perturbation directions to control
product type and defect strength, achieving diversified defect image
generation. Firstly, we leverage a text encoder to separately provide
consistency prompts for background, defect, and fusion parts of the
disentangled integrated architecture, thereby disentangling defects and normal
backgrounds. Secondly, we propose the double-free strategy to generate defect
images through two-stage perturbation of consistency direction, thereby
controlling product type and defect strength by adjusting the perturbation
scale. Besides, DefectDiffu can generate defect mask annotations utilizing
cross-attention maps from the defect part. Finally, to improve the generation
quality of small defects and masks, we propose the adaptive attention-enhance
loss to increase the attention to defects. Experimental results demonstrate
that DefectDiffu surpasses state-of-the-art methods in terms of generation
quality and diversity, thus effectively improving downstream defection
performance. Moreover, defect perturbation directions can be transferred among
various products to achieve zero-shot defect generation, which is highly
beneficial for addressing insufficient data issues. The code are available at
https://github.com/FFDD-diffusion/DefectDiffu.",cs.CV,2024-08-01
On the Limitations and Prospects of Machine Unlearning for Generative AI,,"Generative AI (GenAI), which aims to synthesize realistic and diverse data
samples from latent variables or other data modalities, has achieved remarkable
results in various domains, such as natural language, images, audio, and
graphs. However, they also pose challenges and risks to data privacy, security,
and ethics. Machine unlearning is the process of removing or weakening the
influence of specific data samples or features from a trained model, without
affecting its performance on other data or tasks. While machine unlearning has
shown significant efficacy in traditional machine learning tasks, it is still
unclear if it could help GenAI become safer and aligned with human desire. To
this end, this position paper provides an in-depth discussion of the machine
unlearning approaches for GenAI. Firstly, we formulate the problem of machine
unlearning tasks on GenAI and introduce the background. Subsequently, we
systematically examine the limitations of machine unlearning on GenAI models by
focusing on the two representative branches: LLMs and image generative
(diffusion) models. Finally, we provide our prospects mainly from three
aspects: benchmark, evaluation metrics, and utility-unlearning trade-off, and
conscientiously advocate for the future development of this field.",cs.LG cs.AI,2024-08-01
"A deep spatio-temporal attention model of dynamic functional network
  connectivity shows sensitivity to Alzheimer's in asymptomatic individuals",,"Alzheimer's disease (AD) progresses from asymptomatic changes to clinical
symptoms, emphasizing the importance of early detection for proper treatment.
Functional magnetic resonance imaging (fMRI), particularly dynamic functional
network connectivity (dFNC), has emerged as an important biomarker for AD.
Nevertheless, studies probing at-risk subjects in the pre-symptomatic stage
using dFNC are limited. To identify at-risk subjects and understand alterations
of dFNC in different stages, we leverage deep learning advancements and
introduce a transformer-convolution framework for predicting at-risk subjects
based on dFNC, incorporating spatial-temporal self-attention to capture brain
network dependencies and temporal dynamics. Our model significantly outperforms
other popular machine learning methods. By analyzing individuals with diagnosed
AD and mild cognitive impairment (MCI), we studied the AD progression and
observed a higher similarity between MCI and asymptomatic AD. The interpretable
analysis highlights the cognitive-control network's diagnostic importance, with
the model focusing on intra-visual domain dFNC when predicting asymptomatic AD
subjects.",cs.CE,2024-08-01
"Finding Defective Elements in Intelligent Reflecting Surface via
  Over-the-Air Measurements",,"Due to circuit failures, defective elements that cannot adaptively adjust the
phase shifts of their impinging signals in a desired manner may exist on an
intelligent reflecting surface (IRS). Traditional way to find these defective
IRS elements requires a thorough diagnosis of all the circuits belonging to a
huge number of IRS elements, which is practically challenging. In this paper,
we will devise a novel approach under which a transmitter sends known pilot
signals and a receiver localizes all the defective IRS elements just based on
its over-the-air measurements reflected from the IRS. The key lies in the fact
that the over-the-air measurements at the receiver side are functions of the
set of defective IRS elements. Based on this observation, we propose a
bisection based method to localize all the defective IRS elements.
Specifically, at each time slot, we properly control the desired phase shifts
of all the IRS elements such that half of the considered regime that is not
useful to localize the defective elements can be found based on the received
signals and removed. Via numerical results, it is shown that our proposed
bisection method can exploit the over-the-air measurements to localize all the
defective IRS elements quickly and accurately.",eess.SP cs.IT math.IT,2024-08-01
"Statistical AoI Guarantee Optimization for Supporting xURLLC in
  ISAC-enabled V2I Networks",,"This paper addresses the critical challenge of supporting next-generation
ultra-reliable and low-latency communication (xURLLC) within integrated sensing
and communication (ISAC)-enabled vehicle-to-infrastructure (V2I) networks. We
incorporate channel evaluation and retransmission mechanisms for real-time
reliability enhancement. Using stochastic network calculus (SNC), we establish
a theoretical framework to derive upper bounds for the peak age of information
violation probability (PAVP) via characterized sensing and communication moment
generation functions (MGFs). By optimizing these bounds, we develop power
allocation schemes that significantly reduce the statistical PAVP of sensory
packets in such networks. Simulations validate our theoretical derivations and
demonstrate the effectiveness of our proposed schemes.",cs.IT cs.SY eess.SY math.IT,2024-08-01
"Quantitative Group Testing and Pooled Data in the Linear Regime with
  Sublinear Tests",,"In the pooled data problem, the goal is to identify the categories associated
with a large collection of items via a sequence of pooled tests. Each pooled
test reveals the number of items in the pool belonging to each category. A
prominent special case is quantitative group testing (QGT), which is the case
of pooled data with two categories. We consider these problems in the
non-adaptive and linear regime, where the fraction of items in each category is
of constant order. We propose a scheme with a spatially coupled Bernoulli test
matrix and an efficient approximate message passing (AMP) algorithm for
recovery. We rigorously characterize its asymptotic performance in both the
noiseless and noisy settings, and prove that in the noiseless case, the AMP
algorithm achieves almost-exact recovery with a number of tests sublinear in
the number of items. For both QGT and pooled data, this is the first efficient
scheme that provably achieves recovery in the linear regime with a sublinear
number of tests, with performance degrading gracefully in the presence of
noise. Numerical simulations illustrate the benefits of the spatially coupled
scheme at finite dimensions, showing that it outperforms i.i.d. test designs as
well as other recovery algorithms based on convex programming.",cs.IT eess.SP math.IT,2024-08-01
"What comes after transformers? -- A selective survey connecting ideas in
  deep learning",,"Transformers have become the de-facto standard model in artificial
intelligence since 2017 despite numerous shortcomings ranging from energy
inefficiency to hallucinations. Research has made a lot of progress in
improving elements of transformers, and, more generally, deep learning
manifesting in many proposals for architectures, layers, optimization
objectives, and optimization techniques. For researchers it is difficult to
keep track of such developments on a broader level. We provide a comprehensive
overview of the many important, recent works in these areas to those who
already have a basic understanding of deep learning. Our focus differs from
other works, as we target specifically novel, alternative potentially
disruptive approaches to transformers as well as successful ideas of recent
deep learning. We hope that such a holistic and unified treatment of
influential, recent works and novel ideas helps researchers to form new
connections between diverse areas of deep learning. We identify and discuss
multiple patterns that summarize the key strategies for successful innovations
over the last decade as well as works that can be seen as rising stars.
Especially, we discuss attempts on how to improve on transformers covering
(partially) proven methods such as state space models but also including
far-out ideas in deep learning that seem promising despite not achieving
state-of-the-art results. We also cover a discussion on recent state-of-the-art
models such as OpenAI's GPT series and Meta's LLama models and, Google's Gemini
model family.",cs.LG,2024-08-01
Deepfake Media Forensics: State of the Art and Challenges Ahead,,"AI-generated synthetic media, also called Deepfakes, have significantly
influenced so many domains, from entertainment to cybersecurity. Generative
Adversarial Networks (GANs) and Diffusion Models (DMs) are the main frameworks
used to create Deepfakes, producing highly realistic yet fabricated content.
While these technologies open up new creative possibilities, they also bring
substantial ethical and security risks due to their potential misuse. The rise
of such advanced media has led to the development of a cognitive bias known as
Impostor Bias, where individuals doubt the authenticity of multimedia due to
the awareness of AI's capabilities. As a result, Deepfake detection has become
a vital area of research, focusing on identifying subtle inconsistencies and
artifacts with machine learning techniques, especially Convolutional Neural
Networks (CNNs). Research in forensic Deepfake technology encompasses five main
areas: detection, attribution and recognition, passive authentication,
detection in realistic scenarios, and active authentication. Each area tackles
specific challenges, from tracing the origins of synthetic media and examining
its inherent characteristics for authenticity. This paper reviews the primary
algorithms that address these challenges, examining their advantages,
limitations, and future prospects.",cs.CV,2024-08-01
"Polynomial quasi-Trefftz DG for PDEs with smooth coefficients: elliptic
  problems",,"Trefftz schemes are high-order Galerkin methods whose discrete spaces are
made of elementwise exact solutions of the underlying PDE. Trefftz basis
functions can be easily computed for many PDEs that are linear, homogeneous,
and have piecewise-constant coefficients. However, if the equation has variable
coefficients, exact solutions are generally unavailable. Quasi-Trefftz methods
overcome this limitation relying on elementwise ""approximate solutions"" of the
PDE, in the sense of Taylor polynomials.
  We define polynomial quasi-Trefftz spaces for general linear PDEs with smooth
coefficients and source term, describe their approximation properties and,
under a non-degeneracy condition, provide a simple algorithm to compute a
basis. We then focus on a quasi-Trefftz DG method for variable-coefficient
elliptic diffusion-advection-reaction problems, showing stability and
high-order convergence of the scheme. The main advantage over standard DG
schemes is the higher accuracy for comparable numbers of degrees of freedom.
For non-homogeneous problems with piecewise-smooth source term we propose to
construct a local quasi-Trefftz particular solution and then solve for the
difference. Numerical experiments in 2 and 3 space dimensions show the
excellent properties of the method both in diffusion-dominated and
advection-dominated problems.",math.NA cs.NA,2024-08-01
A Zero-Knowledge Proof of Knowledge for Subgroup Distance Problem,,"In this study, we introduce a novel zero-knowledge identification scheme
based on the hardness of the subgroup distance problem in the Hamming metric.
The proposed protocol, named Subgroup Distance Zero Knowledge Proof (SDZKP),
employs a cryptographically secure pseudorandom number generator to mask
secrets and utilizes a Stern-type algorithm to ensure robust security
properties.",cs.CR math.GR,2024-08-01
"Analysis of continuous data assimilation with large (or even infinite)
  nudging parameters",,"This paper considers continuous data assimilation (CDA) in partial
differential equation (PDE) discretizations where nudging parameters can be
taken arbitrarily large. We prove that long-time optimally accurate solutions
are obtained for such parameters for the heat and Navier-Stokes equations
(using implicit time stepping methods), with error bounds that do not grow as
the nudging parameter gets large. Existing theoretical results either prove
optimal accuracy but with the error scaled by the nudging parameter, or
suboptimal accuracy that is independent of it. The key idea to the improved
analysis is to decompose the error based on a weighted inner product that
incorporates the (symmetric by construction) nudging term, and prove that the
projection error from this weighted inner product is optimal and independent of
the nudging parameter. We apply the idea to BDF2 - finite element
discretizations of the heat equation and Navier-Stokes equations to show that
with CDA, they will admit optimal long-time accurate solutions independent of
the nudging parameter, for nudging parameters large enough. Several numerical
tests are given for the heat equation, fluid transport equation, Navier-Stokes,
and Cahn-Hilliard that illustrate the theory.",math.NA cs.NA,2024-08-01
"In-Context Example Selection via Similarity Search Improves Low-Resource
  Machine Translation",,"The ability of generative large language models (LLMs) to perform in-context
learning has given rise to a large body of research into how best to prompt
models for various natural language processing tasks. In this paper, we focus
on machine translation (MT), a task that has been shown to benefit from
in-context translation examples. However no systematic studies have been
published on how best to select examples, and mixed results have been reported
on the usefulness of similarity-based selection over random selection. We
provide a study covering multiple LLMs and multiple in-context example
retrieval strategies, comparing multilingual sentence embeddings. We cover
several language directions, representing different levels of language
resourcedness (English into French, German, Swahili and Wolof). Contrarily to
previously published results, we find that sentence embedding similarity can
improve MT, especially for low-resource language directions, and discuss the
balance between selection pool diversity and quality. We also highlight
potential problems with the evaluation of LLM-based MT and suggest a more
appropriate evaluation protocol, adapting the COMET metric to the evaluation of
LLMs. Code and outputs are freely available at
https://github.com/ArmelRandy/ICL-MT.",cs.CL,2024-08-01
Log Diameter Rounds MST Verification and Sensitivity in MPC,,"We consider two natural variants of the problem of minimum spanning tree
(MST) of a graph in the parallel setting: MST verification (verifying if a
given tree is an MST) and the sensitivity analysis of an MST (finding the
lowest cost replacement edge for each edge of the MST). These two problems have
been studied extensively for sequential algorithms and for parallel algorithms
in the PRAM model of computation. In this paper, we extend the study to the
standard model of Massive Parallel Computation (MPC).
  It is known that for graphs of diameter $D$, the connectivity problem can be
solved in $O(\log D + \log\log n)$ rounds on an MPC with low local memory (each
machine can store only $O(n^{\delta})$ words for an arbitrary constant $\delta
> 0$) and with linear global memory, that is, with optimal utilization.
However, for the related task of finding an MST, we need $\Omega(\log
D_{\text{MST}})$ rounds, where $D_{\text{MST}}$ denotes the diameter of the
minimum spanning tree. The state of the art upper bound for MST is $O(\log n)$
rounds; the result follows by simulating existing PRAM algorithms. While this
bound may be optimal for general graphs, the benchmark of connectivity and
lower bound for MST suggest the target bound of $O(\log D_{\text{MST}})$
rounds, or possibly $O(\log D_{\text{MST}} + \log\log n)$ rounds. As for now,
we do not know if this bound is achievable for the MST problem on an MPC with
low local memory and linear global memory. In this paper, we show that two
natural variants of the MST problem: MST verification and sensitivity analysis
of an MST, can be completed in $O(\log D_T)$ rounds on an MPC with low local
memory and with linear global memory; here $D_T$ is the diameter of the input
``candidate MST'' $T$. The algorithms asymptotically match our lower bound,
conditioned on the 1-vs-2-cycle conjecture.",cs.DS,2024-08-01
"Unsupervised Pairwise Causal Discovery on Heterogeneous Data using
  Mutual Information Measures",,"A fundamental task in science is to determine the underlying causal relations
because it is the knowledge of this functional structure what leads to the
correct interpretation of an effect given the apparent associations in the
observed data. In this sense, Causal Discovery is a technique that tackles this
challenge by analyzing the statistical properties of the constituent variables.
In this work, we target the generalizability of the discovery method by
following a reductionist approach that only involves two variables, i.e., the
pairwise or bi-variate setting. We question the current (possibly misleading)
baseline results on the basis that they were obtained through supervised
learning, which is arguably contrary to this genuinely exploratory endeavor. In
consequence, we approach this problem in an unsupervised way, using robust
Mutual Information measures, and observing the impact of the different variable
types, which is oftentimes ignored in the design of solutions. Thus, we provide
a novel set of standard unbiased results that can serve as a reference to guide
future discovery tasks in completely unknown environments.",cs.AI cs.LG stat.ME,2024-08-01
"Micro frequency hopping spread spectrum modulation and encryption
  technology",,"By combining traditional frequency hopping ideas with the concepts of
subcarriers and sampling points in OFDM baseband systems, this paper proposes a
frequency hopping technology within the baseband called micro frequency
hopping. Based on the concept of micro frequency hopping, this paper proposes a
micro frequency hopping spread spectrum modulation method based on cyclic
frequency shift and cyclic time shift, as well as a micro frequency hopping
encryption method based on phase scrambling of baseband signals. Specifically,
this paper reveals a linear micro frequency hopping symbol with good
auto-correlation and cross-correlation feature in both time domain and
frequency domain. Linear micro frequency hopping symbols with different root
$R$ have good cross-correlation feature, which can be used in multi-user
communication at same time and same frequency. Moreover, there is a linear
relationship between the time delay and frequency offset of this linear micro
frequency hopping symbol, making it suitable for time delay and frequency
offset estimation, also for ranging, and speed measurement. Finally, this paper
also verifies the advantages of micro frequency hopping technology through an
example of a linear micro frequency hopping spread spectrum multiple access
communication system. The author believes that micro frequency hopping
technology will be widely used in fields such as the Internet of Things,
military communication, satellite communication, satellite positioning, and
radar etc.",cs.IT math.IT,2024-08-01
Task-oriented and Semantics-aware Communications for Augmented Reality,,"Upon the advent of the emerging metaverse and its related applications in
Augmented Reality (AR), the current bit-oriented network struggles to support
real-time changes for the vast amount of associated information, creating a
significant bottleneck in its development. To address the above problem, we
present a novel task-oriented and semantics-aware communication framework for
augmented reality (TSAR) to enhance communication efficiency and effectiveness
significantly. We first present an analysis of the traditional wireless AR
point cloud communication framework, followed by a detailed summary of our
proposed semantic information extraction within the end-to-end communication.
Then, we detail the components of the TSAR framework, incorporating semantics
extraction with deep learning, task-oriented base knowledge selection, and
avatar pose recovery. Through rigorous experimentation, we demonstrate that our
proposed TSAR framework considerably outperforms traditional point cloud
communication framework, reducing wireless AR application transmission latency
by 95.6% and improving communication effectiveness in geometry and color
aspects by up to 82.4% and 20.4%, respectively.",eess.SY cs.SY,2024-08-01
Low-level I/O Monitoring for Scientific Workflows,,"While detailed resource usage monitoring is possible on the low-level using
proper tools, associating such usage with higher-level abstractions in the
application layer that actually cause the resource usage in the first place
presents a number of challenges. Suppose a large-scale scientific data analysis
workflow is run using a distributed execution environment such as a compute
cluster or cloud environment and we want to analyze the I/O behaviour of it to
find and alleviate potential bottlenecks. Different tasks of the workflow can
be assigned to arbitrary compute nodes and may even share the same compute
nodes. Thus, locally observed resource usage is not directly associated with
the individual workflow tasks. By acquiring resource usage profiles of the
involved nodes, we seek to correlate the trace data to the workflow and its
individual tasks. To accomplish that, we select the proper set of metadata
associated with low-level traces that let us associate them with higher-level
task information obtained from log files of the workflow execution as well as
the job management using a task orchestrator such as Kubernetes with its
container management. Ensuring a proper information chain allows the
classification of observed I/O on a logical task level and may reveal the most
costly or inefficient tasks of a scientific workflow that are most promising
for optimization.",cs.DC,2024-08-01
"DriveArena: A Closed-loop Generative Simulation Platform for Autonomous
  Driving",,"This paper presented DriveArena, the first high-fidelity closed-loop
simulation system designed for driving agents navigating in real scenarios.
DriveArena features a flexible, modular architecture, allowing for the seamless
interchange of its core components: Traffic Manager, a traffic simulator
capable of generating realistic traffic flow on any worldwide street map, and
World Dreamer, a high-fidelity conditional generative model with infinite
autoregression. This powerful synergy empowers any driving agent capable of
processing real-world images to navigate in DriveArena's simulated environment.
The agent perceives its surroundings through images generated by World Dreamer
and output trajectories. These trajectories are fed into Traffic Manager,
achieving realistic interactions with other vehicles and producing a new scene
layout. Finally, the latest scene layout is relayed back into World Dreamer,
perpetuating the simulation cycle. This iterative process fosters closed-loop
exploration within a highly realistic environment, providing a valuable
platform for developing and evaluating driving agents across diverse and
challenging scenarios. DriveArena signifies a substantial leap forward in
leveraging generative image data for the driving simulation platform, opening
insights for closed-loop autonomous driving. Code will be available soon on
GitHub: https://github.com/PJLab-ADG/DriveArena",cs.RO cs.AI cs.CV,2024-08-01
"A Batch Update Using Multiplicative Noise Modelling for Extended Object
  Tracking",,"While the tracking of multiple extended targets demands for sophisticated
algorithms to handle the high complexity inherent to the task, it also requires
low runtime for online execution in real-world scenarios. In this work, we
derive a batch update for the recently introduced elliptical-target tracker
called MEM-EKF*. The MEM-EKF* is based on the same likelihood as the
well-established random matrix approach but is derived from the multiplicative
error model (MEM) and uses an extended Kalman filter (EKF) to update the target
state sequentially, i.e., measurement-by-measurement. Our batch variant updates
the target state in a single step based on straightforward sums over all
measurements and the MEM-specific pseudo-measurements. This drastically reduces
the scaling constant for typical implementations and indeed we find a speedup
of roughly 100x in our numerical experiments. At the same time, the estimation
error which we measure using the Gaussian Wasserstein distance stays
significantly below that of the random matrix approach in coordinated turn
scenarios while being comparable otherwise.",eess.SY cs.SY,2024-08-01
Towards Reliable Advertising Image Generation Using Human Feedback,,"In the e-commerce realm, compelling advertising images are pivotal for
attracting customer attention. While generative models automate image
generation, they often produce substandard images that may mislead customers
and require significant labor costs to inspect. This paper delves into
increasing the rate of available generated images. We first introduce a
multi-modal Reliable Feedback Network (RFNet) to automatically inspect the
generated images. Combining the RFNet into a recurrent process, Recurrent
Generation, results in a higher number of available advertising images. To
further enhance production efficiency, we fine-tune diffusion models with an
innovative Consistent Condition regularization utilizing the feedback from
RFNet (RFFT). This results in a remarkable increase in the available rate of
generated images, reducing the number of attempts in Recurrent Generation, and
providing a highly efficient production process without sacrificing visual
appeal. We also construct a Reliable Feedback 1 Million (RF1M) dataset which
comprises over one million generated advertising images annotated by human,
which helps to train RFNet to accurately assess the availability of generated
images and faithfully reflect the human feedback. Generally speaking, our
approach offers a reliable solution for advertising image generation.",cs.CV,2024-08-01
MPT-PAR:Mix-Parameters Transformer for Panoramic Activity Recognition,,"The objective of the panoramic activity recognition task is to identify
behaviors at various granularities within crowded and complex environments,
encompassing individual actions, social group activities, and global
activities. Existing methods generally use either parameter-independent modules
to capture task-specific features or parameter-sharing modules to obtain common
features across all tasks. However, there is often a strong interrelatedness
and complementary effect between tasks of different granularities that previous
methods have yet to notice. In this paper, we propose a model called MPT-PAR
that considers both the unique characteristics of each task and the synergies
between different tasks simultaneously, thereby maximizing the utilization of
features across multi-granularity activity recognition. Furthermore, we
emphasize the significance of temporal and spatial information by introducing a
spatio-temporal relation-enhanced module and a scene representation learning
module, which integrate the the spatio-temporal context of action and global
scene into the feature map of each granularity. Our method achieved an overall
F1 score of 47.5\% on the JRDB-PAR dataset, significantly outperforming all the
state-of-the-art methods.",cs.CV cs.AI,2024-08-01
"Towards Evolutionary-based Automated Machine Learning for Small Molecule
  Pharmacokinetic Prediction",,"Machine learning (ML) is revolutionising drug discovery by expediting the
prediction of small molecule properties essential for developing new drugs.
These properties -- including absorption, distribution, metabolism and
excretion (ADME)-- are crucial in the early stages of drug development since
they provide an understanding of the course of the drug in the organism, i.e.,
the drug's pharmacokinetics. However, existing methods lack personalisation and
rely on manually crafted ML algorithms or pipelines, which can introduce
inefficiencies and biases into the process. To address these challenges, we
propose a novel evolutionary-based automated ML method (AutoML) specifically
designed for predicting small molecule properties, with a particular focus on
pharmacokinetics. Leveraging the advantages of grammar-based genetic
programming, our AutoML method streamlines the process by automatically
selecting algorithms and designing predictive pipelines tailored to the
particular characteristics of input molecular data. Results demonstrate
AutoML's effectiveness in selecting diverse ML algorithms, resulting in
comparable or even improved predictive performances compared to conventional
approaches. By offering personalised ML-driven pipelines, our method promises
to enhance small molecule research in drug discovery, providing researchers
with a valuable tool for accelerating the development of novel therapeutic
drugs.",cs.LG cs.AI,2024-08-01
A Cross-Domain Benchmark for Active Learning,,"Active Learning (AL) deals with identifying the most informative samples for
labeling to reduce data annotation costs for supervised learning tasks. AL
research suffers from the fact that lifts from literature generalize poorly and
that only a small number of repetitions of experiments are conducted. To
overcome these obstacles, we propose \emph{CDALBench}, the first active
learning benchmark which includes tasks in computer vision, natural language
processing and tabular learning. Furthermore, by providing an efficient, greedy
oracle, \emph{CDALBench} can be evaluated with 50 runs for each experiment. We
show, that both the cross-domain character and a large amount of repetitions
are crucial for sophisticated evaluation of AL research. Concretely, we show
that the superiority of specific methods varies over the different domains,
making it important to evaluate Active Learning with a cross-domain benchmark.
Additionally, we show that having a large amount of runs is crucial. With only
conducting three runs as often done in the literature, the superiority of
specific methods can strongly vary with the specific runs. This effect is so
strong, that, depending on the seed, even a well-established method's
performance can be significantly better and significantly worse than random for
the same dataset.",cs.LG,2024-08-01
"Augmenting Channel Simulator and Semi- Supervised Learning for Efficient
  Indoor Positioning",,"This work aims to tackle the labor-intensive and resource-consuming task of
indoor positioning by proposing an efficient approach. The proposed approach
involves the introduction of a semi-supervised learning (SSL) with a biased
teacher (SSLB) algorithm, which effectively utilizes both labeled and unlabeled
channel data. To reduce measurement expenses, unlabeled data is generated using
an updated channel simulator (UCHS), and then weighted by adaptive confidence
values to simplify the tuning of hyperparameters. Simulation results
demonstrate that the proposed strategy achieves superior performance while
minimizing measurement overhead and training expense compared to existing
benchmarks, offering a valuable and practical solution for indoor positioning.",eess.SP cs.AI,2024-08-01
ShellFuzzer: Grammar-based Fuzzing of Shell Interpreters,,"Despite its long-standing popularity and fundamental role in an operating
system, the Unix shell has rarely been a subject of academic research. In
particular, regardless of the significant progress in compiler testing, there
has been hardly any work applying automated testing techniques to detect faults
and vulnerabilities in shell interpreters.
  To address this important shortcoming, we present ShellFuzzer: a technique to
test Unix shell interpreters by automatically generating a large number of
shell scripts. ShellFuzzer combines grammar-based generation with selected
random mutations, so as to produce a diverse range of shell programs with
predictable characteristics (e.g., valid according to the language standard,
and free from destructive behavior).
  In our experimental evaluation, ShellFuzzer generated shell programs that
exposed 8 previously unknown issues that affected a recent version of the mksh
POSIX-compliant shell; the shell maintainers confirmed 7 of these issues, and
addressed them in the latest revisions of the shell's open-source
implementation.",cs.SE,2024-08-01
"A Qualitative Study on Using ChatGPT for Software Security: Perception
  vs. Practicality",,"Artificial Intelligence (AI) advancements have enabled the development of
Large Language Models (LLMs) that can perform a variety of tasks with
remarkable semantic understanding and accuracy. ChatGPT is one such LLM that
has gained significant attention due to its impressive capabilities for
assisting in various knowledge-intensive tasks. Due to the knowledge-intensive
nature of engineering secure software, ChatGPT's assistance is expected to be
explored for security-related tasks during the development/evolution of
software. To gain an understanding of the potential of ChatGPT as an emerging
technology for supporting software security, we adopted a two-fold approach.
Initially, we performed an empirical study to analyse the perceptions of those
who had explored the use of ChatGPT for security tasks and shared their views
on Twitter. It was determined that security practitioners view ChatGPT as
beneficial for various software security tasks, including vulnerability
detection, information retrieval, and penetration testing. Secondly, we
designed an experiment aimed at investigating the practicality of this
technology when deployed as an oracle in real-world settings. In particular, we
focused on vulnerability detection and qualitatively examined ChatGPT outputs
for given prompts within this prominent software security task. Based on our
analysis, responses from ChatGPT in this task are largely filled with generic
security information and may not be appropriate for industry use. To prevent
data leakage, we performed this analysis on a vulnerability dataset compiled
after the OpenAI data cut-off date from real-world projects covering 40
distinct vulnerability types and 12 programming languages. We assert that the
findings from this study would contribute to future research aimed at
developing and evaluating LLMs dedicated to software security.",cs.SE cs.AI cs.CR,2024-08-01
A Search for High-Threshold Qutrit Magic State Distillation Routines,,"Determining the best attainable threshold for qudit magic state distillation
is directly related to the question of whether or not contextuality is
sufficient for universal quantum computation. We carry out a search for
high-threshold magic state distillation routines for a highly-symmetric qutrit
magic state known as the strange state. Our search covers a large class of
$[[n,1]]_3$ qutrit stabilizer codes with up to 23 qutrits, and is facilitated
by a theorem that relates the distillation performance of a qudit stabilizer
code to its weight-enumerators. We could not find any code with $n<23$ qutrits
that distills the strange state with better than linear noise suppression,
other than the 11-qutrit Golay code. However, for $n=23$, we find over 600 CSS
codes that can distill the qutrit strange state with cubic noise suppression.
While none of these codes surpass the threshold of the 11-qutrit Golay code,
their existence suggests that, for large codes, the ability to distill the
qutrit strange state is somewhat generic.",quant-ph cs.IT math.CO math.IT,2024-08-01
"Efficient Patient Fine-Tuned Seizure Detection with a Tensor Kernel
  Machine",,"Recent developments in wearable devices have made accurate and efficient
seizure detection more important than ever. A challenge in seizure detection is
that patient-specific models typically outperform patient-independent models.
However, in a wearable device one typically starts with a patient-independent
model, until such patient-specific data is available. To avoid having to
construct a new classifier with this data, as required in conventional kernel
machines, we propose a transfer learning approach with a tensor kernel machine.
This method learns the primal weights in a compressed form using the canonical
polyadic decomposition, making it possible to efficiently update the weights of
the patient-independent model with patient-specific data. The results show that
this patient fine-tuned model reaches as high a performance as a
patient-specific SVM model with a model size that is twice as small as the
patient-specific model and ten times as small as the patient-independent model.",eess.SP cs.LG stat.ML,2024-08-01
"MonoMM: A Multi-scale Mamba-Enhanced Network for Real-time Monocular 3D
  Object Detection",,"Recent advancements in transformer-based monocular 3D object detection
techniques have exhibited exceptional performance in inferring 3D attributes
from single 2D images. However, most existing methods rely on
resource-intensive transformer architectures, which often lead to significant
drops in computational efficiency and performance when handling long sequence
data. To address these challenges and advance monocular 3D object detection
technology, we propose an innovative network architecture, MonoMM, a
Multi-scale \textbf{M}amba-Enhanced network for real-time Monocular 3D object
detection. This well-designed architecture primarily includes the following two
core modules: Focused Multi-Scale Fusion (FMF) Module, which focuses on
effectively preserving and fusing image information from different scales with
lower computational resource consumption. By precisely regulating the
information flow, the FMF module enhances the model adaptability and robustness
to scale variations while maintaining image details. Depth-Aware Feature
Enhancement Mamba (DMB) Module: It utilizes the fused features from image
characteristics as input and employs a novel adaptive strategy to globally
integrate depth information and visual information. This depth fusion strategy
not only improves the accuracy of depth estimation but also enhances the model
performance under different viewing angles and environmental conditions.
Moreover, the modular design of MonoMM provides high flexibility and
scalability, facilitating adjustments and optimizations according to specific
application needs. Extensive experiments conducted on the KITTI dataset show
that our method outperforms previous monocular methods and achieves real-time
detection.",cs.CV,2024-08-01
"Rapid and Power-Aware Learned Optimization for Modular Receive
  Beamforming",,"Multiple-input multiple-output (MIMO) systems play a key role in wireless
communication technologies. A widely considered approach to realize scalable
MIMO systems involves architectures comprised of multiple separate modules,
each with its own beamforming capability. Such models accommodate cell-free
massive MIMO and partially connected hybrid MIMO architectures. A core issue
with the implementation of modular MIMO arises from the need to rapidly set the
beampatterns of the modules, while maintaining their power efficiency. This
leads to challenging constrained optimization that should be repeatedly solved
on each coherence duration. In this work, we propose a power-oriented
optimization algorithm for beamforming in uplink modular hybrid MIMO systems,
which learns from data to operate rapidly. We derive our learned optimizer by
tackling the rate maximization objective using projected gradient ascent steps
with momentum. We then leverage data to tune the hyperparameters of the
optimizer, allowing it to operate reliably in a fixed and small number of
iterations while completely preserving its interpretable operation. We show how
power efficient beamforming can be encouraged by the learned optimizer, via
boosting architectures with low-resolution phase shifts and with deactivated
analog components. Numerical results show that our learn-to-optimize method
notably reduces the number of iterations and computation latency required to
reliably tune modular MIMO receivers, and that it allows obtaining desirable
balances between power efficient designs and throughput.",eess.SP cs.IT cs.LG math.IT,2024-08-01
"An Empirical Study on Challenges of Event Management in Microservice
  Architectures",,"Microservices emerged as a popular architectural style over the last decade.
Although microservices are designed to be self-contained, they must communicate
to realize business capabilities, creating dependencies among their data and
functionalities. Developers then resort to asynchronous, event-based
communication to fulfill such dependencies while reducing coupling. However,
developers are often oblivious to the inherent challenges of the asynchronous
and event-based paradigm, leading to frustrations and ultimately making them
reconsider the adoption of microservices. To make matters worse, there is a
scarcity of literature on the practices and challenges of designing,
implementing, testing, monitoring, and troubleshooting event-based
microservices.
  To fill this gap, this paper provides the first comprehensive
characterization of event management practices and challenges in microservices
based on a repository mining study of over 8000 Stack Overflow questions.
Moreover, 628 relevant questions were randomly sampled for an in-depth manual
investigation of challenges. We find that developers encounter many problems,
including large event payloads, modeling event schemas, auditing event flows,
and ordering constraints in processing events. This suggests that developers
are not sufficiently served by state-of-the-practice technologies. We provide
actionable implications to developers, technology providers, and researchers to
advance event management in microservices.",cs.SE cs.DB,2024-08-01
"Focus, Distinguish, and Prompt: Unleashing CLIP for Efficient and
  Flexible Scene Text Retrieval",,"Scene text retrieval aims to find all images containing the query text from
an image gallery. Current efforts tend to adopt an Optical Character
Recognition (OCR) pipeline, which requires complicated text detection and/or
recognition processes, resulting in inefficient and inflexible retrieval.
Different from them, in this work we propose to explore the intrinsic potential
of Contrastive Language-Image Pre-training (CLIP) for OCR-free scene text
retrieval. Through empirical analysis, we observe that the main challenges of
CLIP as a text retriever are: 1) limited text perceptual scale, and 2)
entangled visual-semantic concepts. To this end, a novel model termed FDP
(Focus, Distinguish, and Prompt) is developed. FDP first focuses on scene text
via shifting the attention to the text area and probing the hidden text
knowledge, and then divides the query text into content word and function word
for processing, in which a semantic-aware prompting scheme and a distracted
queries assistance module are utilized. Extensive experiments show that FDP
significantly enhances the inference speed while achieving better or
competitive retrieval accuracy compared to existing methods. Notably, on the
IIIT-STR benchmark, FDP surpasses the state-of-the-art model by 4.37% with a 4
times faster speed. Furthermore, additional experiments under phrase-level and
attribute-aware scene text retrieval settings validate FDP's particular
advantages in handling diverse forms of query text. The source code will be
publicly available at https://github.com/Gyann-z/FDP.",cs.CV cs.AI,2024-08-01
"An Experimental Evaluation of TEE technology Evolution: Benchmarking
  Transparent Approaches based on SGX, SEV, and TDX",,"Protection of data-in-use is a key priority, for which Trusted Execution
Environment (TEE) technology has unarguably emerged as a, possibly the most,
promising solution. Multiple server-side TEE offerings have been released over
the years, exhibiting substantial differences with respect to several aspects.
The first comer was Intel SGX, which featured Process-based TEE protection, an
efficient yet difficult to use approach. Some SGX limitations were (partially)
overcome by runtimes, notably: Gramine, Scone, and Occlum. A major paradigm
shift was later brought by AMD SEV, with VM-based TEE protection, which enabled
lift-and-shift deployment of legacy applications. This new paradigm has been
implemented by Intel only recently, in TDX. While the threat model of the
aforementioned TEE solutions has been widely discussed, a thorough performance
comparison is still lacking in the literature. This paper provides a
comparative evaluation of TDX, SEV, Gramine-SGX, and Occlum-SGX. We study
computational overhead and resource usage, under different operational
scenarios and using a diverse suite of legacy applications. By doing so, we
provide a reliable performance assessment under realistic conditions. We
explicitly emphasize that, at the time of writing, TDX was not yet available to
the public. Thus, the evaluation of TDX is a unique feature of this study.",cs.CR,2024-08-01
Ontological Relations from Word Embeddings,,"It has been reliably shown that the similarity of word embeddings obtained
from popular neural models such as BERT approximates effectively a form of
semantic similarity of the meaning of those words. It is therefore natural to
wonder if those embeddings contain enough information to be able to connect
those meanings through ontological relationships such as the one of
subsumption. If so, large knowledge models could be built that are capable of
semantically relating terms based on the information encapsulated in word
embeddings produced by pre-trained models, with implications not only for
ontologies (ontology matching, ontology evolution, etc.) but also on the
ability to integrate ontological knowledge in neural models. In this paper, we
test how embeddings produced by several pre-trained models can be used to
predict relations existing between classes and properties of popular
upper-level and general ontologies. We show that even a simple feed-forward
architecture on top of those embeddings can achieve promising accuracies, with
varying generalisation abilities depending on the input data. To achieve that,
we produce a dataset that can be used to further enhance those models, opening
new possibilities for applications integrating knowledge from web ontologies.",cs.AI,2024-08-01
"DiscipLink: Unfolding Interdisciplinary Information Seeking Process via
  Human-AI Co-Exploration",,"Interdisciplinary studies often require researchers to explore literature in
diverse branches of knowledge. Yet, navigating through the highly scattered
knowledge from unfamiliar disciplines poses a significant challenge. In this
paper, we introduce DiscipLink, a novel interactive system that facilitates
collaboration between researchers and large language models (LLMs) in
interdisciplinary information seeking (IIS). Based on users' topics of
interest, DiscipLink initiates exploratory questions from the perspectives of
possible relevant fields of study, and users can further tailor these
questions. DiscipLink then supports users in searching and screening papers
under selected questions by automatically expanding queries with
disciplinary-specific terminologies, extracting themes from retrieved papers,
and highlighting the connections between papers and questions. Our evaluation,
comprising a within-subject comparative experiment and an open-ended
exploratory study, reveals that DiscipLink can effectively support researchers
in breaking down disciplinary boundaries and integrating scattered knowledge in
diverse fields. The findings underscore the potential of LLM-powered tools in
fostering information-seeking practices and bolstering interdisciplinary
research.",cs.HC cs.AI cs.IR,2024-08-01
"How quantum and evolutionary algorithms can help each other: two
  examples",,"We investigate the potential of bio-inspired evolutionary algorithms for
designing quantum circuits with specific goals, focusing on two particular
tasks. The first one is motivated by the ideas of Artificial Life that are used
to reproduce stochastic cellular automata with given rules. We test the
robustness of quantum implementations of the cellular automata for different
numbers of quantum gates The second task deals with the sampling of quantum
circuits that generate highly entangled quantum states, which constitute an
important resource for quantum computing. In particular, an evolutionary
algorithm is employed to optimize circuits with respect to a fitness function
defined with the Mayer-Wallach entanglement measure. We demonstrate that, by
balancing the mutation rate between exploration and exploitation, we can find
entangling quantum circuits for up to five qubits. We also discuss the
trade-off between the number of gates in quantum circuits and the computational
costs of finding the gate arrangements leading to a strongly entangled state.
Our findings provide additional insight into the trade-off between the
complexity of a circuit and its performance, which is an important factor in
the design of quantum circuits.",quant-ph cs.NE,2024-08-01
Space-Time Isogeometric Method for a Nonlocal Parabolic Problem,,"In the present work, we focus on the space-time isogeometric discretization
of a parabolic problem with a nonlocal diffusion coefficient. The use of a
space-time discretization with smooth basis functions yields advantages in the
approximation of the solution. The existence of the unique solution for
continuous and discrete space-time variational formulations is proven. We also
establish the a priori error estimate for the space-time isogeometric scheme.
The non-linear system is linearized through Picard's method and a suitable
preconditioner for the linearized system is provided. Finally, to confirm the
theoretical findings, results of some numerical experiments are presented.",math.NA cs.NA,2024-08-01
"Reenact Anything: Semantic Video Motion Transfer Using Motion-Textual
  Inversion",,"Recent years have seen a tremendous improvement in the quality of video
generation and editing approaches. While several techniques focus on editing
appearance, few address motion. Current approaches using text, trajectories, or
bounding boxes are limited to simple motions, so we specify motions with a
single motion reference video instead. We further propose to use a pre-trained
image-to-video model rather than a text-to-video model. This approach allows us
to preserve the exact appearance and position of a target object or scene and
helps disentangle appearance from motion. Our method, called motion-textual
inversion, leverages our observation that image-to-video models extract
appearance mainly from the (latent) image input, while the text/image embedding
injected via cross-attention predominantly controls motion. We thus represent
motion using text/image embedding tokens. By operating on an inflated
motion-text embedding containing multiple text/image embedding tokens per
frame, we achieve a high temporal motion granularity. Once optimized on the
motion reference video, this embedding can be applied to various target images
to generate videos with semantically similar motions. Our approach does not
require spatial alignment between the motion reference video and target image,
generalizes across various domains, and can be applied to various tasks such as
full-body and face reenactment, as well as controlling the motion of inanimate
objects and the camera. We empirically demonstrate the effectiveness of our
method in the semantic video motion transfer task, significantly outperforming
existing methods in this context.",cs.CV cs.GR cs.LG,2024-08-01
Designing Efficient LLM Accelerators for Edge Devices,,"The increase in open-source availability of Large Language Models (LLMs) has
enabled users to deploy them on more and more resource-constrained edge devices
to reduce reliance on network connections and provide more privacy. However,
the high computation and memory demands of LLMs make their execution on
resource-constrained edge devices challenging and inefficient. To address this
issue, designing new and efficient edge accelerators for LLM inference is
crucial. FPGA-based accelerators are ideal for LLM acceleration due to their
reconfigurability, as they enable model-specific optimizations and higher
performance per watt. However, creating and integrating FPGA-based accelerators
for LLMs (particularly on edge devices) has proven challenging, mainly due to
the limited hardware design flows for LLMs in existing FPGA platforms.
  To tackle this issue, in this paper we first propose a new design platform,
named SECDA-LLM, that utilizes the SECDA methodology to streamline the process
of designing, integrating, and deploying efficient FPGA-based LLM accelerators
for the llama.cpp inference framework. We then demonstrate, through a case
study, the potential benefits of SECDA-LLM by creating a new MatMul accelerator
that supports block floating point quantized operations for LLMs. Our initial
accelerator design, deployed on the PYNQ-Z1 board, reduces latency 1.7 seconds
per token or ~2 seconds per word) by 11x over the dual-core Arm NEON-based CPU
execution for the TinyLlama model.",cs.AR cs.LG,2024-08-01
"Image Super-Resolution with Taylor Expansion Approximation and Large
  Field Reception",,"Self-similarity techniques are booming in blind super-resolution (SR) due to
accurate estimation of the degradation types involved in low-resolution images.
However, high-dimensional matrix multiplication within self-similarity
computation prohibitively consumes massive computational costs. We find that
the high-dimensional attention map is derived from the matrix multiplication
between Query and Key, followed by a softmax function. This softmax makes the
matrix multiplication between Query and Key inseparable, posing a great
challenge in simplifying computational complexity. To address this issue, we
first propose a second-order Taylor expansion approximation (STEA) to separate
the matrix multiplication of Query and Key, resulting in the complexity
reduction from $\mathcal{O}(N^2)$ to $\mathcal{O}(N)$. Then, we design a
multi-scale large field reception (MLFR) to compensate for the performance
degradation caused by STEA. Finally, we apply these two core designs to
laboratory and real-world scenarios by constructing LabNet and RealNet,
respectively. Extensive experimental results tested on five synthetic datasets
demonstrate that our LabNet sets a new benchmark in qualitative and
quantitative evaluations. Tested on the RealWorld38 dataset, our RealNet
achieves superior visual quality over existing methods. Ablation studies
further verify the contributions of STEA and MLFR towards both LabNet and
RealNet frameworks.",cs.CV cs.AI eess.IV,2024-08-01
"Towards Explainable and Interpretable Musical Difficulty Estimation: A
  Parameter-efficient Approach",,"Estimating music piece difficulty is important for organizing educational
music collections. This process could be partially automatized to facilitate
the educator's role. Nevertheless, the decisions performed by prevalent
deep-learning models are hardly understandable, which may impair the acceptance
of such a technology in music education curricula. Our work employs explainable
descriptors for difficulty estimation in symbolic music representations.
Furthermore, through a novel parameter-efficient white-box model, we outperform
previous efforts while delivering interpretable results. These comprehensible
outcomes emulate the functionality of a rubric, a tool widely used in music
education. Our approach, evaluated in piano repertoire categorized in 9
classes, achieved 41.4% accuracy independently, with a mean squared error (MSE)
of 1.7, showing precise difficulty estimation. Through our baseline, we
illustrate how building on top of past research can offer alternatives for
music difficulty assessment which are explainable and interpretable. With this,
we aim to promote a more effective communication between the Music Information
Retrieval (MIR) community and the music education one.",cs.SD cs.AI cs.IR eess.AS,2024-08-01
"Enhance the Detection of DoS and Brute Force Attacks within the MQTT
  Environment through Feature Engineering and Employing an Ensemble Technique",,"The rapid development of the Internet of Things (IoT) environment has
introduced unprecedented levels of connectivity and automation. The Message
Queuing Telemetry Transport (MQTT) protocol has become recognized in IoT
applications due to its lightweight and efficient features; however, this
simplicity also renders MQTT vulnerable to multiple attacks that can be
launched against the protocol, including denial of service (DoS) and
brute-force attacks. This study aims to improve the detection of intrusion DoS
and brute-force attacks in an MQTT traffic intrusion detection system (IDS).
Our approach utilizes the MQTT dataset for model training by employing
effective feature engineering and ensemble learning techniques. Following our
analysis and comparison, we identified the top 10 features demonstrating the
highest effectiveness, leading to improved model accuracy. We used supervised
machine learning models, including Random Forest, Decision Trees, k-Nearest
Neighbors, and XGBoost, in combination with ensemble classifiers. Stacking,
voting, and bagging ensembles utilize these four supervised machine-learning
methods to combine models. This study's results illustrate the proposed
technique's efficacy in enhancing the accuracy of detecting DoS and brute-force
attacks in MQTT traffic. Stacking and voting classifiers achieved the highest
accuracy of 0.9538. Our approach outperforms the most recent study that
utilized the same dataset.",cs.NI,2024-08-01
"HBot: A Chatbot for Healthcare Applications in Traditional Chinese
  Medicine Based on Human Body 3D Visualization",,"The unique diagnosis and treatment techniques and remarkable clinical
efficacy of traditional Chinese medicine (TCM) make it play an important role
in the field of elderly care and healthcare, especially in the rehabilitation
of some common chronic diseases of the elderly. Therefore, building a TCM
chatbot for healthcare application will help users obtain consultation services
in a direct and natural way. However, concepts such as acupuncture points
(acupoints) and meridians involved in TCM always appear in the consultation,
which cannot be displayed intuitively. To this end, we develop a
\textbf{h}ealthcare chat\textbf{bot} (HBot) based on a human body model in 3D
and knowledge graph, which provides conversational services such as knowledge
Q\&A, prescription recommendation, moxibustion therapy recommendation, and
acupoint search. When specific acupoints are involved in the conversations
between user and HBot, the 3D body will jump to the corresponding acupoints and
highlight them. Moreover, Hbot can also be used in training scenarios to
accelerate the teaching process of TCM by intuitively displaying acupuncture
points and knowledge cards. The demonstration video is available at
https://www.youtube.com/watch?v=UhQhutSKkTU . Our code and dataset are publicly
available at Gitee: https://gitee.com/plabrolin/interactive-3d-acup.git",cs.AI,2024-08-01
A Systematic Review on Long-Tailed Learning,,"Long-tailed data is a special type of multi-class imbalanced data with a very
large amount of minority/tail classes that have a very significant combined
influence. Long-tailed learning aims to build high-performance models on
datasets with long-tailed distributions, which can identify all the classes
with high accuracy, in particular the minority/tail classes. It is a
cutting-edge research direction that has attracted a remarkable amount of
research effort in the past few years. In this paper, we present a
comprehensive survey of latest advances in long-tailed visual learning. We
first propose a new taxonomy for long-tailed learning, which consists of eight
different dimensions, including data balancing, neural architecture, feature
enrichment, logits adjustment, loss function, bells and whistles, network
optimization, and post hoc processing techniques. Based on our proposed
taxonomy, we present a systematic review of long-tailed learning methods,
discussing their commonalities and alignable differences. We also analyze the
differences between imbalance learning and long-tailed learning approaches.
Finally, we discuss prospects and future directions in this field.",cs.LG cs.AI cs.CV cs.MM,2024-08-01
"SF-TIM: A Simple Framework for Enhancing Quadrupedal Robot Jumping
  Agility by Combining Terrain Imagination and Measurement",,"Dynamic jumping on high platforms and over gaps differentiates legged robots
from wheeled counterparts. Compared to walking on rough terrains, dynamic
locomotion on abrupt surfaces requires fusing proprioceptive and exteroceptive
perception for explosive movements. In this paper, we propose SF-TIM (Simple
Framework combining Terrain Imagination and Measurement), a single-policy
method that enhances quadrupedal robot jumping agility, while preserving their
fundamental blind walking capabilities. In addition, we introduce a
terrain-guided reward design specifically to assist quadrupedal robots in high
jumping, improving their performance in this task. To narrow the
simulation-to-reality gap in quadrupedal robot learning, we introduce a stable
and high-speed elevation map generation framework, enabling zero-shot
simulation-to-reality transfer of locomotion ability. Our algorithm has been
deployed and validated on both the small-/large-size quadrupedal robots,
demonstrating its effectiveness in real-world applications: the robot has
successfully traversed various high platforms and gaps, showing the robustness
of our proposed approach. A demo video has been made available at
https://flysoaryun.github.io/SF-TIM.",cs.RO,2024-08-01
"Absolute-value based preconditioner for complex-shifted Laplacian
  systems",,"The complex-shifted Laplacian systems arising in a wide range of
applications. In this work, we propose an absolute-value based preconditioner
for solving the complex-shifted Laplacian system. In our approach, the
complex-shifted Laplacian system is equivalently rewritten as a $2\times 2$
block real linear system. With the Toeplitz structure of uniform-grid
discretization of the constant-coefficient Laplacian operator, the absolute
value of the block real matrix is fast invertible by means of fast sine
transforms. For more general coefficient function, we then average the
coefficient function and take the absolute value of the averaged matrix as our
preconditioner. With assumptions on the complex shift, we theoretically prove
that the eigenvalues of the preconditioned matrix in absolute value are upper
and lower bounded by constants independent of matrix size, indicating a
matrix-size independent linear convergence rate of MINRES solver.
Interestingly, numerical results show that the proposed preconditioner is still
efficient even if the assumptions on the complex shift are not met. The fast
invertibility of the proposed preconditioner and the robust convergence rate of
the preconditioned MINRES solver lead to a linearithmic (nearly optimal)
complexity of the proposed solver. The proposed preconditioner is compared with
several state-of-the-art preconditioners via several numerical examples to
demonstrate the efficiency of the proposed preconditioner.",math.NA cs.NA,2024-08-01
"Multi-label Sewer Pipe Defect Recognition with Mask Attention Feature
  Enhancement and Label Correlation Learning",,"The coexistence of multiple defect categories as well as the substantial
class imbalance problem significantly impair the detection of sewer pipeline
defects. To solve this problem, a multi-label pipe defect recognition method is
proposed based on mask attention guided feature enhancement and label
correlation learning. The proposed method can achieve current approximate
state-of-the-art classification performance using just 1/16 of the Sewer-ML
training dataset and exceeds the current best method by 11.87\% in terms of F2
metric on the full dataset, while also proving the superiority of the model.
The major contribution of this study is the development of a more efficient
model for identifying and locating multiple defects in sewer pipe images for a
more accurate sewer pipeline condition assessment. Moreover, by employing class
activation maps, our method can accurately pinpoint multiple defect categories
in the image which demonstrates a strong model interpretability. Our code is
available at
\href{https://github.com/shengyu27/MA-Q2L}{\textcolor{black}{https://github.com/shengyu27/MA-Q2L.}",cs.CV,2024-08-01
"Graph Representation Learning via Causal Diffusion for
  Out-of-Distribution Recommendation",,"Graph Neural Networks (GNNs)-based recommendation algorithms typically assume
that training and testing data are drawn from independent and identically
distributed (IID) spaces. However, this assumption often fails in the presence
of out-of-distribution (OOD) data, resulting in significant performance
degradation. In this study, we construct a Structural Causal Model (SCM) to
analyze interaction data, revealing that environmental confounders (e.g., the
COVID-19 pandemic) lead to unstable correlations in GNN-based models, thus
impairing their generalization to OOD data. To address this issue, we propose a
novel approach, graph representation learning via causal diffusion
(CausalDiffRec) for OOD recommendation. This method enhances the model's
generalization on OOD data by eliminating environmental confounding factors and
learning invariant graph representations. Specifically, we use backdoor
adjustment and variational inference to infer the real environmental
distribution, thereby eliminating the impact of environmental confounders. This
inferred distribution is then used as prior knowledge to guide the
representation learning in the reverse phase of the diffusion process to learn
the invariant representation. In addition, we provide a theoretical derivation
that proves optimizing the objective function of CausalDiffRec can encourage
the model to learn environment-invariant graph representations, thereby
achieving excellent generalization performance in recommendations under
distribution shifts. Our extensive experiments validate the effectiveness of
CausalDiffRec in improving the generalization of OOD data, and the average
improvement is up to 10.69% on Food, 18.83% on KuaiRec, 22.41% on Yelp2018, and
11.65% on Douban datasets.",cs.LG cs.AI cs.IR cs.SI,2024-08-01
GalleryGPT: Analyzing Paintings with Large Multimodal Models,,"Artwork analysis is important and fundamental skill for art appreciation,
which could enrich personal aesthetic sensibility and facilitate the critical
thinking ability. Understanding artworks is challenging due to its subjective
nature, diverse interpretations, and complex visual elements, requiring
expertise in art history, cultural background, and aesthetic theory. However,
limited by the data collection and model ability, previous works for
automatically analyzing artworks mainly focus on classification, retrieval, and
other simple tasks, which is far from the goal of AI. To facilitate the
research progress, in this paper, we step further to compose comprehensive
analysis inspired by the remarkable perception and generation ability of large
multimodal models. Specifically, we first propose a task of composing paragraph
analysis for artworks, i.e., painting in this paper, only focusing on visual
characteristics to formulate more comprehensive understanding of artworks. To
support the research on formal analysis, we collect a large dataset
PaintingForm, with about 19k painting images and 50k analysis paragraphs. We
further introduce a superior large multimodal model for painting analysis
composing, dubbed GalleryGPT, which is slightly modified and fine-tuned based
on LLaVA architecture leveraging our collected data. We conduct formal analysis
generation and zero-shot experiments across several datasets to assess the
capacity of our model. The results show remarkable performance improvements
comparing with powerful baseline LMMs, demonstrating its superb ability of art
analysis and generalization. \textcolor{blue}{The codes and model are available
at: https://github.com/steven640pixel/GalleryGPT.",cs.CL cs.CV cs.MM,2024-08-01
Explainable Emotion Decoding for Human and Computer Vision,,"Modern Machine Learning (ML) has significantly advanced various research
fields, but the opaque nature of ML models hinders their adoption in several
domains. Explainable AI (XAI) addresses this challenge by providing additional
information to help users understand the internal decision-making process of ML
models. In the field of neuroscience, enriching a ML model for brain decoding
with attribution-based XAI techniques means being able to highlight which brain
areas correlate with the task at hand, thus offering valuable insights to
domain experts. In this paper, we analyze human and Computer Vision (CV)
systems in parallel, training and explaining two ML models based respectively
on functional Magnetic Resonance Imaging (fMRI) and movie frames. We do so by
leveraging the ""StudyForrest"" dataset, which includes functional Magnetic
Resonance Imaging (fMRI) scans of subjects watching the ""Forrest Gump"" movie,
emotion annotations, and eye-tracking data. For human vision the ML task is to
link fMRI data with emotional annotations, and the explanations highlight the
brain regions strongly correlated with the label. On the other hand, for
computer vision, the input data is movie frames, and the explanations are
pixel-level heatmaps. We cross-analyzed our results, linking human attention
(obtained through eye-tracking) with XAI saliency on CV models and brain region
activations. We show how a parallel analysis of human and computer vision can
provide useful information for both the neuroscience community (allocation
theory) and the ML community (biological plausibility of convolutional models).",cs.CV eess.IV q-bio.NC,2024-08-01
"Chance-Constrained Information-Theoretic Stochastic Model Predictive
  Control with Safety Shielding",,"This paper introduces a novel nonlinear stochastic model predictive control
path integral (MPPI) method, which considers chance constraints on system
states. The proposed belief-space stochastic MPPI (BSS-MPPI) applies
Monte-Carlo sampling to evaluate state distributions resulting from underlying
systematic disturbances, and utilizes a Control Barrier Function (CBF) inspired
heuristic in belief space to fulfill the specified chance constraints. Compared
to several previous stochastic predictive control methods, our approach applies
to general nonlinear dynamics without requiring the computationally expensive
system linearization step. Moreover, the BSS-MPPI controller can solve
optimization problems without limiting the form of the objective function and
chance constraints. By multi-threading the sampling process using a GPU, we can
achieve fast real-time planning for time- and safety-critical tasks such as
autonomous racing. Our results on a realistic race-car simulation study show
significant reductions in constraint violation compared to some of the prior
MPPI approaches, while being comparable in computation times.",cs.RO,2024-08-01
"SegStitch: Multidimensional Transformer for Robust and Efficient Medical
  Imaging Segmentation",,"Medical imaging segmentation plays a significant role in the automatic
recognition and analysis of lesions. State-of-the-art methods, particularly
those utilizing transformers, have been prominently adopted in 3D semantic
segmentation due to their superior performance in scalability and
generalizability. However, plain vision transformers encounter challenges due
to their neglect of local features and their high computational complexity. To
address these challenges, we introduce three key contributions: Firstly, we
proposed SegStitch, an innovative architecture that integrates transformers
with denoising ODE blocks. Instead of taking whole 3D volumes as inputs, we
adapt axial patches and customize patch-wise queries to ensure semantic
consistency. Additionally, we conducted extensive experiments on the BTCV and
ACDC datasets, achieving improvements up to 11.48% and 6.71% respectively in
mDSC, compared to state-of-the-art methods. Lastly, our proposed method
demonstrates outstanding efficiency, reducing the number of parameters by 36.7%
and the number of FLOPS by 10.7% compared to UNETR. This advancement holds
promising potential for adapting our method to real-world clinical practice.
The code will be available at https://github.com/goblin327/SegStitch",cs.CV,2024-08-01
"A note about high-order semi-implicit differentiation: application to a
  numerical integration scheme with Taylor-based compensated error",,"In this brief, we discuss the implementation of a third order semi-implicit
differentiator as a complement of the recent work by the author that proposes
an interconnected semi-implicit Euler double differentiators algorithm through
Taylor expansion refinement. The proposed algorithm is dual to the
interconnected approach since it offers alternative flexibility to be tuned and
to be implemented in real-time processes. In particular, an application to a
numerical integration scheme is presented as the Taylor refinement can be of
interest to improve the global convergence. Numerical results are presented to
support the rightness of the proposed method.",math.NA cs.NA cs.SY eess.SY,2024-08-01
"How Effective are Self-Supervised Models for Contact Identification in
  Videos",,"The exploration of video content via Self-Supervised Learning (SSL) models
has unveiled a dynamic field of study, emphasizing both the complex challenges
and unique opportunities inherent in this area. Despite the growing body of
research, the ability of SSL models to detect physical contacts in videos
remains largely unexplored, particularly the effectiveness of methods such as
downstream supervision with linear probing or full fine-tuning. This work aims
to bridge this gap by employing eight different convolutional neural networks
(CNNs) based video SSL models to identify instances of physical contact within
video sequences specifically. The Something-Something v2 (SSv2) and
Epic-Kitchen (EK-100) datasets were chosen for evaluating these approaches due
to the promising results on UCF101 and HMDB51, coupled with their limited prior
assessment on SSv2 and EK-100. Additionally, these datasets feature diverse
environments and scenarios, essential for testing the robustness and accuracy
of video-based models. This approach not only examines the effectiveness of
each model in recognizing physical contacts but also explores the performance
in the action recognition downstream task. By doing so, valuable insights into
the adaptability of SSL models in interpreting complex, dynamic visual
information are contributed.",cs.CV,2024-08-01
"To Change Or To Stick: Unveiling The Consistency Of Cyber Criminal
  Signatures Through Statistical Analysis",,"This study unveils the elusive presence of criminal signatures in cyberspace,
validating for the first time their existence through statistical evidence. By
applying the A priori algorithm to the modus operandi of Advanced Persistent
Threats, extracted from an extensive corpus of over 17,000 articles spanning
2007 to 2020, we highlight the enduring patterns leveraged by sophisticated
cyber criminals. Our findings verify the existence of unique signatures
associated with advanced cybercriminals, bridging a crucial gap in current
understanding of human behavior in cyber-attacks. This pivotal research sets
the foundation for an entirely new academic intersection in cybersecurity and
computational criminology.",cs.CR,2024-08-01
"If It Looks Like a Rootkit and Deceives Like a Rootkit: A Critical
  Examination of Kernel-Level Anti-Cheat Systems",,"Addressing a critical aspect of cybersecurity in online gaming, this paper
systematically evaluates the extent to which kernel-level anti-cheat systems
mirror the properties of rootkits, highlighting the importance of
distinguishing between protective and potentially invasive software. After
establishing a definition for rootkits (making distinctions between rootkits
and simple kernel-level applications) and defining metrics to evaluate such
software, we introduce four widespread kernel-level anti-cheat solutions. We
lay out the inner workings of these types of software, assess them according to
our previously established definitions, and discuss ethical considerations and
the possible privacy infringements introduced by such programs. Our analysis
shows two of the four anti-cheat solutions exhibiting rootkit-like behaviour,
threatening the privacy and the integrity of the system. This paper thus
provides crucial insights for researchers and developers in the field of gaming
security and software engineering, highlighting the need for informed
development practices that carefully consider the intersection of effective
anti-cheat mechanisms and user privacy.",cs.CR cs.CY,2024-08-01
"Quantum Program Testing Through Commuting Pauli Strings on IBM's Quantum
  Computers",,"The most promising applications of quantum computing are centered around
solving search and optimization tasks, particularly in fields such as physics
simulations, quantum chemistry, and finance. However, the current quantum
software testing methods face practical limitations when applied in industrial
contexts: (i) they do not apply to quantum programs most relevant to the
industry, (ii) they require a full program specification, which is usually not
available for these programs, and (iii) they are incompatible with error
mitigation methods currently adopted by main industry actors like IBM. To
address these challenges, we present QOPS, a novel quantum software testing
approach. QOPS introduces a new definition of test cases based on Pauli strings
to improve compatibility with different quantum programs. QOPS also introduces
a new test oracle that can be directly integrated with industrial APIs such as
IBM's Estimator API and can utilize error mitigation methods for testing on
real noisy quantum computers. We also leverage the commuting property of Pauli
strings to relax the requirement of having complete program specifications,
making QOPS practical for testing complex quantum programs in industrial
settings. We empirically evaluate QOPS on 194,982 real quantum programs,
demonstrating effective performance in test assessment compared to the
state-of-the-art with a perfect F1-score, precision, and recall. Furthermore,
we validate the industrial applicability of QOPS by assessing its performance
on IBM's three real quantum computers, incorporating both industrial and
open-source error mitigation methods.",cs.SE,2024-08-01
Hacked in Translation -- from Subtitles to Complete Takeover,,"Check Point researchers revealed a new attack vector which threatens millions
of users worldwide - attack by subtitles. By crafting malicious subtitle files,
which are then downloaded by a victim's media player, attackers can take
complete control over any type of device via vulnerabilities found in many
popular streaming platforms, including VLC, Kodi (XBMC), Popcorn-Time and
strem.io. We estimate there are approximately 200 million video players and
streamers that currently run the vulnerable software, making this one of the
most widespread, easily accessed and zero-resistance vulnerability reported in
recent years.
  Our research reveals a new possible attack vector, using a completely
overlooked technique in which the cyberattack is delivered when movie subtitles
are automatically loaded from online repositories by the user's media player.
These subtitles repositories are, in practice, treated as a trusted source by
the user or media player; our research also reveals that those repositories can
be manipulated and be made to award the attacker's malicious subtitles a high
score, which results in those specific subtitles being served to the user. This
method requires little or no deliberate action on the part of the user, making
it all the more dangerous.
  Unlike traditional attack vectors, which security firms and users are widely
aware of, movie subtitles are perceived as nothing more than benign text files.
This means users, Anti-Virus software, and other security solutions vet them
without trying to assess their real nature, leaving millions of users exposed
to this risk.",cs.CR,2024-08-01
"Spatial Weather, Socio-Economic and Political Risks in Probabilistic
  Load Forecasting",,"Accurate forecasts of the impact of spatial weather and pan-European
socio-economic and political risks on hourly electricity demand for the
mid-term horizon are crucial for strategic decision-making amidst the inherent
uncertainty. Most importantly, these forecasts are essential for the
operational management of power plants, ensuring supply security and grid
stability, and in guiding energy trading and investment decisions. The primary
challenge for this forecasting task lies in disentangling the multifaceted
drivers of load, which include national deterministic (daily, weekly, annual,
and holiday patterns) and national stochastic weather and autoregressive
effects. Additionally, transnational stochastic socio-economic and political
effects add further complexity, in particular, due to their non-stationarity.
To address this challenge, we present an interpretable probabilistic mid-term
forecasting model for the hourly load that captures, besides all deterministic
effects, the various uncertainties in load. This model recognizes transnational
dependencies across 24 European countries, with multivariate modeled
socio-economic and political states and cross-country dependent forecasting.
Built from interpretable Generalized Additive Models (GAMs), the model enables
an analysis of the transmission of each incorporated effect to the
hour-specific load. Our findings highlight the vulnerability of countries
reliant on electric heating under extreme weather scenarios. This emphasizes
the need for high-resolution forecasting of weather effects on pan-European
electricity consumption especially in anticipation of widespread electric
heating adoption.",stat.AP cs.CE econ.GN q-fin.EC q-fin.RM stat.CO,2024-08-01
"Block-Operations: Using Modular Routing to Improve Compositional
  Generalization",,"We explore the hypothesis that poor compositional generalization in neural
networks is caused by difficulties with learning effective routing. To solve
this problem, we propose the concept of block-operations, which is based on
splitting all activation tensors in the network into uniformly sized blocks and
using an inductive bias to encourage modular routing and modification of these
blocks. Based on this concept we introduce the Multiplexer, a new architectural
component that enhances the Feed Forward Neural Network (FNN). We
experimentally confirm that Multiplexers exhibit strong compositional
generalization. On both a synthetic and a realistic task our model was able to
learn the underlying process behind the task, whereas both FNNs and
Transformers were only able to learn heuristic approximations. We propose as
future work to use the principles of block-operations to improve other existing
architectures.",cs.LG,2024-08-01
"Multiscale topology optimization of functionally graded lattice
  structures based on physics-augmented neural network material models",,"We present a new framework for the simultaneous optimiziation of both the
topology as well as the relative density grading of cellular structures and
materials, also known as lattices. Due to manufacturing constraints, the
optimization problem falls into the class of NP-complete mixed-integer
nonlinear programming problems. To tackle this difficulty, we obtain a relaxed
problem from a multiplicative split of the relative density and a penalization
approach. The sensitivities of the objective function are derived such that any
gradient-based solver might be applied for the iterative update of the design
variables. In a next step, we introduce a material model that is parametric in
the design variables of interest and suitable to describe the isotropic
deformation behavior of quasi-stochastic lattices. For that, we derive and
implement further physical constraints and enhance a physics-augmented neural
network from the literature that was formulated initially for rhombic
materials. Finally, to illustrate the applicability of the method, we
incorporate the material model into our computational framework and exemplary
optimize two-and three-dimensional benchmark structures as well as a complex
aircraft component.",cs.CE,2024-08-01
"Comparative Study of Data-driven Area Inertia Estimation Approaches on
  WECC Power Systems",,"With the increasing integration of inverter-based resources into the power
grid, there has been a notable reduction in system inertia, potentially
compromising frequency stability. To assess the suitability of existing area
inertia estimation techniques for real-world power systems, this paper presents
a rigorous comparative analysis of system identification, measurement
reconstruction, and electromechanical oscillation-based area inertia estimation
methodologies, specifically applied to the large-scale and multi-area WECC
240-bus power system. Comprehensive results show that the system
identification-based approach exhibits superior robustness and accuracy
relative to its counterparts.",eess.SY cs.SY,2024-08-01
"FlowGPT: Exploring Domains, Output Modalities, and Goals of
  Community-Generated AI Chatbots",,"The advent of Generative AI and Large Language Models has not only enhanced
the intelligence of interactive applications but also catalyzed the formation
of communities passionate about customizing these AI capabilities. FlowGPT, an
emerging platform for sharing AI prompts and use cases, exemplifies this trend,
attracting many creators who develop and share chatbots with a broader
community. Despite its growing popularity, there remains a significant gap in
understanding the types and purposes of the AI tools created and shared by
community members. In this study, we delve into FlowGPT and present our
preliminary findings on the domain, output modality, and goals of chatbots. We
aim to highlight common types of AI applications and identify future directions
for research in AI-sharing communities.",cs.HC,2024-08-01
"VecAug: Unveiling Camouflaged Frauds with Cohort Augmentation for
  Enhanced Detection",,"Fraud detection presents a challenging task characterized by ever-evolving
fraud patterns and scarce labeled data. Existing methods predominantly rely on
graph-based or sequence-based approaches. While graph-based approaches connect
users through shared entities to capture structural information, they remain
vulnerable to fraudsters who can disrupt or manipulate these connections. In
contrast, sequence-based approaches analyze users' behavioral patterns,
offering robustness against tampering but overlooking the interactions between
similar users. Inspired by cohort analysis in retention and healthcare, this
paper introduces VecAug, a novel cohort-augmented learning framework that
addresses these challenges by enhancing the representation learning of target
users with personalized cohort information. To this end, we first propose a
vector burn-in technique for automatic cohort identification, which retrieves a
task-specific cohort for each target user. Then, to fully exploit the cohort
information, we introduce an attentive cohort aggregation technique for
augmenting target user representations. To improve the robustness of such
cohort augmentation, we also propose a novel label-aware cohort neighbor
separation mechanism to distance negative cohort neighbors and calibrate the
aggregated cohort information. By integrating this cohort information with
target user representations, VecAug enhances the modeling capacity and
generalization capabilities of the model to be augmented. Our framework is
flexible and can be seamlessly integrated with existing fraud detection models.
We deploy our framework on e-commerce platforms and evaluate it on three fraud
detection datasets, and results show that VecAug improves the detection
performance of base models by up to 2.48\% in AUC and 22.5\% in R@P$_{0.9}$,
outperforming state-of-the-art methods significantly.",cs.LG,2024-08-01
"Low-Power Vibration-Based Predictive Maintenance for Industry 4.0 using
  Neural Networks: A Survey",,"The advancements in smart sensors for Industry 4.0 offer ample opportunities
for low-powered predictive maintenance and condition monitoring. However,
traditional approaches in this field rely on processing in the cloud, which
incurs high costs in energy and storage. This paper investigates the potential
of neural networks for low-power on-device computation of vibration sensor data
for predictive maintenance. We review the literature on Spiking Neural Networks
(SNNs) and Artificial Neuronal Networks (ANNs) for vibration-based predictive
maintenance by analyzing datasets, data preprocessing, network architectures,
and hardware implementations. Our findings suggest that no satisfactory
standard benchmark dataset exists for evaluating neural networks in predictive
maintenance tasks. Furthermore frequency domain transformations are commonly
employed for preprocessing. SNNs mainly use shallow feed forward architectures,
whereas ANNs explore a wider range of models and deeper networks. Finally, we
highlight the need for future research on hardware implementations of neural
networks for low-power predictive maintenance applications and the development
of a standardized benchmark dataset.",cs.LG,2024-08-01
A new approach for encoding code and assisting code understanding,,"Some companies(e.g., Microsoft Research and Google DeepMind) have discovered
some of the limitations of GPTs autoregressive paradigm next-word prediction,
manifested in the model lack of planning, working memory, backtracking, and
reasoning skills. GPTs rely on a local and greedy process of generating the
next word, without a global understanding of the task or the output.We have
confirmed the above limitations through specialized empirical studies of code
comprehension. Although GPT4 is good at producing fluent and coherent text, it
cannot handle complex logic and generate new code that haven not been seen, and
it relies too much on the formatting of the prompt to generate the correct
code.We propose a new paradigm for code understanding that goes beyond the
next-word prediction paradigm, inspired by the successful application of
diffusion techniques to image generation(Dalle2, Sora) and protein structure
generation(AlphaFold3), which have no autoregressive constraints.Instead of
encoding the code in a form that mimics natural language, we encode the code as
a heterogeneous image paradigm with a memory of global information that mimics
both images and protein structures.We then refer to Sora's CLIP upstream
text-to-image encoder model to design a text-to-code encoder model that can be
applied to various downstream code understanding tasks.The model learns the
global understanding of code under the new paradigm heterogeneous image,
connects the encoding space of text and code, and encodes the input of text
into the vector of code most similar to it.Using self-supervised comparative
learning on 456,360 text-code pairs, the model achieved a zero-shot prediction
of new data. This work is the basis for future work on code generation using
diffusion techniques under a new paradigm to avoid autoregressive limitations.",cs.AI,2024-08-01
Jailbreaking Text-to-Image Models with LLM-Based Agents,,"Recent advancements have significantly improved automated task-solving
capabilities using autonomous agents powered by large language models (LLMs).
However, most LLM-based agents focus on dialogue, programming, or specialized
domains, leaving gaps in addressing generative AI safety tasks. These gaps are
primarily due to the challenges posed by LLM hallucinations and the lack of
clear guidelines. In this paper, we propose Atlas, an advanced LLM-based
multi-agent framework that integrates an efficient fuzzing workflow to target
generative AI models, specifically focusing on jailbreak attacks against
text-to-image (T2I) models with safety filters. Atlas utilizes a
vision-language model (VLM) to assess whether a prompt triggers the T2I model's
safety filter. It then iteratively collaborates with both LLM and VLM to
generate an alternative prompt that bypasses the filter. Atlas also enhances
the reasoning abilities of LLMs in attack scenarios by leveraging multi-agent
communication, in-context learning (ICL) memory mechanisms, and the
chain-of-thought (COT) approach. Our evaluation demonstrates that Atlas
successfully jailbreaks several state-of-the-art T2I models in a black-box
setting, which are equipped with multi-modal safety filters. In addition, Atlas
outperforms existing methods in both query efficiency and the quality of the
generated images.",cs.CR cs.AI cs.LG,2024-08-01
"Identifying the Hierarchical Emotional Areas in the Human Brain Through
  Information Fusion",,"The brain basis of emotion has consistently received widespread attention,
attracting a large number of studies to explore this cutting-edge topic.
However, the methods employed in these studies typically only model the
pairwise relationship between two brain regions, while neglecting the
interactions and information fusion among multiple brain
regions$\unicode{x2014}$one of the key ideas of the psychological
constructionist hypothesis. To overcome the limitations of traditional methods,
this study provides an in-depth theoretical analysis of how to maximize
interactions and information fusion among brain regions. Building on the
results of this analysis, we propose to identify the hierarchical emotional
areas in the human brain through multi-source information fusion and graph
machine learning methods. Comprehensive experiments reveal that the identified
hierarchical emotional areas, from lower to higher levels, primarily facilitate
the fundamental process of emotion perception, the construction of basic
psychological operations, and the coordination and integration of these
operations. Overall, our findings provide unique insights into the brain
mechanisms underlying specific emotions based on the psychological
constructionist hypothesis.",cs.HC cs.DM cs.LG,2024-08-01
"Hilbert curves for efficient exploratory landscape analysis
  neighbourhood sampling",,"Landscape analysis aims to characterise optimisation problems based on their
objective (or fitness) function landscape properties. The problem search space
is typically sampled, and various landscape features are estimated based on the
samples. One particularly salient set of features is information content, which
requires the samples to be sequences of neighbouring solutions, such that the
local relationships between consecutive sample points are preserved. Generating
such spatially correlated samples that also provide good search space coverage
is challenging. It is therefore common to first obtain an unordered sample with
good search space coverage, and then apply an ordering algorithm such as the
nearest neighbour to minimise the distance between consecutive points in the
sample. However, the nearest neighbour algorithm becomes computationally
prohibitive in higher dimensions, thus there is a need for more efficient
alternatives. In this study, Hilbert space-filling curves are proposed as a
method to efficiently obtain high-quality ordered samples. Hilbert curves are a
special case of fractal curves, and guarantee uniform coverage of a bounded
search space while providing a spatially correlated sample. We study the
effectiveness of Hilbert curves as samplers, and discover that they are capable
of extracting salient features at a fraction of the computational cost compared
to Latin hypercube sampling with post-factum ordering. Further, we investigate
the use of Hilbert curves as an ordering strategy, and find that they order the
sample significantly faster than the nearest neighbour ordering, without
sacrificing the saliency of the extracted features.",cs.LG cs.AI cs.NE,2024-08-01
"Contrastive Learning with Dynamic Localized Repulsion for Brain Age
  Prediction on 3D Stiffness Maps",,"In the field of neuroimaging, accurate brain age prediction is pivotal for
uncovering the complexities of brain aging and pinpointing early indicators of
neurodegenerative conditions. Recent advancements in self-supervised learning,
particularly in contrastive learning, have demonstrated greater robustness when
dealing with complex datasets. However, current approaches often fall short in
generalizing across non-uniformly distributed data, prevalent in medical
imaging scenarios. To bridge this gap, we introduce a novel contrastive loss
that adapts dynamically during the training process, focusing on the localized
neighborhoods of samples. Moreover, we expand beyond traditional structural
features by incorporating brain stiffness, a mechanical property previously
underexplored yet promising due to its sensitivity to age-related changes. This
work presents the first application of self-supervised learning to brain
mechanical properties, using compiled stiffness maps from various clinical
studies to predict brain age. Our approach, featuring dynamic localized loss,
consistently outperforms existing state-of-the-art methods, demonstrating
superior performance and laying the way for new directions in brain aging
research.",cs.LG,2024-08-01
ReSi: A Comprehensive Benchmark for Representational Similarity Measures,,"Measuring the similarity of different representations of neural architectures
is a fundamental task and an open research challenge for the machine learning
community. This paper presents the first comprehensive benchmark for evaluating
representational similarity measures based on well-defined groundings of
similarity. The representational similarity (ReSi) benchmark consists of (i)
six carefully designed tests for similarity measures, (ii) 23 similarity
measures, (iii) eleven neural network architectures, and (iv) six datasets,
spanning over the graph, language, and vision domains. The benchmark opens up
several important avenues of research on representational similarity that
enable novel explorations and applications of neural architectures. We
demonstrate the utility of the ReSi benchmark by conducting experiments on
various neural network architectures, real world datasets and similarity
measures. All components of the benchmark are publicly available and thereby
facilitate systematic reproduction and production of research results. The
benchmark is extensible, future research can build on and further expand it. We
believe that the ReSi benchmark can serve as a sound platform catalyzing future
research that aims to systematically evaluate existing and explore novel ways
of comparing representations of neural architectures.",cs.LG,2024-08-01
"Predicting nonlinear-flow regions in highly heterogeneous porous media
  using adaptive constitutive laws and neural networks",,"In a porous medium featuring heterogeneous permeabilities, a wide range of
fluid velocities may be recorded, so that significant inertial and frictional
effects may arise in high-speed regions. In such parts, the link between
pressure gradient and velocity is typically made via Darcy's law, which may
fail to account for these effects; instead, the Darcy Forchheimer law, which
introduces a nonlinear term, may be more adequate. Applying the Darcy
Forchheimer law globally in the domain is very costly numerically and, rather,
should only be done where strictly necessary. The question of finding a prori
the subdomain where to restrict the use of the Darcy Forchheimer law was
recently answered in FP23 by using an adaptive model: given a threshold on the
flow velocity, the model locally selects the more appropriate law as it is
being solved. At the end of the resolution, each mesh cell is flagged as being
in the Darcy or Darcy Forchheimer subdomain. Still, this model is nonlinear
itself and thus relatively expensive to run. In this paper, to accelerate the
subdivision of the domain into low and high speed regions, we instead exploit
the adaptive model from FP23 to generate partitioning data given an array of
different input parameters, such as boundary conditions and inertial
coefficients, and then train neural networks on these data classifying each
mesh cell as Darcy or not. Two test cases are studied to illustrate the
results, where cost functions, parity plots, precision-recall plots and
receiver operating characteristic curves are analyzed.",math.NA cs.NA physics.flu-dyn,2024-08-01
"The Monetisation of Toxicity: Analysing YouTube Content Creators and
  Controversy-Driven Engagement",,"YouTube is a major social media platform that plays a significant role in
digital culture, with content creators at its core. These creators often engage
in controversial behaviour to drive engagement, which can foster toxicity. This
paper presents a quantitative analysis of controversial content on YouTube,
focusing on the relationship between controversy, toxicity, and monetisation.
We introduce a curated dataset comprising 20 controversial YouTube channels
extracted from Reddit discussions, including 16,349 videos and more than 105
million comments. We identify and categorise monetisation cues from video
descriptions into various models, including affiliate marketing and direct
selling, using lists of URLs and keywords. Additionally, we train a machine
learning model to measure the toxicity of comments in these videos. Our
findings reveal that while toxic comments correlate with higher engagement,
they negatively impact monetisation, indicating that controversy-driven
interaction does not necessarily lead to financial gain. We also observed
significant variation in monetisation strategies, with some creators showing
extensive monetisation despite high toxicity levels. Our study introduces a
curated dataset, lists of URLs and keywords to categorise monetisation, a
machine learning model to measure toxicity, and is a significant step towards
understanding the complex relationship between controversy, engagement, and
monetisation on YouTube. The lists used for detecting and categorising
monetisation cues are available on https://github.com/thalesbertaglia/toxmon.",cs.CY cs.CL,2024-08-01
"High-Quality, ROS Compatible Video Encoding and Decoding for
  High-Definition Datasets",,"Robotic datasets are important for scientific benchmarking and developing
algorithms, for example for Simultaneous Localization and Mapping (SLAM).
Modern robotic datasets feature video data of high resolution and high
framerates. Storing and sharing those datasets becomes thus very costly,
especially if more than one camera is used for the datasets. It is thus
essential to store this video data in a compressed format. This paper
investigates the use of modern video encoders for robotic datasets. We provide
a software that can replay mp4 videos within ROS 1 and ROS 2 frameworks,
supporting the synchronized playback in simulated time. Furthermore, the paper
evaluates different encoders and their settings to find optimal configurations
in terms of resulting size, quality and encoding time. Through this work we
show that it is possible to store and share even highest quality video datasets
within reasonable storage constraints.",cs.RO cs.CV,2024-08-01
Intermittent Semi-working Mask: A New Masking Paradigm for LLMs,,"Multi-turn dialogues are a key interaction method between humans and Large
Language Models (LLMs), as conversations extend over multiple rounds, keeping
LLMs' high generation quality and low latency is a challenge. Mainstream LLMs
can be grouped into two categories based on masking strategy: causal LLM and
prefix LLM. Several works have demonstrated that prefix LLMs tend to outperform
causal ones in scenarios that heavily depend on historical context such as
multi-turn dialogues or in-context learning, thanks to their bidirectional
attention on prefix sequences. However, prefix LLMs have an inherent
inefficient training problem in multi-turn dialogue datasets. In addition, the
attention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV
Cache) across dialogue rounds to reduce generation latency. In this paper, we
propose a novel masking scheme called Intermittent Semi-working Mask (ISM) to
address these problems. Specifically, we apply alternate bidirectional and
unidirectional attention on queries and answers in the dialogue history. In
this way, ISM is able to maintain the high quality of prefix LLM and low
generation latency of causal LLM, simultaneously. Extensive experiments
illustrate that our ISM achieves significant performance.",cs.CL cs.AI,2024-08-01
The Energy Cost of Artificial Intelligence of Things Lifecycle,,"Artificial intelligence (AI)coupled with existing Internet of Things (IoT)
enables more streamlined and autonomous operations across various economic
sectors. Consequently, the paradigm of Artificial Intelligence of Things (AIoT)
having AI techniques at its core implies additional energy and carbon costs
that may become significant with more complex neural architectures. To better
understand the energy and Carbon Footprint (CF) of some AIoT components, very
recent studies employ conventional metrics. However, these metrics are not
designed to capture energy efficiency aspects of inference. In this paper, we
propose a new metric, the Energy Cost of AIoT Lifecycle (eCAL) to capture the
overall energy cost of inference over the lifecycle of an AIoT system. We
devise a new methodology for determining eCAL of an AIoT system by analyzing
the complexity of data manipulation in individual components involved in the
AIoT lifecycle and derive the overall and per bit energy consumption. With eCAL
we show that the better a model is and the more it is used, the more energy
efficient an inference is. For an example AIoT configuration, eCAL for making
$100$ inferences is $1.43$ times higher than for $1000$ inferences. We also
evaluate the CF of the AIoT system by calculating the equivalent CO$_{2}$
emissions based on the energy consumption and the Carbon Intensity (CI) across
different countries. Using 2023 renewable data, our analysis reveals that
deploying an AIoT system in Germany results in emitting $4.62$ times higher
CO$_2$ than in Finland, due to latter using more low-CI energy sources.",cs.ET cs.AI cs.LG,2024-08-01
"Secret Sharing for Secure and Private Information Retrieval: A
  Construction Using Algebraic Geometry Codes",,"Private information retrieval (PIR) considers the problem of retrieving a
data item from a database or distributed storage system without disclosing any
information about which data item was retrieved. Secure PIR complements this
problem by further requiring the contents of the data to be kept secure.
Privacy and security can be achieved by adding suitable noise to the queries
and data using methods from secret sharing. In this paper, a new framework for
homomorphic secret sharing in secure and private information retrieval from
colluding servers is proposed, generalizing the original cross-subspace
alignment (CSA) codes proposed by Jia, Sun, and Jafar. We utilize this
framework to give a secure PIR construction using algebraic geometry codes over
hyperelliptic curves of arbitrary genus. It is shown that the proposed scheme
offers interesting tradeoffs between the field size, file size, number of
colluding servers, and the total number of servers. When the field size is
fixed, this translates in some cases to higher retrieval rates than those of
the original scheme. In addition, the new schemes exist also for some
parameters where the original ones do not.",cs.IT math.IT,2024-08-01
"Illustrating Classic Brazilian Books using a Text-To-Image Diffusion
  Model",,"In recent years, Generative Artificial Intelligence (GenAI) has undergone a
profound transformation in addressing intricate tasks involving diverse
modalities such as textual, auditory, visual, and pictorial generation. Within
this spectrum, text-to-image (TTI) models have emerged as a formidable approach
to generating varied and aesthetically appealing compositions, spanning
applications from artistic creation to realistic facial synthesis, and
demonstrating significant advancements in computer vision, image processing,
and multimodal tasks. The advent of Latent Diffusion Models (LDMs) signifies a
paradigm shift in the domain of AI capabilities. This article delves into the
feasibility of employing the Stable Diffusion LDM to illustrate literary works.
For this exploration, seven classic Brazilian books have been selected as case
studies. The objective is to ascertain the practicality of this endeavor and to
evaluate the potential of Stable Diffusion in producing illustrations that
augment and enrich the reader's experience. We will outline the beneficial
aspects, such as the capacity to generate distinctive and contextually
pertinent images, as well as the drawbacks, including any shortcomings in
faithfully capturing the essence of intricate literary depictions. Through this
study, we aim to provide a comprehensive assessment of the viability and
efficacy of utilizing AI-generated illustrations in literary contexts,
elucidating both the prospects and challenges encountered in this pioneering
application of technology.",cs.AI,2024-08-01
Collecting Larg-Scale Robotic Datasets on a High-Speed Mobile Platform,,"Mobile robotics datasets are essential for research on robotics, for example
for research on Simultaneous Localization and Mapping (SLAM). Therefore the
ShanghaiTech Mapping Robot was constructed, that features a multitude
high-performance sensors and a 16-node cluster to collect all this data. That
robot is based on a Clearpath Husky mobile base with a maximum speed of 1 meter
per second. This is fine for indoor datasets, but to collect large-scale
outdoor datasets a faster platform is needed. This system paper introduces our
high-speed mobile platform for data collection. The mapping robot is secured on
the rear-steered flatbed car with maximum field of view. Additionally two
encoders collect odometry data from two of the car wheels and an external
sensor plate houses a downlooking RGB and event camera. With this setup a
dataset of more than 10km in the underground parking garage and the outside of
our campus was collected and is published with this paper.",cs.RO,2024-08-01
Learning to Embed Distributions via Maximum Kernel Entropy,,"Empirical data can often be considered as samples from a set of probability
distributions. Kernel methods have emerged as a natural approach for learning
to classify these distributions. Although numerous kernels between
distributions have been proposed, applying kernel methods to distribution
regression tasks remains challenging, primarily because selecting a suitable
kernel is not straightforward. Surprisingly, the question of learning a
data-dependent distribution kernel has received little attention. In this
paper, we propose a novel objective for the unsupervised learning of
data-dependent distribution kernel, based on the principle of entropy
maximization in the space of probability measure embeddings. We examine the
theoretical properties of the latent embedding space induced by our objective,
demonstrating that its geometric structure is well-suited for solving
downstream discriminative tasks. Finally, we demonstrate the performance of the
learned kernel across different modalities.",cs.LG cs.AI eess.SP stat.ML,2024-08-01
Mitigating Multilingual Hallucination in Large Vision-Language Models,,"While Large Vision-Language Models (LVLMs) have exhibited remarkable
capabilities across a wide range of tasks, they suffer from hallucination
problems, where models generate plausible yet incorrect answers given the input
image-query pair. This hallucination phenomenon is even more severe when
querying the image in non-English languages, while existing methods for
mitigating hallucinations in LVLMs only consider the English scenarios. In this
paper, we make the first attempt to mitigate this important multilingual
hallucination in LVLMs. With thorough experiment analysis, we found that
multilingual hallucination in LVLMs is a systemic problem that could arise from
deficiencies in multilingual capabilities or inadequate multimodal abilities.
To this end, we propose a two-stage Multilingual Hallucination Removal (MHR)
framework for LVLMs, aiming to improve resistance to hallucination for both
high-resource and low-resource languages. Instead of relying on the intricate
manual annotations of multilingual resources, we fully leverage the inherent
capabilities of the LVLM and propose a novel cross-lingual alignment method,
which generates multiple responses for each image-query input and then
identifies the hallucination-aware pairs for each language. These data pairs
are finally used for direct preference optimization to prompt the LVLMs to
favor non-hallucinating responses. Experimental results show that our MHR
achieves a substantial reduction in hallucination generation for LVLMs.
Notably, on our extended multilingual POPE benchmark, our framework delivers an
average increase of 19.0% in accuracy across 13 different languages. Our code
and model weights are available at https://github.com/ssmisya/MHR",cs.CV cs.AI cs.CL,2024-08-01
Manifold-Based Optimizations for RIS-Aided Massive MIMO Systems,,"Manifold optimization (MO) is a powerful mathematical framework that can be
applied to optimize functions over complex geometric structures, which is
particularly useful in advanced wireless communication systems, such as
reconfigurable intelligent surface (RIS)-aided massive MIMO (mMIMO) and
extra-large scale massive MIMO (XL-MIMO) systems. MO provides a structured
approach to tackling complex optimization problems. By leveraging the geometric
properties of the manifold, more efficient and effective solutions can be found
compared to conventional optimization methods. This paper provides a tutorial
on MO technique and provides some applications of MO in the context of wireless
communications systems. In particular, to corroborate the effectiveness of MO
methodology, we explore five application examples in RIS-aided mMIMO system,
focusing on fairness, energy efficiency (EE) maximization, intracell pilot
reuse interference mitigation, and grant-free (GF) random access (RA).",cs.NI,2024-08-01
"Alleviating Hallucination in Large Vision-Language Models with Active
  Retrieval Augmentation",,"Despite the remarkable ability of large vision-language models (LVLMs) in
image comprehension, these models frequently generate plausible yet factually
incorrect responses, a phenomenon known as hallucination.Recently, in large
language models (LLMs), augmenting LLMs by retrieving information from external
knowledge resources has been proven as a promising solution to mitigate
hallucinations.However, the retrieval augmentation in LVLM significantly lags
behind the widespread applications of LVLM. Moreover, when transferred to
augmenting LVLMs, sometimes the hallucination degree of the model is even
exacerbated.Motivated by the research gap and counter-intuitive phenomenon, we
introduce a novel framework, the Active Retrieval-Augmented large
vision-language model (ARA), specifically designed to address hallucinations by
incorporating three critical dimensions: (i) dissecting the retrieval targets
based on the inherent hierarchical structures of images. (ii) pinpointing the
most effective retrieval methods and filtering out the reliable retrieval
results. (iii) timing the retrieval process to coincide with episodes of low
certainty, while circumventing unnecessary retrieval during periods of high
certainty. To assess the capability of our proposed ARA model in reducing
hallucination, we employ three widely used LVLM models (LLaVA-1.5, Qwen-VL, and
mPLUG-Owl2) across four benchmarks. Our empirical observations suggest that by
utilizing fitting retrieval mechanisms and timing the retrieval judiciously, we
can effectively mitigate the hallucination problem. We hope that this study can
provide deeper insights into how to adapt the retrieval augmentation to LVLMs
for reducing hallucinations with more effective retrieval and minimal retrieval
occurrences.",cs.CV cs.AI cs.CL,2024-08-01
End-to-End Protocol for High-Quality QAOA Parameters with Few Shots,,"The quantum approximate optimization algorithm (QAOA) is a quantum heuristic
for combinatorial optimization that has been demonstrated to scale better than
state-of-the-art classical solvers for some problems. For a given problem
instance, QAOA performance depends crucially on the choice of the parameters.
While average-case optimal parameters are available in many cases, meaningful
performance gains can be obtained by fine-tuning these parameters for a given
instance. This task is especially challenging, however, when the number of
circuit executions (shots) is limited. In this work, we develop an end-to-end
protocol that combines multiple parameter settings and fine-tuning techniques.
We use large-scale numerical experiments to optimize the protocol for the
shot-limited setting and observe that optimizers with the simplest internal
model (linear) perform best. We implement the optimized pipeline on a
trapped-ion processor using up to $32$ qubits and $5$ QAOA layers, and we
demonstrate that the pipeline is robust to small amounts of hardware noise. To
the best of our knowledge, these are the largest demonstrations of QAOA
parameter tuning on a trapped-ion processor.",quant-ph cs.ET,2024-08-01
New Compressed Indices for Multijoins on Graph Databases,,"A recent surprising result in the implementation of worst-case-optimal (wco)
multijoins in graph databases (specifically, basic graph patterns) is that they
can be supported on graph representations that take even less space than a
plain representation, and orders of magnitude less space than classical
indices, while offering comparable performance. In this paper we uncover a wide
set of new wco space-time tradeoffs: we (1) introduce new compact indices that
handle multijoins in wco time, and (2) combine them with new query resolution
strategies that offer better times in practice. As a result, we improve the
average query times of current compact representations by a factor of up to 13
to produce the first 1000 results, and using twice their space, reduce their
total average query time by a factor of 2. Our experiments suggest that there
is more room for improvement in terms of generating better query plans for
multijoins.",cs.DB cs.DS,2024-08-01
"AMFR-W numerical methods for solving high dimensional SABR/LIBOR PDE
  models",,"In this work we mainly develop a new numerical methodology to solve a PDE
model recently proposed in the literature for pricing interest rate
derivatives. More precisely, we use high order in time AMFR-W methods, which
belong to a class of W-methods based on Approximate Matrix Factorization (AMF)
and are especially suitable in the presence of mixed spatial derivatives.
High-order convergence in time allows larger time steps which combined with the
splitting of the involved operators, highly reduces the computational time for
a given accuracy. Moreover, the consideration of a large number of underlying
forward rates makes the PDE problem high dimensional in space, so the use of
AMFR-W methods with a sparse grids combination technique represents another
innovative aspect, making AMFR-W more efficient than with full grids and
opening the possibility of parallelization. Also the consideration of new
homogeneous Neumann boundary conditions provides another original feature to
avoid the difficulties associated to the presence of boundary layers when using
Dirichlet ones, especially in advection-dominated regimes. These Neumann
boundary conditions motivate the introduction of a modified combination
technique to overcome a decrease in the accuracy of the standard combination
technique.",math.NA cs.NA,2024-08-01
"MUFASA: Multi-View Fusion and Adaptation Network with Spatial Awareness
  for Radar Object Detection",,"In recent years, approaches based on radar object detection have made
significant progress in autonomous driving systems due to their robustness
under adverse weather compared to LiDAR. However, the sparsity of radar point
clouds poses challenges in achieving precise object detection, highlighting the
importance of effective and comprehensive feature extraction technologies. To
address this challenge, this paper introduces a comprehensive feature
extraction method for radar point clouds. This study first enhances the
capability of detection networks by using a plug-and-play module, GeoSPA. It
leverages the Lalonde features to explore local geometric patterns.
Additionally, a distributed multi-view attention mechanism, DEMVA, is designed
to integrate the shared information across the entire dataset with the global
information of each individual frame. By employing the two modules, we present
our method, MUFASA, which enhances object detection performance through
improved feature extraction. The approach is evaluated on the VoD and
TJ4DRaDSet datasets to demonstrate its effectiveness. In particular, we achieve
state-of-the-art results among radar-based methods on the VoD dataset with the
mAP of 50.24%.",cs.CV,2024-08-01
"Enhancing Digital Forensics Readiness In Big Data Wireless Medical
  Networks: A Secure Decentralised Framework",,"Wireless medical networks are pivotal for chronic disease management, yet the
sensitive Big Data they generate presents administration challenges and cyber
vulnerability. This Big Data is valuable within both healthcare and legal
contexts, serving as a resource for investigating medical malpractice, civil
cases, criminal activities, and network-related incidents. However, the rapid
evolution of network technologies and data creates complexities in digital
forensics investigations and audits. To address these issues, this paper
proposes a secure decentralised framework aimed at bolstering digital forensics
readiness (DFR) in Big Data wireless medical networks by identifying security
threats, complexities, and gaps in current research efforts. By improving the
network's resilience to cyber threats and aiding in medical malpractice
investigations, this framework significantly advances digital forensics,
wireless networks, and healthcare. It enhances digital forensics readiness,
incident response, and the management of medical malpractice incidents in Big
Data wireless medical networks. A real-world scenario-based evaluation
demonstrated the framework's effectiveness in improving forensic readiness and
response capabilities, validating its practical applicability and impact. A
comparison of the proposed framework with existing frameworks concluded that it
is an advancement in framework design for DFR, especially in regard to Big Data
processing, decentralised DFR storage and scalability.",cs.DC cs.NI,2024-08-01
Analyzing the Effectiveness of Quantum Annealing with Meta-Learning,,"The field of Quantum Computing has gathered significant popularity in recent
years and a large number of papers have studied its effectiveness in tackling
many tasks. We focus in particular on Quantum Annealing (QA), a meta-heuristic
solver for Quadratic Unconstrained Binary Optimization (QUBO) problems. It is
known that the effectiveness of QA is dependent on the task itself, as is the
case for classical solvers, but there is not yet a clear understanding of which
are the characteristics of a problem that makes it difficult to solve with QA.
In this work, we propose a new methodology to study the effectiveness of QA
based on meta-learning models. To do so, we first build a dataset composed of
more than five thousand instances of ten different optimization problems. We
define a set of more than a hundred features to describe their characteristics,
and solve them with both QA and three classical solvers. We publish this
dataset online for future research. Then, we train multiple meta-models to
predict whether QA would solve that instance effectively and use them to probe
which are the features with the strongest impact on the effectiveness of QA.
Our results indicate that it is possible to accurately predict the
effectiveness of QA, validating our methodology. Furthermore, we observe that
the distribution of the problem coefficients representing the bias and coupling
terms is very informative to identify the probability of finding good
solutions, while the density of these coefficients alone is not enough. The
methodology we propose allows to open new research directions to further our
understanding of the effectiveness of QA, by probing specific dimensions or by
developing new QUBO formulations that are better suited for the particular
nature of QA. Furthermore, the proposed methodology is flexible and can be
extended or used to study other quantum or classical solvers.",quant-ph cs.LG,2024-08-01
"Fluctuating Line-of-Sight Fading Distribution: Statistical
  Characterization and Applications",,"We introduce the fluctuating Line-of-Sight (fLoS) fading model, characterized
by parameters $K$, $k$, $\lambda$, and $\Omega$. The fLoS fading distribution
is expressed in terms of the multivariate confluent hypergeometric functions
$\Psi_2$, $\Phi_3^{(n)}$, and $\Phi_3 = \Phi_3^{(2)}$ and encompasses
well-known distributions, such as the Nakagami-$m$, Hoyt, Rice, and Rician
shadowed fading distributions as special cases. An efficient method to
numerically compute the fLoS fading distribution is also addressed. Notably,
for a positive integer $k$, the fLoS fading distribution simplifies to a finite
mixture of $\kappa$-$\mu$ distributions. Additionally, we analyze the outage
probability and Ergodic capacity, presenting a tailored Prony's approximation
method for the latter. Numerical results are presented to show the impact of
the fading parameters and verify the accuracy of the proposed approximation.
Moreover, we illustrate an application of the proposed fLoS fading distribution
for characterizing wireless systems affected by channel aging.",cs.IT eess.SP math.IT,2024-08-01
"Dimension reduction for large-scale stochastic systems with non-zero
  initial states and controlled diffusion",,"In this paper, we establish new strategies to reduce the dimension of
large-scale controlled stochastic differential equations with non-zero initial
states. The first approach transforms the original setting into a stochastic
system with zero initial states. This transformation naturally leads to
equations with controlled diffusion. A detailed analysis of dominant subspaces
and bounds for the reduction error is provided in this controlled diffusion
framework. Subsequently, we introduce a reduced system for the original
framework and prove an a-priori error bound for the first ansatz. This bound
involves so-called Hankel singular values that are linked to a new pair of
Gramians. A second strategy is presented that is based on the idea of reducing
control and initial state dynamics separately. Here, different Gramians are
used in order to derive a reduced model and their relation to dominant
subspaces are pointed out. We also show an a posteriori error bound for the
second approach involving two types of Hankel singular values.",math.NA cs.NA math.OC math.PR,2024-08-01
"Non Verbis, Sed Rebus: Large Language Models are Weak Solvers of Italian
  Rebuses",,"Rebuses are puzzles requiring constrained multi-step reasoning to identify a
hidden phrase from a set of images and letters. In this work, we introduce a
large collection of verbalized rebuses for the Italian language and use it to
assess the rebus-solving capabilities of state-of-the-art large language
models. While general-purpose systems such as LLaMA-3 and GPT-4o perform poorly
on this task, ad-hoc fine-tuning seems to improve models' performance. However,
we find that performance gains from training are largely motivated by
memorization. Our results suggest that rebus solving remains a challenging test
bed to evaluate large language models' linguistic proficiency and sequential
instruction-following skills.",cs.CL cs.AI,2024-08-01
"Closing the gap between open-source and commercial large language models
  for medical evidence summarization",,"Large language models (LLMs) hold great promise in summarizing medical
evidence. Most recent studies focus on the application of proprietary LLMs.
Using proprietary LLMs introduces multiple risk factors, including a lack of
transparency and vendor dependency. While open-source LLMs allow better
transparency and customization, their performance falls short compared to
proprietary ones. In this study, we investigated to what extent fine-tuning
open-source LLMs can further improve their performance in summarizing medical
evidence. Utilizing a benchmark dataset, MedReview, consisting of 8,161 pairs
of systematic reviews and summaries, we fine-tuned three broadly-used,
open-sourced LLMs, namely PRIMERA, LongT5, and Llama-2. Overall, the fine-tuned
LLMs obtained an increase of 9.89 in ROUGE-L (95% confidence interval:
8.94-10.81), 13.21 in METEOR score (95% confidence interval: 12.05-14.37), and
15.82 in CHRF score (95% confidence interval: 13.89-16.44). The performance of
fine-tuned LongT5 is close to GPT-3.5 with zero-shot settings. Furthermore,
smaller fine-tuned models sometimes even demonstrated superior performance
compared to larger zero-shot models. The above trends of improvement were also
manifested in both human and GPT4-simulated evaluations. Our results can be
applied to guide model selection for tasks demanding particular domain
knowledge, such as medical evidence summarization.",cs.CL cs.AI,2024-07-25
Regional quality estimation for echocardiography using deep learning,,"Automatic estimation of cardiac ultrasound image quality can be beneficial
for guiding operators and ensuring the accuracy of clinical measurements.
Previous work often fails to distinguish the view correctness of the
echocardiogram from the image quality. Additionally, previous studies only
provide a global image quality value, which limits their practical utility. In
this work, we developed and compared three methods to estimate image quality:
1) classic pixel-based metrics like the generalized contrast-to-noise ratio
(gCNR) on myocardial segments as region of interest and left ventricle lumen as
background, obtained using a U-Net segmentation 2) local image coherence
derived from a U-Net model that predicts coherence from B-Mode images 3) a deep
convolutional network that predicts the quality of each region directly in an
end-to-end fashion. We evaluate each method against manual regional image
quality annotations by three experienced cardiologists. The results indicate
poor performance of the gCNR metric, with Spearman correlation to the
annotations of \r{ho} = 0.24. The end-to-end learning model obtains the best
result, \r{ho} = 0.69, comparable to the inter-observer correlation, \r{ho} =
0.63. Finally, the coherence-based method, with \r{ho} = 0.58, outperformed the
classical metrics and is more generic than the end-to-end approach.",eess.IV cs.CV,2024-08-01
"Learned Compression of Point Cloud Geometry and Attributes in a Single
  Model through Multimodal Rate-Control",,"Point cloud compression is essential to experience volumetric multimedia as
it drastically reduces the required streaming data rates. Point attributes,
specifically colors, extend the challenge of lossy compression beyond geometric
representation to achieving joint reconstruction of texture and geometry.
State-of-the-art methods separate geometry and attributes to compress them
individually. This comes at a computational cost, requiring an encoder and a
decoder for each modality. Additionally, as attribute compression methods
require the same geometry for encoding and decoding, the encoder emulates the
decoder-side geometry reconstruction as an input step to project and compress
the attributes. In this work, we propose to learn joint compression of geometry
and attributes using a single, adaptive autoencoder model, embedding both
modalities into a unified latent space which is then entropy encoded. Key to
the technique is to replace the search for trade-offs between rate, attribute
quality and geometry quality, through conditioning the model on the desired
qualities of both modalities, bypassing the need for training model ensembles.
To differentiate important point cloud regions during encoding or to allow
view-dependent compression for user-centered streaming, conditioning is
pointwise, which allows for local quality and rate variation. Our evaluation
shows comparable performance to state-of-the-art compression methods for
geometry and attributes, while reducing complexity compared to related
compression methods.",cs.CV cs.MM eess.IV,2024-08-01
"Ground-to-UAV and RIS-assisted UAV-to-Ground Communication Under Channel
  Aging: Statistical Characterization and Outage Performance",,"This paper studies the statistical characterization of ground-to-air (G2A)
and reconfigurable intelligent surface (RIS)-assisted air-to-ground (A2G)
communications in RIS-assisted UAV networks under the impact of channel aging.
A comprehensive channel model is presented, which incorporates the time-varying
fading, three-dimensional (3D) mobility, Doppler shifts, and the effects of
channel aging on array antenna structures. We provide analytical expressions
for the G2A signal-to-noise ratio (SNR) probability density function (PDF) and
cumulative distribution function (CDF), demonstrating that the G2A SNR follows
a mixture of noncentral $\chi^2$ distributions. The A2G communication is
characterized under RIS arbitrary phase-shift configurations, showing that the
A2G SNR can be represented as the product of two correlated noncentral $\chi^2$
random variables (RVs). Additionally, we present the PDF and the CDF of the
product of two independently distributed noncentral $\chi^2$ RVs, which
accurately characterize the A2G SNR's distribution. Our paper confirms the
effectiveness of RISs in mitigating channel aging effects within the coherence
time. Finally, we propose an adaptive spectral efficiency method that ensures
consistent system performance and satisfactory outage levels when the UAV and
the ground user equipments are in motion.",cs.IT eess.SP math.IT,2024-08-01
AutoPV: Automatically Design Your Photovoltaic Power Forecasting Model,,"Photovoltaic power forecasting (PVPF) is a critical area in time series
forecasting (TSF), enabling the efficient utilization of solar energy. With
advancements in machine learning and deep learning, various models have been
applied to PVPF tasks. However, constructing an optimal predictive architecture
for specific PVPF tasks remains challenging, as it requires cross-domain
knowledge and significant labor costs. To address this challenge, we introduce
AutoPV, a novel framework for the automated search and construction of PVPF
models based on neural architecture search (NAS) technology. We develop a brand
new NAS search space that incorporates various data processing techniques from
state-of-the-art (SOTA) TSF models and typical PVPF deep learning models. The
effectiveness of AutoPV is evaluated on diverse PVPF tasks using a dataset from
the Daqing Photovoltaic Station in China. Experimental results demonstrate that
AutoPV can complete the predictive architecture construction process in a
relatively short time, and the newly constructed architecture is superior to
SOTA predefined models. This work bridges the gap in applying NAS to TSF
problems, assisting non-experts and industries in automatically designing
effective PVPF models.",cs.LG,2024-08-01
"In-Hand Singulation and Scooping Manipulation with a 5 DOF Tactile
  Gripper",,"Manipulation tasks often require a high degree of dexterity, typically
necessitating grippers with multiple degrees of freedom (DoF). While a robotic
hand equipped with multiple fingers can execute precise and intricate
manipulation tasks, the inherent redundancy stemming from its extensive DoF
often adds unnecessary complexity. In this paper, we introduce the design of a
tactile sensor-equipped gripper with two fingers and five DoF. We present a
novel design integrating a GelSight tactile sensor, enhancing sensing
capabilities and enabling finer control during specific manipulation tasks. To
evaluate the gripper's performance, we conduct experiments involving two
challenging tasks: 1) retrieving, singularizing, and classification of various
objects embedded in granular media, and 2) executing scooping manipulations of
credit cards in confined environments to achieve precise insertion. Our results
demonstrate the efficiency of the proposed approach, with a high success rate
for singulation and classification tasks, particularly for spherical objects at
high as 94.3%, and a 100% success rate for scooping and inserting credit cards.",cs.RO,2024-08-01
"Using CSNNs to Perform Event-based Data Processing & Classification on
  ASL-DVS",,"Recent advancements in bio-inspired visual sensing and neuromorphic computing
have led to the development of various highly efficient bio-inspired solutions
with real-world applications. One notable application integrates event-based
cameras with spiking neural networks (SNNs) to process event-based sequences
that are asynchronous and sparse, making them difficult to handle. In this
project, we develop a convolutional spiking neural network (CSNN) architecture
that leverages convolutional operations and recurrent properties of a spiking
neuron to learn the spatial and temporal relations in the ASL-DVS gesture
dataset. The ASL-DVS gesture dataset is a neuromorphic dataset containing hand
gestures when displaying 24 letters (A to Y, excluding J and Z due to the
nature of their symbols) from the American Sign Language (ASL). We performed
classification on a pre-processed subset of the full ASL-DVS dataset to
identify letter signs and achieved 100\% training accuracy. Specifically, this
was achieved by training in the Google Cloud compute platform while using a
learning rate of 0.0005, batch size of 25 (total of 20 batches), 200
iterations, and 10 epochs.",cs.NE cs.LG,2024-08-01
Downstream bias mitigation is all you need,,"The advent of transformer-based architectures and large language models
(LLMs) have significantly advanced the performance of natural language
processing (NLP) models. Since these LLMs are trained on huge corpuses of data
from the web and other sources, there has been a major concern about harmful
prejudices that may potentially be transferred from the data. In many
applications, these pre-trained LLMs are fine-tuned on task specific datasets,
which can further contribute to biases. This paper studies the extent of biases
absorbed by LLMs during pre-training as well as task-specific behaviour after
fine-tuning. We found that controlled interventions on pre-trained LLMs, prior
to fine-tuning, have minimal effect on lowering biases in classifiers. However,
the biases present in domain-specific datasets play a much bigger role, and
hence mitigating them at this stage has a bigger impact. While pre-training
does matter, but after the model has been pre-trained, even slight changes to
co-occurrence rates in the fine-tuning dataset has a significant effect on the
bias of the model.",cs.CL,2024-08-01
"Unlocking Fair Use in the Generative AI Supply Chain: A Systematized
  Literature Review",,"Through a systematization of generative AI (GenAI) stakeholder goals and
expectations, this work seeks to uncover what value different stakeholders see
in their contributions to the GenAI supply line. This valuation enables us to
understand whether fair use advocated by GenAI companies to train model
progresses the copyright law objective of promoting science and arts. While
assessing the validity and efficacy of the fair use argument, we uncover
research gaps and potential avenues for future works for researchers and
policymakers to address.",cs.AI cs.CY cs.LG,2024-08-01
"Harnessing Uncertainty-aware Bounding Boxes for Unsupervised 3D Object
  Detection",,"Unsupervised 3D object detection aims to identify objects of interest from
unlabeled raw data, such as LiDAR points. Recent approaches usually adopt
pseudo 3D bounding boxes (3D bboxes) from clustering algorithm to initialize
the model training, and then iteratively updating both pseudo labels and the
trained model. However, pseudo bboxes inevitably contain noises, and such
inaccurate annotation accumulates to the final model, compromising the
performance. Therefore, in an attempt to mitigate the negative impact of pseudo
bboxes, we introduce a new uncertainty-aware framework. In particular, Our
method consists of two primary components: uncertainty estimation and
uncertainty regularization. (1) In the uncertainty estimation phase, we
incorporate an extra auxiliary detection branch alongside the primary detector.
The prediction disparity between the primary and auxiliary detectors is
leveraged to estimate uncertainty at the box coordinate level, including
position, shape, orientation. (2) Based on the assessed uncertainty, we
regularize the model training via adaptively adjusting every 3D bboxes
coordinates. For pseudo bbox coordinates with high uncertainty, we assign a
relatively low loss weight. Experiment verifies that the proposed method is
robust against the noisy pseudo bboxes, yielding substantial improvements on
nuScenes and Lyft compared to existing techniques, with increases of 6.9% in
AP$_{BEV}$ and 2.5% in AP$_{3D}$ on nuScenes, and 2.2% in AP$_{BEV}$ and 1.0%
in AP$_{3D}$ on Lyft.",cs.CV,2024-08-01
Are Bigger Encoders Always Better in Vision Large Models?,,"In recent years, multimodal large language models (MLLMs) have shown strong
potential in real-world applications. They are developing rapidly due to their
remarkable ability to comprehend multimodal information and their inherent
powerful cognitive and reasoning capabilities. Among MLLMs, vision language
models (VLM) stand out for their ability to understand vision information.
However, the scaling trend of VLMs under the current mainstream paradigm has
not been extensively studied. Whether we can achieve better performance by
training even larger models is still unclear. To address this issue, we
conducted experiments on the pretraining stage of MLLMs. We conduct our
experiment using different encoder sizes and large language model (LLM) sizes.
Our findings indicate that merely increasing the size of encoders does not
necessarily enhance the performance of VLMs. Moreover, we analyzed the effects
of LLM backbone parameter size and data quality on the pretraining outcomes.
Additionally, we explored the differences in scaling laws between LLMs and
VLMs.",cs.CV cs.CL,2024-08-01
"CAVE: Crowdsourcing Passing-By Vehicles for Reliable In-Vehicle Edge
  Computing",,"In-vehicle edge computing is a much anticipated paradigm to serve
ever-increasing computation demands originated from the ego vehicle, such as
passenger entertainments. In this paper, we explore the unique idea of
crowdsourcing passing-by vehicles to augment computing of the ego vehicle. The
challenges lie in the high dynamics of passing-by vehicles, time-correlated
task computation, and the stringent requirement of computing reliability for
individual user tasks. To this end, we formulate an optimization problem to
minimize the end-to-end latency by optimizing the task assignment and resource
allocation of user tasks. To address the complex problem, we propose a new
algorithm (named CAVE) with multiple key designs. We build an end-to-end
network and compute simulator and conduct extensive simulation to evaluate the
performance of the proposed algorithm.",cs.NI,2024-08-01
"SynesLM: A Unified Approach for Audio-visual Speech Recognition and
  Translation via Language Model and Synthetic Data",,"In this work, we present SynesLM, an unified model which can perform three
multimodal language understanding tasks: audio-visual automatic speech
recognition(AV-ASR) and visual-aided speech/machine translation(VST/VMT).
Unlike previous research that focused on lip motion as visual cues for speech
signals, our work explores more general visual information within entire
frames, such as objects and actions. Additionally, we use synthetic image data
to enhance the correlation between image and speech data. We benchmark SynesLM
against the How2 dataset, demonstrating performance on par with
state-of-the-art (SOTA) models dedicated to AV-ASR while maintaining our
multitasking framework. Remarkably, for zero-shot AV-ASR, SynesLM achieved SOTA
performance by lowering the Word Error Rate (WER) from 43.4% to 39.4% on the
VisSpeech Dataset. Furthermore, our results in VST and VMT outperform the
previous results, improving the BLEU score to 43.5 from 37.2 for VST, and to
54.8 from 54.4 for VMT.",eess.AS cs.CL cs.CV,2024-08-01
Factorization of a prime matrix in even blocks,,"In this paper, a matrix is said to be prime if the row and column of this
matrix are both prime numbers. We establish various necessary and sufficient
conditions for developing matrices into the sum of tensor products of prime
matrices. For example, if the diagonal of a matrix blocked evenly are pairwise
commutative, it yields such a decomposition. The computational complexity of
multiplication of these algorithms is shown to be $O(n^{5/2})$. In the section
5, a decomposition is proved to hold if and only if every even natural number
greater than 2 is the sum of two prime numbers.",math.NA cs.NA math.OC math.RA,2024-08-01
"Empowering Snapshot Compressive Imaging: Spatial-Spectral State Space
  Model with Across-Scanning and Local Enhancement",,"Snapshot Compressive Imaging (SCI) relies on decoding algorithms such as CNN
or Transformer to reconstruct the hyperspectral image (HSI) from its compressed
measurement. Although existing CNN and Transformer-based methods have proven
effective, CNNs are limited by their inadequate modeling of long-range
dependencies, while Transformer ones face high computational costs due to
quadratic complexity. Recent Mamba models have demonstrated superior
performance over CNN and Transformer-based architectures in some visual tasks,
but these models have not fully utilized the local similarities in both spatial
and spectral dimensions. Moreover, the long-sequence modeling capability of SSM
may offer an advantage in processing the numerous spectral bands for HSI
reconstruction, which has not yet been explored. In this paper, we introduce a
State Space Model with Across-Scanning and Local Enhancement, named ASLE-SSM,
that employs a Spatial-Spectral SSM for global-local balanced context encoding
and cross-channel interaction promoting. Specifically, we introduce local
scanning in the spatial dimension to balance the global and local receptive
fields, and then propose our across-scanning method based on spatial-spectral
local cubes to leverage local similarities between adjacent spectral bands and
pixels to guide the reconstruction process. These two scanning mechanisms
extract the HSI's local features while balancing the global perspective without
any additional costs. Experimental results illustrate ASLE-SSM's superiority
over existing state-of-the-art methods, with an inference speed 2.4 times
faster than Transformer-based MST and saving 0.12 (M) of parameters, achieving
the lowest computational cost and parameter count.",cs.CV eess.IV,2024-08-01
"DisTrack: a new Tool for Semi-automatic Misinformation Tracking in
  Online Social Networks",,"Introduction: This article introduces DisTrack, a methodology and a tool
developed for tracking and analyzing misinformation within Online Social
Networks (OSNs). DisTrack is designed to combat the spread of misinformation
through a combination of Natural Language Processing (NLP) Social Network
Analysis (SNA) and graph visualization. The primary goal is to detect
misinformation, track its propagation, identify its sources, and assess the
influence of various actors within the network.
  Methods: DisTrack's architecture incorporates a variety of methodologies
including keyword search, semantic similarity assessments, and graph generation
techniques. These methods collectively facilitate the monitoring of
misinformation, the categorization of content based on alignment with known
false claims, and the visualization of dissemination cascades through detailed
graphs. The tool is tailored to capture and analyze the dynamic nature of
misinformation spread in digital environments.
  Results: The effectiveness of DisTrack is demonstrated through three case
studies focused on different themes: discredit/hate speech, anti-vaccine
misinformation, and false narratives about the Russia-Ukraine conflict. These
studies show DisTrack's capabilities in distinguishing posts that propagate
falsehoods from those that counteract them, and tracing the evolution of
misinformation from its inception.
  Conclusions: The research confirms that DisTrack is a valuable tool in the
field of misinformation analysis. It effectively distinguishes between
different types of misinformation and traces their development over time. By
providing a comprehensive approach to understanding and combating
misinformation in digital spaces, DisTrack proves to be an essential asset for
researchers and practitioners working to mitigate the impact of false
information in online social environments.",cs.SI cs.AI,2024-08-01
"Evaluation Metrics and Methods for Generative Models in the Wireless PHY
  Layer",,"Generative models are typically evaluated by direct inspection of their
generated samples, e.g., by visual inspection in the case of images. Further
evaluation metrics like the Fr\'echet inception distance or maximum mean
discrepancy are intricate to interpret and lack physical motivation. These
observations make evaluating generative models in the wireless PHY layer
non-trivial. This work establishes a framework consisting of evaluation metrics
and methods for generative models applied to the wireless PHY layer. The
proposed metrics and methods are motivated by wireless applications,
facilitating interpretation and understandability for the wireless community.
In particular, we propose a spectral efficiency analysis for validating the
generated channel norms and a codebook fingerprinting method to validate the
generated channel directions. Moreover, we propose an application cross-check
to evaluate the generative model's samples for training machine learning-based
models in relevant downstream tasks. Our analysis is based on real-world
measurement data and includes the Gaussian mixture model, variational
autoencoder, diffusion model, and generative adversarial network as generative
models. Our results under a fair comparison in terms of model architecture
indicate that solely relying on metrics like the maximum mean discrepancy
produces insufficient evaluation outcomes. In contrast, the proposed metrics
and methods exhibit consistent and explainable behavior.",cs.IT eess.SP math.IT,2024-08-01
"Deep Learning in Medical Image Classification from MRI-based Brain Tumor
  Images",,"Brain tumors are among the deadliest diseases in the world. Magnetic
Resonance Imaging (MRI) is one of the most effective ways to detect brain
tumors. Accurate detection of brain tumors based on MRI scans is critical, as
it can potentially save many lives and facilitate better decision-making at the
early stages of the disease. Within our paper, four different types of
MRI-based images have been collected from the database: glioma tumor, no tumor,
pituitary tumor, and meningioma tumor. Our study focuses on making predictions
for brain tumor classification. Five models, including four pre-trained models
(MobileNet, EfficientNet-B0, ResNet-18, and VGG16) and one new model,
MobileNet-BT, have been proposed for this study.",cs.CV,2024-08-01
"CrystalTac: 3D-Printed Vision-Based Tactile Sensor Family through Rapid
  Monolithic Manufacturing Technique",,"Recently, vision-based tactile sensors (VBTSs) have gained popularity in
robotics systems. The sensing mechanisms of most VBTSs can be categorised based
on the type of tactile features they capture. Each category requires specific
structural designs to convert physical contact into optical information. The
complex architectures of VBTSs pose challenges for traditional manufacturing
techniques in terms of design flexibility, cost-effectiveness, and quality
stability. Previous research has shown that monolithic manufacturing using
multi-material 3D printing technology can partially address these challenges.
This study introduces the CrystalTac family, a series of VBTSs designed with a
unique sensing mechanism and fabricated through rapid monolithic manufacturing.
Case studies on CrystalTac-type sensors demonstrate their effective performance
in tasks involving tactile perception, along with impressive cost-effectiveness
and design flexibility. The CrystalTac family aims to highlight the potential
of monolithic manufacturing in VBTS development and inspire further research in
tactile sensing and manipulation.",cs.RO eess.SP,2024-08-01
"Privacy-preserving datasets by capturing feature distributions with
  Conditional VAEs",,"Large and well-annotated datasets are essential for advancing deep learning
applications, however often costly or impossible to obtain by a single entity.
In many areas, including the medical domain, approaches relying on data sharing
have become critical to address those challenges. While effective in increasing
dataset size and diversity, data sharing raises significant privacy concerns.
Commonly employed anonymization methods based on the k-anonymity paradigm often
fail to preserve data diversity, affecting model robustness. This work
introduces a novel approach using Conditional Variational Autoencoders (CVAEs)
trained on feature vectors extracted from large pre-trained vision foundation
models. Foundation models effectively detect and represent complex patterns
across diverse domains, allowing the CVAE to faithfully capture the embedding
space of a given data distribution to generate (sample) a diverse,
privacy-respecting, and potentially unbounded set of synthetic feature vectors.
Our method notably outperforms traditional approaches in both medical and
natural image domains, exhibiting greater dataset diversity and higher
robustness against perturbations while preserving sample privacy. These results
underscore the potential of generative models to significantly impact deep
learning applications in data-scarce and privacy-sensitive environments. The
source code is available at
https://github.com/francescodisalvo05/cvae-anonymization .",cs.LG cs.CV eess.IV,2024-08-01
"AMAES: Augmented Masked Autoencoder Pretraining on Public Brain MRI Data
  for 3D-Native Segmentation",,"This study investigates the impact of self-supervised pretraining of 3D
semantic segmentation models on a large-scale, domain-specific dataset. We
introduce BRAINS-45K, a dataset of 44,756 brain MRI volumes from public
sources, the largest public dataset available, and revisit a number of design
choices for pretraining modern segmentation architectures by simplifying and
optimizing state-of-the-art methods, and combining them with a novel
augmentation strategy. The resulting AMAES framework is based on
masked-image-modeling and intensity-based augmentation reversal and balances
memory usage, runtime, and finetuning performance. Using the popular U-Net and
the recent MedNeXt architecture as backbones, we evaluate the effect of
pretraining on three challenging downstream tasks, covering single-sequence,
low-resource settings, and out-of-domain generalization. The results highlight
that pretraining on the proposed dataset with AMAES significantly improves
segmentation performance in the majority of evaluated cases, and that it is
beneficial to pretrain the model with augmentations, despite pretraing on a
large-scale dataset. Code and model checkpoints for reproducing results, as
well as the BRAINS-45K dataset are available at
\url{https://github.com/asbjrnmunk/amaes}.",eess.IV cs.AI cs.CV,2024-08-01
"Enhancing Ethereum Fraud Detection via Generative and Contrastive
  Self-supervision",,"The rampant fraudulent activities on Ethereum hinder the healthy development
of the blockchain ecosystem, necessitating the reinforcement of regulations.
However, multiple imbalances involving account interaction frequencies and
interaction types in the Ethereum transaction environment pose significant
challenges to data mining-based fraud detection research. To address this, we
first propose the concept of meta-interactions to refine interaction behaviors
in Ethereum, and based on this, we present a dual self-supervision enhanced
Ethereum fraud detection framework, named Meta-IFD. This framework initially
introduces a generative self-supervision mechanism to augment the interaction
features of accounts, followed by a contrastive self-supervision mechanism to
differentiate various behavior patterns, and ultimately characterizes the
behavioral representations of accounts and mines potential fraud risks through
multi-view interaction feature learning. Extensive experiments on real Ethereum
datasets demonstrate the effectiveness and superiority of our framework in
detecting common Ethereum fraud behaviors such as Ponzi schemes and phishing
scams. Additionally, the generative module can effectively alleviate the
interaction distribution imbalance in Ethereum data, while the contrastive
module significantly enhances the framework's ability to distinguish different
behavior patterns. The source code will be released on GitHub soon.",cs.LG,2024-08-01
"Coverage Path Planning For Minimizing Expected Time to Search For an
  Object With Continuous Sensing",,"In this paper, we present several results of both theoretical as well as
practical interests. First, we propose the quota lawn mowing problem, an
extension of the classic lawn mowing problem in computational geometry, as
follows: given a quota of coverage, compute the shortest lawn mowing route to
achieve said quota. We give constant-factor approximations for the quota lawn
mowing problem.
  Second, we investigate the expected detection time minimization problem in
geometric coverage path planning with local, continuous sensory information. We
provide the first approximation algorithm with provable error bounds with
pseudopolynomial running time. Our ideas also extend to another search
mechanism, namely visibility-based search, which is related to the watchman
route problem. We complement our theoretical analysis with some simple but
effective heuristics for finding an object in minimum expected time, on which
we provide simulation results.",cs.RO,2024-08-01
"Towards End-to-End Explainable Facial Action Unit Recognition via
  Vision-Language Joint Learning",,"Facial action units (AUs), as defined in the Facial Action Coding System
(FACS), have received significant research interest owing to their diverse
range of applications in facial state analysis. Current mainstream FAU
recognition models have a notable limitation, i.e., focusing only on the
accuracy of AU recognition and overlooking explanations of corresponding AU
states. In this paper, we propose an end-to-end Vision-Language joint learning
network for explainable FAU recognition (termed VL-FAU), which aims to
reinforce AU representation capability and language interpretability through
the integration of joint multimodal tasks. Specifically, VL-FAU brings together
language models to generate fine-grained local muscle descriptions and
distinguishable global face description when optimising FAU recognition.
Through this, the global facial representation and its local AU representations
will achieve higher distinguishability among different AUs and different
subjects. In addition, multi-level AU representation learning is utilised to
improve AU individual attention-aware representation capabilities based on
multi-scale combined facial stem feature. Extensive experiments on DISFA and
BP4D AU datasets show that the proposed approach achieves superior performance
over the state-of-the-art methods on most of the metrics. In addition, compared
with mainstream FAU recognition methods, VL-FAU can provide local- and
global-level interpretability language descriptions with the AUs' predictions.",cs.CV,2024-08-01
Token Interdependency Parsing (Tipping) -- Fast and Accurate Log Parsing,,"In the last decade, an impressive increase in software adaptions has led to a
surge in log data production, making manual log analysis impractical and
establishing the necessity for automated methods. Conversely, most automated
analysis tools include a component designed to separate log templates from
their parameters, commonly referred to as a ""log parser"". This paper aims to
introduce a new fast and accurate log parser, named ""Tipping"". Tipping combines
rule-based tokenizers, interdependency token graphs, strongly connected
components, and various techniques to ensure rapid, scalable, and precise log
parsing. Furthermore, Tipping is parallelized and capable of running on
multiple processing cores with close to linear efficiency. We evaluated Tipping
against other state-of-the-art log parsers in terms of accuracy, performance,
and the downstream task of anomaly detection. Accordingly, we found that
Tipping outperformed existing methods in accuracy and performance in our
evaluations. More in-depth, Tipping can parse 11 million lines of logs in less
than 20 seconds on a laptop machine. Furthermore, we re-implemented a
parallelized version of the past IpLom algorithm to demonstrate the effect of
parallel processing, and it became the second-fastest parser. As logs keep
growing in volume and complexity, the software engineering community needs to
ensure automated log analysis tools keep up with the demand, being capable of
efficiently handling massive volumes of logs with high accuracy. Tipping's
robustness, versatility, efficiency, and scalability make it a viable tool for
the modern automated log analysis task.",cs.SE,2024-08-01
"Counterclockwise Dissipativity, Potential Games and Evolutionary Nash
  Equilibrium Learning",,"We use system-theoretic passivity methods to study evolutionary Nash
equilibria learning in large populations of agents engaged in strategic,
non-cooperative interactions. The agents follow learning rules (rules for
short) that capture their strategic preferences and a payoff mechanism ascribes
payoffs to the available strategies. The population's aggregate strategic
profile is the state of an associated evolutionary dynamical system.
Evolutionary Nash equilibrium learning refers to the convergence of this state
to the Nash equilibria set of the payoff mechanism. Most approaches consider
memoryless payoff mechanisms, such as potential games. Recently, methods using
$\delta$-passivity and equilibrium independent passivity (EIP) have introduced
dynamic payoff mechanisms. However, $\delta$-passivity does not hold when
agents follow rules exhibiting ``imitation"" behavior, such as in replicator
dynamics. Conversely, EIP applies to the replicator dynamics but not to
$\delta$-passive rules. We address this gap using counterclockwise
dissipativity (CCW). First, we prove that continuous memoryless payoff
mechanisms are CCW if and only if they are potential games. Subsequently, under
(possibly dynamic) CCW payoff mechanisms, we establish evolutionary Nash
equilibrium learning for any rule within a convex cone spanned by imitation
rules and continuous $\delta$-passive rules.",cs.GT cs.SY eess.SY math.DS math.OC,2024-08-01
"Enhancing Multistep Prediction of Multivariate Market Indices Using
  Weighted Optical Reservoir Computing",,"We propose and experimentally demonstrate an innovative stock index
prediction method using a weighted optical reservoir computing system. We
construct fundamental market data combined with macroeconomic data and
technical indicators to capture the broader behavior of the stock market. Our
approach shows significant higher performance than state-of-the-art methods
such as linear regression, decision trees, and neural network architectures
including long short-term memory. It captures well the market's high volatility
and nonlinear behaviors despite limited data, demonstrating great potential for
real-time, parallel, multi-dimensional data processing and predictions.",cs.LG physics.app-ph,2024-08-01
"SF3D: Stable Fast 3D Mesh Reconstruction with UV-unwrapping and
  Illumination Disentanglement",,"We present SF3D, a novel method for rapid and high-quality textured object
mesh reconstruction from a single image in just 0.5 seconds. Unlike most
existing approaches, SF3D is explicitly trained for mesh generation,
incorporating a fast UV unwrapping technique that enables swift texture
generation rather than relying on vertex colors. The method also learns to
predict material parameters and normal maps to enhance the visual quality of
the reconstructed 3D meshes. Furthermore, SF3D integrates a delighting step to
effectively remove low-frequency illumination effects, ensuring that the
reconstructed meshes can be easily used in novel illumination conditions.
Experiments demonstrate the superior performance of SF3D over the existing
techniques. Project page: https://stable-fast-3d.github.io",cs.CV cs.GR,2024-08-01
Aligning Multiple Knowledge Graphs in a Single Pass,,"Entity alignment (EA) is to identify equivalent entities across different
knowledge graphs (KGs), which can help fuse these KGs into a more comprehensive
one. Previous EA methods mainly focus on aligning a pair of KGs, and to the
best of our knowledge, no existing EA method considers aligning multiple (more
than two) KGs. To fill this research gap, in this work, we study a novel
problem of aligning multiple KGs and propose an effective framework named
MultiEA to solve the problem. First, we embed the entities of all the candidate
KGs into a common feature space by a shared KG encoder. Then, we explore three
alignment strategies to minimize the distances among pre-aligned entities. In
particular, we propose an innovative inference enhancement technique to improve
the alignment performance by incorporating high-order similarities. Finally, to
verify the effectiveness of MultiEA, we construct two new real-world benchmark
datasets and conduct extensive experiments on them. The results show that our
MultiEA can effectively and efficiently align multiple KGs in a single pass.",cs.CL cs.LG,2024-08-01
"AutoM3L: An Automated Multimodal Machine Learning Framework with Large
  Language Models",,"Automated Machine Learning (AutoML) offers a promising approach to streamline
the training of machine learning models. However, existing AutoML frameworks
are often limited to unimodal scenarios and require extensive manual
configuration. Recent advancements in Large Language Models (LLMs) have
showcased their exceptional abilities in reasoning, interaction, and code
generation, presenting an opportunity to develop a more automated and
user-friendly framework. To this end, we introduce AutoM3L, an innovative
Automated Multimodal Machine Learning framework that leverages LLMs as
controllers to automatically construct multimodal training pipelines. AutoM3L
comprehends data modalities and selects appropriate models based on user
requirements, providing automation and interactivity. By eliminating the need
for manual feature engineering and hyperparameter optimization, our framework
simplifies user engagement and enables customization through directives,
addressing the limitations of previous rule-based AutoML approaches. We
evaluate the performance of AutoM3L on six diverse multimodal datasets spanning
classification, regression, and retrieval tasks, as well as a comprehensive set
of unimodal datasets. The results demonstrate that AutoM3L achieves competitive
or superior performance compared to traditional rule-based AutoML methods.
Furthermore, a user study highlights the user-friendliness and usability of our
framework, compared to the rule-based AutoML methods.",cs.LG,2024-08-01
"Absorbing Boundary Conditions for Variable Potential Schr\""odinger
  Equations via Titchmarsh-Weyl Theory",,"We propose a novel approach to simulate the solution of the time-dependent
Schr\""odinger equation with a general variable potential. The key idea is to
approximate the Titchmarsh-Weyl m-function (exact Dirichlet-to-Neumann
operator) by a rational function with respect to an appropriate spectral
parameter. By using this method, we overcome the usual high-frequency
restriction associated with absorbing boundary conditions in general variable
potential problems. The resulting fast computational algorithm for absorbing
boundary conditions ensures accuracy over the entire frequency range.",math.NA cs.NA physics.comp-ph,2024-08-01
ExpertAF: Expert Actionable Feedback from Video,,"Feedback is essential for learning a new skill or improving one's current
skill-level. However, current methods for skill-assessment from video only
provide scores or compare demonstrations, leaving the burden of knowing what to
do differently on the user. We introduce a novel method to generate actionable
feedback from video of a person doing a physical activity, such as basketball
or soccer. Our method takes a video demonstration and its accompanying 3D body
pose and generates (1) free-form expert commentary describing what the person
is doing well and what they could improve, and (2) a visual expert
demonstration that incorporates the required corrections. We show how to
leverage Ego-Exo4D's videos of skilled activity and expert commentary together
with a strong language model to create a weakly-supervised training dataset for
this task, and we devise a multimodal video-language model to infer coaching
feedback. Our method is able to reason across multi-modal input combinations to
output full-spectrum, actionable coaching -- expert commentary, expert video
retrieval, and the first-of-its-kind expert pose generation -- outperforming
strong vision-language models on both established metrics and human preference
studies.",cs.CV,2024-08-01
"Modeling stochastic eye tracking data: A comparison of quantum
  generative adversarial networks and Markov models",,"We explore the use of quantum generative adversarial networks QGANs for
modeling eye movement velocity data. We assess whether the advanced
computational capabilities of QGANs can enhance the modeling of complex
stochastic distribution beyond the traditional mathematical models,
particularly the Markov model. The findings indicate that while QGANs
demonstrate potential in approximating complex distributions, the Markov model
consistently outperforms in accurately replicating the real data distribution.
This comparison underlines the challenges and avenues for refinement in time
series data generation using quantum computing techniques. It emphasizes the
need for further optimization of quantum models to better align with real-world
data characteristics.",cs.NE quant-ph,2024-08-01
ChordSync: Conformer-Based Alignment of Chord Annotations to Music Audio,,"In the Western music tradition, chords are the main constituent components of
harmony, a fundamental dimension of music. Despite its relevance for several
Music Information Retrieval (MIR) tasks, chord-annotated audio datasets are
limited and need more diversity. One way to improve those resources is to
leverage the large number of chord annotations available online, but this
requires aligning them with music audio. However, existing audio-to-score
alignment techniques, which typically rely on Dynamic Time Warping (DTW), fail
to address this challenge, as they require weakly aligned data for precise
synchronisation. In this paper, we introduce ChordSync, a novel conformer-based
model designed to seamlessly align chord annotations with audio, eliminating
the need for weak alignment. We also provide a pre-trained model and a
user-friendly library, enabling users to synchronise chord annotations with
audio tracks effortlessly. In this way, ChordSync creates opportunities for
harnessing crowd-sourced chord data for MIR, especially in audio chord
estimation, thereby facilitating the generation of novel datasets.
Additionally, our system extends its utility to music education, enhancing
music learning experiences by providing accurately aligned annotations, thus
enabling learners to engage in synchronised musical practices.",cs.SD cs.LG cs.MM eess.AS,2024-08-01
Leveraging Entailment Judgements in Cross-Lingual Summarisation,,"Synthetically created Cross-Lingual Summarisation (CLS) datasets are prone to
include document-summary pairs where the reference summary is unfaithful to the
corresponding document as it contains content not supported by the document
(i.e., hallucinated content). This low data quality misleads model learning and
obscures evaluation results. Automatic ways to assess hallucinations and
improve training have been proposed for monolingual summarisation,
predominantly in English. For CLS, we propose to use off-the-shelf
cross-lingual Natural Language Inference (X-NLI) to evaluate faithfulness of
reference and model generated summaries. Then, we study training approaches
that are aware of faithfulness issues in the training data and propose an
approach that uses unlikelihood loss to teach a model about unfaithful summary
sequences. Our results show that it is possible to train CLS models that yield
more faithful summaries while maintaining comparable or better informativess.",cs.CL,2024-08-01
"An effect analysis of the balancing techniques on the counterfactual
  explanations of student success prediction models",,"In the past decade, we have experienced a massive boom in the usage of
digital solutions in higher education. Due to this boom, large amounts of data
have enabled advanced data analysis methods to support learners and examine
learning processes. One of the dominant research directions in learning
analytics is predictive modeling of learners' success using various machine
learning methods. To build learners' and teachers' trust in such methods and
systems, exploring the methods and methodologies that enable relevant
stakeholders to deeply understand the underlying machine-learning models is
necessary. In this context, counterfactual explanations from explainable
machine learning tools are promising. Several counterfactual generation methods
hold much promise, but the features must be actionable and causal to be
effective. Thus, obtaining which counterfactual generation method suits the
student success prediction models in terms of desiderata, stability, and
robustness is essential. Although a few studies have been published in recent
years on the use of counterfactual explanations in educational sciences, they
have yet to discuss which counterfactual generation method is more suitable for
this problem. This paper analyzed the effectiveness of commonly used
counterfactual generation methods, such as WhatIf Counterfactual Explanations,
Multi-Objective Counterfactual Explanations, and Nearest Instance
Counterfactual Explanations after balancing. This contribution presents a case
study using the Open University Learning Analytics dataset to demonstrate the
practical usefulness of counterfactual explanations. The results illustrate the
method's effectiveness and describe concrete steps that could be taken to alter
the model's prediction.",cs.LG stat.ML,2024-08-01
"Alpha-VI DeepONet: A prior-robust variational Bayesian approach for
  enhancing DeepONets with uncertainty quantification",,"We introduce a novel deep operator network (DeepONet) framework that
incorporates generalised variational inference (GVI) using R\'enyi's
$\alpha$-divergence to learn complex operators while quantifying uncertainty.
By incorporating Bayesian neural networks as the building blocks for the branch
and trunk networks, our framework endows DeepONet with uncertainty
quantification. The use of R\'enyi's $\alpha$-divergence, instead of the
Kullback-Leibler divergence (KLD), commonly used in standard variational
inference, mitigates issues related to prior misspecification that are
prevalent in Variational Bayesian DeepONets. This approach offers enhanced
flexibility and robustness. We demonstrate that modifying the variational
objective function yields superior results in terms of minimising the mean
squared error and improving the negative log-likelihood on the test set. Our
framework's efficacy is validated across various mechanical systems, where it
outperforms both deterministic and standard KLD-based VI DeepONets in
predictive accuracy and uncertainty quantification. The hyperparameter
$\alpha$, which controls the degree of robustness, can be tuned to optimise
performance for specific problems. We apply this approach to a range of
mechanics problems, including gravity pendulum, advection-diffusion, and
diffusion-reaction systems. Our findings underscore the potential of
$\alpha$-VI DeepONet to advance the field of data-driven operator learning and
its applications in engineering and scientific domains.",stat.ML cs.LG,2024-08-01
Learning in Multi-Objective Public Goods Games with Non-Linear Utilities,,"Addressing the question of how to achieve optimal decision-making under risk
and uncertainty is crucial for enhancing the capabilities of artificial agents
that collaborate with or support humans. In this work, we address this question
in the context of Public Goods Games. We study learning in a novel
multi-objective version of the Public Goods Game where agents have different
risk preferences, by means of multi-objective reinforcement learning. We
introduce a parametric non-linear utility function to model risk preferences at
the level of individual agents, over the collective and individual reward
components of the game. We study the interplay between such preference
modelling and environmental uncertainty on the incentive alignment level in the
game. We demonstrate how different combinations of individual preferences and
environmental uncertainties sustain the emergence of cooperative patterns in
non-cooperative environments (i.e., where competitive strategies are dominant),
while others sustain competitive patterns in cooperative environments (i.e.,
where cooperative strategies are dominant).",cs.MA cs.AI cs.GT,2024-08-01
"Assessing the Variety of a Concept Space Using an Unbiased Estimate of
  Rao's Quadratic Index",,"Past research relates design creativity to 'divergent thinking,' i.e., how
well the concept space is explored during the early phase of design.
Researchers have argued that generating several concepts would increase the
chances of producing better design solutions. 'Variety' is one of the
parameters by which one can quantify the breadth of a concept space explored by
the designers. It is useful to assess variety at the conceptual design stage
because, at this stage, designers have the freedom to explore different
solution principles so as to satisfy a design problem with substantially novel
concepts. This article elaborates on and critically examines the existing
variety metrics from the engineering design literature, discussing their
limitations. A new distance-based variety metric is proposed, along with a
prescriptive framework to support the assessment process. This framework uses
the SAPPhIRE model of causality as a knowledge representation scheme to measure
the real-valued distance between two design concepts. The proposed framework is
implemented in a software tool called 'VariAnT.' Furthermore, the tool's
application is demonstrated through an illustrative example.",cs.CL,2024-08-01
"Can Developers Prompt? A Controlled Experiment for Code Documentation
  Generation",,"Large language models (LLMs) bear great potential for automating tedious
development tasks such as creating and maintaining code documentation. However,
it is unclear to what extent developers can effectively prompt LLMs to create
concise and useful documentation. We report on a controlled experiment with 20
professionals and 30 computer science students tasked with code documentation
generation for two Python functions. The experimental group freely entered
ad-hoc prompts in a ChatGPT-like extension of Visual Studio Code, while the
control group executed a predefined few-shot prompt. Our results reveal that
professionals and students were unaware of or unable to apply prompt
engineering techniques. Especially students perceived the documentation
produced from ad-hoc prompts as significantly less readable, less concise, and
less helpful than documentation from prepared prompts. Some professionals
produced higher quality documentation by just including the keyword Docstring
in their ad-hoc prompts. While students desired more support in formulating
prompts, professionals appreciated the flexibility of ad-hoc prompting.
Participants in both groups rarely assessed the output as perfect. Instead,
they understood the tools as support to iteratively refine the documentation.
Further research is needed to understand which prompting skills and preferences
developers have and which support they need for certain tasks.",cs.AI cs.HC cs.SE,2024-08-01
"Speed Limit Reduction Enhances Urban Worker Safety: Evidence from a
  Decade of Traffic Incidents in Santiago, Chile",,"Work-related traffic incidents significantly impact urban mobility and
productivity. This study analyzes a decade of work-related traffic incident
data (2012--2021) in Santiago, Chile, using records from a major social
insurance company. We explore temporal, spatial, and demographic patterns in
these incidents in urban and rural areas. We also evaluate the impact of a 2018
urban speed limit reduction law on incident injury severity. Using negative
binomial regression, we assess how various factors, including the speed limit
change, affect injury severity measured by prescribed medical leave days.
  Our analysis reveals distinct incident occurrence and severity patterns
across different times, locations, and demographic groups. We find that
motorcycles and cycles are associated with more severe injuries, with marginal
effects of 26.94 and 13.06 additional days of medical leave, respectively,
compared to motorized vehicles. Female workers tend to have less severe
injuries, with an average of 7.57 fewer days of medical leave. Age is also a
significant factor, with each year associated with 0.57 additional days of
leave.
  Notably, the urban speed limit reduction is associated with a decrease of
4.26 days in prescribed medical leave for incidents in urban areas, suggesting
that lower speed limits contribute to reduced injury severity in work-related
traffic incidents. Our results provide insights for urban planning,
transportation policy, and workplace safety initiatives, highlighting the
potential benefits of speed management in urban areas for improving road safety
and minimizing the economic impact of work-related incidents.",cs.CY,2024-08-01
"Kernel-based multi-step predictors for data-driven analysis and control
  of nonlinear systems through the velocity form",,"We propose kernel-based approaches for the construction of a single-step and
multi-step predictor of the velocity form of nonlinear (NL) systems, which
describes the time-difference dynamics of the corresponding NL system and
admits a highly structured representation. The predictors in turn allow to
formulate completely data-driven representations of the velocity form. The
kernel-based formulation that we derive, inherently respects the structured
quasi-linear and specific time-dependent relationship of the velocity form.
This results in an efficient multi-step predictor for the velocity form and
hence for nonlinear systems. Moreover, by using the velocity form, our methods
open the door for data-driven behavioral analysis and control of nonlinear
systems with global stability and performance guarantees.",eess.SY cs.SY,2024-08-01
Convergence analysis of inner-iteration preconditioned GMRES,,"The objective of this paper is to understand the superlinear convergence
behavior of the inner-iteration preconditioned GMRES method. In order to
understand the phenomenon, we analyze the convergence using the Vandermonde
matrix which is defined using the eigenvalues of the coefficient matrix.
Although eigenvalues alone cannot explain the convergence, they may provide an
upper bound of the residual, together with the right hand side vector and the
eigenvectors. For the diagonalizable case, if the eigenvalues of the
coefficient matrix are clustered, the upper bound of the convergence curve
shows superlinear convergence, when the norm of the matrix obtained by
decomposing the right hand side vector into the eigenvector components is not
so large. We especially analyze the effect of inner-iteration preconditioning
for least squares problems, where the eigenvalues cluster towards 1.",math.NA cs.NA,2024-08-01
Accelerating Full Waveform Inversion By Transfer Learning,,"Full waveform inversion (FWI) is a powerful tool for reconstructing material
fields based on sparsely measured data obtained by wave propagation. For
specific problems, discretizing the material field with a neural network (NN)
improves the robustness and reconstruction quality of the corresponding
optimization problem. We call this method NN-based FWI. Starting from an
initial guess, the weights of the NN are iteratively updated to fit the
simulated wave signals to the sparsely measured data set. For gradient-based
optimization, a suitable choice of the initial guess, i.e., a suitable NN
weight initialization, is crucial for fast and robust convergence.
  In this paper, we introduce a novel transfer learning approach to further
improve NN-based FWI. This approach leverages supervised pretraining to provide
a better NN weight initialization, leading to faster convergence of the
subsequent optimization problem. Moreover, the inversions yield physically more
meaningful local minima. The network is pretrained to predict the unknown
material field using the gradient information from the first iteration of
conventional FWI. In our computational experiments on two-dimensional domains,
the training data set consists of reference simulations with arbitrarily
positioned elliptical voids of different shapes and orientations. We compare
the performance of the proposed transfer learning NN-based FWI with three other
methods: conventional FWI, NN-based FWI without pretraining and conventional
FWI with an initial guess predicted from the pretrained NN. Our results show
that transfer learning NN-based FWI outperforms the other methods in terms of
convergence speed and reconstruction quality.",cs.LG cs.AI,2024-08-01
"Granular-Balls based Fuzzy Twin Support Vector Machine for
  Classification",,"The twin support vector machine (TWSVM) classifier has attracted increasing
attention because of its low computational complexity. However, its performance
tends to degrade when samples are affected by noise. The granular-ball fuzzy
support vector machine (GBFSVM) classifier partly alleviates the adverse
effects of noise, but it relies solely on the distance between the
granular-ball's center and the class center to design the granular-ball
membership function. In this paper, we first introduce the granular-ball twin
support vector machine (GBTWSVM) classifier, which integrates granular-ball
computing (GBC) with the twin support vector machine (TWSVM) classifier. By
replacing traditional point inputs with granular-balls, we demonstrate how to
derive a pair of non-parallel hyperplanes for the GBTWSVM classifier by solving
a quadratic programming problem. Subsequently, we design the membership and
non-membership functions of granular-balls using Pythagorean fuzzy sets to
differentiate the contributions of granular-balls in various regions.
Additionally, we develop the granular-ball fuzzy twin support vector machine
(GBFTSVM) classifier by incorporating GBC with the fuzzy twin support vector
machine (FTSVM) classifier. We demonstrate how to derive a pair of non-parallel
hyperplanes for the GBFTSVM classifier by solving a quadratic programming
problem. We also design algorithms for the GBTSVM classifier and the GBFTSVM
classifier. Finally, the superior classification performance of the GBTWSVM
classifier and the GBFTSVM classifier on 20 benchmark datasets underscores
their scalability, efficiency, and robustness in tackling classification tasks.",cs.LG,2024-08-01
"You Can't Ignore Either: Unifying Structure and Feature Denoising for
  Robust Graph Learning",,"Recent research on the robustness of Graph Neural Networks (GNNs) under
noises or attacks has attracted great attention due to its importance in
real-world applications. Most previous methods explore a single noise source,
recovering corrupt node embedding by reliable structures bias or developing
structure learning with reliable node features. However, the noises and attacks
may come from both structures and features in graphs, making the graph
denoising a dilemma and challenging problem. In this paper, we develop a
unified graph denoising (UGD) framework to unravel the deadlock between
structure and feature denoising. Specifically, a high-order neighborhood
proximity evaluation method is proposed to recognize noisy edges, considering
features may be perturbed simultaneously. Moreover, we propose to refine noisy
features with reconstruction based on a graph auto-encoder. An iterative
updating algorithm is further designed to optimize the framework and acquire a
clean graph, thus enabling robust graph learning for downstream tasks. Our UGD
framework is self-supervised and can be easily implemented as a plug-and-play
module. We carry out extensive experiments, which proves the effectiveness and
advantages of our method. Code is avalaible at
https://github.com/YoungTimmy/UGD.",cs.LG,2024-08-01
Joint Neural Networks for One-shot Object Recognition and Detection,,"This paper presents a novel joint neural networks approach to address the
challenging one-shot object recognition and detection tasks. Inspired by
Siamese neural networks and state-of-art multi-box detection approaches, the
joint neural networks are able to perform object recognition and detection for
categories that remain unseen during the training process. Following the
one-shot object recognition/detection constraints, the training and testing
datasets do not contain overlapped classes, in other words, all the test
classes remain unseen during training. The joint networks architecture is able
to effectively compare pairs of images via stacked convolutional layers of the
query and target inputs, recognising patterns of the same input query category
without relying on previous training around this category. The proposed
approach achieves 61.41% accuracy for one-shot object recognition on the
MiniImageNet dataset and 47.1% mAP for one-shot object detection when trained
on the COCO dataset and tested using the Pascal VOC dataset. Code available at
https://github.com/cjvargasc/JNN recog and https://github.com/cjvargasc/JNN
detection/",cs.CV,2024-08-01
Future Directions in Human Mobility Science,,"We provide a brief review of human mobility science and present three key
areas where we expect to see substantial advancements. We start from the mind
and discuss the need to better understand how spatial cognition shapes mobility
patterns. We then move to societies and argue the importance of better
understanding new forms of transportation. We conclude by discussing how
algorithms shape mobility behaviour and provide useful tools for modellers.
Finally, we discuss how progress in these research directions may help us
address some of the challenges our society faces today.",physics.soc-ph cs.CY,2024-08-01
Future of Artificial Intelligence in Agile Software Development,,"The advent of Artificial intelligence has promising advantages that can be
utilized to transform the landscape of software project development. The
Software process framework consists of activities that constantly require
routine human interaction, leading to the possibility of errors and
uncertainties. AI can assist software development managers, software testers,
and other team members by leveraging LLMs, GenAI models, and AI agents to
perform routine tasks, risk analysis and prediction, strategy recommendations,
and support decision making. AI has the potential to increase efficiency and
reduce the risks encountered by the project management team while increasing
the project success rates. Additionally, it can also break down complex notions
and development processes for stakeholders to make informed decisions. In this
paper, we propose an approach in which AI tools and technologies can be
utilized to bestow maximum assistance for agile software projects, which have
become increasingly favored in the industry in recent years.",cs.SE cs.AI,2024-08-01
Segment-Based Test Case Prioritization: A Multi-objective Approach,,"Regression testing of software is a crucial but time-consuming task,
especially in the context of user interface (UI) testing where multiple
microservices must be validated simultaneously. Test case prioritization (TCP)
is a cost-efficient solution to address this by scheduling test cases in an
execution order that maximizes an objective function, generally aimed at
increasing the fault detection rate. While several techniques have been
proposed for TCP, most rely on source code information which is usually not
available for UI testing. In this paper, we introduce a multi-objective
optimization approach to prioritize UI test cases, using evolutionary search
algorithms and four coverage criteria focusing on web page elements as
objectives for the optimization problem. Our method, which does not require
source code information, is evaluated using two evolutionary algorithms
(AGE-MOEA and NSGA-II) and compared with other TCP methods on a self-collected
dataset of 11 test suites. The results show that our approach significantly
outperforms other methods in terms of Average Percentage of Faults Detected
(APFD) and APFD with Cost (APFDc), achieving the highest scores of 87.8\% and
79.2\%, respectively. We also introduce a new dataset and demonstrate the
significant improvement of our approach over existing ones via empirical
experiments. The paper's contributions include the application of web page
segmentation in TCP, the construction of a new dataset for UI TCP, and
empirical comparisons that demonstrate the improvement of our approach.",cs.SE,2024-08-01
Point-supervised Brain Tumor Segmentation with Box-prompted MedSAM,,"Delineating lesions and anatomical structure is important for image-guided
interventions. Point-supervised medical image segmentation (PSS) has great
potential to alleviate costly expert delineation labeling. However, due to the
lack of precise size and boundary guidance, the effectiveness of PSS often
falls short of expectations. Although recent vision foundational models, such
as the medical segment anything model (MedSAM), have made significant
advancements in bounding-box-prompted segmentation, it is not straightforward
to utilize point annotation, and is prone to semantic ambiguity. In this
preliminary study, we introduce an iterative framework to facilitate
semantic-aware point-supervised MedSAM. Specifically, the semantic box-prompt
generator (SBPG) module has the capacity to convert the point input into
potential pseudo bounding box suggestions, which are explicitly refined by the
prototype-based semantic similarity. This is then succeeded by a prompt-guided
spatial refinement (PGSR) module that harnesses the exceptional
generalizability of MedSAM to infer the segmentation mask, which also updates
the box proposal seed in SBPG. Performance can be progressively improved with
adequate iterations. We conducted an evaluation on BraTS2018 for the
segmentation of whole brain tumors and demonstrated its superior performance
compared to traditional PSS methods and on par with box-supervised methods.",cs.CV cs.AI cs.LG eess.IV physics.med-ph,2024-08-01
"Synthetic dual image generation for reduction of labeling efforts in
  semantic segmentation of micrographs with a customized metric function",,"Training of semantic segmentation models for material analysis requires
micrographs and their corresponding masks. It is quite unlikely that perfect
masks will be drawn, especially at the edges of objects, and sometimes the
amount of data that can be obtained is small, since only a few samples are
available. These aspects make it very problematic to train a robust model. We
demonstrate a workflow for the improvement of semantic segmentation models of
micrographs through the generation of synthetic microstructural images in
conjunction with masks. The workflow only requires joining a few micrographs
with their respective masks to create the input for a Vector
Quantised-Variational AutoEncoder model that includes an embedding space, which
is trained such that a generative model (PixelCNN) learns the distribution of
each input, transformed into discrete codes, and can be used to sample new
codes. The latter will eventually be decoded by VQ-VAE to generate images
alongside corresponding masks for semantic segmentation. To evaluate the
synthetic data, we have trained U-Net models with different amounts of these
synthetic data in conjunction with real data. These models were then evaluated
using non-synthetic images only. Additionally, we introduce a customized metric
derived from the mean Intersection over Union (mIoU). The proposed metric
prevents a few falsely predicted pixels from greatly reducing the value of the
mIoU. We have achieved a reduction in sample preparation and acquisition times,
as well as the efforts, needed for image processing and labeling tasks, are
less when it comes to training semantic segmentation model. The approach could
be generalized to various types of image data such that it serves as a
user-friendly solution for training models with a small number of real images.",cs.CV cs.CE cs.LG,2024-08-01
"Investigating Brain Connectivity and Regional Statistics from EEG for
  early stage Parkinson's Classification",,"We evaluate the effectiveness of combining brain connectivity metrics with
signal statistics for early stage Parkinson's Disease (PD) classification using
electroencephalogram data (EEG). The data is from 5 arousal states - wakeful
and four sleep stages (N1, N2, N3 and REM). Our pipeline uses an Ada Boost
model for classification on a challenging early stage PD classification task
with with only 30 participants (11 PD , 19 Healthy Control). Evaluating 9 brain
connectivity metrics we find the best connectivity metric to be different for
each arousal state with Phase Lag Index achieving the highest individual
classification accuracy of 86\% on N1 data. Further to this our pipeline using
regional signal statistics achieves an accuracy of 78\%, using brain
connectivity only achieves an accuracy of 86\% whereas combining the two
achieves a best accuracy of 91\%. This best performance is achieved on N1 data
using Phase Lag Index (PLI) combined with statistics derived from the frequency
characteristics of the EEG signal. This model also achieves a recall of 80 \%
and precision of 96\%. Furthermore we find that on data from each arousal
state, combining PLI with regional signal statistics improves classification
accuracy versus using signal statistics or brain connectivity alone. Thus we
conclude that combining brain connectivity statistics with regional EEG
statistics is optimal for classifier performance on early stage Parkinson's.
Additionally, we find outperformance of N1 EEG for classification of
Parkinson's and expect this could be due to disrupted N1 sleep in PD. This
should be explored in future work.",q-bio.NC cs.AI eess.SP,2024-08-01
MotionFix: Text-Driven 3D Human Motion Editing,,"The focus of this paper is 3D motion editing. Given a 3D human motion and a
textual description of the desired modification, our goal is to generate an
edited motion as described by the text. The challenges include the lack of
training data and the design of a model that faithfully edits the source
motion. In this paper, we address both these challenges. We build a methodology
to semi-automatically collect a dataset of triplets in the form of (i) a source
motion, (ii) a target motion, and (iii) an edit text, and create the new
MotionFix dataset. Having access to such data allows us to train a conditional
diffusion model, TMED, that takes both the source motion and the edit text as
input. We further build various baselines trained only on text-motion pairs
datasets, and show superior performance of our model trained on triplets. We
introduce new retrieval-based metrics for motion editing and establish a new
benchmark on the evaluation set of MotionFix. Our results are encouraging,
paving the way for further research on finegrained motion generation. Code and
models will be made publicly available.",cs.CV cs.GR,2024-08-01
SAM 2: Segment Anything in Images and Videos,,"We present Segment Anything Model 2 (SAM 2), a foundation model towards
solving promptable visual segmentation in images and videos. We build a data
engine, which improves model and data via user interaction, to collect the
largest video segmentation dataset to date. Our model is a simple transformer
architecture with streaming memory for real-time video processing. SAM 2
trained on our data provides strong performance across a wide range of tasks.
In video segmentation, we observe better accuracy, using 3x fewer interactions
than prior approaches. In image segmentation, our model is more accurate and 6x
faster than the Segment Anything Model (SAM). We believe that our data, model,
and insights will serve as a significant milestone for video segmentation and
related perception tasks. We are releasing a version of our model, the dataset
and an interactive demo.",cs.CV cs.AI cs.LG,2024-08-01
"A Natural Language Processing Framework for Hotel Recommendation Based
  on Users' Text Reviews",,"Recently, the application of Artificial Intelligence algorithms in hotel
recommendation systems has become an increasingly popular topic. One such
method that has proven to be effective in this field is Deep Learning,
especially Natural Language processing models, which are able to extract
semantic knowledge from user's text reviews to create more efficient
recommendation systems. This can lead to the development of intelligent models
that can classify a user's preferences and emotions based on their feedback in
the form of text reviews about their hotel stay experience. In this study, we
propose a Natural Language Processing framework that utilizes customer text
reviews to provide personalized recommendations for the most appropriate hotel
based on their preferences. The framework is based on Bidirectional Encoder
Representations from Transformers (BERT) and a fine-tuning/validation pipeline
that categorizes customer hotel review texts into ""Bad,"" ""Good,"" or ""Excellent""
recommended hotels. Our findings indicate that the hotel recommendation system
we propose can significantly enhance the user experience of booking
accommodations by providing personalized recommendations based on user
preferences and previous booking history.",cs.LG,2024-08-01
"A Multi-Reference Relaxation Enforced Neighborhood Search Heuristic in
  SCIP",,"This paper proposes and evaluates a Multi-Reference Relaxation Enforced
Neighborhood Search (MRENS) heuristic within the SCIP solver. This study marks
the first integration and evaluation of MRENS in a full-fledged MILP solver,
specifically coupled with the recently-introduced Lagromory separator for
generating multiple reference solutions. Computational experiments on the
MIPLIB 2017 benchmark set show that MRENS, with multiple reference solutions,
improves the solver's ability to find higher-quality feasible solutions
compared to single-reference approaches. This study highlights the potential of
multi-reference heuristics in enhancing primal heuristics in MILP solvers.",math.OC cs.MS,2024-08-01
"Pathway to Secure and Trustworthy 6G for LLMs: Attacks, Defense, and
  Opportunities",,"Recently, large language models (LLMs) have been gaining a lot of interest
due to their adaptability and extensibility in emerging applications, including
communication networks. It is anticipated that 6G mobile edge computing
networks will be able to support LLMs as a service, as they provide ultra
reliable low-latency communications and closed loop massive connectivity.
However, LLMs are vulnerable to data and model privacy issues that affect the
trustworthiness of LLMs to be deployed for user-based services. In this paper,
we explore the security vulnerabilities associated with fine-tuning LLMs in 6G
networks, in particular the membership inference attack. We define the
characteristics of an attack network that can perform a membership inference
attack if the attacker has access to the fine-tuned model for the downstream
task. We show that the membership inference attacks are effective for any
downstream task, which can lead to a personal data breach when using LLM as a
service. The experimental results show that the attack success rate of maximum
92% can be achieved on named entity recognition task. Based on the experimental
analysis, we discuss possible defense mechanisms and present possible research
directions to make the LLMs more trustworthy in the context of 6G networks.",cs.CR cs.AI cs.DC,2024-08-01
"An Empirical Analysis of Compute-Optimal Inference for Problem-Solving
  with Language Models",,"The optimal training configurations of large language models (LLMs) with
respect to model sizes and compute budgets have been extensively studied. But
how to optimally configure LLMs during inference has not been explored in
sufficient depth. We study compute-optimal inference: designing models and
inference strategies that optimally trade off additional inference-time compute
for improved performance. As a first step towards understanding and designing
compute-optimal inference methods, we assessed the effectiveness and
computational efficiency of multiple inference strategies such as Greedy
Search, Majority Voting, Best-of-N, Weighted Voting, and their variants on two
different Tree Search algorithms, involving different model sizes and
computational budgets. We found that a smaller language model with a novel tree
search algorithm typically achieves a Pareto-optimal trade-off. These results
highlight the potential benefits of deploying smaller models equipped with more
sophisticated decoding algorithms in budget-constrained scenarios, e.g., on
end-devices, to enhance problem-solving accuracy. For instance, we show that
the Llemma-7B model can achieve competitive accuracy to a Llemma-34B model on
MATH500 while using $2\times$ less FLOPs. Our findings could potentially apply
to any generation task with a well-defined measure of success.",cs.AI,2024-08-01
"Improving Retrieval-Augmented Generation in Medicine with Iterative
  Follow-up Questions",,"The emergent abilities of large language models (LLMs) have demonstrated
great potential in solving medical questions. They can possess considerable
medical knowledge, but may still hallucinate and are inflexible in the
knowledge updates. While Retrieval-Augmented Generation (RAG) has been proposed
to enhance the medical question-answering capabilities of LLMs with external
knowledge bases, it may still fail in complex cases where multiple rounds of
information-seeking are required. To address such an issue, we propose
iterative RAG for medicine (i-MedRAG), where LLMs can iteratively ask follow-up
queries based on previous information-seeking attempts. In each iteration of
i-MedRAG, the follow-up queries will be answered by a vanilla RAG system and
they will be further used to guide the query generation in the next iteration.
Our experiments show the improved performance of various LLMs brought by
i-MedRAG compared with vanilla RAG on complex questions from clinical vignettes
in the United States Medical Licensing Examination (USMLE), as well as various
knowledge tests in the Massive Multitask Language Understanding (MMLU) dataset.
Notably, our zero-shot i-MedRAG outperforms all existing prompt engineering and
fine-tuning methods on GPT-3.5, achieving an accuracy of 69.68\% on the MedQA
dataset. In addition, we characterize the scaling properties of i-MedRAG with
different iterations of follow-up queries and different numbers of queries per
iteration. Our case studies show that i-MedRAG can flexibly ask follow-up
queries to form reasoning chains, providing an in-depth analysis of medical
questions. To the best of our knowledge, this is the first-of-its-kind study on
incorporating follow-up queries into medical RAG.",cs.CL cs.AI,2024-08-01
CERT-ED: Certifiably Robust Text Classification for Edit Distance,,"With the growing integration of AI in daily life, ensuring the robustness of
systems to inference-time attacks is crucial. Among the approaches for
certifying robustness to such adversarial examples, randomized smoothing has
emerged as highly promising due to its nature as a wrapper around arbitrary
black-box models. Previous work on randomized smoothing in natural language
processing has primarily focused on specific subsets of edit distance
operations, such as synonym substitution or word insertion, without exploring
the certification of all edit operations. In this paper, we adapt Randomized
Deletion (Huang et al., 2023) and propose, CERTified Edit Distance defense
(CERT-ED) for natural language classification. Through comprehensive
experiments, we demonstrate that CERT-ED outperforms the existing Hamming
distance method RanMASK (Zeng et al., 2023) in 4 out of 5 datasets in terms of
both accuracy and the cardinality of the certificate. By covering various
threat models, including 5 direct and 5 transfer attacks, our method improves
empirical robustness in 38 out of 50 settings.",cs.CL cs.CR cs.LG,2024-08-01
Litmus: Fair Pricing for Serverless Computing,,"Serverless computing has emerged as a market-dominant paradigm in modern
cloud computing, benefiting both cloud providers and tenants. While service
providers can optimize their machine utilization, tenants only need to pay for
the resources they use. To maximize resource utilization, these serverless
systems co-run numerous short-lived functions, bearing frequent system
condition shifts. When the system gets overcrowded, a tenant's function may
suffer from disturbing slowdowns. Ironically, tenants also incur higher costs
during these slowdowns, as commercial serverless platforms determine costs
proportional to their execution times.
  This paper argues that cloud providers should compensate tenants for losses
incurred when the server is over-provisioned. However, estimating tenants'
losses is challenging without pre-profiled information about their functions.
Prior studies have indicated that assessing tenant losses leads to heavy
overheads. As a solution, this paper introduces a new pricing model that offers
discounts based on the machine's state while presuming the tenant's loss under
that state. To monitor the machine state accurately, Litmus pricing frequently
conducts Litmus tests, an effective and lightweight solution for measuring
system congestion. Our experiments show that Litmus pricing can accurately
gauge the impact of system congestion and offer nearly ideal prices, with only
a 0.2% price difference on average, in a heavily congested system.",cs.DC,2024-08-01
TurboEdit: Text-Based Image Editing Using Few-Step Diffusion Models,,"Diffusion models have opened the path to a wide range of text-based image
editing frameworks. However, these typically build on the multi-step nature of
the diffusion backwards process, and adapting them to distilled, fast-sampling
methods has proven surprisingly challenging. Here, we focus on a popular line
of text-based editing frameworks - the ``edit-friendly'' DDPM-noise inversion
approach. We analyze its application to fast sampling methods and categorize
its failures into two classes: the appearance of visual artifacts, and
insufficient editing strength. We trace the artifacts to mismatched noise
statistics between inverted noises and the expected noise schedule, and suggest
a shifted noise schedule which corrects for this offset. To increase editing
strength, we propose a pseudo-guidance approach that efficiently increases the
magnitude of edits without introducing new artifacts. All in all, our method
enables text-based image editing with as few as three diffusion steps, while
providing novel insights into the mechanisms behind popular text-based editing
approaches.",cs.CV cs.GR,2024-08-01
"Virchow 2: Scaling Self-Supervised Mixed Magnification Models in
  Pathology",,"Foundation models are rapidly being developed for computational pathology
applications. However, it remains an open question which factors are most
important for downstream performance with data scale and diversity, model size,
and training algorithm all playing a role. In this work, we present the result
of scaling both data and model size, surpassing previous studies in both
dimensions, and introduce two new models: Virchow 2, a 632M parameter vision
transformer, and Virchow 2G, a 1.85B parameter vision transformer, each trained
with 3.1M histopathology whole slide images. To support this scale, we propose
domain-inspired adaptations to the DINOv2 training algorithm, which is quickly
becoming the default method in self-supervised learning for computational
pathology. We achieve state of the art performance on twelve tile-level tasks,
as compared to the top performing competing models. Our results suggest that
data diversity and domain-specific training can outperform models that only
scale in the number of parameters, but, on average, performance benefits from
domain-tailoring, data scale, and model scale.",cs.CV,2024-08-01
"DynamoLLM: Designing LLM Inference Clusters for Performance and Energy
  Efficiency",,"The rapid evolution and widespread adoption of generative large language
models (LLMs) have made them a pivotal workload in various applications. Today,
LLM inference clusters receive a large number of queries with strict Service
Level Objectives (SLOs). To achieve the desired performance, these models
execute on power-hungry GPUs causing the inference clusters to consume large
amount of energy and, consequently, result in excessive carbon emissions.
Fortunately, we find that there is a great opportunity to exploit the
heterogeneity in inference compute properties and fluctuations in inference
workloads, to significantly improve energy-efficiency. However, such a diverse
and dynamic environment creates a large search-space where different system
configurations (e.g., number of instances, model parallelism, and GPU
frequency) translate into different energy-performance trade-offs. To address
these challenges, we propose DynamoLLM, the first energy-management framework
for LLM inference environments. DynamoLLM automatically and dynamically
reconfigures the inference cluster to optimize for energy and cost of LLM
serving under the service's performance SLOs. We show that at a service-level,
DynamoLLM conserves 53% energy and 38% operational carbon emissions, and
reduces 61% cost to the customer, while meeting the latency SLOs.",cs.AI cs.AR cs.DC,2024-08-01
"Collaborative Vision-Text Representation Optimizing for Open-Vocabulary
  Segmentation",,"Pre-trained vision-language models, e.g. CLIP, have been increasingly used to
address the challenging Open-Vocabulary Segmentation (OVS) task, benefiting
from their well-aligned vision-text embedding space. Typical solutions involve
either freezing CLIP during training to unilaterally maintain its zero-shot
capability, or fine-tuning CLIP vision encoder to achieve perceptual
sensitivity to local regions. However, few of them incorporate vision-text
collaborative optimization. Based on this, we propose the Content-Dependent
Transfer to adaptively enhance each text embedding by interacting with the
input image, which presents a parameter-efficient way to optimize the text
representation. Besides, we additionally introduce a Representation
Compensation strategy, reviewing the original CLIP-V representation as
compensation to maintain the zero-shot capability of CLIP. In this way, the
vision and text representation of CLIP are optimized collaboratively, enhancing
the alignment of the vision-text feature space. To the best of our knowledge,
we are the first to establish the collaborative vision-text optimizing
mechanism within the OVS field. Extensive experiments demonstrate our method
achieves superior performance on popular OVS benchmarks. In open-vocabulary
semantic segmentation, our method outperforms the previous state-of-the-art
approaches by +0.5, +2.3, +3.4, +0.4 and +1.1 mIoU, respectively on A-847,
A-150, PC-459, PC-59 and PAS-20. Furthermore, in a panoptic setting on ADE20K,
we achieve the performance of 27.1 PQ, 73.5 SQ, and 32.9 RQ. Code will be
available at https://github.com/jiaosiyu1999/MAFT-Plus.git .",cs.CV,2024-08-01
Leaf Angle Estimation using Mask R-CNN and LETR Vision Transformer,,"Modern day studies show a high degree of correlation between high yielding
crop varieties and plants with upright leaf angles. It is observed that plants
with upright leaf angles intercept more light than those without upright leaf
angles, leading to a higher rate of photosynthesis. Plant scientists and
breeders benefit from tools that can directly measure plant parameters in the
field i.e. on-site phenotyping. The estimation of leaf angles by manual means
in a field setting is tedious and cumbersome. We mitigate the tedium using a
combination of the Mask R-CNN instance segmentation neural network, and Line
Segment Transformer (LETR), a vision transformer. The proposed Computer Vision
(CV) pipeline is applied on two image datasets, Summer 2015-Ames ULA and Summer
2015- Ames MLA, with a combined total of 1,827 plant images collected in the
field using FieldBook, an Android application aimed at on-site phenotyping. The
leaf angles estimated by the proposed pipeline on the image datasets are
compared to two independent manual measurements using ImageJ, a Java-based
image processing program developed at the National Institutes of Health and the
Laboratory for Optical and Computational Instrumentation. The results, when
compared for similarity using the Cosine Similarity measure, exhibit 0.98
similarity scores on both independent measurements of Summer 2015-Ames ULA and
Summer 2015-Ames MLA image datasets, demonstrating the feasibility of the
proposed pipeline for on-site measurement of leaf angles.",cs.CV cs.AI cs.LG,2024-08-01
"Algebraic power series and their automatic complexity II: modulo prime
  powers",,"Christol and, independently, Denef and Lipshitz showed that an algebraic
sequence of $p$-adic integers (or integers) is $p$-automatic when reduced
modulo $p^\alpha$. Previously, the best known bound on the minimal automaton
size for such a sequence was doubly exponential in $\alpha$. We improve this
bound to the order of $p^{\alpha^3 h d}$, where $h$ and $d$ are the height and
degree of the minimal annihilating polynomial. We achieve this bound by showing
that all states in the automaton are naturally represented in a new numeration
system. This significantly restricts the set of possible states. Since our
approach embeds algebraic sequences as diagonals of rational functions, we also
obtain bounds more generally for diagonals of multivariate rational functions.",math.NT cs.FL cs.SC,2024-08-01
"A Policy-Gradient Approach to Solving Imperfect-Information Games with
  Iterate Convergence",,"Policy gradient methods have become a staple of any single-agent
reinforcement learning toolbox, due to their combination of desirable
properties: iterate convergence, efficient use of stochastic trajectory
feedback, and theoretically-sound avoidance of importance sampling corrections.
In multi-agent imperfect-information settings (extensive-form games), however,
it is still unknown whether the same desiderata can be guaranteed while
retaining theoretical guarantees. Instead, sound methods for extensive-form
games rely on approximating counterfactual values (as opposed to Q values),
which are incompatible with policy gradient methodologies. In this paper, we
investigate whether policy gradient can be safely used in two-player zero-sum
imperfect-information extensive-form games (EFGs). We establish positive
results, showing for the first time that a policy gradient method leads to
provable best-iterate convergence to a regularized Nash equilibrium in
self-play.",cs.GT cs.AI cs.LG stat.ML,2024-08-01
"A deep learning-enabled smart garment for versatile sleep behaviour
  monitoring",,"Continuous monitoring and accurate detection of complex sleep patterns
associated to different sleep-related conditions is essential, not only for
enhancing sleep quality but also for preventing the risk of developing chronic
illnesses associated to unhealthy sleep. Despite significant advances in
research, achieving versatile recognition of various unhealthy and sub-healthy
sleep patterns with simple wearable devices at home remains a significant
challenge. Here, we report a robust and durable ultrasensitive strain sensor
array printed on a smart garment, in its collar region. This solution allows
detecting subtle vibrations associated with multiple sleep patterns at the
extrinsic laryngeal muscles. Equipped with a deep learning neural network, it
can precisely identify six sleep states-nasal breathing, mouth breathing,
snoring, bruxism, central sleep apnea (CSA), and obstructive sleep apnea
(OSA)-with an impressive accuracy of 98.6%, all without requiring specific
positioning. We further demonstrate its explainability and generalization
capabilities in practical applications. Explainable artificial intelligence
(XAI) visualizations reflect comprehensive signal pattern analysis with low
bias. Transfer learning tests show that the system can achieve high accuracy
(overall accuracy of 95%) on new users with very few-shot learning (less than
15 samples per class). The scalable manufacturing process, robustness, high
accuracy, and excellent generalization of the smart garment make it a promising
tool for next-generation continuous sleep monitoring.",eess.SP cs.AI,2024-08-01
"Coarse Correspondence Elicit 3D Spacetime Understanding in Multimodal
  Language Model",,"Multimodal language models (MLLMs) are increasingly being implemented in
real-world environments, necessitating their ability to interpret 3D spaces and
comprehend temporal dynamics. Despite their potential, current top models
within our community still fall short in adequately understanding spatial and
temporal dimensions. We introduce Coarse Correspondence, a simple,
training-free, effective, and general-purpose visual prompting method to elicit
3D and temporal understanding in multimodal LLMs. Our method uses a lightweight
tracking model to find object correspondences between frames in a video or
between sets of image viewpoints. It selects the most frequent object instances
and visualizes them with markers with unique IDs in the image. With this simple
approach, we achieve state-of-the-art results on 3D understanding benchmarks
including ScanQA (+20.5\%) and a subset of OpenEQA (+9.7\%), and on long-form
video benchmarks such as EgoSchema (+6.0\%). We also curate a small diagnostic
dataset to evaluate whether MLLMs can reason about space from a described
viewpoint other than the camera viewpoint. Again, Coarse Correspondence
improves spatial perspective-taking abilities but we highlight that MLLMs
struggle with this task. Together, we demonstrate that our simple prompting
method can significantly aid downstream tasks that require 3D or temporal
reasoning.",cs.CV cs.LG,2024-08-01
Text-Guided Video Masked Autoencoder,,"Recent video masked autoencoder (MAE) works have designed improved masking
algorithms focused on saliency. These works leverage visual cues such as motion
to mask the most salient regions. However, the robustness of such visual cues
depends on how often input videos match underlying assumptions. On the other
hand, natural language description is an information dense representation of
video that implicitly captures saliency without requiring modality-specific
assumptions, and has not been explored yet for video MAE. To this end, we
introduce a novel text-guided masking algorithm (TGM) that masks the video
regions with highest correspondence to paired captions. Without leveraging any
explicit visual cues for saliency, our TGM is competitive with state-of-the-art
masking algorithms such as motion-guided masking. To further benefit from the
semantics of natural language for masked reconstruction, we next introduce a
unified framework for joint MAE and masked video-text contrastive learning. We
show that across existing masking algorithms, unifying MAE and masked
video-text contrastive learning improves downstream performance compared to
pure MAE on a variety of video recognition tasks, especially for linear probe.
Within this unified framework, our TGM achieves the best relative performance
on five action recognition and one egocentric datasets, highlighting the
complementary nature of natural language for masked video modeling.",cs.CV,2024-08-01
"Smoothed Energy Guidance: Guiding Diffusion Models with Reduced Energy
  Curvature of Attention",,"Conditional diffusion models have shown remarkable success in visual content
generation, producing high-quality samples across various domains, largely due
to classifier-free guidance (CFG). Recent attempts to extend guidance to
unconditional models have relied on heuristic techniques, resulting in
suboptimal generation quality and unintended effects. In this work, we propose
Smoothed Energy Guidance (SEG), a novel training- and condition-free approach
that leverages the energy-based perspective of the self-attention mechanism to
enhance image generation. By defining the energy of self-attention, we
introduce a method to reduce the curvature of the energy landscape of attention
and use the output as the unconditional prediction. Practically, we control the
curvature of the energy landscape by adjusting the Gaussian kernel parameter
while keeping the guidance scale parameter fixed. Additionally, we present a
query blurring method that is equivalent to blurring the entire attention
weights without incurring quadratic complexity in the number of tokens. In our
experiments, SEG achieves a Pareto improvement in both quality and the
reduction of side effects. The code is available at
\url{https://github.com/SusungHong/SEG-SDXL}.",cs.CV cs.AI cs.LG,2024-08-01
"UniTalker: Scaling up Audio-Driven 3D Facial Animation through A Unified
  Model",,"Audio-driven 3D facial animation aims to map input audio to realistic facial
motion. Despite significant progress, limitations arise from inconsistent 3D
annotations, restricting previous models to training on specific annotations
and thereby constraining the training scale. In this work, we present
UniTalker, a unified model featuring a multi-head architecture designed to
effectively leverage datasets with varied annotations. To enhance training
stability and ensure consistency among multi-head outputs, we employ three
training strategies, namely, PCA, model warm-up, and pivot identity embedding.
To expand the training scale and diversity, we assemble A2F-Bench, comprising
five publicly available datasets and three newly curated datasets. These
datasets contain a wide range of audio domains, covering multilingual speech
voices and songs, thereby scaling the training data from commonly employed
datasets, typically less than 1 hour, to 18.5 hours. With a single trained
UniTalker model, we achieve substantial lip vertex error reductions of 9.2% for
BIWI dataset and 13.7% for Vocaset. Additionally, the pre-trained UniTalker
exhibits promise as the foundation model for audio-driven facial animation
tasks. Fine-tuning the pre-trained UniTalker on seen datasets further enhances
performance on each dataset, with an average error reduction of 6.3% on
A2F-Bench. Moreover, fine-tuning UniTalker on an unseen dataset with only half
the data surpasses prior state-of-the-art models trained on the full dataset.
The code and dataset are available at the project page
https://github.com/X-niper/UniTalker.",cs.CV,2024-08-01
"AgentGen: Enhancing Planning Abilities for Large Language Model based
  Agent via Environment and Task Generation",,"Large Language Model (LLM) based agents have garnered significant attention
and are becoming increasingly popular. Furthermore, planning ability is a
crucial component of an LLM-based agent, involving interaction with the
environment and executing actions to complete a planning task, which generally
entails achieving a desired goal from an initial state. This paper investigates
enhancing the planning abilities of LLMs through instruction tuning, referred
to as agent training. Recent studies have demonstrated that utilizing
expert-level trajectory for instruction-tuning LLMs effectively enhances their
planning capabilities. However, existing work primarily focuses on synthesizing
trajectories from manually designed planning tasks and environments. The
labor-intensive nature of creating these environments and tasks impedes the
generation of sufficiently varied and extensive trajectories. To address this
limitation, this paper explores the automated synthesis of diverse environments
and a gradual range of planning tasks, from easy to difficult. We introduce a
framework, AgentGen, that leverages LLMs first to generate environments and
subsequently generate planning tasks conditioned on these environments.
Specifically, to improve environmental diversity, we propose using an
inspiration corpus composed of various domain-specific text segments as the
context for synthesizing environments. Moreover, to increase the difficulty
diversity of generated planning tasks, we propose a bidirectional evolution
method, Bi-Evol, that evolves planning tasks from easier and harder directions
to synthesize a task set with a smoother difficulty curve. The evaluation
results derived from AgentBoard show that AgentGen greatly improves LLMs'
planning ability, e.g., the AgentGen instruction-tuned Llama-3 8B surpasses
GPT-3.5 in overall performance. Moreover, in certain tasks, it even outperforms
GPT-4.",cs.CL cs.AI cs.LG,2024-08-01
"MM-Vet v2: A Challenging Benchmark to Evaluate Large Multimodal Models
  for Integrated Capabilities",,"MM-Vet, with open-ended vision-language questions targeting at evaluating
integrated capabilities, has become one of the most popular benchmarks for
large multimodal model evaluation. MM-Vet assesses six core vision-language
(VL) capabilities: recognition, knowledge, spatial awareness, language
generation, OCR, and math. However, its question format is restricted to single
image-text pairs, lacking the interleaved image and text sequences prevalent in
real-world scenarios. To address this limitation, we introduce MM-Vet v2, which
includes a new VL capability called ""image-text sequence understanding"",
evaluating models' ability to process VL sequences. Furthermore, we maintain
the high quality of evaluation samples while further expanding the evaluation
set size. Using MM-Vet v2 to benchmark large multimodal models, we found that
Claude 3.5 Sonnet is the best model with a score of 71.8, slightly
outperforming GPT-4o which scored 71.0. Among open-weight models,
InternVL2-Llama3-76B leads with a score of 68.4.",cs.CV cs.AI cs.CL,2024-08-01
"Optimizing Diffusion Models for Joint Trajectory Prediction and
  Controllable Generation",,"Diffusion models are promising for joint trajectory prediction and
controllable generation in autonomous driving, but they face challenges of
inefficient inference steps and high computational demands. To tackle these
challenges, we introduce Optimal Gaussian Diffusion (OGD) and Estimated Clean
Manifold (ECM) Guidance. OGD optimizes the prior distribution for a small
diffusion time $T$ and starts the reverse diffusion process from it. ECM
directly injects guidance gradients to the estimated clean manifold,
eliminating extensive gradient backpropagation throughout the network. Our
methodology streamlines the generative process, enabling practical applications
with reduced computational overhead. Experimental validation on the large-scale
Argoverse 2 dataset demonstrates our approach's superior performance, offering
a viable solution for computationally efficient, high-quality joint trajectory
prediction and controllable generation for autonomous driving. Our project
webpage is at https://yixiaowang7.github.io/OptTrajDiff_Page/.",cs.CV,2024-08-01
